{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "accelerating-data-ingestion-new-innovations-auto-loaders-performance", "title": "Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala", "Streaming"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including: You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect. /Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "ai-agents-action-structuring-unstructured-data-demand-databricks-and", "title": "AI Agents in Action: Structuring Unstructured Data on Demand with Databricks and Unstructured", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Head of Product and Engineering, Unstructured"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Product and Engineering"}
{"session_id": "apache-spark-ask-us-anything", "title": "Apache Spark \u2014 Ask Us Anything", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Staff Developer Advocate:Technical, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an interactive Ask Me Anything (AMA) session on the latest advancements in Apache Spark 4, including Spark Connect\u2014the new client-server architecture enabling seamless integration with IDEs, notebooks and custom applications. Learn about performance improvements, enhanced APIs and best practices for leveraging Spark\u2019s next-generation features. Whether you're a data engineer, Spark developer or big data enthusiast, bring your questions on architecture, real-world use cases and how these innovations can optimize your workflows. Don\u2019t miss this chance to dive deep into the future of distributed computing with Spark! /Staff Software Engineer\nDatabricks /Senior Engineering Manager\nDatabricks /Staff Developer Advocate:Technical Staff"}
{"session_id": "authoring-data-pipelines-new-dlt-editor", "title": "Authoring Data Pipelines With the New DLT Editor", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re introducing a new developer experience for DLT designed for data practitioners who prefer a code-first approach and expect robust developer tooling. The new multi-file editor brings an IDE-like environment to declarative pipeline development, making it easy to structure transformation logic, configure pipelines throughout the development lifecycle and iterate efficiently. Features like contextual data previews and selective table updates enable step-by-step development. UI-driven tools, such as DAG previews and DAG-based actions, enhance productivity for experienced users and provide a bridge for those transitioning to declarative workflows. In this session, we\u2019ll showcase the new editor in action, highlighting how these enhancements simplify declarative coding and improve development for production-ready data pipelines. Whether you\u2019re an experienced developer or new to declarative data engineering, join us to see how DLT can enhance your data practice. /Sr. Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "automating-engineering-ai-llms-metadata-driven-frameworks", "title": "Automating Engineering with AI - LLMs in Metadata Driven Frameworks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Quality"], "speakers": ["CTO, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for data engineering keeps growing, but data teams are bored by repetitive tasks, stumped by growing complexity and endlessly harassed by an unrelenting need for speed. What if AI could take the heavy lifting off your hands? What if we make the move away from code-generation and into config-generation \u2014 how much more could we achieve? In this session, we\u2019ll explore how AI is revolutionizing data engineering, turning pain points into innovation. Whether you\u2019re grappling with manual schema generation or struggling to ensure data quality, this session offers practical solutions to help you work smarter, not harder. You\u2019ll walk away with a good idea of where AI is going to disrupt the data engineering workload, some good tips around how to accelerate your own workflows and an impending sense of doom around the future of the industry! /CTO"}
{"session_id": "bayadas-snowflake-databricks-migration-transforming-data-speed", "title": "Bayada\u2019s Snowflake-to-Databricks Migration: Transforming Data for Speed & Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Integration", "Machine Learning", "Real-time", "SQL", "Scala"], "speakers": ["CDAO, BAYADA Home Health Care"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayada is transforming its data ecosystem by consolidating Matillion+Snowflake and SSIS+SQL Server into a unified Enterprise Data Platform powered by Databricks. Using Databricks' Medallion architecture, this platform enables seamless data integration, advanced analytics and machine learning across critical domains like general ledger, recruitment and activity-based costing. Databricks was selected for its scalability, real-time analytics and ability to handle both structured and unstructured data, positioning Bayada for future growth. The migration aims to reduce data processing times by 35%, improve reporting accuracy and cut reconciliation efforts by 40%. Operational costs are projected to decrease by 20%, while real-time analytics is expected to boost efficiency by 15%. Join this session to learn how Bayada is leveraging Databricks to build a high-performance data platform that accelerates insights, drives efficiency and fosters innovation organization-wide. /Head of Data Architecture & Governance\nBAYADA Home Health Care /Sr. Director - HLS\nTredence Inc /CDAO"}
{"session_id": "better-together-change-data-feed-streaming-data-flow", "title": "Better Together: Change Data Feed in a Streaming Data Flow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Streaming"], "speakers": ["Data Engineer & Architect, 84.51 LLC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditional streaming works great when your data source is append-only, but what if your data source includes updates and deletes? At 84.51 we used DLT and Delta Lake to build a streaming data flow that consumes inserts, updates and deletes while still taking advantage of streaming checkpoints. We combined this flow with a materialized view and Enzyme incremental refresh for a low-code, efficient and robust end-to-end data flow. We process around 8 million sales transactions each day with 80 million items purchased. This flow not only handles new transactions but also handles updates to previous transactions. Join us to learn how 84.51 combined change data feed, data streaming and materialized views to deliver a \u201cbetter together\u201d solution. 84.51 is a retail insights, media & marketing company. We use first-party retail data from 60 million households sourced through a loyalty card program to drive Kroger\u2019s customer-centric journey. /Lead Data Engineer\n84.51\u02da /Data Engineer & Architect"}
{"session_id": "breaking-barriers-building-custom-spark-40-data-connectors-python", "title": "Breaking Barriers: Building Custom Spark 4.0 Data Connectors with Python", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python", "Scala", "Streaming"], "speakers": ["Resident Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building a custom Spark data source connector once required Java or Scala expertise, making it complex and limiting. This left many proprietary data sources without public SDKs disconnected from Spark. Additionally, data sources with Python SDKs couldn't harness Spark\u2019s distributed power. Spark 4.0 changes this with a new Python API for data source connectors, allowing developers to build fully functional connectors without Java or Scala. This unlocks new possibilities, from integrating proprietary systems to leveraging untapped data sources. Supporting both batch and streaming, this API makes data ingestion more flexible than ever. In this talk, we\u2019ll demonstrate how to build a Spark connector for Excel using Python, showcasing schema inference, data reads/writes and streaming support. Whether you're a data engineer or Spark enthusiast, you\u2019ll gain the knowledge to integrate Spark with any data source \u2014 entirely in Python. /Senior Resident Solutions Architect\nDatabricks /Resident Solutions Architect"}
{"session_id": "breaking-spark-versions-better-way-manage-workload-compatibility", "title": "Breaking With Spark Versions: A Better Way to Manage Workload Compatibility + Dependency Management", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Product Manager - serverless jedi, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explains how we\u2019ve made Apache Spark\u2122 versionless for end users by introducing a stable client API, environment versioning and automatic remediation. These capabilities have enabled auto-upgrade of hundreds of millions of workloads with minimal disruption. We\u2019ll also introduce a new approach to dependency management using environments. Admins will learn how to speed up package installation with Default Base Environments, and users will see how to manage custom environments for their own workloads. /Staff Product Manager - serverless jedi"}
{"session_id": "building-real-time-sport-model-insights-spark-structured-streaming", "title": "Building Real-Time Sport Model Insights with Spark Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Lead Data Science Engineer, Draftkings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the dynamic world of sports betting, precision and adaptability are key. Sports traders must navigate risk management, limitations of data feeds, and much more to prevent small model miscalculations from causing significant losses. To ensure accurate real-time pricing of hundreds of interdependent markets, traders provide key inputs such as player skill-level adjustments, whilst maintaining precise correlations. Black-box models aren\u2019t enough\u2014 constant feedback loops drive informed, accurate decisions. Join DraftKings as we showcase how we expose real-time metrics from our simulation engine, to empower traders with deeper insights into how their inputs shape the model. Using Spark Structured Streaming, Kafka, and Databricks dashboards, we transform raw simulation outputs into actionable data. This transparency into our engines enables fine-grained control over pricing\u2015 leading to more accurate odds, a more efficient sportsbook, and an elevated customer experience. /Lead Machine Learning Engineer\nDraftkings /Lead Data Science Engineer"}
{"session_id": "building-real-time-trading-dashboards-dlt-and-databricks-apps", "title": "Building Real-Time Trading Dashboards With DLT and Databricks Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Barclays Post Trade real-time trade monitoring platform was historically built on a complex set of legacy technologies including Java, Solace, and custom micro-services.This ssession will demonstrate how the power of DLT new real-time mode, in conjunction with the foreach_batch_sink, can enable simple, cost-effective streaming pipelines that can load high volumes of data into our OLTP database with very low latency. Once in our OLTP database, this can be used to update real-time trading dashboards, securely hosted in Databricks Apps, with the latest stock trades - enabling better, more responsive decision-making and alerting.The session will walk-through the architecture, and demonstrate how simple it is to create and manage the pipelines and apps within the Databricks environment. /Senior Specialist Solution Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "building-scalable-real-time-concurrency-prediction-service", "title": "Building a Scalable, Real-Time Concurrency Prediction Service", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science", "Machine Learning", "Real-time"], "speakers": ["AVP Data Science & Machine Learning, Dream 11"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dream11's rapid growth has posed critical challenges in scaling infrastructure to handle millions of concurrent users during high-traffic events. Concurrency Prediction Service provides real-time forecasts of peak user activity in 30-minute intervals to optimize resource allocation by the Scaler Service. This presentation covers the critical aspects of building and optimizing the Concurrency Prediction Service, including: /AVP Data Science & Machine Learning"}
{"session_id": "building-self-service-data-platform-small-data-team", "title": "Building a Self-Service Data Platform With a Small Data Team", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Head of Architecture, Dodo Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Dodo Brands, a global pizza and coffee business with over 1,200 retail locations and 40k employees, revolutionized their analytics infrastructure by creating a self-service data platform. This session explores the approach to empowering analysts, data scientists and ML engineers to independently build analytical pipelines with minimal involvement from data engineers. By leveraging Databricks as the backbone of their platform, the team developed automated tools like a \"job-generator\" that uses Jinja templates to streamline the creation of data jobs. This approach minimized manual coding and enabled non-data engineers to create over 1,420 data jobs \u2014 90% of which were auto-generated by user configurations. Supporting thousands of weekly active users via tools like Apache Superset. This session provides actionable insights for organizations seeking to scale their analytics capabilities efficiently without expanding their data engineering teams. /Senior Data Engineer\nDodo Brands /Head of Architecture"}
{"session_id": "cicd-databricks-advanced-asset-bundles-and-github-actions", "title": "CI/CD for Databricks: Advanced Asset Bundles and GitHub Actions", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Asset Bundles (DABs) provide a way to use the command line to deploy and run a set of Databricks assets \u2014 like notebooks, Python code, DLT pipelines and workflows. To automate deployments, you create a deployment pipeline that uses the power of DABs along with other validation steps to ensure high quality deployments. In this session you will learn how to automate CI/CD processes for Databricks while following best practices to keep deployments easy to scale and maintain. After a brief explanation of why Databricks Asset Bundles are a good option for CI/CD, we will walk through a working project including advanced variables, target-specific overrides, linting, integration testing and automatic deployment upon code review approval. You will leave the session clear on how to build your first GitHub Action using DABs. /Sr. Specialist Solutions Architect"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Real-time", "Streaming"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Databricks"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /"}
