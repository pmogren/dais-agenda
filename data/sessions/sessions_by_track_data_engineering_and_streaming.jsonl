{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["R", "ETL", "Delta Lake", "ELT", "SQL", "Streaming", "AI", "Data Quality"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "accelerating-data-ingestion-new-innovations-auto-loaders-performance", "title": "Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Streaming", "Scala", "R"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including: You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect. /Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["R", "Real-time", "Streaming", "Apache Spark", "AI"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Databricks"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DELTA LAKE", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "R"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /"}
