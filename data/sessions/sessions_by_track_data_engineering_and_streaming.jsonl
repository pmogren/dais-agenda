{"session_id": "10-hours-10-minutes-unleashing-power-dlt", "title": "From 10 Hours to 10 Minutes: Unleashing the Power of DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Batch Processing", "Data Integration", "Data Pipeline", "Real-time", "SQL"], "speakers": ["Data Engineer, Accenture"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How do you transform a data pipeline from sluggish 10-hour batch processing into a real-time powerhouse that delivers insights in just 10 minutes? This was the challenge we tackled at one of France's largest manufacturing companies, where data integration and analytics were mission-critical for supply chain optimization. Power BI dashboards needed to refresh every 15 minutes. Our team struggled with legacy Azure Data Factory batch pipelines. These outdated processes couldn\u2019t keep up, delaying insights and generating up to three daily incident tickets. We identified DLTs and Databricks SQL as the game-changing solution to modernize our workflow, implement quality checks, and reduce processing times.In this session, we\u2019ll dive into the key factors behind our success: /Senior Data Engineer\nAccenture /Data Engineer"}
{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "ai-agents-action-structuring-unstructured-data-demand-databricks-and", "title": "AI Agents in Action: Structuring Unstructured Data on Demand With Databricks and Unstructured", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Head of Product and Engineering, Unstructured"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "LLM agents aren\u2019t just answering questions \u2014 they\u2019re running entire workflows. In this talk, we\u2019ll show how agents can autonomously ingest, process and structure unstructured data using Unstructured, with outputs flowing directly into Databricks. Powered by the Model Context Protocol (MCP), agents can interface with Unstructured\u2019s full suite of capabilities \u2014 discovering documents across sources, building ephemeral workflows and exporting structured insights into Delta tables. We\u2019ll walk through a demo where an agent responds to a natural language request, dynamically pulls relevant documents, transforms them into usable data and surfaces insights \u2014 fast. Join us for a sneak peek into the future of AI-native data workflows, where LLMs don\u2019t just assist \u2014 they operate. /Head of Product and Engineering"}
{"session_id": "apache-airflow-lakeflow-jobs-guide-workflow-modernization", "title": "From Apache Airflow to Lakeflow Jobs: A Guide for Workflow Modernization", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "Scala"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This is an overview of migrating from Apache Airflow to Lakeflow Jobs for modern data orchestration. It covers key differences, best practices and practical examples of transitioning from traditional Airflow DAGs orchestrating legacy systems to declarative, incremental ETL pipelines with Lakeflow. Attendees will gain actionable tips on how to improve efficiency, scalability and maintainability in their workflows. /Product Manager\nDatabricks /Databricks"}
{"session_id": "apache-spark-ask-us-anything", "title": "Apache Spark \u2014 Ask Us Anything", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Staff Developer Advocate:Technical, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an interactive Ask Me Anything (AMA) session on the latest advancements in Apache Spark 4, including Spark Connect \u2014 the new client-server architecture enabling seamless integration with IDEs, notebooks and custom applications. Learn about performance improvements, enhanced APIs and best practices for leveraging Spark\u2019s next-generation features. Whether you're a data engineer, Spark developer or big data enthusiast, bring your questions on architecture, real-world use cases and how these innovations can optimize your workflows. Don\u2019t miss this chance to dive deep into the future of distributed computing with Spark! /Staff Software Engineer\nDatabricks /Senior Engineering Manager\nDatabricks /Staff Developer Advocate:Technical Staff"}
{"session_id": "authoring-data-pipelines-new-dlt-editor", "title": "Authoring Data Pipelines With the New DLT Editor", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re introducing a new developer experience for DLT designed for data practitioners who prefer a code-first approach and expect robust developer tooling. The new multi-file editor brings an IDE-like environment to declarative pipeline development, making it easy to structure transformation logic, configure pipelines throughout the development lifecycle and iterate efficiently. Features like contextual data previews and selective table updates enable step-by-step development. UI-driven tools, such as DAG previews and DAG-based actions, enhance productivity for experienced users and provide a bridge for those transitioning to declarative workflows. In this session, we\u2019ll showcase the new editor in action, highlighting how these enhancements simplify declarative coding and improve development for production-ready data pipelines. Whether you\u2019re an experienced developer or new to declarative data engineering, join us to see how DLT can enhance your data practice. /Sr. Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "automating-engineering-ai-llms-metadata-driven-frameworks", "title": "Automating Engineering with AI - LLMs in Metadata Driven Frameworks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Quality"], "speakers": ["CTO, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for data engineering keeps growing, but data teams are bored by repetitive tasks, stumped by growing complexity and endlessly harassed by an unrelenting need for speed. What if AI could take the heavy lifting off your hands? What if we make the move away from code-generation and into config-generation \u2014 how much more could we achieve? In this session, we\u2019ll explore how AI is revolutionizing data engineering, turning pain points into innovation. Whether you\u2019re grappling with manual schema generation or struggling to ensure data quality, this session offers practical solutions to help you work smarter, not harder. You\u2019ll walk away with a good idea of where AI is going to disrupt the data engineering workload, some good tips around how to accelerate your own workflows and an impending sense of doom around the future of the industry! /CTO"}
{"session_id": "bayadas-snowflake-databricks-migration-transforming-data-speed", "title": "Bayada\u2019s Snowflake-to-Databricks Migration: Transforming Data for Speed & Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Integration", "Machine Learning", "Real-time", "SQL", "Scala"], "speakers": ["CDAO, BAYADA Home Health Care"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayada is transforming its data ecosystem by consolidating Matillion+Snowflake and SSIS+SQL Server into a unified Enterprise Data Platform powered by Databricks. Using Databricks' Medallion architecture, this platform enables seamless data integration, advanced analytics and machine learning across critical domains like general ledger, recruitment and activity-based costing. Databricks was selected for its scalability, real-time analytics and ability to handle both structured and unstructured data, positioning Bayada for future growth. The migration aims to reduce data processing times by 35%, improve reporting accuracy and cut reconciliation efforts by 40%. Operational costs are projected to decrease by 20%, while real-time analytics is expected to boost efficiency by 15%. Join this session to learn how Bayada is leveraging Databricks to build a high-performance data platform that accelerates insights, drives efficiency and fosters innovation organization-wide. /Head of Data Architecture & Governance\nBAYADA Home Health Care /Sr. Director - HLS\nTredence Inc /CDAO"}
{"session_id": "better-together-change-data-feed-streaming-data-flow", "title": "Better Together: Change Data Feed in a Streaming Data Flow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Streaming"], "speakers": ["Data Engineer & Architect, 84.51 LLC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditional streaming works great when your data source is append-only, but what if your data source includes updates and deletes? At 84.51 we used DLT and Delta Lake to build a streaming data flow that consumes inserts, updates and deletes while still taking advantage of streaming checkpoints. We combined this flow with a materialized view and Enzyme incremental refresh for a low-code, efficient and robust end-to-end data flow. We process around 8 million sales transactions each day with 80 million items purchased. This flow not only handles new transactions but also handles updates to previous transactions. Join us to learn how 84.51 combined change data feed, data streaming and materialized views to deliver a \u201cbetter together\u201d solution. 84.51 is a retail insights, media & marketing company. We use first-party retail data from 60 million households sourced through a loyalty card program to drive Kroger\u2019s customer-centric journey. /Lead Data Engineer\n84.51\u02da /Data Engineer & Architect"}
{"session_id": "breaking-barriers-building-custom-spark-40-data-connectors-python", "title": "Breaking Barriers: Building Custom Spark 4.0 Data Connectors with Python", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python", "Scala", "Streaming"], "speakers": ["Resident Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building a custom Spark data source connector once required Java or Scala expertise, making it complex and limiting. This left many proprietary data sources without public SDKs disconnected from Spark. Additionally, data sources with Python SDKs couldn't harness Spark\u2019s distributed power. Spark 4.0 changes this with a new Python API for data source connectors, allowing developers to build fully functional connectors without Java or Scala. This unlocks new possibilities, from integrating proprietary systems to leveraging untapped data sources. Supporting both batch and streaming, this API makes data ingestion more flexible than ever. In this talk, we\u2019ll demonstrate how to build a Spark connector for Excel using Python, showcasing schema inference, data reads/writes and streaming support. Whether you're a data engineer or Spark enthusiast, you\u2019ll gain the knowledge to integrate Spark with any data source \u2014 entirely in Python. /Senior Resident Solutions Architect\nDatabricks /Resident Solutions Architect"}
{"session_id": "breaking-spark-versions-better-way-manage-workload-compatibility", "title": "Breaking With Spark Versions: A Better Way to Manage Workload Compatibility + Dependency Management", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Product Manager - serverless jedi, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explains how we\u2019ve made Apache Spark\u2122 versionless for end users by introducing a stable client API, environment versioning and automatic remediation. These capabilities have enabled auto-upgrade of hundreds of millions of workloads with minimal disruption. We\u2019ll also introduce a new approach to dependency management using environments. Admins will learn how to speed up package installation with Default Base Environments, and users will see how to manage custom environments for their own workloads. /Staff Product Manager - serverless jedi"}
{"session_id": "building-real-time-sport-model-insights-spark-structured-streaming", "title": "Building Real-Time Sport Model Insights with Spark Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Lead Data Science Engineer, Draftkings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the dynamic world of sports betting, precision and adaptability are key. Sports traders must navigate risk management, limitations of data feeds, and much more to prevent small model miscalculations from causing significant losses. To ensure accurate real-time pricing of hundreds of interdependent markets, traders provide key inputs such as player skill-level adjustments, whilst maintaining precise correlations. Black-box models aren\u2019t enough\u2014 constant feedback loops drive informed, accurate decisions. Join DraftKings as we showcase how we expose real-time metrics from our simulation engine, to empower traders with deeper insights into how their inputs shape the model. Using Spark Structured Streaming, Kafka, and Databricks dashboards, we transform raw simulation outputs into actionable data. This transparency into our engines enables fine-grained control over pricing\u2015 leading to more accurate odds, a more efficient sportsbook, and an elevated customer experience. /Lead Machine Learning Engineer\nDraftkings /Lead Data Science Engineer"}
{"session_id": "building-real-time-trading-dashboards-dlt-and-databricks-apps", "title": "Building Real-Time Trading Dashboards With DLT and Databricks Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Barclays Post Trade real-time trade monitoring platform was historically built on a complex set of legacy technologies including Java, Solace, and custom micro-services.This ssession will demonstrate how the power of DLT new real-time mode, in conjunction with the foreach_batch_sink, can enable simple, cost-effective streaming pipelines that can load high volumes of data into our OLTP database with very low latency. Once in our OLTP database, this can be used to update real-time trading dashboards, securely hosted in Databricks Apps, with the latest stock trades - enabling better, more responsive decision-making and alerting.The session will walk-through the architecture, and demonstrate how simple it is to create and manage the pipelines and apps within the Databricks environment. /Senior Specialist Solution Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "building-self-service-data-platform-small-data-team", "title": "Building a Self-Service Data Platform With a Small Data Team", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Head of Architecture, Dodo Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Dodo Brands, a global pizza and coffee business with over 1,200 retail locations and 40k employees, revolutionized their analytics infrastructure by creating a self-service data platform. This session explores the approach to empowering analysts, data scientists and ML engineers to independently build analytical pipelines with minimal involvement from data engineers. By leveraging Databricks as the backbone of their platform, the team developed automated tools like a \"job-generator\" that uses Jinja templates to streamline the creation of data jobs. This approach minimized manual coding and enabled non-data engineers to create over 1,420 data jobs \u2014 90% of which were auto-generated by user configurations. Supporting thousands of weekly active users via tools like Apache Superset. This session provides actionable insights for organizations seeking to scale their analytics capabilities efficiently without expanding their data engineering teams. /Senior Data Engineer\nDodo Brands /Head of Architecture"}
{"session_id": "cicd-databricks-advanced-asset-bundles-and-github-actions", "title": "CI/CD for Databricks: Advanced Asset Bundles and GitHub Actions", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Asset Bundles (DABs) provide a way to use the command line to deploy and run a set of Databricks assets \u2014 like notebooks, Python code, DLT pipelines and workflows. To automate deployments, you create a deployment pipeline that uses the power of DABs along with other validation steps to ensure high quality deployments. In this session you will learn how to automate CI/CD processes for Databricks while following best practices to keep deployments easy to scale and maintain. After a brief explanation of why Databricks Asset Bundles are a good option for CI/CD, we will walk through a working project including advanced variables, target-specific overrides, linting, integration testing and automatic deployment upon code review approval. You will leave the session clear on how to build your first GitHub Action using DABs. /Sr. Specialist Solutions Architect"}
{"session_id": "code-insights-leveraging-advanced-infrastructure-and-ai-capabilities", "title": "From Code to Insights: Leveraging Advanced Infrastructure and AI Capabilities.", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Director, Insulet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this talk, we will explore how AI and advanced infrastructure are transforming Insulet's development and operations. We'll highlight how our innovations have reduced scrap part costs through manufacturing analytics, showcasing efficiency and cost savings. On leveraging Databricks AI solutions and productivity, it not only identifies errors but also fixes code and assists in writing complex queries. This goes beyond suggestions, providing actual solutions. On the infrastructure side, integrating Spark with Databricks simplifies setup and reduces costs. Additionally Databricks Lakeflow Connect enables real-time updates and simplification without much coding as we integrate with Salesforce. We'll also discuss real-time processing of patient data, demonstrating how Databricks drives efficiency and productivity. Join us to learn how these innovations enhance efficiency, cost savings and performance. /Director"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Real-time", "Streaming"], "speakers": ["Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Product Management"}
{"session_id": "creating-custom-pyspark-stream-reader-pyspark-40", "title": "Creating a Custom PySpark Stream Reader with PySpark 4.0", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Delta Lake", "ELT", "SQL", "Streaming"], "speakers": ["Head of Data Engineering, Entrada"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark supports many data sources out of the box, such as Apache Kafka, JDBC, ODBC, Delta Lake, etc. However, some older systems, such as systems that use JMS protocol, are not supported by default and require considerable extra work for developers to read from them. One such example is ActiveMQ for streaming. Traditionally, users of ActiveMQ have to use a middle-man in order to read the stream with Spark (such as writing to a MySQL DB using Java code and reading that table with Spark JDBC). With PySpark 4.0\u2019s custom data sources (supported in DBR 15.3+) we are able to cut out the middle-man processing using batch or Spark Streaming and consume the queues directly from PySpark, saving developers considerable time and complexity in getting source data into your Delta Lake and governed by Unity Catalog and orchestrated with Databricks Workflows. /Head of Data Engineering"}
{"session_id": "crypto-scale-building-cost-efficient-high-performance-platform-real", "title": "Crypto at Scale: Building a Cost-Efficient, High-Performance Platform for Real-Time Blockchain Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Scala", "Streaming"], "speakers": ["Lead Data Engineer, Elliptic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s fast-evolving crypto landscape, organizations require fast, reliable intelligence to manage risk, investigate financial crime, and stay ahead of evolving threats. In this session, discover how Elliptic built a scalable, high-performance data intelligence platform that delivers real-time, actionable blockchain insights\u2014empowering businesses to future-proof their crypto risk strategies and law enforcement to streamline investigations. We\u2019ll walk you through how we've built a high-performance data platform, leveraging key components of the Databricks ecosystem such as Structured Streaming and Delta Lake. This transformation has fundamentally changed the way we deliver user-facing analytics\u2014improving not only speed and scalability, but also enabling analytics to directly enhance the accuracy and intelligence of our operational systems. Along the way, we\u2019ll share how we overcame critical challenges such as building efficient incremental pipelines, designing robust partitioning strategies for large-scale crypto datasets, and enabling seamless data sharing with our customers.Whether you\u2019re looking to enhance your streaming capabilities, expand your knowledge of how crypto analytics works or simply discover novel approaches to data processing at scale, this session will provide concrete strategies and valuable lessons learned from the front lines of crypto attribution /Specialist Solutions Architect\nDatabricks /Lead Data Engineer"}
{"session_id": "data-triggers-and-advanced-control-flow-lakeflow-jobs", "title": "Data Triggers and Advanced Control Flow With Lakeflow Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Jobs is the production-ready fully managed orchestrator for the entire Lakehouse with 99.95% uptime. Join us for a dive into how you can orchestrate your enterprise data operations, from triggering your jobs only when your data is ready to advanced control flow with conditionals, looping and job modularity \u2014 with demos! Attendees will gain practical insights into optimizing their data operations by orchestrating with Lakeflow Jobs: /Product Manager"}
{"session_id": "databricks-backbone-mlops-orchestration-inference", "title": "Databricks as the Backbone of MLOps: From Orchestration to Inference", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Senior Expert - MLOps, Globe Telecoms"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As machine learning (ML) models scale in complexity and impact, organizations must establish a robust MLOps foundation to ensure seamless model deployment, monitoring and retraining. In this session, we\u2019ll share how we leverage Databricks as the backbone of our MLOps ecosystem \u2014 handling everything from workflow orchestration to large-scale inference. We\u2019ll walk through our journey of transitioning from fragmented workflows to an integrated, scalable system powered by Databricks Workflows. You\u2019ll learn how we built an automated pipeline that streamlines model development, inference and monitoring while ensuring reliability in production. We\u2019ll also discuss key challenges we faced, lessons learned and best practices for organizations looking to operationalize ML with Databricks. /Asst. Director MLOps\nGlobe Telecoms /Senior Expert - MLOps"}
{"session_id": "databricks-lakeflow-foundation-data-ai-innovation-your-industry", "title": "Databricks Lakeflow: the Foundation of Data + AI Innovation for Your Industry", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Pipeline", "Real-time"], "speakers": ["Sr. Manager Product Marketing, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Every analytics, BI and AI project relies on high-quality data. This is why data engineering, the practice of building reliable data pipelines that ingest and transform data, is consequential to the success of these projects. In this session, we'll show how you can use Lakeflow to accelerate innovation in multiple parts of the organization. We'll review real-world examples of Databricks customers using Lakeflow in different industries such as automotive, healthcare and retail. We'll touch on how the foundational data engineering capabilities Lakeflow provides help power initiatives that improve customer experiences, make real-time decisions and drive business results. /Sr. Manager Product Marketing"}
{"session_id": "days-seconds-reducing-query-times-large-geospatial-datasets-99", "title": "From Days to Seconds \u2014 Reducing Query Times on Large Geospatial Datasets by 99%", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL", "Streaming"], "speakers": ["Associate Director of Technology, Global Water Security Center"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Global Water Security Center translates environmental science into actionable insights for the U.S. Department of Defense. Prior to incorporating Databricks, responding to these requests required querying approximately five hundred thousand raster files representing over five hundred billion points. By leveraging lakehouse architecture, Databricks Auto Loader, Spark Streaming, Databricks Spatial SQL, H3 geospatial indexing and Databricks Liquid Clustering, we were able to drastically reduce our \u201ctime to analysis\u201d from multiple business days to a matter of seconds. Now, our data scientists execute queries on pre-computed tables in Databricks, resulting in a \u201ctime to analysis\u201d that is 99% faster, giving our teams more time for deeper analysis of the data. Additionally, we\u2019ve incorporated Databricks Workflows, Databricks Asset Bundles, Git and Git Actions to support CI/CD across workspaces. We completed this work in close partnership with Databricks. /Sr. Solutions Archtect\nDatabricks /Associate Director of Technology"}
{"session_id": "de-risking-investment-decisions-qcgs-smarter-deal-evaluation-process", "title": "De-Risking Investment Decisions: QCG's Smarter Deal Evaluation Process Leveraging Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT", "SQL", "Scala"], "speakers": ["Head of Engineering, Quantum Capital Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Quantum Capital Group (QCG) screens hundreds of deals across the global Sustainable Energy Ecosystem, requiring deep technical due diligence. With over 1.5 billion records sourced from public, premium and proprietary datasets, their challenge was how to efficiently curate, analyze and share this data to drive smarter investment decisions. QCG partnered with Databricks & Tiger Analytics to modernize its data landscape. Using Delta tables, Spark SQL, and Unity Catalog, the team built a golden dataset that powers proprietary evaluation models and automates complex workflows. Data is now seamlessly curated, enriched and distributed \u2014 both internally and to external stakeholders \u2014 in a secure, governed and scalable way. This session explores how QCG\u2019s investment in data intelligence has turned an overwhelming volume of information into a competitive advantage, transforming deal evaluation into a faster, more strategic process. /Head of Engineering"}
{"session_id": "declarative-pipelines-simplifying-data-engineering-workloads", "title": "Declarative Pipelines: Simplifying Data Engineering Workloads", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT has made it dramatically easier to build production-grade pipelines, using a declarative framework that abstracts away orchestration and complexity. It\u2019s become a go-to solution for teams who want reliable, maintainable pipelines without reinventing the wheel. But we\u2019re just getting started. In this session, we\u2019ll take a step back and share how DLT fits into a broader vision for the future of data engineering pipelines \u2014 one that opens the door to a new level of openness, standardization and community momentum. We\u2019ll cover the core concepts behind declarative pipelines, where the architecture is headed, and what this shift means for data engineers building procedural code. Don\u2019t miss this session \u2014 we\u2019ll be sharing something new that sets the direction for what comes next. /Distinguished Engineer\nDatabricks /Software Engineer"}
{"session_id": "delivering-sub-second-latency-operational-workloads-databricks", "title": "Delivering Sub-Second Latency for Operational Workloads on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Real-time", "Streaming"], "speakers": ["Head of Streaming, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As enterprise streaming adoption accelerates, more teams are turning to real-time processing to support operational workloads that require sub-second response times. To address this need, Databricks introduced Project Lightspeed in 2022, which recently delivered Real-Time Mode in Apache Spark\u2122 Structured Streaming. This new mode achieves consistent p99 latencies under 300ms for a wide range of stateless and stateful streaming queries. In this session, we\u2019ll define what constitutes an operational use case, outline typical latency requirements and walk through how to meet those SLAs using Real-Time Mode in Structured Streaming. /Staff Software Engineer\nDatabricks /Head of Streaming"}
{"session_id": "delta-rs-turning-five-growing-pains-and-life-lessons", "title": "Delta-rs Turning Five: Growing Pains and Life Lessons", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Python"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Five years ago, the delta-rs project embarked on a journey to bring Delta Lake's robust capabilities to the Rust & Python ecosystem. In this talk, we'll delve into the triumphs, tribulations and lessons learned along the way. We'll explore how delta-rs has matured alongside the thriving Rust data ecosystem, adapting to its evolving landscape and overcoming the challenges of maintaining a complex data project. Join us as we share insights into the project's evolution, the symbiotic relationship between delta-rs and the Rust community, and the current hurdles and future directions that lie ahead. /Staff Developer Advocate"}
{"session_id": "deploying-databricks-asset-bundles-dabs-scale", "title": "Deploying Databricks Asset Bundles (DABs) at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Managing data and AI workloads in Databricks can be complex. Databricks Asset Bundles (DABs) simplify this by enabling declarative, Git-driven deployment workflows for notebooks, jobs, DLT pipelines, dashboards, ML models and more. Join the DABs Team for a Deep Dive and learn about: If you're a data engineer, ML practitioner or platform architect, this talk will provide practical insights to improve reliability, efficiency and compliance in your Databricks workflows. /Sr. Staff Software Engineer\nDatabricks /Product Management"}
{"session_id": "dlt-integrations-and-interoperability-get-data-and-anywhere", "title": "DLT Integrations and Interoperability: Get Data From \u2014 and to \u2014 Anywhere", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "ELT", "ETL", "Python"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, you will learn how to integrate DLT with external systems in order to ingest and send data virtually anywhere. DLT is most often used in ingestion and ETL into the Lakehouse. New DLT capabilities like the DLT Sinks API and added support for Python Data Source and ForEachBatch have opened up DLT to support almost any integration. This includes popular Apache Spark\u2122 integrations like JDBC, Kafka, External and managed Delta tables, Azure CosmosDB, MongoDB and more. /Sr. Staff Product Manager"}
{"session_id": "federated-data-pipelines", "title": "Federated Data Pipelines", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline"], "speakers": ["Senior Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Are you struggling to keep up with rapid business changes that demand constant updates to your data pipelines? Is your data engineering team growing rapidly just to manage this complexity? Databricks was not immune to this challenge either. Managing our BI with contributions from hundreds of Product Engineering Teams across the company while maintaining central oversight and quality posed significant hurdles. Join us to learn how we developed a config-driven data pipeline framework using Metric Store and UC Metrics that helped us reduce engineering effort \u2014 achieving the work of 100 classical data engineers with just two platform engineers. /Senior Software Engineer\nDatabricks /Senior Software Engineer"}
{"session_id": "genai-observability-customer-care", "title": "GenAI Observability in Customer Care", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Real-time"], "speakers": ["Senior Machine Learning Engineer, EarnIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Customer support is going through the GenAI revolution, but how can we use AI to foster deeper empathy with our end users? To enable this, Earnin has built its GenAI observability platform on Databricks, leveraging DLTs, Kafka and Databricks AI/BI. This session covers how we use DLT to monitor our customer care chatbot in near real-time and how we leverage Databricks to better anticipate our customers' needs. /Senior Staff Software Engineer\nEarnin /Senior Machine Learning Engineer"}
{"session_id": "genie-engineering-optimizing-hvac-design-and-operational-insights-data", "title": "Genie for Engineering: Optimizing HVAC Design and Operational Insights With Data and AI", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Data Science", "Delta Lake", "ELT", "Real-time"], "speakers": ["Principal AI Engineer, Lennox International"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will explore how Genie, an AI-driven platform transformed HVAC operational insights by leveraging Databricks offerings like Apache Spark, Delta Lake and the Databricks Data Intelligence Platform. Key contributions: By analyzing real-time data from HVAC installations, Genie identified discrepancies between design specs and field performance, allowing engineers to optimize algorithms, reduce inefficiencies and improve customer satisfaction. Discover how Genie revolutionized HVAC management and apply to your projects. /Manager, Data Science & AI\nLennox /Principal AI Engineer"}
{"session_id": "getting-most-out-dlt-deep-dive-whats-new-and-best-practices", "title": "Getting the Most Out of DLT: A Deep Dive on What\u2019s New and Best Practices", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline", "Scala"], "speakers": ["Distinguished Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This deep dive covers advanced usage patterns, tips and best practices for maximizing the potential of DLT. Attendees will explore new features, enhanced workflows and cost-optimization strategies through a demo-heavy presentation. The session will also address complex use cases, showcasing how DLT simplifies the management of robust data pipelines while maintaining scalability and efficiency across diverse data engineering challenges. /Distinguished Engineer"}
{"session_id": "getting-started-lakeflow-connect", "title": "Getting Started With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Hundreds of customers are already ingesting data with Lakeflow Connect from SQL Server, Salesforce, ServiceNow, Google Analytics, SharePoint, PostgreSQL and more to unlock the full power of their data. Lakeflow Connect introduces built-in, no-code ingestion connectors from SaaS applications, databases and file sources to help unlock data intelligence. In this demo-packed session, you\u2019ll learn how to ingest ready-to-use data for analytics and AI with a few clicks in the UI or a few lines of code. We\u2019ll also demonstrate how Lakeflow Connect is fully integrated with the Databricks Data Intelligence Platform for built-in governance, observability, CI/CD, automated pipeline maintenance and more. Finally, we\u2019ll explain how to use Lakeflow Connect in combination with downstream analytics and AI tools to tackle common business challenges and drive business impact. /Sr. Product Marketing Manager\nDatabricks /Senior Product Manager"}
{"session_id": "gpu-accelerated-spark-connect", "title": "GPU Accelerated Spark Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "ETL", "SQL"], "speakers": ["Sr. Manager, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Spark Connect, first included for SQL/DataFrame API in Apache Spark 3.4 and recently extended to MLlib in 4.0, introduced a new way to run Spark applications over a gRPC protocol. This has many benefits, including easier adoption for non-JVM clients, version independence from applications and increased stability and security of the associated Spark clusters. The recent Spark Connect extension for ML also included a plugin interface to configure enhanced server-side implementations of the MLlib algorithms when launching the server. In this talk, we shall demonstrate how this new interface, together with Spark SQL\u2019s existing plugin interface, can be used with NVIDIA GPU-accelerated plugins for ML and SQL to enable no-code change, end-to-end GPU acceleration of Spark ETL and ML applications over Spark Connect, with optimal performance up to 9x at 80% cost reduction compared to CPU baselines. /Principal Distributed Systems Engineer\nNVIDIA /Sr. Manager"}
{"session_id": "harnessing-databricks-asset-bundles-transforming-pipeline-management", "title": "Harnessing Databricks Asset Bundles: Transforming Pipeline Management at Scale at Stack Overflow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Security", "Scala"], "speakers": ["Staff Data Engineer, Stack Overflow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Stack Overflow optimized its data engineering workflows using Databricks Asset Bundles (DABs) for scalable and efficient pipeline deployments. This session explores the structured pipeline architecture, emphasizing code reusability, modular design and bundle variables to ensure clarity and data isolation across projects. Learn how the data team leverages enterprise infrastructure to streamline deployment across multiple environments. Key topics include DRY-principled modular design, essential DAB features for automation and data security strategies using Unity Catalog. Designed for data engineers and teams managing multi-project workflows, this talk offers actionable insights on optimizing pipelines with Databricks evolving toolset. /Staff Data Engineer"}
{"session_id": "harnessing-real-time-data-and-ai-retail-innovation", "title": "Harnessing Real-Time Data and AI for Retail Innovation", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "Machine Learning", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk explores using advanced data processing and generative AI techniques to revolutionize the retail industry. Using Databricks, we will discuss how cutting-edge technologies enable real-time data analysis and machine learning applications, creating a powerful ecosystem for large-scale, data-driven retail solutions. Attendees will gain insights into architecting scalable data pipelines for retail operations and implementing advanced analytics on streaming customer data. Discover how these integrated technologies drive innovation in retail, enhancing customer experiences, streamlining operations and enabling data-driven decision-making. Learn how retailers can leverage these tools to gain a competitive edge in the rapidly evolving digital marketplace, ultimately driving growth and adaptability in the face of changing consumer behaviors and market dynamics. /Lead Solutions Architect\nDatabricks /Senior Specialist Solutions Architect"}
{"session_id": "health-data-delivered-how-dlt-powers-healthverity-marketplace", "title": "Health Data, Delivered: How DLT Powers the HealthVerity Marketplace", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "ETL", "Scala"], "speakers": ["Principal Data Engineer, HealthVerity"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building scalable, reliable ETL pipelines is a challenge for organizations managing large, diverse data sources. Theseus, our custom ETL framework, streamlines data ingestion and transformation by fully leveraging Databricks-native capabilities, including DLT, auto loader and event-driven orchestration. By decoupling supplier logic and implementing structured bronze, silver, and gold layers, Theseus ensures high-performance, fault-tolerant data processing with minimal operational overhead. The result? Faster time-to-value, simplified governance and improved data quality \u2014 all within a declarative framework that reduces engineering effort. In this session, we\u2019ll explore how Theseus automates complex data workflows, optimizes cost efficiency and enhances scalability, showcasing how Databricks-native tools drive real business outcomes. /Principal Data Engineer"}
{"session_id": "healthcare-interoperability-end-end-streaming-fhir-pipelines-databricks", "title": "Healthcare Interoperability: End-to-End Streaming FHIR Pipelines With Databricks & Redox", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Redox & Databricks direct integration can streamline your interoperability workflows from responding in record time to preauthorization requests to letting attending physicians know about a change in risk for sepsis and readmission in near real time from ADTs. Data engineers will learn how to create fully-streaming ETL pipelines for ingesting, parsing and acting on insights from Redox FHIR bundles delivered directly to Unity Catalog volumes. Once available in the Lakehouse, AI/BI Dashboards and Agentic Frameworks help write FHIR messages back to Redox for direct push down to EMR systems. Parsing FHIR bundle resources has never been easier with SQL combined with the new VARIANT data type in Delta and streaming table creation against Serverless DBSQL Warehouses. We'll also use Databricks accelerators dbignite and redoxwrite for writing and posting FHIR bundles back to Redox integrated EMRs and we'll extend AI/BI with Unity Catalog SQL UDFs and the Redox API for use in Genie. /Field CTO\nRedox, Inc. /Solutions Architect"}
{"session_id": "highways-and-hexagons-processing-large-geospatial-datasets-h3", "title": "Highways and Hexagons: Processing Large Geospatial Datasets With H3", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Scala"], "speakers": ["Austroads Ltd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The problem of matching GPS locations to roads and local government areas (LGAs) involves handling large datasets and a number of geospatial operations. In this deep dive, we will outline the challenges of developing scalable solutions for these tasks. We will discuss our multi-step approach, first focusing on the use of H3 indexing to isolate matches with single candidates, then explaining use of different geospatial computational techniques to accurately match points with multiple candidates. From technical perspective, the talk will showcase the use of broadcasting and partitioning techniques, their effect on autoscaling, memory usage and effective data parallelization. This session is for anyone interested in geospatial data, spark performance optimization and the real-world challenges of large-scale data engineering. This session will be co-presented by Prad Dias (Austroads Senior Implementation Manager) and Petr Andreev (Mantel Group Senior Data Engineer) /Senior Data Engineer\nMantel Group /Austroads Ltd"}
{"session_id": "hitchhikers-guide-delta-lake-streaming-agentic-universe", "title": "The Hitchhiker's Guide to Delta Lake Streaming in an Agentic Universe", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT", "Streaming"], "speakers": ["Distinguished Software Engineer, Nike"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data engineering continues to evolve the shift from batch-oriented to streaming-first has become standard across the enterprise. The reality is these changes have been taking shape for the past decade \u2014 we just now also happen to be standing on the precipice of true disruption through automation, the likes of which we could only dream about before. Yes, AI Agents and LLMs are already a large part of our daily lives, but we (as data engineers) are ultimately on the frontlines ensuring that the future of AI is powered by consistent, just-in-time data \u2014 and Delta Lake is critical to help us get there. This session will provide you with best practices learned the hard way by one of the authors of The Delta Lake Definitive Guide including: /Distinguished Software Engineer"}
{"session_id": "how-databricks-powers-real-time-threat-detection-barracuda-xdr", "title": "How Databricks Powers Real-Time Threat Detection at Barracuda XDR", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DLT", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Machine Learning", "Real-time", "Scala", "Streaming"], "speakers": ["Manager of Detection Engineering, Barracuda Networks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As cybersecurity threats grow in volume and complexity, organizations must efficiently process security telemetry for best-in-class detection and mitigation. Barracuda\u2019s XDR platform is redefining security operations by layering advanced detection methodologies over a broad range of supported technologies. Our vision is to deliver unparalleled protection through automation, machine learning and scalable detection frameworks, ensuring threats are identified and mitigated quickly. To achieve this, we have adopted Databricks as the foundation of our security analytics platform, providing greater control and flexibility while decoupling from traditional SIEM tools. By leveraging DLTs, Spark Structured Streaming and detection-as-code CI/CD pipelines, we have built a real-time detection engine that enhances scalability, accuracy and cost efficiency. This session explores how Databricks is shaping the future of XDR through real-time analytics and cloud-native security. /Director, SOC Offensive Security\nBarracuda Networks /Manager of Detection Engineering"}
{"session_id": "imperative-declarative-paradigm-rebuilding-cicd-infrastructure-using", "title": "From Imperative to Declarative Paradigm: Rebuilding a CI/CD Infrastructure Using Hatch and DABs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Site Reliability Engineer, FreeWheel, a Comcast Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building and deploying Pyspark pipelines to Databricks should be effortless. However, our team at FreeWheel has, for the longest time, struggled with a convoluted and hard-to-maintain CI/CD infrastructure. It followed an imperative paradigm, demanding that every project implement custom scripts to build artifacts and deploy resources, and resulting in redundant boilerplate code and awkward interactions with the Databricks REST API. We set our mind on rebuilding it from scratch, following a declarative paradigm instead. We will share how we were able to eliminate thousands of lines of code from our repository, create a fully configuration-driven infrastructure where projects can be easily onboarded, and improve the quality of our codebase using Hatch and Databricks Asset Bundles as our tools of choice. In particular, DAB has made deploying across our 3 environments a breeze, and has allowed us to quickly adopt new features as soon as they are released by Databricks. /Sr. Software Engineer\nFreeWheel, a Comcast Company /Sr. Site Reliability Engineer"}
{"session_id": "innovating-retail-data-unilevers-transformation-databricks-dlt", "title": "Innovating Retail Data: Unilever\u2019s Transformation with Databricks DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Data Science Manager, Unilever"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retail data is expanding at an unprecedented rate, demanding a scalable, cost-efficient, and near real-time architecture. At Unilever, we transformed our data management approach by leveraging Databricks DLT, achieving approximately $500K in cost savings while accelerating computation speeds by 200\u2013500%. By adopting a streaming-driven architecture, we built a system where data flows continuously across processing layers, enabling real-time updates with minimal latency. DLT\u2019s serverless simplicity replaced complex-dependency management, reducing maintenance overhead, and improving pipeline reliability. DLT Direct Publishing further enhanced data segmentation, concurrency, and governance, ensuring efficient and scalable data operations while simplifying workflows. This transformation empowers Unilever to manage data with greater efficiency, scalability, and reduced costs, creating a future-ready infrastructure that evolves with the needs of our retail partners and customers. /Senior Data Science Manager"}
{"session_id": "inside-spark-41-preview-latest-innovations", "title": "Inside Spark 4.1: A Preview of the Latest Innovations", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Quality", "ETL", "Python", "Real-time"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance. In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility \u2014 including the simplified Python Data Source API \u2014 and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution. /Databricks"}
{"session_id": "introducing-lakeflow-future-data-engineering-databricks", "title": "Introducing Lakeflow: The Future of Data Engineering on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Scala"], "speakers": ["Sr. Director of Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to explore Lakeflow, Databricks' end-to-end solution for simplifying and unifying the most complex data engineering workflows. This session builds on keynote announcements, offering an accessible introduction for newcomers while emphasizing the transformative value Lakeflow delivers. We\u2019ll cover: Discover how Lakeflow equips data teams with a seamless experience for ingestion, transformation and orchestration, reducing complexity and driving productivity. By unifying these capabilities, Lakeflow lays the groundwork for scalable, reliable and efficient data pipelines in a governed and high-performing environment. /Distinguished Engineer\nDatabricks /Sr. Director of Product Management"}
{"session_id": "introducing-simplified-state-tracking-apache-sparktm-structured", "title": "Introducing Simplified State Tracking in Apache Spark\u2122 Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Streaming"], "speakers": ["Sr. SSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation will review the new change feed and snapshot capabilities in Apache Spark\u2122 Structured Streaming\u2019s State Reader API. The State Reader API enables users to access and analyze Structured Streaming's internal state data. Readers will learn how to leverage the new features to debug, troubleshoot and analyze state changes efficiently, making streaming workloads easier to manage at scale. /Sr. SSA"}
{"session_id": "lakeflow-connect-easy-efficient-ingestion-databases", "title": "Lakeflow Connect: Easy, Efficient Ingestion From Databases", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Member of Technical Staff, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Connect streamlines the ingestion of incremental data from popular databases like SQL Server and PostgreSQL. In this session, we\u2019ll review best practices for networking, security, minimizing database load, monitoring and more \u2014 tailored to common industry scenarios. Join us to gain practical insights into Lakeflow Connect's functionality so that you\u2019re ready to build your own pipelines. Whether you're looking to optimize data ingestion or enhance your database integrations, this session will provide you with a deep understanding of how Lakeflow Connect works with databases. /Senior Product Manager\nDatabricks /Member of Technical Staff"}
{"session_id": "lakeflow-connect-game-changer-complex-event-driven-architectures", "title": "Lakeflow Connect: The Game-Changer for Complex Event-Driven Architectures", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["DLT", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Real-time"], "speakers": ["Cloud Solutions Architect, European Food Safety Authority"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In 2020, Delaware implemented a state-of-the-art, event-driven architecture for EFSA, enabling a highly decoupled system landscape, presented at the Data&AI Summit 2021. By centrally brokering events in near real-time, consumer applications react instantly to events from producer applications as they occur. Event producers are decoupled from consumers via a publisher/subscriber mechanism. Over the past years, we noticed some drawbacks. The processing of these custom events, primarily aimed for process integration weren\u2019t covering all edge cases, the data quality was not always optimal due to missing events and we needed to create a complex logic for SCD2 tables. Lakeflow Connect allows us to extract the data directly from the source without the complex architecture in between, avoiding data loss and thus, data quality issues, and with some simple adjustments, an SCD2 table is created automatically. Lakeflow Connect allows us to create more efficient and intelligent data provisioning. /Senior Consultant\ndelaware /Business Analyst\ndelaware /Cloud Solutions Architect"}
{"session_id": "lakeflow-connect-seamless-data-ingestion-enterprise-apps", "title": "Lakeflow Connect: Seamless Data Ingestion From Enterprise Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, TRAVEL AND HOSPITALITY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics"], "speakers": ["Director, Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Connect enables you to easily and efficiently ingest data from enterprise applications like Salesforce, ServiceNow, Google Analytics, SharePoint, NetSuite, Dynamics 365 and more. In this session, we\u2019ll dive deep on using connectors for the most popular SaaS applications, reviewing common use cases such as analyzing consumer behavior, predicting churn and centralizing HR analytics. You'll also hear from an early customer about how Lakeflow Connect helped unify their customer data to drive an improved automotive experience. We\u2019ll wrap up with a Q&A so you have the opportunity to learn from our experts. /Team Lead Data-Driven Business Solutions\nPorsche Informatik GmbH /Director, Product Management"}
{"session_id": "lakeflow-connect-simplify-ingestion-iot-clickstreams-and-telemetry", "title": "Lakeflow Connect: Simplify Ingestion for IoT, Clickstreams and Telemetry", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Sr. Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we\u2019ll explore new patterns that simplify real-time data ingestion into your lakehouse \u2014 especially for sources like IoT, clickstreams, telemetry and more. We\u2019ll begin with a quick look at the evolution of the ingestion landscape and the growing need to embed data flows closer to the source. Then, we\u2019ll introduce a modern approach that helps you \u201cshift left\u201d on ingestion \u2014 making it easier to integrate analytics and AI directly into operational systems, rather than treating them as afterthoughts. This shift can lead to simpler architectures that reduce unnecessary hops and scale with your operations. To close, we\u2019ll share a practical decision framework to help you determine which ingestion options best fit your use case, followed by a live Q&A to help you get started quickly. /Member of Technical Staff\nDatabricks /Sr. Staff Software Engineer"}
{"session_id": "lakeflow-connect-smarter-simpler-file-ingestion-next-generation-auto", "title": "Lakeflow Connect: Smarter, Simpler File Ingestion With the Next Generation of Auto Loader", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "ELT"], "speakers": ["Sr Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is the definitive tool for ingesting data from cloud storage into your lakehouse. In this session, we\u2019ll unveil new features and best practices that simplify every aspect of cloud storage ingestion. We\u2019ll demo out-of-the-box observability for pipeline health and data quality, walk through improvements for schema management, introduce a series of new data formats and unveil recent strides in Auto Loader performance. Along the way, we\u2019ll provide examples and best practices for optimizing cost and performance. Finally, we\u2019ll introduce a preview of what\u2019s coming next \u2014 including a REST API for pushing files directly to Delta, a UI for creating cloud storage pipelines and more. Join us to help shape the future of file ingestion on Databricks. /Staff Software Engineer\nDatabricks /Sr Staff Software Engineer"}
{"session_id": "lakeflow-effect", "title": "The LakeFlow Effect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Director of Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow brings much excitement, simplicity and unification to Databricks\u2019 engineering experience. Databricks\u2019 Bilal Aslam (Sr. Director of Product Management) and Josue A. Bogran (Databricks MVP & content creator) provide an overview of the history of Lakeflow, current value to your organization and the direction its capabilities are going toward. The session covers: The session will also provide you with an opportunity to ask questions to the team behind Lakeflow. /VP of Data + AI Architecture\nJosueBogran.com & zeb.co /Sr. Director of Product Management"}
{"session_id": "lakeflow-observability-ui-monitoring-deep-analytics", "title": "Lakeflow Observability: From UI Monitoring to Deep Analytics", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Monitoring data pipelines is key to reliability at scale. In this session, we\u2019ll dive into the observability experience in Lakeflow, Databricks\u2019 unified DE solution \u2014 from intuitive UI monitoring to advanced event analysis, cost observability and custom dashboards. We\u2019ll walk through the revamped UX for Lakeflow observability, showing how to: This session will help you unlock full visibility into your data workflows. /Product Management\nDatabricks /Product Manager"}
{"session_id": "lakeflow-production-cicd-testing-and-monitoring-scale", "title": "Lakeflow in Production: CI/CD, Testing and Monitoring at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline", "Data Quality", "Scala"], "speakers": ["Principal Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building robust, production-grade data pipelines goes beyond writing transformation logic \u2014 it requires rigorous testing, version control, automated CI/CD workflows and a clear separation between development and production. In this talk, we\u2019ll demonstrate how Lakeflow, paired with Databricks Asset Bundles (DABs), enables Git-based workflows, automated deployments and comprehensive testing for data engineering projects. We\u2019ll share best practices for unit testing, CI/CD automation, data quality monitoring and environment-specific configurations. Additionally, we\u2019ll explore observability techniques and performance tuning to ensure your pipelines are scalable, maintainable and production-ready. /Sr. Staff Product Manager\nDatabricks /Principal Engineer"}
{"session_id": "leveraging-genai-synthetic-data-generation-improve-spark-testing-and", "title": "Leveraging GenAI for Synthetic Data Generation to Improve Spark Testing and Performance in Big Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "LLAMA", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Principal Data Engineer, Independent Community"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Testing Spark jobs in local environments is often difficult due to the lack of suitable datasets, especially under tight timelines. This creates challenges when jobs work in development clusters but fail in production, or when they run locally but encounter issues in staging clusters due to inadequate documentation or checks. In this session, we\u2019ll discuss how these challenges can be overcome by leveraging Generative AI to create custom synthetic datasets for local testing. By incorporating variations and sampling, a testing framework can be introduced to solve some of these challenges, allowing for the generation of realistic data to aid in performance and load testing. We\u2019ll show how this approach helps identify performance bottlenecks early, optimize job performance and recognize scalability issues while keeping costs low. This methodology fosters better deployment practices and enhances the reliability of Spark jobs across environments. /Principal Data Engineer"}
{"session_id": "mastering-change-data-capture-dlt", "title": "Mastering Change Data Capture With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Streaming"], "speakers": ["Software Engineer, Square"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Transactional systems are a common source of data for analytics, and Change Data Capture (CDC) offers an efficient way to extract only what\u2019s changed. However, ingesting CDC data into an analytics system comes with challenges, such as handling out-of-order events or maintaining global order across multiple streams. These issues often require complex, stateful stream processing logic. This session will explore how DLT simplifies CDC ingestion using the Apply Changes function. With Apply Changes, global ordering across multiple change feeds is handled automatically \u2014 there is no need to manually manage state or understand advanced streaming concepts like watermarks. It supports both snapshot-based inputs from cloud storage and continuous change feeds from systems like message buses, reducing complexity for common streaming use cases. /Product Management\nDatabricks /Software Engineer"}
{"session_id": "maximize-retail-data-insights-genie-deltasharing-crisps-collaborative", "title": "Maximize Retail Data Insights in Genie with DeltaSharing via Crisp\u2019s Collaborative Commerce Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Crisp"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Crisp"}
{"session_id": "metadata-driven-streaming-ingestion-using-dlt-azure-event-hubs-and", "title": "Metadata-Driven Streaming Ingestion Using DLT, Azure Event Hubs and a Schema Registry", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Streaming"], "speakers": ["Principal Data Engineer, Plexure"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Plexure, we ingest hundreds of millions of customer activities and transactions into our data platform every day, fuelling our personalisation engine and providing insights into the effectiveness of marketing campaigns. We're on a journey to transition from infrequent batch ingestion to near real-time streaming using Azure Event Hubs and DLT. This transformation will allow us to react to customer behaviour as it happens, rather than hours or even days later. It also enables us to move faster in other ways. By leveraging a Schema Registry, we've created a metadata-driven framework that allows data producers to: Join us to learn more about our journey and see how we're implementing this with DLT meta-programming - including a live demo of the end-to-end process! /Principal Data Engineer"}
{"session_id": "next-level-pyspark-udf-debugging", "title": "Next-Level PySpark UDF Debugging", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Debugging PySpark User Defined Functions (UDFs) has long been challenging due to the distributed execution model and limited runtime visibility. Traditional methods often require manually searching through scattered logs, making debugging slow and inefficient. In this talk, we introduce a set of powerful UDF debugging improvements, including a new logging framework that provides structured, queryable insights into UDF execution. We also cover timeouts to stop long-running tasks, better error messages for easier debugging, and best practices for common UDF issues. /Sr. Software Engineer\nDatabricks /Senior Software Engineer"}
{"session_id": "no-code-change-your-python-udf-arrow-optimization", "title": "No-Code Change in Your Python UDF for Arrow Optimization", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Python"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark\u2122 has introduced Arrow-optimized APIs such as Pandas UDFs and the Pandas Functions API, providing high performance for Python workloads. Yet, many users continue to rely on regular Python UDFs due to their simple interface, especially when advanced Python expertise is not readily available. This talk introduces a powerful new feature in Apache Spark that brings Arrow optimization to regular Python UDFs. With this enhancement, users can leverage performance gains without modifying their existing UDFs \u2014 simply by enabling a configuration setting or toggling a UDF-level parameter. Additionally, we will dive into practical tips and features for using Arrow-optimized Python UDFs effectively, exploring their strengths and limitations. Whether you\u2019re a Spark beginner or an experienced user, this session will allow you to achieve the best of both simplicity and performance in your workflows with regular Python UDFs. /Staff Software Engineer"}
{"session_id": "orchestration-lakeflow-jobs", "title": "Orchestration With Lakeflow Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "ETL", "Machine Learning"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious about orchestrating data pipelines on Databricks? Join us for an introduction to Lakeflow Jobs (formerly Databricks Workflows) \u2014 an easy-to-use orchestration service built into the Databricks Data Intelligence Platform. Lakeflow Jobs simplifies automating your data and AI workflows, from ETL pipelines to machine learning model training. In this beginner-friendly session, you'll learn how to: We\u2019ll walk through common use cases, share demos and offer tips to help you get started quickly. If you're new to orchestration or just getting started with Databricks, this session is for you. /Product Management\nDatabricks /Product Manager"}
{"session_id": "pdf-document-ingestion-accelerator-genai-applications", "title": "PDF Document Ingestion Accelerator for GenAI Applications", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Streaming"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Financial Service customers in the GenAI space have a common use case of ingestion and processing of unstructured documents \u2014 PDF/images \u2014 then performing downstream GenAI tasks such as entity extraction and RAG based knowledge Q&A. The pain points for the customers for these types of use cases are: In this talk we will present an optimized structured streaming workflow for complex PDF ingestion. The key techniques include Apache Spark\u2122 optimization, multi-threading, PDF object extraction, skew handling and auto retry logics /Lead SSA\nDatabricks /Specialist Solution Architect"}
{"session_id": "pella-next-generation-implement-optimization-techniques-and-automated", "title": "Pella Next Generation: Implement Optimization Techniques and Automated Deployments for Cost Savings", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PROFESSIONAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Data Quality"], "speakers": ["Pella Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will explore the \u201cPella Next Generation\u201d project, towards performance optimization and cost savings using various tools within Databricks data analytics in an Azure cloud. Liquid clustering, managed schema/table within Unity Catalog, BI analytical dashboard to monitor job performance and cost \u2014 compute and storage \u2014 are used towards performance optimization. Auto Loader and DLT with data quality checks/constraints and a common framework to support data ingestion into the medallion layers of the data lake. Databricks Asset Bundles for workflows/jobs deployment resulted in significant cost savings of $30K/year \u2014 conservative number \u2014 with a potential savings of up to $40K/year for the organization. This presentation will provide valuable takeaways for professionals looking to implement similar solutions in their own organizations. /Pella Corporation"}
{"session_id": "perks-using-unity-catalog-managed-tables", "title": "The Perks of Using Unity Catalog Managed Tables", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, HEALTH AND LIFE SCIENCES, MANUFACTURING", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Architecture"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session provides actionable insights into how organizations can transition to Unity Catalog managed tables to unlock the full potential of predictive optimization and future-proof their data architecture. Whether you\u2019re managing thousands of tables or looking to streamline operations, this talk will equip you with the tools and strategies to succeed in the era of intelligent data management. Key highlights include: /Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": ["CEO, Acadford"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /CEO"}
{"session_id": "race-real-time-low-latency-streaming-etl-next-gen-oltp-db", "title": "Race to Real-Time: Low-Latency Streaming ETL With Next-Gen OLTP-DB", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Pipeline", "ETL", "Real-time", "Scala", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s digital economy, real-time insights and rapid responsiveness are paramount to delivering exceptional user experiences and lowering TCO. In this session, discover a pioneering approach that leverages a low-latency streaming ETL pipeline built with Apache Spark\u2122 Structured Streaming and a next-gen OLTP-DB solution. Validated in a live customer scenario, this architecture achieves sub-2 second end-to-end latency by seamlessly ingesting streaming data from Kinesis and merging it into OLTP-DB. This breakthrough not only enhances performance and scalability but also provides a replicable blueprint for transforming data pipelines across various verticals. Join us as we delve into the advanced optimization techniques and best practices that underpin this innovation, demonstrating how Databricks next-generation solutions can revolutionize real-time data processing and unlock a myriad of new use cases in data landscape. /Specialist Solutions Architect"}
{"session_id": "real-time-analytics-pipeline-iot-device-monitoring-and-reporting", "title": "Real-Time Analytics Pipeline for IoT Device Monitoring and Reporting", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Quality", "ELT", "Real-time", "Streaming"], "speakers": ["Lead Data Engineer, CKDelta"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will show how we implemented a solution to support high-frequency data ingestion from smart meters. We implemented a robust API endpoint that interfaces directly with IoT devices. This API processes messages in real time from millions of distributed IoT devices and meters across the network.The architecture leverages cloud storage as a landing zone for the raw data, followed by a streaming pipeline built on DLT. This pipeline implements a multi-layer medallion architecture to progressively clean, transform and enrich the data. The pipeline operates continuously to maintain near real-time data freshness in our gold layer tables. These datasets connect directly to Databricks Dashboards, providing stakeholders with immediate insights into their operational metrics. This solution demonstrates how modern data architecture can handle high-volume IoT data streams while maintaining data quality and providing accessible real-time analytics for business users. /Data Scientist\nCK Delta /Lead Data Engineer"}
{"session_id": "real-time-mode-technical-deep-dive-how-we-built-sub-300-millisecond", "title": "Real-Time Mode Technical Deep Dive: How We Built Sub-300 Millisecond Streaming Into Apache Spark\u2122", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Machine Learning", "Real-time", "SQL", "Streaming"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Real-time mode is a new low-latency execution mode for Apache Spark\u2122 Structured Streaming. It can consistently provide p99 latencies less than 300 milliseconds for a broad set of stateless and stateful streaming queries. Our talk focuses on the technical aspects of making this possible in Spark. We\u2019ll dive into the core architecture that enables these dramatic latency improvements, including a concurrent stage scheduler and a non-blocking shuffle. We\u2019ll explore how we maintained Spark\u2019s fault-tolerance guarantees, and we\u2019ll also share specific optimizations we made to our streaming SQL operators. These architectural improvements have already enabled Databricks customers to build workloads with latencies up to 10x lower than before. Early adopters in our Private Preview have successfully implemented real-time enrichment pipelines and feature engineering for machine learning \u2014 use cases that were previously impossible at these latencies. /Staff Software Engineer\nDatabricks /Databricks"}
{"session_id": "saas-data-ingestion-lakeflow-connect", "title": "SaaS Data Ingestion With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality"], "speakers": ["Sr. Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Are you looking to make informed business decisions by analyzing data from multiple SaaS platforms, but struggling with custom APIs, governance and compliance? Join us as we showcase how Databricks built our ingestion platform using our own Lakeflow Connect product that seamlessly ingests data from all our SaaS systems while addressing challenges like fragmentation, data quality, discoverability, observability and governance. Our streamlined approach empowers your teams to focus on strategic insights instead of the technical burdens of data management, ensuring robust compliance and a faster path to actionable intelligence. /Software Engineer\nDatabricks /Sr. Engineering Manager"}
{"session_id": "saving-millions-millions-navigating-towards-cost-efficiency-pinterests", "title": "Saving Millions From Millions: Navigating Towards Cost-Efficiency in Pinterest's Spark Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Software Engineer, Pinterest"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "While Spark offers powerful processing capabilities for massive data volumes, cost-efficiency challenges are always bothering users operating at large scales. At Pinterest, where we run millions of Spark jobs monthly, maintaining infra cost efficiency is crucial to support our rapid business growth. To tackle this challenge, we have developed several strategies that have saved us tens of millions of dollars across numerous job instances. We will share our analytical methodology for identifying performance bottlenecks, and the technical solutions to overcome various challenges. Our approach includes extracting insights from billions of collected metrics, leveraging remote shuffle services to address shuffle slowness and improve memory utilization and reduce costs while hosting hundreds of millions of pods. The presentation aims to trigger more discussions about cost efficiency topics of Apache Spark in the community and help the community to tackle the common challenge. /Staff Software Engineer"}
{"session_id": "scaling-data-engineering-pipelines-preparing-credit-card-transactions", "title": "Scaling Data Engineering Pipelines: Preparing Credit Card Transactions Data for Machine Learning", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Delta Lake", "ELT"], "speakers": ["Lead Data Engineer, Mastercard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We discuss two real-world use cases in big data engineering, focusing on constructing stable pipelines and managing storage at a petabyte scale. The first use case highlights the implementation of Delta Lake to optimize data pipelines, resulting in an 80% reduction in query time and a 70% reduction in storage space. The second use case demonstrates the effectiveness of the Workflows \u2018ForEach\u2019 operator in executing compute-intensive pipelines across multiple clusters, significantly reducing processing time from months to days. This approach involves a reusable design pattern that isolates notebooks into units of work, enabling data scientists to independently test and develop. /Director, Data Scientist\nMastercard /Lead Data Engineer"}
{"session_id": "scaling-data-quality-zillow-migrating-and-enhancing-data-quality", "title": "Scaling Data Quality at Zillow: Migrating and Enhancing Data Quality Systems on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Scala"], "speakers": ["Sr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Zillow has well-established, comprehensive systems for defining and enforcing data quality contracts and detecting anomalies. In this session, we will share how we evaluated Databricks\u2019 native data quality features and why we chose DLT expectations for DLT pipelines, along with a combination of enforced constraints and self-defined queries for other job types. Our evaluation considered factors such as performance overhead, cost and scalability. We\u2019ll highlight key improvements over our previous system and demonstrate how these choices have enabled Zillow to enforce scalable, production-grade data quality. Additionally, we are actively testing Databricks\u2019 latest data quality innovations, including enhancements to lakehouse monitoring and the newly released DQX project from Databricks Labs. In summary, we will cover Zillow\u2019s approach to data quality in the lakehouse, key lessons from our migration and actionable takeaways. /Software Dev Engineer, Big Data\nZillow /Sr. Solutions Architect"}
{"session_id": "scaling-identity-graph-ingestion-1m-eventssec-spark-streaming-delta", "title": "Scaling Identity Graph Ingestion to 1M Events/Sec with Spark Streaming & Delta Lake", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Sr Software Engineer, Adobe"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Adobe\u2019s Real-Time Customer Data Platform relies on the identity graph to connect over 70 billion identities and deliver personalized experiences. This session will showcase how the platform leverages Databricks, Spark Streaming and Delta Lake, along with 25+ Databricks deployments across multiple regions and clouds \u2014 Azure & AWS \u2014 to process terabytes of data daily and handle over a million records per second. The talk will highlight the platform\u2019s ability to scale, demonstrating a 10x increase in ingestion pipeline capacity to accommodate peak traffic during events like the Super Bowl. Attendees will learn about the technical strategies employed, including migrating from Flink to Spark Streaming, optimizing data deduplication, and implementing robust monitoring and anomaly detection. Discover how these optimizations enable Adobe to deliver real-time identity resolution at scale while ensuring compliance and privacy. /Sr. Data Engineer\nAdobe /Sr Software Engineer"}
{"session_id": "serverless-new-easy-button-how-hp-inc-used-serverless-turbocharge-their", "title": "Serverless as the New \"Easy Button\": How HP Inc. Used Serverless to Turbocharge Their Data Pipeline", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Scala"], "speakers": ["Senior Databricks Engineer, Zahlen Solutions"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How do you wrangle over 8TB of granular \u201chit-level\u201d website analytics data with hundreds of columns, all while eliminating the overhead of cluster management, decreasing runtime and saving money? In this session, we\u2019ll dive into how we helped HP Inc. use Databricks serverless compute and DLT to streamline Adobe Analytics data ingestion while making it faster, cheaper and easier to operate. We\u2019ll walk you through our full migration story \u2014 from managing unwieldy custom-defined AWS-based Apache Spark\u2122 clusters to spinning up Databricks serverless pipelines and workflows with on-demand scalability and near-zero overhead. If you want to simplify infrastructure, optimize performance and get more out of your Databricks workloads, this talk is for you. /CEO\nZahlen Solutions LLC /Senior Databricks Engineer"}
{"session_id": "simplify-data-ingest-and-egress-new-python-data-source-api", "title": "Simplify Data Ingest and Egress with the New Python Data Source API", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Data Integration", "Python"], "speakers": ["Sr. SSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data engineering teams are frequently tasked with building bespoke ingest and/or egress solutions for myriad custom, proprietary, or industry-specific data sources or sinks. Many teams find this work cumbersome and time-consuming. Recognizing these challenges, Databricks interviewed numerous companies across different industries to better understand their diverse data integration needs. This comprehensive feedback led us to develop the Python Data Source API for Apache Spark\u2122. /Sr. SSA"}
{"session_id": "simplifying-data-pipelines-dlt-beginners-guide", "title": "Simplifying Data Pipelines With DLT: A Beginner\u2019s Guide", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Pipeline", "ETL", "Real-time", "SQL", "Streaming"], "speakers": ["Data Engineer, 84.51"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As part of the new Lakeflow data engineering experience, DLT makes it easy to build and manage reliable data pipelines. It unifies batch and streaming, reduces operational complexity and ensures dependable data delivery at scale \u2014 from batch ETL to real-time processing. DLT excels at declarative change data capture, batch and streaming workloads, and efficient SQL-based pipelines. In this session, you\u2019ll learn how we\u2019ve reimagined data pipelining with DLT, including: Join us to see how DLT powers better analytics and AI with reliable, unified pipelines. /Senior Product Marketing Manager\nDatabricks /Data Engineer"}
{"session_id": "somebody-set-us-bomb-identifying-list-bombing-end-users-email-anti-spam", "title": "Somebody Set Up Us the Bomb: Identifying List Bombing of End Users in an Email Anti-Spam Context", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "ETL", "Streaming"], "speakers": ["Security Research Technical Leader, Cisco Talos"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditionally, spam emails are messages a user does not want, containing some kind of threat like phishing. Because of this, detection systems can focus on malicious content or sender behavior. List bombing upends this paradigm. By abusing public forms such as marketing signups, attackers can fill a user's inbox with high volumes of legitimate mail. These emails don't contain threats, and each sender is following best practices to confirm the recipient wants to be subscribed, but the net effect for an end user is their inbox being flooded with dozens of emails per minute. This talk covers the the exploration and implementation for identifying this attack in our company's anti-spam telemetry: from reading and writing to Kafka, Delta table streaming for ETL workflows, multi-table liquid clustering design for efficient table joins, curating gold tables to speed up critical queries and using Delta tables as an auditable integration point for interacting with external services. /Security Research Technical Leader"}
{"session_id": "spaghetti-bowl-pipeline-dlt-efficiency", "title": "From Spaghetti Bowl Pipeline to DLT Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Apache Spark", "Data Engineering", "Delta Lake", "ELT", "Scala"], "speakers": ["Analytics Engineer, Intermountain Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today's data-driven world, the ability to efficiently manage and transform data is crucial for any organization. This presentation will explore the process of converting a complex and messy workflow into a clean and simple DLT pipeline at a large integrated health system, Intermountain Health. Alteryx is a powerful tool for data preparation and blending, but as workflows grow in complexity, they can become difficult to manage and maintain. DLT, on the other hand, offers a more democratized, streamlined and scalable approach to data engineering, leveraging the power of Apache Spark and Delta Lake. We will begin by examining a typical legacy workflow, identifying common pain points such as tangled logic, performance bottlenecks and maintenance challenges. Next, we will demonstrate how to translate this workflow into a DLT pipeline, highlighting key steps such as data transformation, validation and delivery. /Analytics Engineer"}
{"session_id": "spark-40-and-delta-40-streaming-data", "title": "Spark 4.0 and Delta 4.0 For Streaming Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Real-time"], "speakers": ["Chief Digital Technology Advisor, Shell"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Real-time data is one of the most important datasets for any Data and AI Platform across any industry. Spark 4.0 and Delta 4.0 include new features that make ingestion and querying of real-time data better than ever before. Features such as: In this presentation you will learn how data teams can leverage these latest features to build industry-leading, real-time data products using Spark and Delta and includes real world examples and metrics of the improvements they make in performance and processing of data in the real time space. /Chief Digital Technology Advisor"}
{"session_id": "spark-connect-flexible-local-access-apache-spark-scale", "title": "Spark Connect: Flexible, Local Access to Apache Spark at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "What if you could run Spark jobs without worrying about clusters, versions and upgrades? Did you know Spark has this functionality built-in today? Join us to take a look at this functionality \u2014 Spark Connect. Join us to dig into how Spark Connect works \u2014 abstracting away Spark clusters away in favor of the DataFrame API and unresolved logical plans. You will learn some of the cool things Spark Connect unlocks, including: /Databricks"}
{"session_id": "spark-databricks-tips-and-tricks", "title": "Spark on Databricks: Tips and Tricks", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores a collection of advanced and lesser-known use cases in Apache Spark\u2122, drawn from real-world scenarios and internal experimentation. Topics include: We'll also cover additional patterns and tooling tips that can help solve operational challenges and optimize performance in production Spark environments. /Specialist Solutions Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "sponsored-astronomer-scaling-data-teams-future", "title": "Sponsored by: Astronomer | Scaling Data Teams for the Future", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering"], "speakers": ["CD, and Infrastructure-as-code) from the DevOps movement are gradually making their way into data engineering. We believe these changes have led to rise of DataOps a new wave best practices that will transform discipline But how do you reactive team proactive force for innovation? We\u2019ll explore key principles building resilient, high-impact team\u2014from structuring collaboration, testing, automation, leveraging modern orchestration tools. Whether you\u2019re leading or looking future-proof your career, you\u2019ll walk away with actionable insights on stay ahead in rapidly changing landscape."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The role of data teams and data engineers is evolving. No longer just pipeline builders or dashboard creators, today\u2019s data teams must evolve to drive business strategy, enable automation, and scale with growing demands. Best practices seen in the software engineering world (Agile development, CI/CD, and Infrastructure-as-code) from the DevOps movement are gradually making their way into data engineering. We believe these changes have led to the rise of DataOps and a new wave of best practices that will transform the discipline of data engineering. But how do you transform a reactive team into a proactive force for innovation? We\u2019ll explore the key principles for building a resilient, high-impact data team\u2014from structuring for collaboration, testing, automation, to leveraging modern orchestration tools. Whether you\u2019re leading a team or looking to future-proof your career, you\u2019ll walk away with actionable insights on how to stay ahead in the rapidly changing data landscape."}
{"session_id": "sponsored-astronomer-unlocking-future-data-orchestration-introducing", "title": "Sponsored by: Astronomer | Unlocking the Future of Data Orchestration: Introducing Apache Airflow\u00ae 3", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Airflow 3 is here, bringing a new era of flexibility, scalability, and security to data orchestration. This release makes building, running, and managing data pipelines easier than ever. In this session, we will cover the key benefits of Airflow 3, including: (1) Ease of Use: Airflow 3 rethinks the user experience\u2014from an intuitive, upgraded UI to DAG Versioning and scheduler-integrated backfills that let teams manage pipelines more effectively than ever before (2) Stronger Security: By decoupling task execution from direct database connections, Airflow 3 enforces task isolation and minimal-privilege access. This meets stringent compliance standards while reducing the risk of unauthorized data exposure. (3) Ultimate Flexibility: Run tasks anywhere, anytime with remote execution and event-driven scheduling. Airflow 3 is designed for global, heterogeneous modern data environments with an architecture that facilitates edge and hybrid-cloud to GPU-based deployments."}
{"session_id": "sponsored-coalesce-bringing-order-chaos-how-succeed-data-analytics", "title": "Sponsored by: Coalesce | Bringing Order to Chaos: How to Succeed in a Data & Analytics World", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Priorities shift, requirements change, resources fluctuate, and the demands on data teams are only continuing to grow. Join this session, led by Coalesce Sales Engineering Director, Michael Tantrum, to hear about the most efficient way to deliver high quality data to your organization at the speed they need to consume it. Learn how to sidestep the common pitfalls of data development for maximum data team productivity."}
{"session_id": "sponsored-coalesce-raw-data-real-time-retention-powering-customer", "title": "Sponsored by: Coalesce | From Raw Data to Real-Time Retention: Powering Customer Health Scores on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Understanding customer engagement and retention isn\u2019t optional\u2014it\u2019s mission-critical. Join us for a live demo to see how you can build a scalable, governed customer health scoring model by transforming raw signals into actionable insights. Discover how Coalesce\u2019s low-code development platform works seamlessly with Databricks\u2019 lakehouse architecture to unify and operationalize customer data at scale. With built-in governance, automation, and metadata intelligence, you\u2019ll deliver trusted scores that support proactive decision-making across the business. Why Attend?"}
{"session_id": "sponsored-dbt-labs-cooking-data-success-how-chick-fil-optimizes-trust", "title": "Sponsored by: dbt Labs | Cooking Up Data Success: How Chick-fil-A Optimizes Trust, Efficiency and Speed with dbt & Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DBT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Accelerating development, gaining better control over data models, and increasing confidence in data accuracy benefit both internal stakeholders and customers. Join this session to discover how Chick-fil-A\u2014a leading fast-food chain renowned for its efficiency, customer service, and iconic chicken sandwiches\u2014is transforming its data operations with dbt and Databricks. Learn how their \u201cdata recipe,\u201d has enhanced trust, velocity, efficiency, and governance. Through streamlined development, improved collaboration, and higher data quality, Chick-fil-A is achieving faster time-to-market and reducing errors, driving a significant impact across its organization."}
{"session_id": "sponsored-dbt-labs-leveling-data-engineering-riot-how-we-rolled-out-dbt", "title": "Sponsored by: dbt Labs | Leveling Up Data Engineering at Riot: How We Rolled Out dbt and Transformed the Developer Experience", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DBT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Riot Games reduced its Databricks compute spend and accelerated development cycles by transforming its data engineering workflows\u2014migrating from bespoke Databricks notebooks and Spark pipelines to a scalable, testable, and developer-friendly dbt-based architecture. In this talk, members of the Developer Experience & Automation (DEA) team will walk through how they designed and operationalized dbt to support Riot\u2019s evolving data needs."}
{"session_id": "sponsored-firebolt-power-low-latency-data-ai-apps", "title": "Sponsored by: Firebolt | The Power of Low-latency Data for AI Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retrieval-augmented generation (RAG) has transformed AI applications by grounding responses with external data. It can be better. By pairing RAG with low latency SQL analytics, you can enrich responses with instant insights, leading to a more interactive and insightful user experience with fresh, data-driven intelligence. In this talk, we\u2019ll demo how low latency SQL combined with an AI application can deliver speed, accuracy, and trust."}
{"session_id": "sponsored-fivetran-scalable-data-ingestion-building-custom-pipelines", "title": "Sponsored by: Fivetran | Scalable Data Ingestion: Building custom pipelines with the Fivetran Connector SDK and Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Integration", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Organizations have hundreds of data sources, some of which are very niche or difficult to access. Incorporating this data into your lakehouse requires significant time and resources, hindering your ability to work on more value-add projects. Enter the Fivetran Connector SDK- a powerful new tool that enables your team to create custom pipelines for niche systems, custom APIs, and sources with specific data filtering requirements, seamlessly integrating with Databricks. During this session, Fivetran will demonstrate how to (1) Leverage the Connector SDK to build scalable connectors, enabling the ingestion of diverse data into Databricks (2) Gain flexibility and control over historical and incremental syncs, delete capture, state management, multithreading data extraction, and custom schemas (3) Utilize practical examples, code snippets, and architectural considerations to overcome data integration challenges and unlock the full potential of your Databricks environment."}
{"session_id": "sponsored-fivetran-under-hood-general-motors-data-engine-supply-chain", "title": "Sponsored by: Fivetran | Under the Hood: General Motor\u2019s Data Engine for Supply Chain Innovation", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING", "technologies": ["DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "General Motors, a global leader in the automotive and manufacturing industry, is on a mission to modernize its data infrastructure to drive supply chain efficiency and unlock new software-based revenue opportunities. By leveraging Fivetran\u2019s data movement platform, GM can reliably capture real-time data from a complex landscape of SaaS and on-premises sources and securely ingest it into the Databricks Data Intelligence Platform. Join this session to learn:"}
{"session_id": "sql-first-etl-building-easy-efficient-data-pipelines-dlt", "title": "SQL-First ETL: Building Easy, Efficient Data Pipelines With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "SQL"], "speakers": ["Sr Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores how SQL-based ETL can accelerate development, simplify maintenance and make data transformation more accessible to both engineers and analysts. We'll walk through how Databricks DLT and Databricks SQL warehouse support building production-grade pipelines using familiar SQL constructs. Topics include: By the end of the session, you\u2019ll understand how SQL-first approaches can streamline ETL development and support both operational and analytical use cases. /Sr Staff Product Manager"}
{"session_id": "streaming-meets-governance-building-ai-ready-tables-confluent-tableflow", "title": "Streaming Meets Governance: Building AI-Ready Tables With Confluent Tableflow and Unity Catalog", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "ELT", "Real-time", "Streaming"], "speakers": ["Senior Product Manager, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how Databricks and Confluent are simplifying the path from real-time data to governed, analytics- and AI-ready tables. This session will cover how Confluent Tableflow automatically materializes Kafka topics into Delta tables and registers them with Unity Catalog \u2014 eliminating the need for custom streaming pipelines. We\u2019ll walk through how this integration helps data engineers reduce ingestion complexity, enforce data governance and make real-time data immediately usable for analytics and AI. /Member of Technical Staff\nDatabricks /Senior Product Manager"}
{"session_id": "supercharging-sales-intelligence-processing-billions-events-structured", "title": "Supercharging Sales Intelligence: Processing Billions of Events via Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "ELT", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Data Engineer, DigiCert"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DigiCert is a digital security company that provides digital certificates, encryption and authentication services and serves 88% of the Fortune 500, securing over 28 billion web connections daily. Our project aggregates and analyzes certificate transparency logs via public APIs to provide comprehensive market and competitive intelligence. Instead of relying on third-party providers with limited data, our project gives full control, deeper insights and automation. Databricks has helped us reliably poll public APIs in a scalable manner that fetches millions of events daily, deduplicate and store them in our Delta tables. We specifically use Spark for parallel processing, structured streaming for real-time ingestion and deduplication, Delta tables for data reliability, pools and jobs to ensure our costs are optimized. These technologies help us keep our data fresh, accurate and cost effective. This data has helped our sales team with real-time intelligence, ensuring DigiCert's success. /Director-Data Engineering\nDigiCert /Senior Data Engineer"}
{"session_id": "top-performance-and-cost-optimizations-dlt", "title": "Top Performance and Cost Optimizations for DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT simplifies pipeline development and management \u2014 but how do you optimize for performance and cost? In this session, we\u2019ll explore practical strategies for tuning DLT pipelines, including when and how to use autoscaling, Photon and different node types. We'll also cover how to monitor resource usage and decide when serverless is the right choice. You'll learn best practices drawn from real-world customer implementations, along with an overview of the latest performance enhancements available in serverless DLT. /Principal Solutions Architect"}
{"session_id": "transforming-customer-processes-and-gaining-productivity-dlt", "title": "Transforming Customer Processes and Gaining Productivity With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Integration"], "speakers": ["Sr Manager, Banco Bradesco S.A."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bradesco Bank is one of the largest private banks in Latin America, with over 75 million customers and over 80 years of presence in FSI. In the digital business, velocity to react to customer interactions is crucial to succeed. In the legacy landscape, acquiring data points on interactions over digital and marketing channels was complex, costly and lacking integrity due to typical fragmentation of tools. With the new in-house Customer Data Platform powered by Databricks Intelligent Platform, it was possible to completely transform the data strategy around customer data. Using some key components such Uniform and DLT, it was possible to increase data integrity, reduce latency and processing time and, most importantly, boost personal productivity and business agility. Months of reprocessing, weeks of human labor and cumbersome and complex data integrations were dramatically simplified achieving significant operational efficiency. /senior manager\nBradesco Bank /Sr Manager"}
{"session_id": "transforming-data-pipeline-management-targeted-proof-concept", "title": "Transforming Data Pipeline Management With a Targeted Proof of Concept", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Machine Learning", "Scala"], "speakers": ["VP, Capital One Financial"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Capital One, data-driven decision making is paramount to our success. This session explores how a focused proof of concept (POC) accelerated a shift in our data pipeline management strategy, resulting in operational improvements and expanded analytical capabilities. We'll cover the business challenges that motivated POC initiation, including data latency, cost savings and scalability limitations, and real-world results. We'll also dive into an examination of the before-and-after architecture with highlights for key technological levers. This session offers insights for data engineering and machine learning practitioners seeking to optimize their data pipelines for improved performance, scalability and business value. /VP Data Engineering CARD\nCapital One Financial /VP"}
{"session_id": "unifying-customer-data-drive-new-automotive-experience-lakeflow-connect", "title": "Unifying Customer Data to Drive a New Automotive Experience With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Integration", "Data Science"], "speakers": ["Team Lead Data-Driven Business Solutions, Porsche Informatik GmbH"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Data Intelligence Platform and Lakeflow Connect have transformed how Porsche manages and uses its customer data. By opting to use Lakeflow Connect instead of building a custom solution, the company has reaped the benefits of both operational efficiency and cost management. Internally, teams at Porsche now spend less time managing data integration processes. \u201cLakeflow Connect has enabled our dedicated CRM and Data Science teams to be more productive as they can now focus on their core work to help innovate, instead of spending valuable time on the data ingestion integration with Salesforce,\u201d says Gruber. This shift in focus is aligned with broader industry trends, where automotive companies are redirecting significant portions of their IT budgets toward customer experience innovations and digital transformation initiatives. This story was also shared as part of a Databricks Success Story \u2014 Elise Georis, Giselle Goicochea. /Team Lead Data-Driven Business Solutions"}
{"session_id": "unifying-human-curated-data-ingestion-and-real-time-updates-databricks", "title": "Unifying Human-Curated Data Ingestion and Real-Time Updates with Databricks DLT, Protobuf and BSR", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Data Pipeline", "Real-time", "Streaming"], "speakers": ["Data Platform Architect, Clinician Nexus"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Red Stapler is a streaming-native system on Databricks that merges file-based ingestion and real-time user edits into one DLT pipeline for near real-time feedback. Protobuf definitions, managed in the Buf Schema Registry (BSR), govern schema and data-quality rules, ensuring backward compatibility. All records \u2014 valid or not \u2014 are stored in an SCD Type 2 table, capturing every version for full history and immediate quarantine views of invalid data. This unified approach boosts data governance, simplifies auditing and streamlines error fixes. Running on DLT Serverless and the Kafka-compatible Bufstream keeps costs low by scaling down to zero when idle. Red Stapler\u2019s configuration-driven Protobuf logic adapts easily to evolving survey definitions without risking production. The result is consistent validation, quick updates and a complete audit trail \u2014 all critical for trustworthy, flexible data pipelines. /Data Platform Architect"}
{"session_id": "unlock-your-use-cases-deep-dive-structured-streamings-new", "title": "Unlock Your Use Cases: A Deep Dive on Structured Streaming\u2019s New TransformWithState API", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Streaming"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don\u2019t you just hate telling your customers \u201cNo\u201d? \u201cNo, I can\u2019t get you the data that quickly\u201d, or \u201cNo that logic isn\u2019t possible to implement\u201d really aren\u2019t fun to say. But what if you had a tool that would allow you to implement those use cases? What if it was in a technology you were already familiar with \u2014 say, Spark Structured Streaming? There is a brand new arbitrary stateful operations API called TransformWithState, and after attending this deep dive you won\u2019t have to say \u201cNo\u201d anymore. During this presentation we\u2019ll go through some real-world use cases and build them step-by-step. Everything from state variables, process vs. event time, watermarks, timers, state TTL, and even how you can initialize state with the checkpoint of another stream. Unlock your use cases with the power of Structured Streaming\u2019s TransformWithState! /Lead Solutions Architect\nDatabricks /Software Engineer"}
{"session_id": "unlocking-streaming-power-how-sega-wins-dlt", "title": "Unlocking Streaming Power: How SEGA Wins With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Quality", "Streaming"], "speakers": ["Head of Data Services, SEGA Europe Limited"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Streaming data is hard and costly \u2014 that's the default opinion, but it doesn\u2019t have to be. In this session, discover how SEGA simplified complex streaming pipelines and turned them into a competitive edge. SEGA sees over 40,000 events per second. That's no easy task, but enabling personalised gaming experiences for over 50 million gamers drives a huge competitive advantage. If you\u2019re wrestling with streaming challenges, this talk is your next checkpoint. We\u2019ll unpack how DLT helped SEGA, from automated schema evolution and simple data quality management to seamless streaming reliability. Learn how DLT drives value by transforming chaos emeralds into clarity, delivering results for a global gaming powerhouse. We'll step through the architecture, approach and challenges we overcame. Join Craig Porteous, Microsoft MVP from Advancing Analytics, and Felix Baker, Head of Data Services at SEGA Europe, for a fast-paced, hands-on journey into DLT\u2019s unique powers. /Associate Head of Data Engineering\nAdvancing Analytics /Head of Data Services"}
{"session_id": "upcoming-apache-spark-41-next-chapter-unified-analytics", "title": "The Upcoming Apache Spark 4.1: The Next Chapter in Unified Analytics", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Quality", "ETL", "Python", "Real-time"], "speakers": ["Senior Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance. In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility (including the simplified Python Data Source API) and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution. Don\u2019t miss this chance to see Spark\u2019s next chapter! /Engineering Director\nDatabricks /Senior Engineering Manager"}
{"session_id": "using-delta-rs-and-delta-kernel-rs-serve-cdc-feeds", "title": "Using Delta-rs and Delta-Kernel-rs to Serve CDC Feeds", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Python", "Scala"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Change data feeds are a common tool for synchronizing changes between tables and performing data processing in a scalable fashion. Serverless architectures offer a compelling solution for organizations looking to avoid the complexity of managing infrastructure. But how can you bring CDFs into a serverless environment? In this session, we'll explore how to integrate Change Data Feeds into serverless architectures using Delta-rs and Delta-kernel-rs\u2014open-source projects that allow you to read Delta tables and their change data feeds in Rust or Python. We\u2019ll demonstrate how to use these tools with Lakestore\u2019s serverless platform to easily stream and process changes. You\u2019ll learn how to: /Senior Resident Solutions Architect\nDatabricks /Software Engineer"}
{"session_id": "whats-new-apache-sparktm-40", "title": "What\u2019s New in Apache Spark\u2122 4.0?", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Sr. Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this session for a concise tour of Apache Spark\u2122 4.0\u2019s most notable enhancements: Whether you\u2019re a seasoned Spark user or new to the ecosystem, this talk will prepare you to leverage Spark 4.0\u2019s latest innovations for modern data and AI pipelines. /Senior Staff Software Engineer\nDatabricks /Sr. Staff Software Engineer"}
{"session_id": "why-you-should-move-dlt-serverless", "title": "Why You Should Move to DLT Serverless", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "ETL"], "speakers": ["Sr Backline Technical Solutions Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT Serverless offers a range of benefits that make it an attractive option for organizations looking to optimize their ETL (Extract, Transform, Load) processes.Key benefits of DLT Serverless: By moving to DLT Serverless, organizations can achieve faster, more reliable, and cost-effective data pipeline management, ultimately driving better business insights and outcomes. /Sr Backline Technical Solutions Engineer"}
