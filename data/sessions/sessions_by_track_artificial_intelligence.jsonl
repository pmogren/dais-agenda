{"session_id": "accelerate-end-end-multi-agents-databricks-and-dspy", "title": "Accelerate End-to-End Multi-Agents on Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A production-ready GenAI application is more than the framework itself. Like ML, you need a unified platform to create an end-to-end workflow for production quality applications. Below is an example of how this works on Databricks: In this session, learn how to build agents to access all your data and models through function calling. Then, learn how DSPy enables agent interaction with each other to ensure the question is answered correctly. We will demonstrate a chatbot, powered by multiple agents, to be able to answer questions and reason answers the base LLM does not know and very specialized topics. /Delivery Solutions Architect"}
{"session_id": "accelerating-model-development-and-fine-tuning-databricks-twelvelabs", "title": "Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Scaling large language models (LLMs) and multimodal architectures requires efficient data management and computational power. NVIDIA NeMo Framework Megatron-LM on Databricks is an open source solution that integrates GPU acceleration and advanced parallelism with Databricks Delta Lakehouse, streamlining workflows for pre-training and fine-tuning models at scale. This session highlights context parallelism, a unique NeMo capability for parallelizing over sequence lengths, making it ideal for video datasets with large embeddings. Through the case study of TwelveLabs\u2019 Pegasus-1 model, learn how NeMo empowers scalable multimodal AI development, from text to video processing, setting a new standard for LLM workflows. /Chief Technology Officer & Co-Founder\nTwelve Labs, Inc /Solutions Architect - NVIDIA"}
{"session_id": "achieve-your-mission-ai-driven-decisions", "title": "Achieve Your Mission With AI-Driven Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Government leaders overwhelmingly recognize the potential benefits of AI as critical to long-term strategic goals of efficiency, but implementation challenges and security concerns could be obstacles to success. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "achieving-precision-ai-retrieving-right-data-using-ai-agents", "title": "Achieving Precision in AI: Retrieving the Right Data Using AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director"}
{"session_id": "advanced-rag-overview-thawing-your-frozen-rag-pipeline", "title": "Advanced RAG Overview \u2014 Thawing Your Frozen RAG Pipeline", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ML Innovation, Experian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The most common RAG systems rely on a frozen RAG system \u2014 one where there\u2019s a single embedding model and single vector index. We\u2019ve achieved a modicum of success with that, but when it comes to increasing accuracy for production systems there is only so much this approach solves. In this session we will explore how to move from the frozen systems to adaptive RAG systems which produce more tailored outputs with higher accuracy. Databricks services: Lakehouse, Unity Catalog, Mosaic, Sweeps, Vector Search, Agent Evaluation, Managed Evaluation, Inference Tables /Head of AI/ML Innovation"}
{"session_id": "agentic-architectures-create-realistic-conversations-using-genai-teach", "title": "Agentic Architectures to Create Realistic Conversations: Using GenAI to Teach Empathy in Healthcare", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Providence Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Medical providers often receive less than 15 minutes of instruction in how to interact with patients during emotionally charged end of life interactions. Continuing education for clinicians is critical to hone these skills but is difficult to scale traditional approaches that require professional patients and instructors. Here, we describe a custom chatbot that plays the role of patient and coach to provide a scaling learning experience. A critical challenge was how to mitigate the persistently cheerful and helpful tone which results from standard pretraining in the Patient Persona AI. We accomplished this by implementing a multi-agent architecture based upon a graphical model of the conversation. System prompts reflecting the patient\u2019s cognitive state are dynamically updated as the conversation progresses. Future extensions of the work are intended to focus on additional custom model fine-tuning in the Mosaic AI platform to further improve the realism of the conversation. /Head of Data Science\nProvidence Health /Senior Data Scientist\nTegria Consulting/Providence Healthcare"}
{"session_id": "ai-agents-marketing-leveraging-mosaic-ai-create-multi-purpose-agentic", "title": "AI Agents for Marketing: Leveraging Mosaic AI to Create a Multi-Purpose Agentic Marketing Assistant", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Head of AI Center Excellence, 7-Eleven Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing professionals build campaigns, create content and use effective copywriting to tell a good story to promote a product/offer. All of this requires a thorough and meticulous process for every individual campaign. In order to assist marketing professionals at 7-Eleven, we built a multi-purpose assistant that could: We will walk you through how we created multiple agents as different personas with LangGraph and Mosaic AI to create a chat assistant that assumes a different persona based on the user query. We will also explain our evaluation methodology in choosing models and prompts and how we implemented guardrails for high reliability with sensitive marketing content. This assistant by 7-Eleven was showcased at the Databricks booth at NRF earlier this year. /Head of AI Center of Excellence"}
{"session_id": "ai-driven-drug-discovery-accelerating-molecular-insights-nvidia-and", "title": "AI-Driven Drug Discovery: Accelerating Molecular Insights With NVIDIA and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the race to revolutionize healthcare and drug discovery, biopharma companies are turning to AI to streamline workflows and unlock new scientific insights. This session, we will explore how NVIDIA BioNeMo, combined with Databricks Delta Lakehouse, can be used for advancing drug discovery for critical applications like molecular structure modeling, protein folding and diagnostics. We\u2019ll demonstrate how BioNeMo pre-trained models can run inference on data securely stored in Delta Lake, delivering actionable insights. By leveraging containerized solutions on Databricks\u2019 ML Runtime with GPU acceleration, users can achieve significant performance gains compared to traditional CPU-based computation. /Solutions Architect - NVIDIA"}
{"session_id": "amplifying-human-human-connection-face-mental-health-crisis-using", "title": "Amplifying Human-to-Human Connection in the Face of Mental Health Crisis Using Agentic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "LLAMA", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Data Scientist, Crisis Text Line"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Crisis Text Line has been innovating for ten years in text-based mental health crisis intervention and is now leading the next wave of GenAI use cases in the space. With over 300 million messages exchanged since 2013 and a decade of expertise, Crisis Text Line is unlocking the potential of AI to amplify human connection at a global scale.We will discuss how we leveraged our bedrock application to co-navigate crisis care through a set of early AI agent workflows. First, a simulator that reproduces texter behavior to train responders in taking conversations ranging in difficulty where the texter is in imminent risk of suicide or self-harm. Second, a tool that automatically monitors clinical quality of conversations. Third, predicted summarization to capture key context before conversations are transferred. Through the power of suggestion, this compound system aims to reduce burden and drive efficiency, such that our responders can focus on what they do best \u2014 support people in need. /Principal Product Manager\nCrisis Text Line /Lead Data Scientist"}
{"session_id": "anomaly-detection-apple-large-scale-data-using-apache-spark-and-flink", "title": "Anomaly Detection at Apple for Large-Scale Data Using Apache Spark and Flink", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Software Engineer, Apple Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Anomaly detection in time series data is crucial for identifying unusual patterns and trends, enabling better alerting and action when data deviates from normal. Most anomaly detection algorithms perform adequately on a single node machine with public datasets, but do not scale well with distributed processing frameworks used in modern big data environments. This talk will focus on how we scaled anomaly detection for large-scale datasets using Apache Spark and Flink for both batch and near real-time use cases. We will also discuss how we leveraged Apache Spark to parallelize and scale common anomaly detection algorithms, enabling support for large-scale data processing. We'll highlight some of the challenges faced and how we resolved them to make it useful for massive datasets with varying degree of anomalies. Finally, we will demonstrate how our anomaly detection framework works in batch for petabytes of data and in streaming mode for hundreds of thousands of transactions per second. /Senior Machine Learning Engineer\nApple Inc /Principal Software Engineer"}
{"session_id": "att-autoclassify-unified-multi-head-binary-classification-unlabeled", "title": "AT&T AutoClassify: Unified Multi-Head Binary Classification From Unlabeled Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, AT&T"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present AT&T AutoClassify, built jointly between AT&T's Chief Data Office (CDO) and Databricks professional services, a novel end-to-end system for automatic multi-head binary classifications from unlabeled text data. Our approach automates the challenge of creating labeled datasets and training multi-head binary classifiers with minimal human intervention. Starting only from a corpus of unlabeled text and a list of desired labels, AT&T AutoClassify leverages advanced natural language processing techniques to automatically mine relevant examples from raw text, fine-tune embedding models and train individual classifier heads for multiple true/false labels. This solution can reduce LLM classification costs by 1,000x, making it an efficient solution in operational costs. The end result is a highly optimized and low-cost model servable in Databricks capable of taking raw text and producing multiple binary classifications. An example use case using call transcripts will be examined. /Staff Data Scientist\nDatabricks /Senior Data Scientist"}
{"session_id": "automating-taxonomy-generation-compound-ai-databricks", "title": "Automating Taxonomy Generation With Compound AI on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Consultant, Data & AI, Lovelytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Taxonomy generation is a challenge across industries such as retail, manufacturing and e-commerce. Incomplete or inconsistent taxonomies can lead to fragmented data insights, missed monetization opportunities and stalled revenue growth. In this session, we will explore a modern approach to solving this problem by leveraging Databricks platform to build a scalable compound AI architecture for automated taxonomy generation. The first half of the session will walk you through the business significance and implications of taxonomy, followed by a technical deep dive in building an architecture for taxonomy implementation on the Databricks platform using a compound AI architecture. We will walk attendees through the anatomy of taxonomy generation, showcasing an innovative solution that combines multimodal and text-based LLMs, internal data sources and external API calls. This ensemble approach ensures more accurate, comprehensive and adaptable taxonomies that align with business needs. /Managing Director, GenAI\nLovelytics /Lead Consultant, Data & AI"}
{"session_id": "autonomous-ai-agents-ai-infrastructure", "title": "Autonomous AI Agents in AI Infrastructure", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Real-time", "Scala"], "speakers": ["Principal Software Engineer, Walmart Global Tech"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Autonomous AI agents are transforming industries by enabling systems to perform tasks, make decisions and adapt in real time without human intervention. In this talk, I will delve into the architecture and design principles required to build these agents within scalable AI infrastructure. Key topics will include constructing modular, reusable frameworks, optimizing resource allocation and enabling interoperability between agents and data pipelines. I will discuss practical use cases in which attendees will learn how to leverage containerization and orchestration techniques to enhance the flexibility and performance of these agents while ensuring low-latency decision-making. This session will also highlight challenges like ensuring robustness, ethical considerations and strategies for real-time feedback loops. Participants will gain actionable insights into building autonomous AI agents that drive efficiency, scalability and innovation in modern AI ecosystems. /Principal Software Engineer"}
{"session_id": "best-practices-building-user-facing-ai-systems-databricks", "title": "Best Practices for Building User-Facing AI Systems on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI agents into business systems requires tailored approaches for different maturity levels (crawl-walk-run) that balance scalability, accuracy and usability. This session addresses the critical challenge of making AI agents accessible to business users. We will explore four key integration methods: We'll compare these approaches, discussing their strengths, challenges and ideal use cases to help businesses select the most suitable integration strategy for their specific needs. /Senior Specialist Solutions Architect\nDatabricks /Senior Solutions Architect"}
{"session_id": "beyond-ai-accuracy-building-trustworthy-and-responsible-ai-application", "title": "Beyond AI Accuracy: Building Trustworthy and Responsible AI Application Through Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Generic LLM metrics are useless until it meets your business needs.In this session we will dive deep into creating bespoke custom state-of-the-art AI metrics that matters to you. Discuss best practices on LLM evaluation strategies, when to use LLM judge vs. statistical metrics and many more. Through a live demo using Mosaic AI Framework, we will showcase: By the end of this session, you'll be equipped to create AI solutions that are not only powerful but also relevant to your organizations needs. Join us to transform your AI strategy and make a tangible impact on your business! /Specialist Solution Architect"}
{"session_id": "beyond-privacy-utility-tradeoff-differential-privacy-tabular-data", "title": "Beyond the Privacy-Utility Tradeoff: Differential Privacy in Tabular Data Synthesis", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Scientist, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly leverage sensitive data for AI applications, generating high quality synthetic data with mathematical guarantees of privacy has become crucial. This talk explores the use of Gretel Navigator to generate differentially private synthetic data that maintains high fidelity to the source data and high utility on downstream tasks across heterogeneous datasets. Our analysis covers a framework for privacy-preserving synthetic data generation with two use cases: patient events and e-commerce reviews. We reveal nuanced strategies for: calibrating privacy parameters \u03b5 and \u03b4 for mixed-modal data, leveraging both record-level and user-level differential privacy depending on which entity in the dataset requires protection, maintaining statistical properties and high utility on downstream classification tasks under stringent privacy constraints (e.g., <0.05 difference in AUC when using DP), and quantifying resilience to membership inference and attribute inference attacks. /Research Scientist"}
{"session_id": "boosting-data-science-and-ai-productivity-databricks-notebooks", "title": "Boosting Data Science and AI Productivity With Databricks Notebooks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to accelerate your team's data science workflow? This session reveals how Databricks Notebooks can transform your productivity through an optimized environment designed specifically for data science and AI work. Discover how notebooks serve as a central collaboration hub where code, visualizations, documentation and results coexist seamlessly, enabling faster iteration and development. Key takeaways: You'll leave with practical techniques to enhance your notebook-based workflow and deliver AI projects faster with higher-quality results. /Databricks"}
{"session_id": "bridging-big-data-and-ai-empowering-pyspark-lance-format-multi-modal-ai", "title": "Bridging Big Data and AI: Empowering PySpark With Lance Format for Multi-Modal AI Data Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Machine Learning", "Python", "SQL"], "speakers": ["Database Engineer, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark has long been a cornerstone of big data processing, excelling in data preparation, analytics and machine learning tasks within traditional data lakes. However, the rise of multimodal AI and vector search introduces challenges beyond its capabilities. Spark\u2019s new Python data source API enables integration with emerging AI data lakes built on the multi-modal Lance format. Lance delivers unparalleled value with its zero-copy schema evolution capability and robust support for large record-size data (e.g., images, tensors, embeddings, etc), simplifying multimodal data storage. Its advanced indexing for semantic and full-text search, combined with rapid random access, enables high-performance AI data analytics to the level of SQL. By unifying PySpark's robust processing capabilities with Lance's AI-optimized storage, data engineers and scientists can efficiently manage and analyze the diverse data types required for cutting-edge AI applications within a familiar big data framework. /Staff Software Engineer\nDatabricks /Database Engineer"}
{"session_id": "bringing-ai-your-data-using-sql-functions-databricks", "title": "Bringing AI to Your Data: Using SQL Functions in Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks AI Functions make it easy for analysts and data engineers to integrate advanced AI capabilities into data workflows \u2014 no ML expertise is required. These built-in SQL functions let you apply tasks like sentiment analysis, text summarization and language translation directly to data in Databricks, whether you're working in SQL queries, notebooks, DLT or Jobs. This session will walk through practical applications of both general-purpose and task-specific AI Functions, showing how to analyze text data at scale using simple SQL. You'll learn how to convert unstructured content into structured insights, and how to embed AI directly into batch pipelines and analytics processes. What you\u2019ll learn: /Databricks"}
{"session_id": "building-ai-models-health-care-using-semi-synthetic-data", "title": "Building AI Models In Health Care Using Semi-Synthetic Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Co-founder, Fight Health Insurance INC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Regulated or restricted fields like Health Care make collecting training data complicated. We all want to do the right thing, but how? This talk will look at how Fight Health Insurance used de-identified public and proprietary information to create a semi-synthetic training set for use in fine-tuning machine learning models to power Fight Paperwork. We'll explore how to incorporate the latest \"reasoning\" techniques in fine tuning as well as how to make models that you can afford to serve \u2014 think single GPU inference instead of a cluster of A100s. In addition to the talk we have the code used in a public GitHub repo \u2014 although it is a little rough, so you might want to use it more as a source of inspiration rather than directly forking it. /Co-founder"}
{"session_id": "building-and-scaling-production-ai-systems-mosaic-ai", "title": "Building and Scaling Production AI Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP of Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ready to go beyond the basics of Mosaic AI? This session will walk you through how to architect and scale production-grade AI systems on the Databricks Data Intelligence Platform. We\u2019ll cover practical techniques for building end-to-end AI pipelines \u2014 from processing structured and unstructured data to applying Mosaic AI tools and functions for model development, deployment and monitoring. You\u2019ll learn how to integrate experiment tracking with MLflow, apply performance tuning and use built-in frameworks to manage the full AI lifecycle. By the end, you\u2019ll be equipped to design, deploy and maintain AI systems that deliver measurable outcomes at enterprise scale. /CTO, Neural Networks\nDatabricks /VP of Engineering"}
{"session_id": "building-intelligent-ai-agents-claude-models-and-databricks-mosaic-ai", "title": "Building Intelligent AI Agents With Claude Models and Databricks Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Technical Staff, Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how Anthropic's frontier models power AI agents in Databricks Mosaic AI Agent Framework. Learn to leverage Claude's state-of-the-art capabilities for complex agentic workflows while benefiting from Databricks unified governance, credential management and evaluation tools. We'll demonstrate how Anthropic's models integrate seamlessly to create production-ready applications that combine Claude's reasoning with Databricks data intelligence capabilities. /Technical Staff"}
{"session_id": "building-knowledge-agents-automate-document-workflows", "title": "Building Knowledge Agents to Automate Document Workflows", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-founder and CEO, LlamaIndex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "One of the biggest promises for LLM agents is automating all knowledge work over unstructured data \u2014 we call these \"knowledge agents\". To date, while there are fragmented tools around data connectors, storage and agent orchestration, AI engineers have trouble building and shipping production-grade agents beyond basic chatbots. In this session, we first outline the highest-value knowledge agent use cases we see being built and deployed at various enterprises. These are: We then define the core architectural components around knowledge management and agent orchestration required to build these use cases. By the end you'll not only have an understanding of the core technical concepts, but also an appreciation of the ROI you can generate for end-users by shipping these use cases to production. /Co-founder and CEO"}
{"session_id": "building-tool-calling-agents-databricks-agent-framework-and-mcp", "title": "Building Tool-Calling Agents With Databricks Agent Framework and MCP", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to create AI agents that can do more than just generate text? Join us to explore how combining Databricks' Mosaic AI Agent Framework with the Model Context Protocol (MCP) unlocks powerful tool-calling capabilities. We'll show you how MCP provides a standardized way for AI agents to interact with external tools, data and APIs, solving the headache of fragmented integration approaches. Learn to build agents that can retrieve both structured and unstructured data, execute custom code and tackle real enterprise challenges. Key takeaways: Whether you're building customer service bots or data analysis assistants, you'll leave with practical know-how to create powerful, governed AI agents. /Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "no-code-ml-forecasting-platform-retail-and-cpg-companies", "title": "A No-Code ML Forecasting Platform for Retail and CPG companies", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Director, Antuit - A Zebra Technologies company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Director"}
