{"session_id": "accelerate-end-end-multi-agents-databricks-and-dspy", "title": "Accelerate End-to-End Multi-Agents on Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A production-ready GenAI application is more than the framework itself. Like ML, you need a unified platform to create an end-to-end workflow for production quality applications. Below is an example of how this works on Databricks: In this session, learn how to build agents to access all your data and models through function calling. Then, learn how DSPy enables agent interaction with each other to ensure the question is answered correctly. We will demonstrate a chatbot, powered by multiple agents, to be able to answer questions and reason answers the base LLM does not know and very specialized topics. /Delivery Solutions Architect"}
{"session_id": "accelerating-model-development-and-fine-tuning-databricks-twelvelabs", "title": "Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Scaling large language models (LLMs) and multimodal architectures requires efficient data management and computational power. NVIDIA NeMo Framework Megatron-LM on Databricks is an open source solution that integrates GPU acceleration and advanced parallelism with Databricks Delta Lakehouse, streamlining workflows for pre-training and fine-tuning models at scale. This session highlights context parallelism, a unique NeMo capability for parallelizing over sequence lengths, making it ideal for video datasets with large embeddings. Through the case study of TwelveLabs\u2019 Pegasus-1 model, learn how NeMo empowers scalable multimodal AI development, from text to video processing, setting a new standard for LLM workflows. /Chief Technology Officer & Co-Founder\nTwelve Labs, Inc /Solutions Architect - NVIDIA"}
{"session_id": "achieve-your-mission-ai-driven-decisions", "title": "Achieve Your Mission With AI-Driven Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Government leaders overwhelmingly recognize the potential benefits of AI as critical to long-term strategic goals of efficiency, but implementation challenges and security concerns could be obstacles to success. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "achieving-precision-ai-retrieving-right-data-using-ai-agents", "title": "Achieving Precision in AI: Retrieving the Right Data Using AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director"}
{"session_id": "advanced-rag-overview-thawing-your-frozen-rag-pipeline", "title": "Advanced RAG Overview \u2014 Thawing Your Frozen RAG Pipeline", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ML Innovation, Experian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The most common RAG systems rely on a frozen RAG system \u2014 one where there\u2019s a single embedding model and single vector index. We\u2019ve achieved a modicum of success with that, but when it comes to increasing accuracy for production systems there is only so much this approach solves. In this session we will explore how to move from the frozen systems to adaptive RAG systems which produce more tailored outputs with higher accuracy. Databricks services: Lakehouse, Unity Catalog, Mosaic, Sweeps, Vector Search, Agent Evaluation, Managed Evaluation, Inference Tables /Head of AI/ML Innovation"}
{"session_id": "agentic-architectures-create-realistic-conversations-using-genai-teach", "title": "Agentic Architectures to Create Realistic Conversations: Using GenAI to Teach Empathy in Healthcare", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Providence Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Medical providers often receive less than 15 minutes of instruction in how to interact with patients during emotionally charged end of life interactions. Continuing education for clinicians is critical to hone these skills but is difficult to scale traditional approaches that require professional patients and instructors. Here, we describe a custom chatbot that plays the role of patient and coach to provide a scaling learning experience. A critical challenge was how to mitigate the persistently cheerful and helpful tone which results from standard pretraining in the Patient Persona AI. We accomplished this by implementing a multi-agent architecture based upon a graphical model of the conversation. System prompts reflecting the patient\u2019s cognitive state are dynamically updated as the conversation progresses. Future extensions of the work are intended to focus on additional custom model fine-tuning in the Mosaic AI platform to further improve the realism of the conversation. /Senior Data Scientist\nTegria Consulting/Providence Healthcare"}
{"session_id": "ai-agents-marketing-leveraging-mosaic-ai-create-multi-purpose-agentic", "title": "AI Agents for Marketing: Leveraging Mosaic AI to Create a Multi-Purpose Agentic Marketing Assistant", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Head of AI Center Excellence, 7-Eleven Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing professionals build campaigns, create content and use effective copywriting to tell a good story to promote a product/offer. All of this requires a thorough and meticulous process for every individual campaign. In order to assist marketing professionals at 7-Eleven, we built a multi-purpose assistant that could: We will walk you through how we created multiple agents as different personas with LangGraph and Mosaic AI to create a chat assistant that assumes a different persona based on the user query. We will also explain our evaluation methodology in choosing models and prompts and how we implemented guardrails for high reliability with sensitive marketing content. This assistant by 7-Eleven was showcased at the Databricks booth at NRF earlier this year. /Head of AI Center of Excellence"}
{"session_id": "ai-driven-drug-discovery-accelerating-molecular-insights-nvidia-and", "title": "AI-Driven Drug Discovery: Accelerating Molecular Insights With NVIDIA and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the race to revolutionize healthcare and drug discovery, biopharma companies are turning to AI to streamline workflows and unlock new scientific insights. This session, we will explore how NVIDIA BioNeMo, combined with Databricks Delta Lakehouse, can be used for advancing drug discovery for critical applications like molecular structure modeling, protein folding and diagnostics. We\u2019ll demonstrate how BioNeMo pre-trained models can run inference on data securely stored in Delta Lake, delivering actionable insights. By leveraging containerized solutions on Databricks\u2019 ML Runtime with GPU acceleration, users can achieve significant performance gains compared to traditional CPU-based computation. /Solutions Architect - NVIDIA"}
{"session_id": "ai-evaluation-first-principles-you-cant-manage-what-you-cant-measure", "title": "AI Evaluation from First Principles: You Can't Manage What You Can't Measure", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Research Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Chief Scientist - Neural Networks\nDatabricks /Research Scientist"}
{"session_id": "ai-regulation-dilemma-spur-innovation-or-guardrails-where-are-we-and", "title": "The AI Regulation Dilemma: Spur Innovation, or Guardrails? \u2014 Where Are We and the Impact of Trump 2", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Chief Public Affairs Officer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Trump 2 AI agenda prioritizes US leadership by opposing regulation on bias and frontier AI risks, favoring innovation and AI expansion. With comprehensive federal AI regulation unlikely, states are advancing AI laws on bias, harmful content and transparency (e.g., Colorado). Meanwhile, the EU AI Act imposes global obligations. The emerging patchwork of state rules will burden US companies more than a unified federal approach, undermining Trump\u2019s deregulatory goals. Ironically, the Trump agenda may accelerate state-level regulation and impede innovation. A light federal AI law preempting state rules is politically unlikely, leaving US companies with a fragmented landscape similar to privacy regulation where the EU AI Act \u2014 in the role of GDPR \u2014 has set the stage, and the states are asserting themselves with various incremental requirements. Important developments to then cover: EU GPAI Code of Practice (effective 8/2/25), newly enacted state laws, Korean AI law, Japan \u2014 if final. /Chief Public Affairs Officer"}
{"session_id": "amplifying-human-human-connection-face-mental-health-crisis-using", "title": "Amplifying Human-to-Human Connection in the Face of Mental Health Crisis Using Agentic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "LLAMA", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Data Scientist, Crisis Text Line"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Crisis Text Line has been innovating for ten years in text-based mental health crisis intervention and is now leading the next wave of GenAI use cases in the space. With over 300 million messages exchanged since 2013 and a decade of expertise, Crisis Text Line is unlocking the potential of AI to amplify human connection at a global scale.We will discuss how we leveraged our bedrock application to co-navigate crisis care through a set of early AI agent workflows. First, a simulator that reproduces texter behavior to train responders in taking conversations ranging in difficulty where the texter is in imminent risk of suicide or self-harm. Second, a tool that automatically monitors clinical quality of conversations. Third, predicted summarization to capture key context before conversations are transferred. Through the power of suggestion, this compound system aims to reduce burden and drive efficiency, such that our responders can focus on what they do best \u2014 support people in need. /Principal Product Manager\nCrisis Text Line /Lead Data Scientist"}
{"session_id": "anomaly-detection-apple-large-scale-data-using-apache-spark-and-flink", "title": "Anomaly Detection at Apple for Large-Scale Data Using Apache Spark and Flink", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Software Engineer, Apple Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Anomaly detection in time series data is crucial for identifying unusual patterns and trends, enabling better alerting and action when data deviates from normal. Most anomaly detection algorithms perform adequately on a single node machine with public datasets, but do not scale well with distributed processing frameworks used in modern big data environments. This talk will focus on how we scaled anomaly detection for large-scale datasets using Apache Spark and Flink for both batch and near real-time use cases. We will also discuss how we leveraged Apache Spark to parallelize and scale common anomaly detection algorithms, enabling support for large-scale data processing. We'll highlight some of the challenges faced and how we resolved them to make it useful for massive datasets with varying degree of anomalies. Finally, we will demonstrate how our anomaly detection framework works in batch for petabytes of data and in streaming mode for hundreds of thousands of transactions per second. /Senior Machine Learning Engineer\nApple Inc /Principal Software Engineer"}
{"session_id": "att-autoclassify-unified-multi-head-binary-classification-unlabeled", "title": "AT&T AutoClassify: Unified Multi-Head Binary Classification From Unlabeled Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, AT&T"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present AT&T AutoClassify, built jointly between AT&T's Chief Data Office (CDO) and Databricks professional services, a novel end-to-end system for automatic multi-head binary classifications from unlabeled text data. Our approach automates the challenge of creating labeled datasets and training multi-head binary classifiers with minimal human intervention. Starting only from a corpus of unlabeled text and a list of desired labels, AT&T AutoClassify leverages advanced natural language processing techniques to automatically mine relevant examples from raw text, fine-tune embedding models and train individual classifier heads for multiple true/false labels. This solution can reduce LLM classification costs by 1,000x, making it an efficient solution in operational costs. The end result is a highly optimized and low-cost model servable in Databricks capable of taking raw text and producing multiple binary classifications. An example use case using call transcripts will be examined. /Staff Data Scientist\nDatabricks /Senior Data Scientist"}
{"session_id": "automating-taxonomy-generation-compound-ai-databricks", "title": "Automating Taxonomy Generation With Compound AI on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Consultant, Data & AI, Lovelytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Taxonomy generation is a challenge across industries such as retail, manufacturing and e-commerce. Incomplete or inconsistent taxonomies can lead to fragmented data insights, missed monetization opportunities and stalled revenue growth. In this session, we will explore a modern approach to solving this problem by leveraging Databricks platform to build a scalable compound AI architecture for automated taxonomy generation. The first half of the session will walk you through the business significance and implications of taxonomy, followed by a technical deep dive in building an architecture for taxonomy implementation on the Databricks platform using a compound AI architecture. We will walk attendees through the anatomy of taxonomy generation, showcasing an innovative solution that combines multimodal and text-based LLMs, internal data sources and external API calls. This ensemble approach ensures more accurate, comprehensive and adaptable taxonomies that align with business needs. /Managing Director, GenAI\nLovelytics /Lead Consultant, Data & AI"}
{"session_id": "autonomous-ai-agents-ai-infrastructure", "title": "Autonomous AI Agents in AI Infrastructure", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Real-time", "Scala"], "speakers": ["Principal Software Engineer, Walmart Global Tech"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Autonomous AI agents are transforming industries by enabling systems to perform tasks, make decisions and adapt in real time without human intervention. In this talk, I will delve into the architecture and design principles required to build these agents within scalable AI infrastructure. Key topics will include constructing modular, reusable frameworks, optimizing resource allocation and enabling interoperability between agents and data pipelines. I will discuss practical use cases in which attendees will learn how to leverage containerization and orchestration techniques to enhance the flexibility and performance of these agents while ensuring low-latency decision-making. This session will also highlight challenges like ensuring robustness, ethical considerations and strategies for real-time feedback loops. Participants will gain actionable insights into building autonomous AI agents that drive efficiency, scalability and innovation in modern AI ecosystems. /Principal Software Engineer"}
{"session_id": "best-practices-building-user-facing-ai-systems-databricks", "title": "Best Practices for Building User-Facing AI Systems on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI agents into business systems requires tailored approaches for different maturity levels (crawl-walk-run) that balance scalability, accuracy and usability. This session addresses the critical challenge of making AI agents accessible to business users. We will explore four key integration methods: We'll compare these approaches, discussing their strengths, challenges and ideal use cases to help businesses select the most suitable integration strategy for their specific needs. /Senior Specialist Solutions Architect\nDatabricks /Senior Solutions Architect"}
{"session_id": "beyond-ai-accuracy-building-trustworthy-and-responsible-ai-application", "title": "Beyond AI Accuracy: Building Trustworthy and Responsible AI Application Through Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Generic LLM metrics are useless until it meets your business needs.In this session we will dive deep into creating bespoke custom state-of-the-art AI metrics that matters to you. Discuss best practices on LLM evaluation strategies, when to use LLM judge vs. statistical metrics and many more. Through a live demo using Mosaic AI Framework, we will showcase: By the end of this session, you'll be equipped to create AI solutions that are not only powerful but also relevant to your organizations needs. Join us to transform your AI strategy and make a tangible impact on your business! /Specialist Solution Architect"}
{"session_id": "beyond-privacy-utility-tradeoff-differential-privacy-tabular-data", "title": "Beyond the Privacy-Utility Tradeoff: Differential Privacy in Tabular Data Synthesis", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Scientist, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly leverage sensitive data for AI applications, generating high quality synthetic data with mathematical guarantees of privacy has become crucial. This talk explores the use of Gretel Navigator to generate differentially private synthetic data that maintains high fidelity to the source data and high utility on downstream tasks across heterogeneous datasets. Our analysis covers a framework for privacy-preserving synthetic data generation with two use cases: patient events and e-commerce reviews. We reveal nuanced strategies for: calibrating privacy parameters \u03b5 and \u03b4 for mixed-modal data, leveraging both record-level and user-level differential privacy depending on which entity in the dataset requires protection, maintaining statistical properties and high utility on downstream classification tasks under stringent privacy constraints (e.g., <0.05 difference in AUC when using DP), and quantifying resilience to membership inference and attribute inference attacks. /Research Scientist"}
{"session_id": "boosting-data-science-and-ai-productivity-databricks-notebooks", "title": "Boosting Data Science and AI Productivity With Databricks Notebooks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to accelerate your team's data science workflow? This session reveals how Databricks Notebooks can transform your productivity through an optimized environment designed specifically for data science and AI work. Discover how notebooks serve as a central collaboration hub where code, visualizations, documentation and results coexist seamlessly, enabling faster iteration and development. Key takeaways: You'll leave with practical techniques to enhance your notebook-based workflow and deliver AI projects faster with higher-quality results. /Databricks"}
{"session_id": "bridging-big-data-and-ai-empowering-pyspark-lance-format-multi-modal-ai", "title": "Bridging Big Data and AI: Empowering PySpark With Lance Format for Multi-Modal AI Data Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Machine Learning", "Python", "SQL"], "speakers": ["Database Engineer, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark has long been a cornerstone of big data processing, excelling in data preparation, analytics and machine learning tasks within traditional data lakes. However, the rise of multimodal AI and vector search introduces challenges beyond its capabilities. Spark\u2019s new Python data source API enables integration with emerging AI data lakes built on the multi-modal Lance format. Lance delivers unparalleled value with its zero-copy schema evolution capability and robust support for large record-size data (e.g., images, tensors, embeddings, etc), simplifying multimodal data storage. Its advanced indexing for semantic and full-text search, combined with rapid random access, enables high-performance AI data analytics to the level of SQL. By unifying PySpark's robust processing capabilities with Lance's AI-optimized storage, data engineers and scientists can efficiently manage and analyze the diverse data types required for cutting-edge AI applications within a familiar big data framework. /Staff Software Engineer\nDatabricks /Database Engineer"}
{"session_id": "bringing-ai-your-data-using-sql-functions-databricks", "title": "Bringing AI to Your Data: Using SQL Functions in Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks AI Functions make it easy for analysts and data engineers to integrate advanced AI capabilities into data workflows \u2014 no ML expertise is required. These built-in SQL functions let you apply tasks like sentiment analysis, text summarization and language translation directly to data in Databricks, whether you're working in SQL queries, notebooks, DLT or Jobs. This session will walk through practical applications of both general-purpose and task-specific AI Functions, showing how to analyze text data at scale using simple SQL. You'll learn how to convert unstructured content into structured insights, and how to embed AI directly into batch pipelines and analytics processes. What you\u2019ll learn: /Databricks"}
{"session_id": "building-ai-models-health-care-using-semi-synthetic-data", "title": "Building AI Models In Health Care Using Semi-Synthetic Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Co-founder, Fight Health Insurance INC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Regulated or restricted fields like Health Care make collecting training data complicated. We all want to do the right thing, but how? This talk will look at how Fight Health Insurance used de-identified public and proprietary information to create a semi-synthetic training set for use in fine-tuning machine learning models to power Fight Paperwork. We'll explore how to incorporate the latest \"reasoning\" techniques in fine tuning as well as how to make models that you can afford to serve \u2014 think single GPU inference instead of a cluster of A100s. In addition to the talk we have the code used in a public GitHub repo \u2014 although it is a little rough, so you might want to use it more as a source of inspiration rather than directly forking it. /Co-founder"}
{"session_id": "building-ai-models-human-cell-tahoe-therapeutics-databricks", "title": "Building AI models of human cell: Tahoe Therapeutics on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Co-Founder, CEO, Tahoe Therapeutics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Tahoe Therapeutics (formerly Vevo) is generating gigascale single-cell data that map how drugs interact with cells from cancer patients. They are using that to find better therapeutics, and to build AI models that can predict drug-patient interactions on Databricks. Their technology enabled the landmark Tahoe-100M atlas, the world\u2019s largest dataset of drug responses-profiling 100 million cells across 60,000 conditions. Learn how we use Databricks to process this massive data, enabling AI models that predict drug efficacy and resistance at the cellular level. Recognized as the Grand Prize Winner of the Databricks Generative AI Startup Challenge, Tahoe sets a new standard for scalable, data-driven drug discovery. /Co-Founder, CEO"}
{"session_id": "building-and-scaling-production-ai-systems-mosaic-ai", "title": "Building and Scaling Production AI Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP of Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ready to go beyond the basics of Mosaic AI? This session will walk you through how to architect and scale production-grade AI systems on the Databricks Data Intelligence Platform. We\u2019ll cover practical techniques for building end-to-end AI pipelines \u2014 from processing structured and unstructured data to applying Mosaic AI tools and functions for model development, deployment and monitoring. You\u2019ll learn how to integrate experiment tracking with MLflow, apply performance tuning and use built-in frameworks to manage the full AI lifecycle. By the end, you\u2019ll be equipped to design, deploy and maintain AI systems that deliver measurable outcomes at enterprise scale. /CTO, Neural Networks\nDatabricks /VP of Engineering"}
{"session_id": "building-intelligent-ai-agents-claude-models-and-databricks-mosaic-ai", "title": "Building Intelligent AI Agents With Claude Models and Databricks Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Technical Staff, Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how Anthropic's frontier models power AI agents in Databricks Mosaic AI Agent Framework. Learn to leverage Claude's state-of-the-art capabilities for complex agentic workflows while benefiting from Databricks unified governance, credential management and evaluation tools. We'll demonstrate how Anthropic's models integrate seamlessly to create production-ready applications that combine Claude's reasoning with Databricks data intelligence capabilities. /Technical Staff"}
{"session_id": "building-knowledge-agents-automate-document-workflows", "title": "Building Knowledge Agents to Automate Document Workflows", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-founder and CEO, LlamaIndex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "One of the biggest promises for LLM agents is automating all knowledge work over unstructured data \u2014 we call these \"knowledge agents\". To date, while there are fragmented tools around data connectors, storage and agent orchestration, AI engineers have trouble building and shipping production-grade agents beyond basic chatbots. In this session, we first outline the highest-value knowledge agent use cases we see being built and deployed at various enterprises. These are: We then define the core architectural components around knowledge management and agent orchestration required to build these use cases. By the end you'll not only have an understanding of the core technical concepts, but also an appreciation of the ROI you can generate for end-users by shipping these use cases to production. /Co-founder and CEO"}
{"session_id": "building-tool-calling-agents-databricks-agent-framework-and-mcp", "title": "Building Tool-Calling Agents With Databricks Agent Framework and MCP", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to create AI agents that can do more than just generate text? Join us to explore how combining Databricks' Mosaic AI Agent Framework with the Model Context Protocol (MCP) unlocks powerful tool-calling capabilities. We'll show you how MCP provides a standardized way for AI agents to interact with external tools, data and APIs, solving the headache of fragmented integration approaches. Learn to build agents that can retrieve both structured and unstructured data, execute custom code and tackle real enterprise challenges. Key takeaways: Whether you're building customer service bots or data analysis assistants, you'll leave with practical know-how to create powerful, governed AI agents. /Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "code-completion-autonomous-software-engineering-agents", "title": "From Code Completion to Autonomous Software Engineering Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Software Engineer, Princeton University"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As language models have advanced, they have moved beyond code completion and are beginning to tackle software engineering tasks in a more autonomous, agentic way. However, evaluating agentic capabilities is challenging. To address this, we first introduce SWE-bench, a benchmark built from real GitHub issues that has become the standard for assessing AI\u2019s ability to resolve complex software tasks in large codebases. We will discuss the current state of the field, the limitations of today\u2019s models, and how far we still are from truly autonomous AI developers. Next, we will explore the fundamentals of agents based on hands-on demonstrations with SWE-agent, a simple yet powerful agent framework designed for software engineering but adaptable to a variety of domains. By the end of this session, you will have a clear understanding of the current frontier of agentic AI in software engineering, the challenges ahead and how you can experiment with AI agents in your own workflows. /Research Software Engineer"}
{"session_id": "composing-high-accuracy-ai-systems-slms-and-mini-agents", "title": "Composing High-Accuracy AI Systems With SLMs and Mini-Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["CEO & Cofounder, Lamini"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "For most companies, building compound AI systems remains aspirational. LLMs are powerful, but imperfect, and their non-deterministic nature makes steering them to high accuracy a challenge. In this session, we\u2019ll demonstrate how to build compound AI systems using SLMs and highly accurate mini-agents that can be integrated into agentic workflows. You'll learn about breakthrough techniques, including: memory RAG, an embedding algorithm that reduces hallucinations using embed-time compute to generate contextual embeddings, improving indexing and retrieval, and memory tuning, a finetuning algorithm that reduces hallucinations using a Mixture of Memory Experts (MoME) to specialize models with proprietary data. We\u2019ll also share real-world examples (text-to-SQL, factual reasoning, function calling, code analysis and more) across various industries. With these building blocks, we\u2019ll demonstrate how to create high accuracy mini-agents that can be composed into larger AI systems. /CEO & Cofounder"}
{"session_id": "comprehensive-guide-mlops-databricks", "title": "Comprehensive Guide to MLOps on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Staff Data Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This in-depth session explores advanced MLOps practices for implementing production-grade machine learning workflows on Databricks. We'll examine the complete MLOps journey from foundational principles to sophisticated implementation patterns, covering essential tools including MLflow, Unity Catalog, Feature Stores and version control with Git. Dive into Databricks' latest MLOps capabilities including MLflow 3.0, which enhances the entire ML lifecycle from development to deployment with particular focus on generative AI applications. Key session takeaways include: /Software Engineer\nDatabricks /Staff Data Scientist"}
{"session_id": "cracking-complex-documents-databricks-mosaic-ai", "title": "Cracking Complex Documents with Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Chief AI Officer, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share how we are transforming the way organizations process unstructured and non-standard documents using Mosaic AI and agentic patterns within the Databricks ecosystem. We have developed a scalable pipeline that turns complex legal and regulatory content into structured, tabular data.We will walk through the full architecture, which includes Unity Catalog for secure and governed data access, Databricks Vector Search for intelligent indexing and retrieval and Databricks Apps to deliver clear insights to business users. The solution supports multiple languages and formats, making it suitable for teams working across different regions. We will also discuss some of the key technical challenges we addressed, including handling parsing inconsistencies, grounding model responses and ensuring traceability across the entire process. If you are exploring how to apply GenAI and large language models, this session is for you. /Chief AI Officer"}
{"session_id": "creating-llm-judges-measure-domain-specific-agent-quality", "title": "Creating LLM judges to Measure Domain-Specific Agent Quality", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores comprehensive methodologies for assessing agent performance across specialized knowledge domains, tailored workflows and task-specific objectives. We'll demonstrate practical approaches to designing robust evaluation metrics that align with your business goals and provide meaningful insights into agent capabilities and limitations. Key session takeaways include: Join us to learn how proper evaluation methodologies can transform your domain-specific agents from experimental tools to trusted enterprise solutions with measurable business value. /Databricks /Software Engineer"}
{"session_id": "data-intelligence-marketing-breakout-agentic-systems-bayesian-mmm-and", "title": "Data Intelligence for Marketing Breakout: Agentic Systems for Bayesian MMM and Consumer Testing", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Partner, PyMC Labs"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk dives into leveraging GenAI to scale sophisticated decision intelligence. Learn how an AI copilot interface simplifies running complex Bayesian probabilistic models, accelerating insight generation, and accurate decision making at the enterprise level. We talk through techniques for deploying AI agents at scale to simulate market dynamics or product feature impacts, providing robust, data-driven foresight for high-stakes innovation and strategy directly within your Databricks environment. For marketing teams, this approach will help you leverage autonomous AI agents to dynamically manage media channel allocation while simulating real-world consumer behavior through synthetic testing environments. /Marketing Solutions GTM\nDatabricks /Partner"}
{"session_id": "databricks-databricks-transforming-sales-experience-using-genai-agents", "title": "Databricks on Databricks: Transforming the Sales Experience using GenAI Agents at Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Vice President, IT & Deputy CIO, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks is transforming its sales experience with a GenAI agent \u2014 built and deployed entirely on Databricks \u2014 to automate tasks, streamline data retrieval, summarize content, and enable conversational AI for over 4,000 sellers. This agent leverages the AgentEval framework, AI Bricks, and Model Serving to process both structured and unstructured data within Databricks, unlocking deep sales insights. The agent seamlessly integrates across multiple data sources including Salesforce, Google Drive, and Glean securely via OAuth. This session includes a live demonstration and explores the business impact, architecture as well as agent development and evaluation strategies, providing a blueprint for deploying secure, scalable GenAI agents in large enterprises. /Enterprise Architect\nDatabricks /Senior Vice President, IT & Deputy CIO"}
{"session_id": "days-minutes-ai-transforms-audit-kpmg", "title": "From Days to Minutes - AI Transforms Audit at KPMG", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Staff Data Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Imagine performing complex regulatory checks in minutes instead of days. We made this a reality using GenAI on the Databricks Data Intelligence Platform. Join us for a deep dive into our journey from POC to a production-ready AI audit tool. Discover how we automated thousands of legal requirement checks in annual reports with remarkable speed and accuracy. Learn our blueprint for:High-Performance AI: Building a scalable, >90% accurate AI system with an optimized RAG pipeline that auditors praise.Robust Productionization: Achieving secure, governed deployment using Unity Catalog, MLflow, LLM-based evaluation, and MLOps best practices.This session provides actionable insights for deploying impactful, compliant GenAI in the enterprise. We\u2019re now able to perform thousands of legal and regulatory compliance checks on high complexity documents in minutes\u2014a task that used to take auditors days and weeks. We\u2019ll wrap up with a view on the broader impact AI is having on the audit, accounting and finance sector\u2014and how the solution we\u2019ve built is providing the foundations to accelerate future use cases. /Head of Audit Analytics & AI\nKPMG /Staff Data Scientist"}
{"session_id": "defending-revenue-genai", "title": "Defending Revenue With GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Managing Director of Product, Blueprint"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Managing Director of Product"}
{"session_id": "deploy-and-scale-ai-models-mosaic-model-serving", "title": "Deploy and Scale AI Models With Mosaic Model Serving", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ever struggled with getting your AI models into production? Join us to discover how Databricks' Mosaic AI Model Serving takes the headache out of deploying both traditional ML and generative AI models at scale. This session demonstrates how to implement a highly available, auto-scaling service that optimizes performance while controlling costs. Discover deployment strategies for various model types \u2014 from custom models to foundation models like Llama 4 and external models from OpenAI and Anthropic \u2014 with proper governance through Mosaic AI Gateway. Key takeaways: /Databricks"}
{"session_id": "dspy-30-and-dspy-databricks", "title": "DSPy 3.0 \u2014 and DSPy at Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The DSPy OSS team at Databricks and beyond is excited to present DSPy 3.0, targeted for release close to DAIS 2025. We will present what DSPy is and how it evolved over the past year. We will discuss greatly improved prompt optimization and finetuning/RL capabilities, improved productionization and observability via thorough and native integration with MLflow, and lessons from usage of DSPy in various Databricks R&D and professional services contexts. /Research Scientist\nDatabricks /Research Engineer"}
{"session_id": "empowering-fundraising-ai-journey-databricks-mosaic-ai", "title": "Empowering Fundraising With AI: A Journey With Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PUBLIC SECTOR", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Director of Data Science, Doctors Without Borders"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Artificial Intelligence (AI) is more than a corporate tool; it\u2019s a force for good. At Doctors Without Borders/M\u00e9decins Sans Fronti\u00e8res (MSF), we use AI to optimize fundraising, ensuring that every dollar raised directly supports life-saving medical aid worldwide. With Databricks, Mosaic AI and Unity Catalog, we analyze donor behavior, predict giving patterns and personalize outreach, increasing contributions while upholding ethical AI principles. This session will showcase how AI maximizes fundraising impact, enabling faster crisis response and resource allocation. We\u2019ll explore predictive modeling for donor engagement, secure AI governance with Unity Catalog and our vision for generative AI in fundraising, leveraging AI-assisted storytelling to deepen donor connections. AI is not just about efficiency; it\u2019s about saving lives. Join us to see how AI-driven fundraising is transforming humanitarian aid on a global scale. /Director of Data Science"}
{"session_id": "empowering-warfighter-ai", "title": "Empowering the Warfighter With AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Science", "Delta Lake", "ELT"], "speakers": ["Data Science Director, Navy"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The new Budget Execution Validation process has transformed how the Navy reviews unspent funds. Powered by Databricks Workflows, MLflow, Delta Lake and Apache Spark\u2122, this data-driven model predicts which financial transactions are most likely to have errors, streamlining reviews and increasing accuracy. In FY24, it helped review $40 billion, freeing $1.1 billion for other priorities, including $260 million from active projects. By reducing reviews by 80%, cutting job runtime by over 50% and lowering costs by 60%, it saved 218,000 work hours and $6.7 million in labor costs. With automated workflows and robust data management, this system exemplifies how advanced tools can improve financial decision-making, save resources and ensure efficient use of taxpayer dollars. /Data Science Director\nNavy /Data Science Director"}
{"session_id": "entity-resolution-best-outcomes-your-data", "title": "Entity Resolution for the Best Outcomes on Your Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING", "technologies": ["DATABRICKS APPS", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["DSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many ways to implement entity resolution (ER) system \u2014 both using vendor software and open-source libraries that enable DIY Entity Resolution. However, generally we see common challenges with any approach \u2014 scalability, bound to a single model architecture, lack of metrics and explainability, and stagnant implementations that do not \"learn\" with experience. Recent experiments with transformer-based approaches, fast lookups with vector search and Databricks components such as Databricks Apps and Agent Eval provide the foundations for a composable ER system that can get better with time on your data. In this presentation, we include a demo of how to use these components to build a composable ER that has the best outcomes for your data. /Staff Data Scientist\nDatabricks /DSA"}
{"session_id": "evaluation-driven-development-workflows-best-practices-and-real-world", "title": "Evaluation-Driven Development Workflows: Best Practices and Real-World Scenarios", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In enterprise AI, Evaluation-Driven Development (EDD) ensures reliable, efficient systems by embedding continuous assessment and improvement into the AI development lifecycle. High-quality evaluation datasets are created using techniques like document analysis, synthetic data generation via Mosaic AI\u2019s synthetic data generation API, SME validation, and relevance filtering, reducing manual effort and accelerating workflows. EDD focuses on metrics such as context relevance, groundedness, and response accuracy to identify and address issues like retrieval errors or model limitations. Custom LLM judges, tailored to domain-specific needs like PII detection or tone assessment, enhance evaluations. By leveraging tools like Mosaic AI Agent Framework and Agent Evaluation, MLflow, EDD automates data tracking, streamlines workflows, and quantifies improvements, transforming AI development for delivering scalable, high-performing systems that drive measurable organizational value. /Senior Specialist Solutions Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "evolving-agent-complexity-building-multi-agent-systems-mosaic-ai", "title": "Evolving Agent Complexity: Building Multi-Agent Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Machine Learning Engineer, Greenlight Financial Technology"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session dives into building multi-agent systems on the Mosaic AI Platform, exploring the techniques, architectures and lessons learned from experiences building Greenlight\u2019s real-world agent applications. This presentation is well suited for executives, product managers and engineers alike, breaking down AI Agents into easy-to-understand concepts, while presenting an architecture for building complex systems. We\u2019ll examine the core components of generative AI Agents and different ways to assemble them into agents, including different prompting and reasoning techniques. We\u2019ll cover how the Mosaic AI Platform has enabled our small team to build, deploy and monitor our AI Agents, touching on vector search, feature and model serving endpoints, and the evaluation framework. Finally, we\u2019ll discuss the pros and cons of building a multi-agent system consisting of specialized agents vs. a single large agent for Greenlight\u2019s AI Assistant, and the challenges we encountered. /Staff Machine Learning Engineer\nGreenlight Financial Technology /Machine Learning Engineer"}
{"session_id": "fueling-efficiency-how-pilot-uses-vector-stores-data-quality-and-genai", "title": "Fueling Efficiency: How Pilot Uses Vector Stores, Data Quality, and GenAI to Deliver Business Value", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, TRAVEL AND HOSPITALITY", "technologies": ["AI/BI", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning"], "speakers": ["Sr Manager, Machine Learning Engineering, Pilot Travel Centers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Sr Manager, Machine Learning Engineering"}
{"session_id": "future-anti-cheat", "title": "The Future of Anti-Cheat", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science", "Machine Learning", "Scala"], "speakers": ["Rebel Data Science"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As online gaming evolves, so do cheating methods that exploit client-server vulnerabilities. Traditional anti-cheat, such as kernel-level drivers and runtime detections, has long been the primary defense. However, recent high-profile failures expose the risks of operating in kernel space. More critically, advanced cheats like Direct Memory Access (DMA) exploits and AI-powered Computer Vision (CV) hacks increasingly render client-side detection ineffective. This presentation examines the escalating arms race between cheat creators and developers, highlighting client-side limitations. With CV cheats mimicking human behavior, anti-cheat must shift toward server-side, data-driven detection. By leveraging AI, machine learning, and behavioral analytics to analyze player patterns, input anomalies, and decision inconsistencies, future solutions can move beyond static detection to adaptive security models, ensuring fair play at scale. /Rebel Data Science"}
{"session_id": "gaining-insight-image-data-databricks-using-multi-modal-foundation", "title": "Gaining Insight From Image Data in Databricks Using Multi-Modal Foundation Model API", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the hidden potential in your image data without specialized computer vision expertise! This session explores how to leverage Databricks' multi-modal Foundation Model APIs to analyze, classify and extract insights from visual content. Learn how Databricks provides a unified API to understand images using powerful foundation models within your data workflows. Key takeaways: Whether analyzing product images, processing visual documents or building content moderation systems, you'll discover how to extract valuable insights from your image data within the Databricks ecosystem. /Sr. Software Engineer"}
{"session_id": "genai-powered-shopping-assistant-prada-e-commerce-search-bar", "title": "GenAI-Powered Shopping Assistant for Prada e-Commerce Search Bar", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Data Scientist, Data Reply IT"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Prada has developed a complex solution, leveraging MosaicAI to propose an interactive and natural language product discovery capability that could improve its e-commerce search bar. The backbone is a 70B model and a Vector Store, which collaborates with additional filterings and AI solutions to suggest not only the perfect outfit for each occasion, but also provide alternative solutions and similar items. /Lead Data Scientist\nPrada Group /Data Scientist"}
{"session_id": "generating-laughter-testing-and-evaluating-success-llms-comedy", "title": "Generating Laughter: Testing and Evaluating the Success of LLMs for Comedy", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Sr. Developer Experience Engineer, Galileo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Nondeterministic AI models, like large language models (LLMs), offer immense creative potential but require new approaches to testing and scalability. Drawing from her experience running New York Times-featured Generative AI comedy shows, Erin uncovers how traditional benchmarks may fall short and how embracing unpredictability can lead to innovative, laugh-inducing results. This talk will explore methods like multi-tiered feedback loops, chaos testing and exploratory user testing, where AI outputs are evaluated not by rigid accuracy standards but by their adaptability and resonance across different contexts \u2014 from comedy generation to functional applications. Erin will emphasize the importance of establishing a root source of truth \u2014 a reliable dataset or core principle \u2014 to manage consistency while embracing creativity. Whether you\u2019re looking to generate a few laughs of your own or explore creative uses of Generative AI, this talk will inspire and delight enthusiasts of all levels. /Sr. Developer Experience Engineer"}
{"session_id": "generating-zero-shot-hard-case-hallucinations-synthetic-and-open-data", "title": "Generating Zero-Shot Hard-Case Hallucinations: A Synthetic and Open Data Approach", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal Research Scientist, Nvidia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present a novel framework for designing and inducing controlled hallucinations in long-form content generation by LLMs across diverse domains. The purpose is to create fully-synthetic benchmarks and mine hard cases for iterative refinement of zero-shot hallucination detectors. We will first demonstrate how Gretel Navigator can be used to design realistic, high-quality long-context datasets across various domains. Second, we will describe our reasoning-based approach to hard-case mining. Specifically, our methodology relies on chain-of-thought-based generation of both faithful and deceptive question-answer pairs based upon long-context samples. Subsequently, a consensus labeling and detector framework is employed to filter synthetic examples to zero-shot hard cases. The result of this process is a fully-automated system, operating under open data licenses such as Apache-2.0, for the generation of hallucinations at the edge-of-capabilities for a target LLM to detect. /Principal Research Scientist"}
{"session_id": "generative-ai-merchant-matching", "title": "Generative AI Merchant Matching", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LLAMA", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, Mastercard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Our project demonstrates building enterprise AI systems cost-effectively, focusing on matching merchant descriptors to known businesses. Using fine-tuned LLMs and advanced search, we created a solution rivaling alternatives at minimal cost. The system works in three steps: A fine-tuned Llama 3 8B model parses merchant descriptors into standardized components. A hybrid search system uses these components to find candidate matches in our database. A Llama 3 70B model then evaluates top candidates, with an AI judge reviewing results for hallucination. We achieved a 400% latency improvement while maintaining accuracy and keeping costs low and each fine-tuning round cost hundreds of dollars. Through careful optimization and simple architecture for a balance between cost, speed and accuracy, we show that small teams with modest budgets can tackle complex problems effectively using this technology. We share key insights on prompt engineering, fine-tuning and cost and latency management. /Senior Data Scientist"}
{"session_id": "harnessing-databricks-advanced-llm-time-series-models-healthcare", "title": "Harnessing Databricks for Advanced LLM Time-Series Models in Healthcare Forecasting", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["AI Scientist Dir, IQVIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This research introduces a groundbreaking method for healthcare time-series forecasting using a Large Language Model (LLM) foundation model. By leveraging a comprehensive dataset of over 50 million IQVIA time-series trends, which includes data on procedure demands, sales and prescriptions (TRx), alongside publicly available data spanning two decades, the model aims to significantly enhance predictive accuracy in various healthcare applications. The model's transformer-based architecture incorporates self-attention mechanisms to effectively capture complex temporal dependencies within historical time-series trends, offering a sophisticated approach to understanding patterns, trends and cyclical variations. /AI Scientist Dir"}
{"session_id": "high-throughput-ml-mastering-efficient-model-serving-enterprise-scale", "title": "High-Throughput ML: Mastering Efficient Model Serving at Enterprise Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ever wondered how industry leaders handle thousands of ML predictions per second? This session reveals the architecture behind high-performance model serving systems on Databricks. We'll explore how to build inference pipelines that efficiently scale to handle massive request volumes while maintaining low latency. You'll learn how to leverage Feature Store for consistent, low-latency feature lookups and implement auto-scaling strategies that optimize both performance and cost. Key takeaways: Whether you're serving recommender systems or real-time fraud detection models, you'll gain practical strategies for building enterprise-grade ML serving systems."}
{"session_id": "how-anthropic-transforms-financial-services-teams-genai", "title": "How Anthropic Transforms Financial Services Teams With GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Anthropic"}
{"session_id": "improve-ai-training-first-synthetic-personas-dataset-aligned-real-world", "title": "Improve AI Training With the First Synthetic Personas Dataset Aligned to Real-World Distributions", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Staff Applied Scientist, Gretel.ai"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A big challenge in LLM development and synthetic data generation is ensuring data quality and diversity. While data incorporating varied perspectives and reasoning traces consistently improves model performance, procuring such data remains impossible for most enterprises. Human-annotated data struggles to scale, while purely LLM-based generation often suffers from distribution clipping and low entropy. In a novel compound AI approach, we combine LLMs with probabilistic graphical models and other tools to generate synthetic personas grounded in real demographic statistics. The approach allows us to address major limitations in bias, licensing and persona skew of existing methods. We release the first open source dataset aligned with real-world distributions and show how enterprises can leverage it with its Gretel Navigator extensions to bring diversity and quality to model training on the Databricks Platform, all while addressing model collapse and data provenance concerns head-on. /Principal Research Scientist\nNVIDIA /Staff Applied Scientist"}
{"session_id": "intelligent-document-processing-building-ai-bi-and-analytics-systems", "title": "Intelligent Document Processing: Building AI, BI, and Analytics Systems on Unstructured Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Most enterprise data is trapped in unstructured formats \u2014 documents, PDFs, scanned images and tables \u2014 making it difficult to access, analyze and use. This session shows how to unlock that hidden value by building intelligent document processing workflows on the Databricks Data Intelligence Platform. You\u2019ll learn how to ingest unstructured content using Lakeflow Connect, extract structured data with AI Parse \u2014 even from complex tables and scanned documents \u2014 and apply analytics or AI to this newly structured data. What you\u2019ll learn: /Product Manager\nDatabricks /Product Manager"}
{"session_id": "intro-mosaic-ai-platform-building-data-intelligence-your-ai-solutions", "title": "Intro to the Mosaic AI Platform: Building Data Intelligence Into Your AI Solutions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Technical Marketing Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Take a front-row seat for a comprehensive, high-level introduction to Mosaic AI through the lens of Data Intelligence. In this session, we\u2019ll spotlight the Databricks Platform\u2019s newest features and announcements, showcase how Mosaic AI transforms raw enterprise data into actionable insights and share real-world examples of success. Whether you\u2019re beginning your AI journey or scaling your existing efforts, this talk will provide you with the foundational knowledge and inspiration to fully leverage Mosaic AI for Data Intelligence and next-generation GenAI solutions. /AI/ML Product Mgmt\nDatabricks /Staff Technical Marketing Manager"}
{"session_id": "kill-bill-ing-revenge-dish-best-served-optimized-genai", "title": "Kill Bill-ing? Revenge is a Dish Best Served Optimized with GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, PROFESSIONAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sportsbet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era where cloud costs can spiral out of control, Sportsbet achieved a remarkable 49% reduction in Total Cost of Ownership (TCO) through an innovative AI-powered solution called 'Kill Bill.' This presentation reveals how we transformed Databricks' consumption-based pricing model from a challenge into a strategic advantage through an intelligent automation and optimization. Attendees will leave with a clear understanding of how to implement AI within Databricks solutions to address similar cost challenges in their environments. /Sportsbet"}
{"session_id": "lakehouse-powerhouse-reckitts-enterprise-ai-transformation-story", "title": "Lakehouse to Powerhouse: Reckitt's Enterprise AI Transformation Story", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Boston Consulting Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we showcase Reckitt\u2019s journey to develop and implement a state-of-the-art Gen AI platform, designed to transform enterprise operations starting with the marketing function. We will explore the unique technical challenges encountered and the innovative architectural solutions employed to overcome them. Attendees will gain insights into how cutting-edge Gen AI technologies were integrated to meet Reckitt\u2019s specific needs. This session will not only highlight the transformative impacts on Reckitt\u2019s marketing operations but also serve as a blueprint for AI-driven innovation in the Consumer Goods sector, demonstrating a successful model of partnership in technology and business transformation. /VP - Global Data & Analytics @ Reckitt\nReckitt /Boston Consulting Group"}
{"session_id": "lancedb-complete-search-and-analytical-store-serving-production-scale", "title": "LanceDB: A Complete Search and Analytical Store for Serving Production-scale AI Applications", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Co-founder, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "If you're building AI applications, chances are you're solving a retrieval problem somewhere along the way. This is why vector databases are popular today. But if we zoom out from just vector search, serving AI applications also requires handling KV workloads like a traditional feature store, as well as analytical workloads to explore and visualize data. This means that building an AI application often requires multiple data stores, which means multiple data copies, manual syncing, and extra infrastructure expenses. LanceDB is the first and only system that supports all of these workloads in one system. Powered by Lance columnar format, LanceDB completely breaks open the impossible triangle of performance, scalability, and cost for AI serving. Serving AI applications is different from previous waves of technology, and a new paradigm demands new tools. /Software Engineer\nDatabricks /CEO / Co-founder"}
{"session_id": "learn-how-virtue-foundation-saves-lives-optimizing-health-care-delivery", "title": "Learn How the Virtue Foundation Saves Lives by Optimizing Health Care Delivery Across the Globe", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-Founder and President, Virtue Foundation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Virtue Foundation uses cutting-edge techniques in AI to optimize global health care delivery to save lives. With Unity Catalog as a foundation, they are using advanced Gen AI with model serving, vector search and MLflow to radically change how they map volunteer health resources with the right locations and facilities. /Co-Founder and President"}
{"session_id": "learn-program-not-write-prompts-dspy", "title": "Learn to Program Not Write Prompts with DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Writing prompts for our GenAI applications is long, tedious, and unmaintainable. A proper software development lifecycle requires proper testing and maintenance, something incredibly difficult to do on a block of text. Our current prompt engineering best practices have largely been manual trial and error, testing which of our prompts work well in certain situations. This process worsens as our prompts become more complex, adding multiple tasks and functionality within one long singular prompt. Enter DSPy, your PROGRAMATIC way of building GenAI Applications. Learn how DSPy allows you to modularize your prompt into modules and enforce typing through signatures. Then, utilize state of the art algorithms to optimize the prompts and weights against your evaluation datasets, just like machine learning! We will compare DSPy to a restaurant to help illustrate and demo DSPy\u2019s capabilities. It's time to start programming, rather than prompting, again! /Delivery Solutions Architect"}
{"session_id": "let-llm-write-prompts-intro-dspy-compound-ai-pipelines", "title": "Let the LLM Write the Prompts: An Intro to DSPy in Compound AI Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["AI/BI", "DSPY", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Data Science Leader & Strategist, Overture Maps Foundation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Large Language Models (LLMs) excel at understanding messy, real-world data, but integrating them into production systems remains challenging. Prompts can be unruly to write, vary by model and can be difficult to manage in the large context of a pipeline. In this session, we'll demonstrate incorporating LLMs into a geospatial conflation pipeline, using DSPy. We'll discuss how DSPy works under the covers and highlight the benefits it provides pipeline creators and managers. /Data Science Leader & Strategist"}
{"session_id": "llmops-intermountain-health-case-study-ai-inventory-agents", "title": "LLMOps at Intermountain Health: A Case Study on AI Inventory Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead MLOps Engineer, Intermountain Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will delve into the creation of an infrastructure, CI/CD processes and monitoring systems that facilitate the responsible and efficient deployment of Large Language Models (LLMs) at Intermountain Healthcare. Using the \"AI Inventory Agents\" project as a case study, we will showcase how an LLM Agent can assist in effort and impact estimates, as well as provide insights into various AI products, both custom-built and third-party hosted. This includes their responsible AI certification status, development status and monitoring status (lights on, performance, drift, etc.). Attendees will learn how to build and customize their own LLMOps infrastructure to ensure seamless deployment and monitoring of LLMs, adhering to responsible AI practices. /Lead MLOps Engineer"}
{"session_id": "low-emission-oil-gas-engineering-balance-between-clean-and-reliable", "title": "Low-Emission Oil & Gas: Engineering the Balance Between Clean and Reliable", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science"], "speakers": ["Technical lead, BP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Manager of Data Science and Applied AI\nNOV /Lead Architect\nbp /Director of Data Science and Analytics\nNOV /Technical lead"}
{"session_id": "machine-learning-aimbot-detection-call-duty", "title": "Machine Learning Aimbot Detection in Call of Duty", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Machine Learning", "Scala"], "speakers": ["Machine Learning Research Engineer, Activision"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As online gaming grows, maintaining fair play is increasingly difficult. Call of Duty, a highly competitive first-person shooter, faces a surge in aimbot usage\u2014cheats that enable near-perfect accuracy, undermining ranked play. Traditional detection methods are ineffective against advanced cheats that mimic human behavior. Machine learning presents a scalable and adaptive solution. We developed a data pipeline that collects features such as angle velocity, acceleration, etc. to train a deep neural network and deployed it. We are processing 30 million rows of data per hour for this detection on Databricks Platform. As cheat developers evolve, so must detection techniques. This session will explore our methodologies, challenges and future directions, demonstrating how machine learning is transforming anti-cheat strategies and preserving competitive integrity in online gaming and how Databricks enabling us to do so. /Machine Learning Research Engineer"}
{"session_id": "managing-data-and-ai-security-risks-dasf-20-and-customer-story", "title": "Managing Data and AI Security Risks With DASF 2.0 \u2014 and a Customer Story", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Artificial Intelligence, US AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Security team led a broad working group that significantly evolved the Databricks AI Security Framework (DASF) to its 2.0 version since its first release by closely collaborating with the top cyber security researchers at industry organizations such as OWASP, Gartner, NIST, HITRUST, FAIR Institute and several Fortune 100 companies to address the evolving risks and associated controls of AI systems in enterprises. Join us to to learn how The CLEVER GenAI pipeline, an AI-driven innovation in healthcare, processes over 1.5 million clinical notes daily to classify social determinants impacting veteran care while adhering to robust security measures like NIST 800-53 controls and by leveraging Databricks AI Security Framework. We will discuss robust AI security guidelines to help data and AI teams understand how to deploy their AI applications securely. This session will give a security framework for security teams, AI practitioners, data engineers and governance teams. /Principal Staff Security Field Engineer\nDatabricks /Director of Artificial Intelligence"}
{"session_id": "marketing-runs-your-data-why-it-holds-keys-customer-growth", "title": "Marketing Runs on Your Data: Why IT Holds the Keys to Customer Growth", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time"], "speakers": ["Epsilon"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing owns the outcomes, but IT owns the infrastructure that makes those outcomes possible. In today\u2019s data-driven landscape, the success of customer engagement and personalization strategies depends on a tight partnership between marketing and IT. This session explores how leading brands are using Databricks and Epsilon to unlock the full value of first-party data \u2014 transforming raw data into rich customer profiles, real-time engagement and measurable marketing ROI. Join Epsilon to see how a unified data foundation powers marketing to drive outcomes \u2014 with IT as the enabler of scale, governance and innovation. Key takeaways: /Epsilon"}
{"session_id": "measure-what-matters-quality-focused-monitoring-production-ai-agents", "title": "Measure What Matters: Quality-Focused Monitoring for Production AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Sr Staff Data Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How do you know if your AI agents are really delivering value in production? This session dives into Databricks' Mosaic Agent Monitoring solution, your window into AI agent performance and quality. We'll show you how the system automatically tracks everything from basic operational metrics to sophisticated quality indicators, giving you real-time visibility into how your agents are performing in the wild. Key takeaways: Transform your AI operations from reactive firefighting to proactive excellence, ensuring your agents consistently deliver value in production. /Databricks /Sr Staff Data Scientist"}
{"session_id": "meet-goose-open-source-ai-agent", "title": "Meet Goose, an Open Source AI Agent", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal AI Engineer, Block"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Principal AI Engineer"}
{"session_id": "metadata-agents-building-future-content-understanding-coactive-ai", "title": "From Metadata to Agents: Building the future of content understanding with Coactive AI + Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "ELT"], "speakers": ["SVP Data Science, Media Group, NBC Universal"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Media enterprises generate vast amounts of visual content, but unlocking its full potential requires multimodal AI at scale. Coactive AI and NBCUniversal\u2019s Corporate Decision Sciences team are transforming how enterprises discover and understand visual content. We explore how Coactive AI and Databricks \u2014 from Delta Share to Genie \u2014 can revolutionize media content search, tagging and enrichment, enabling new levels of collaboration. Attendees will see how this AI-powered approach fuels AI workflows, enhances BI insights and drives new applications \u2014 from automating cut sheet generation to improving content compliance and recommendations. By structuring and sharing enriched media metadata, Coactive AI and NBCU are unlocking deeper intelligence and laying the groundwork for agentic AI systems that retrieve, interpret and act on visual content. This session will showcase real-world examples of these AI agents and how they can reshape future content discovery and media workflows. /Field CTO & Co-Founder\nCoactive Systems Inc /SVP Data Science, Media Group"}
{"session_id": "mlops-databricks", "title": "MLOps With Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["MLOps Tech Lead, Marvelous MLOps"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/MLOps Tech Lead"}
{"session_id": "mlops-ships-accelerating-ai-deployment-vizient-databricks", "title": "MLOps That Ships: Accelerating AI Deployment at Vizient with Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Lead Machine Learning Engineer, Vizient"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Deploying AI models efficiently and consistently is a challenge many organizations face. This session will explore how Vizient built a standardized MLOps stack using Databricks, Azure DevOps and GitHub Actions to streamline model development, deployment and monitoring. Attendees will gain insights into how Databricks Asset Bundles were leveraged to create reproducible, scalable pipelines and how Infrastructure-as-Code principles accelerated onboarding for new AI projects.The talk will cover: By the end of this session, participants will have a roadmap for implementing a scalable, reusable MLOps framework that enhances operational efficiency across AI initiatives. /Director- Technology Delivery, Data & AI\nVizient Inc. /Lead Machine Learning Engineer"}
{"session_id": "moodys-ai-screening-agent-automating-compliance-decisions", "title": "Moody's AI Screening Agent: Automating Compliance Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Assc Dir - Machine Learning, Moody's"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The AI Screening Agent automates Level 1 (L1) screening process, essential for Know Your Customer (KYC) and compliance due diligence during customer onboarding. This system aims to minimize false positives, significantly reducing human review time and costs. Beyond typical Retrieval-Augmented Generation (RAG) applications like summarization and chat-with-your-data (CWYD), the AI Screening Agent employs a ReAct architecture with intelligent tools, enabling it to perform complex compliance decision-making with human-like accuracy and greater consistency. In this talk, I will explore the screening agent architecture, demonstrating its ability to meet evolving client policies. I will discuss evaluation and configuration management using MLflow LLM-as-judge and Unity Catalog, and discuss challenges, such as, data fidelity and customization. This session underscores the transformative potential of AI agents in compliance workflows, emphasizing their adaptability, accuracy, and consistency. /Assc Dir - Machine Learning"}
{"session_id": "next-gen-data-science-how-posit-and-databricks-are-transforming", "title": "Next-Gen Data Science: How Posit and Databricks Are Transforming Analytics at Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Python", "Scala"], "speakers": ["Senior Product Mgr. Cloud Integrations, Posit, PBC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Modern data science teams face the challenge of navigating complex landscapes of languages, tools and infrastructure. Positron, Posit\u2019s next-generation IDE, offers a powerful environment tailored for data science, seamlessly integrating with Databricks to empower teams working in Python and R. Now integrated within Posit Workbench, Positron enables data scientists to efficiently develop, iterate and analyze data with Databricks \u2014 all while maintaining their preferred workflows. In this session, we\u2019ll explore how Python and R users can develop, deploy and scale their data science workflows by combining Posit tools with Databricks. We\u2019ll showcase how Positron simplifies development for both Python and R and how Posit Connect enables seamless deployment of applications, reports and APIs powered by Databricks. Join us to see how Posit + Databricks create a frictionless, scalable and collaborative data science experience \u2014 so your teams can focus on insights, not infrastructure. /Senior Product Mgr. Cloud Integrations"}
{"session_id": "next-wave-ai-applications-driven-agentic-workflow-adidas-using", "title": "The Next Wave of AI Applications Driven by Agentic Workflow at Adidas Using Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["MLFLOW", "MOSAIC AI", "PARTNER CONNECT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["ML Engineer, Adidas AG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious to know how Adidas is transforming customer experience and business impact with agentic workflows, powered by Databricks? By leveraging cutting-edge tools like MosaicML\u2019s deployment capabilities, Mosaic AI Gateway, and MLflow, Adidas built a scalable GenAI agentic infrastructure that delivers actionable insights from growing 2 million product reviews annually. With remarkable results: Join us to explore how Adidas turned agentic workflows infra into a strategic advantage using Databricks and learn how you can do the same! /Resident Solutions Architect\nDatabricks /ML Engineer"}
{"session_id": "no-code-ml-forecasting-platform-retail-and-cpg-companies", "title": "A No-Code ML Forecasting Platform for Retail and CPG Companies", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Director, Antuit - A Zebra Technologies company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retail and CPG companies face growing pressure to better forecast demand, optimize pricing and manage inventory \u2014 yet traditional approaches take months to deploy and often require extensive engineering support. In this session, we will showcase Workcloud Modeling Studio, a low-code/no-code ML platform designed for data scientists working in retail and CPG. Learn how this tool improves forecasting accuracy and accelerates time-to-value from months to hours. We will walk through a real-world use case of demand forecasting for a retailer using Zebra's Modeling Studio. This talk will demonstrate how to build, train and deploy an ML forecasting pipeline \u2014 without reinventing the wheel. /Product Director"}
{"session_id": "no-time-dad-bod-automating-life-ai-and-databricks", "title": "No Time for the Dad Bod: Automating Life with AI and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DELTA LAKE", "DSPY"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Real-time", "Scala"], "speakers": ["AI Entrepreneur in Residence, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Life as a father, tech leader, and fitness enthusiast demands efficiency. To reclaim my time, I\u2019ve built AI-driven solutions that automate everyday tasks\u2014from research agents that prep for podcasts to multi-agent systems that plan meals\u2014all powered by real-time data and automation. This session dives into the technical foundations of these solutions, focusing on event-driven agent design and scalable patterns for robust AI systems. You\u2019ll discover how Databricks technologies like Delta Lake, for reliable and scalable data management, and DSPy, for streamlining the development of generative AI workflows, empower seamless decision-making and deliver actionable insights. Through detailed architecture diagrams and a live demo, I\u2019ll showcase how to design systems that process data in motion to tackle complex, real-world problems. Whether you\u2019re an engineer, architect, or data scientist, you\u2019ll leave with practical strategies to integrate AI-driven automation into your workflows. /AI Entrepreneur in Residence"}
{"session_id": "one-stop-machine-translation-solution-game-domain-real-time-ugc-content", "title": "One-Stop Machine Translation Solution in Game Domain From Real-Time UGC Content to In-Game Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior AI researcher, Tencent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present Level Infinite AI Translation, a translation engine developed by Tencent, tailored specifically for the gaming industry. The primary challenge in game machine translation (MT) lies in accurately interpreting the intricate context of game texts, effectively handling terminology and adapting to the highly diverse translation formats and stylistic requirements across different games. Traditional MT approaches cannot effectively address the aforementioned challenges due to their weak context representation ability and lack of common knowledge. Leveraging large language model and related technology, our engine is crafted to capture the subtleties of localized language expression while ensuring optimization for domain-specific terminology, jargon and required formats and styles. To date, the engine has been successfully implemented in 15 international projects, translating over one billion words across 23 languages, and has demonstrated cost savings exceeding 25% for partners. /Lead Researcher\nProxima Beta (Tencent) /Senior AI researcher"}
{"session_id": "optimize-cost-and-user-value-through-model-routing-ai-agent", "title": "Optimize Cost and User Value Through Model Routing AI Agent", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Machine Learning Lead, Meta"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Each LLM has unique strengths and weaknesses, and there is no one-size-fits-all solution. Companies strive to balance cost reduction with maximizing the value of their use cases by considering various factors such as latency, multi-modality, API costs, user need, and prompt complexity. Model routing helps in optimizing performance and cost along with enhanced scalability and user satisfaction. Overview of cost-effective models training using AI gateway logs, user feedback, prompt, and model features to design an intelligent model-routing AI agent. Covers different strategies for model routing, deployment in Mosaic AI, re-training, and evaluation through A/B testing and end-to-end Databricks workflows. Additionally, it will delve into the details of training data collection, feature engineering, prompt formatting, custom loss functions, architectural modifications, addressing cold-start problems, query embedding generation and clustering through VectorDB, and RL policy-based exploration. /Machine Learning Lead"}
{"session_id": "optimizing-ev-charging-experience-machine-learning-accurate-charge-time", "title": "Optimizing EV Charging Experience: Machine Learning for Accurate Charge Time Estimation", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "ELT", "Machine Learning", "Real-time", "Scala"], "speakers": ["AI Engineer, Rivian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Accurate charge time estimation is key to vehicle performance and user experience. We developed a scalable ML model that enhances real-time charge predictions in vehicle controls. Traditional rule-based methods struggle with dynamic factors like environment, vehicle state, and charging conditions. Our adaptive ML solution improves accuracy by 10%. We use Unity Catalog for data governance, Delta Tables for storage, and Liquid Clustering for data layout. Job schedulers manage data processing, while AutoML accelerates model selection. MLflow streamlines tracking, versioning, and deployment. A dedicated serving endpoint enables A/B testing and real-time insights. As our data ecosystem grew, scalability became critical. Our flexible ML framework was integrated into vehicle control systems within months. With live accuracy tracking and software-driven blending, we support 50,000+ weekly charge sessions, improving energy management and user experience. /Senior Manager, Machine Learning & AI\nRivian Automotive, LLC /Sr. Machine Learning/AI Engineer"}
{"session_id": "patients-are-waiting-accelerating-healthcare-innovation-data-ai-and", "title": "Patients Are Waiting... Accelerating Healthcare Innovation With Data, AI and Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture", "Scala"], "speakers": ["Head of Automation & Digital Innovation, Novo Nordisk"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era of exponential data growth, organizations across industries face common challenges in transforming raw data into actionable insights. This presentation showcases how Novo Nordisk is pioneering insights generation approaches to clinical data management and AI.Using our clinical trials platform FounData, built on Databricks, we demonstrate how proper data architecture enables advanced AI applications. We'll introduce a multi-agent AI framework that revolutionizes data interaction, combining specialized AI agents to guide users through complex datasets. While our focus is on clinical data, these principles apply across sectors \u2013 from manufacturing to financial services.Learn how democratizing access to data and AI capabilities can transform organizational efficiency while maintaining governance. Through this real-world implementation, participants will gain insights on building scalable data architectures and leveraging multi-agent AI frameworks for responsible innovation. /Principal Platform Architect\nNovo Nordisk /VP, Data & AI Platform Engineering\nNovo Nordisk A/S /Head of Automation & Digital Innovation"}
{"session_id": "petrobras-mlops-transformation-mlflow-and-databricks", "title": "Petrobras MLOps Transformation With MLflow and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Scala"], "speakers": ["Consultant, Petrobras"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As a global energy leader, Petrobras relies on machine learning to optimize operations, but manual model deployment and validation processes once created bottlenecks that delayed critical insights. In this session, we\u2019ll reveal how we revolutionized our MLOps framework using MLflow, Databricks Asset Bundles (DABs) and Unity Catalog to: Discover how we enabled data scientists to focus on innovation\u2014not infrastructure\u2014through standardized pipelines while ensuring compliance and scalability in one of the world\u2019s most complex energy ecosystems. /Sr. Solutions Architect\nDatabricks /Consultant"}
{"session_id": "practical-ai-solutions-customer-care-supply-chain-excellence", "title": "Practical AI Solutions: From Customer Care to Supply Chain Excellence", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Senior Programmer, Hypertherm Associates"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director of AI\nLippert /VP, Data + AI\nLippert /Director - AI, Analytics & Automation\nHypertherm Associates /Senior Programmer"}
{"session_id": "real-time-botnet-defense-cvs-ai-driven-detection-and-mitigation", "title": "Real-Time Botnet Defense at CVS: AI-Driven Detection and Mitigation on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Principal Data Scientist, CVS"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Botnet attacks mobilize digital armies of compromised devices that continuously evolve, challenging traditional security frameworks with their high-speed, high-volume nature. In this session, we will reveal our advanced system \u2014 developed on the Databricks platform \u2014 that leverages cutting-edge AI/ML capabilities to detect and mitigate bot attacks in near-real time. We will dive into the system\u2019s robust architecture, including scalable data ingestion, feature engineering, MLOps strategies & production deployment of the system. We will address the unique challenges of processing bulk HTTP traffic data, time-series anomaly detection and attack signature identification. We will demonstrate key business values through downtime minimization and threat response automation. With sectors like healthcare facing heightened risks, ensuring data integrity and service continuity is vital. Join us to uncover lessons learned while building an enterprise-grade solution that stays ahead of adversaries. /Sr. Data Scientist\nCVS /Principal Data Scientist"}
{"session_id": "recsys-topic-modeling-and-agents-bridging-genai-traditional-ml-divide", "title": "RecSys, Topic Modeling and Agents: Bridging the GenAI-Traditional ML Divide", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr GenAI Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The rise of GenAI has led to a complete reinvention of how we conceptualize Data + AI. In this breakout, we will recontextualize the rise of GenAI in traditional ML paradigms, and hopefully unite the pre- and post-LLM eras. We will demonstrate when and where GenAI may prove more effective than traditional ML algorithms, and highlight problems for which the wheel is unnecessarily being reinvented with GenAI. This session will also highlight how MLflow provides a unified means of benchmarking traditional ML against GenAI, and lay out a vision for bridging the divide between Traditional ML and GenAI practitioners. /Sr GenAI Product Specialist"}
{"session_id": "responsible-ai-scale-balancing-democratization-and-regulation-financial", "title": "Responsible AI at Scale: Balancing Democratization and Regulation in the Financial Sector", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["EVP and Chief Architect for State Street, State Street"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/EVP and Chief Architect for State Street"}
{"session_id": "revolutionizing-insurance-how-drive-growth-and-innovation", "title": "Revolutionizing Insurance: How to Drive Growth and Innovation", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Modeling", "Real-time"], "speakers": ["Standard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The insurance industry is rapidly evolving as advances in data and artificial intelligence (AI) drive innovation, enabling more personalized customer experiences, streamlined operations, and improved efficiencies. With powerful data analytics and AI-driven solutions, insurers can automate claims processing, enhance risk management, and make real-time decisions. Leveraging insights from large and complex datasets, organizations are delivering more customer-centric products and services than ever before. Key takeaways: Real-world applications of data and AI in claims automation, underwriting, and customer engagementHow predictive analytics and advanced data modeling help anticipate risks and meet customer needs. Personalization of policies, optimized pricing, and more efficient workflows for greater ROI. Discover how data and AI are fueling growth, improving protection, and shaping the future of the insurance industry! /Principal Solutions Architect\nDatabricks /Standard"}
{"session_id": "revolutionizing-nuclear-ai-hive-and-bertha-databricks-architecture", "title": "Revolutionizing Nuclear AI With HiVE and Bertha on Databricks Architecture", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["CTO, EVP R&D and Innovation, Westinghouse Electric Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session we will explore the revolutionary advancements in nuclear AI capabilities with HiVE and Bertha on Databricks architecture. HiVE, developed by Westinghouse, leverages over a century of proprietary data to deliver unparalleled AI capabilities. At its core is Bertha, a generative AI model designed to tackle the unique challenges of the nuclear industry. This session will delve into the technical architecture of HiVE and Bertha, showcasing how Databricks' scalable environment enhances their performance. We will discuss the secure data infrastructure supporting HiVE, ensuring data integrity and compliance. Real-world applications and use cases will demonstrate the impact of HiVE and Bertha on improving efficiency, innovation and safety in nuclear operations. Discover how the fusion of HiVE and Bertha with Databricks architecture is transforming the nuclear AI landscape and driving the future of nuclear technology. /CTO, EVP R&D and Innovation"}
{"session_id": "route-success-scalable-routing-agents-databricks-and-dspy", "title": "Route to Success: Scalable Routing Agents With Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Data Scientist - ML Practice, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As companies increasingly adopt Generative AI, they're faced with a new challenge: managing multiple AI assistants. What if you could have a single, intuitive interface that automatically directs questions to the best assistant for the task? Join us to discover how to implement a flexible Routing Agent that streamlines working with multiple AI Assistants. We'll show you how to leverage Databricks and DSPy 3.0 to simplify adding this powerful pattern to your system. We'll dive into the essential aspects including: We'll share real-world examples that you can apply today. You'll leave with the knowledge to make your AI system run smoothly and efficiently. /Staff Data Scientist - ML Practice"}
{"session_id": "scaling-blockchain-ml-databricks-graph-analytics-graph-machine-learning", "title": "Scaling Blockchain ML With Databricks: From Graph Analytics to Graph Machine Learning", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "MLFLOW", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Scala"], "speakers": ["Staff ML Engineer, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Coinbase leverages Databricks to scale ML on blockchain data, turning vast transaction networks into actionable insights. This session explores how Databricks\u2019 scalable infrastructure, powered by Delta Lake, enables real-time processing for ML applications like NFT floor price predictions. We\u2019ll show how GraphFrames helps us analyze billion-node transaction graphs (e.g., Bitcoin) for clustering and fraud detection, uncovering structural patterns in blockchain data. But traditional graph analytics has limits. We\u2019ll go further with Graph Neural Networks (GNNs) using Kumo AI, which learn from the transaction network itself rather than relying on hand-engineered features. By encoding relationships directly into the model, GNNs adapt to new fraud tactics, capturing subtle relationships that evolve over time. Join us to see how Coinbase is advancing blockchain ML with Databricks and deep learning on graphs. /Staff ML Engineer"}
{"session_id": "scaling-demand-forecasting-nikon-automating-camera-accessories-sales", "title": "Scaling Demand Forecasting at Nikon: Automating Camera Accessories Sales Planning with Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Associate Researcher, Nikon Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Nikon, camera accessories are essential in meeting the diverse needs of professional photographers worldwide, making their timely availability a priority. Forecasting accessories, however, presents unique challenges including dependencies on parent products, sparse demand patterns, and managing predictions for thousands of items across global subsidiaries. To address this, we leveraged Databricks' unified data and AI platform to develop and deploy an automated, scalable solution for accessory sales planning. Our solution employs a hybrid approach that auto-selects best algorithm from a suite of ML and time-series models, incorporating anomaly detection and methods to handle sparse and low-demand scenarios. MLflow is utilized to automate model logging and versioning, enabling efficient management, and scalable deployment. The framework includes data preparation, model selection and training, performance tracking, prediction generation, and output processing for downstream systems. /Senior Associate Researcher"}
{"session_id": "scaling-genai-inference-prototype-production-real-world-lessons-speed", "title": "Scaling GenAI Inference From Prototype to Production: Real-World Lessons in Speed & Cost", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "EDUCATION, MEDIA AND ENTERTAINMENT", "technologies": ["DATA MARKETPLACE", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Engineer, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This lightning talk dives into real-world GenAI projects that scaled from prototype to production using Databricks\u2019 fully managed tools. Facing cost and time constraints, we leveraged four key Databricks features\u2014Workflows, Model Serving, Serverless Compute, and Notebooks\u2014to build an AI inference pipeline processing millions of documents (text and audiobooks). This approach enables rapid experimentation, easy tuning of GenAI prompts and compute settings, seamless data iteration and efficient quality testing\u2014allowing Data Scientists and Engineers to collaborate effectively. Learn how to design modular, parameterized notebooks that run concurrently, manage dependencies and accelerate AI-driven insights. Whether you're optimizing AI inference, automating complex data workflows or architecting next-gen serverless AI systems, this session delivers actionable strategies to maximize performance while keeping costs low. /Lead Engineer"}
{"session_id": "scaling-generative-ai-batch-inference-strategies-foundation-models", "title": "Scaling Generative AI: Batch Inference Strategies for Foundation Models", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious how to apply resource-intensive generative AI models across massive datasets without breaking the bank? This session reveals efficient batch inference strategies for foundation models on Databricks. Learn how to architect scalable pipelines that process large volumes of data through LLMs, text-to-image models and other generative AI systems while optimizing for throughput, cost and quality. Key takeaways: You'll discover how to process any scale of data through your generative AI models efficiently. /Engineering Lead, AI Serving\nDatabricks /Software Engineer"}
{"session_id": "scaling-real-time-fraud-detection-databricks-lessons-draftkings", "title": "Scaling Real-Time Fraud Detection With Databricks: Lessons From DraftKings", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Data Science Engineer, DraftKings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At DraftKings, ensuring secure, fair gaming requires detecting fraud in real time with both speed and precision. In this talk, we\u2019ll share how Databricks powers our fraud detection pipeline, integrating real-time streaming, machine learning and rule-based detection within a PySpark framework. Our system enables rapid model training, real-time inference and seamless feature transformation across historical and live data. We use shadow mode to test models and rules in live environments before deployment. Collaborating with Databricks, we push online feature store performance and enhance real-time PySpark capabilities. We'll cover PySpark-based feature transformations, real-time inference, scaling challenges and our migration from a homegrown system to Databricks. This session is for data engineers and ML practitioners optimizing real-time AI workloads, featuring a deep dive, code snippets and lessons from building and scaling fraud detection. /Principal Software Engineers\nDraftkings /Principal Data Science Engineer"}
{"session_id": "scaling-sales-excellence-how-databricks-uses-its-own-tech-train-gtm", "title": "Scaling Sales Excellence: How Databricks Uses Its Own Tech to Train GTM Teams", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Sales Performance, APJ, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, discover how Databricks leverages the power of Gen AI, MosaicML, Model Serving and Databricks Apps to revolutionize sales enablement. We\u2019ll showcase how we built an advanced chatbot that equips our go-to-market team with the tools and knowledge needed to excel in customer-facing interactions. This AI-driven solution not only trains our salespeople but also enhances their confidence and effectiveness in demonstrating the transformative potential of Databricks to future customers. Attendees will gain insights into the architecture, development process and practical applications of this innovative approach. The session will conclude with an interactive demo, offering a firsthand look at the chatbot in action. Join us to explore how Databricks is using its own platform to drive sales excellence through cutting-edge AI solutions. /Senior Solutions Architect\nDatabricks /Director, Sales Performance, APJ"}
{"session_id": "scaling-smarter-technical-dive-how-databricks-optimizes-model-serving", "title": "Scaling Smarter: Technical Dive Into How Databricks Optimizes Model Serving", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn from the experts on how Databricks\u2019 Mosaic AI Model Serving delivers unparalleled speed and scalability for deploying AI models. This session delves into the architecture and innovations that showcase the impressive improvements in throughput for the AI-serving infrastructure that powers Mosaic AI. /Software Engineer\nDatabricks /Databricks"}
{"session_id": "scaling-xgboost-spark-connect-ml-grace-blackwell", "title": "Scaling XGBoost With Spark Connect ML on Grace Blackwell", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Engineer, NVIDIA Semiconductor Co., Ltd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "XGBoost is one of the off-the-shelf gradient boosting algorithms for analyzing tabular datasets. Unlike deep learning, gradient-boosting decision trees require the entire dataset to be in memory for efficient model training. To overcome the limitation, XGBoost features a distributed out-of-core implementation that fetches data in batch, which benefits significantly from the latest NVIDIA GPUs and the NVLink-C2C\u2019s ultra bandwidth. In this talk, we will share our work on optimizing XGBoost using the Grace Blackwell super chip. The fast chip-to-chip link between the CPU and the GPU enables XGBoost to scale up without compromising performance. Our work has effectively increased XGBoost\u2019s training capacity to over 1.2TB on a single node. The approach is scalable to GPU clusters using Spark, enabling XGBoost to handle terabytes of data efficiently. We will demonstrate combining XGBoost out-of-core algorithms with the latest connect ML from Spark 4.0 for large model training workflows. /Engineer\nNvidia Corp /Engineer"}
{"session_id": "searching-meaning-age-ai", "title": "Searching for Meaning in the Age of AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["CoFounder and CTO, You.com"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bryan McCann, You.com\u2019s co-founder and CTO, shares his journey from studying philosophy and meaning to the Stanford Computer Science Department working on groundbreaking AI research alongside Richard Socher. Right now, AI is reshaping everything we hold dear \u2014 our jobs, creativity, and identities. It\u2019s also our greatest source of inspiration. The Age of AI is simultaneously a Renaissance, Enlightenment, Industrial Revolution and likely source of humanity\u2019s greatest existential crisis. To surmount this, Bryan will discuss how he uses AI responses as new starting points rather than answers, building teams like neural networks optimized for learning and how the answer to our meaning crisis may be for humans to be more like AI. Exploring AI\u2019s impact on politics, economics, healthcare, education and culture, Bryan asserts that we must all take part in authoring humanity\u2019s new story \u2014 AI can inspire us to become something new, rather than merely replace what we are now. /CoFounder and CTO"}
{"session_id": "securing-capital-markets-ai-powered-risk-management-resilience", "title": "Securing Capital Markets: AI-Powered Risk Management for Resilience", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Machine Learning"], "speakers": ["Chief Data Officer, Moody's Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Mitigating risk is vital for protecting reputation, assets and clients in capital markets. This session highlights how firms use Databricks\u2019 Data Intelligence Platform to enhance risk management, ensure compliance and safeguard operations from emerging threats. Discover how advanced analytics and machine learning models enable anomaly detection, fraud prevention and precise regulatory management. Industry leaders share proactive risk strategies that balance security with operational efficiency. Key takeaways: Learn how data intelligence transforms risk management in capital markets, securing the future while driving success! /Senior Vice President\nState Street /Global Head of FS Industry Marketing\nDatabricks /Chief Data Officer"}
{"session_id": "self-improving-agents-and-agent-evaluation-arize-databricks-ml-flow", "title": "Self-Improving Agents and Agent Evaluation With Arize & Databricks ML Flow", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-Founder and Chief Product Officer, Arize"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As autonomous agents become increasingly sophisticated and widely deployed, the ability for these agents to evaluate their own performance and continuously self-improve is essential. However, the growing complexity of these agents amplifies potential risks, including exposure to malicious inputs and generation of undesirable outputs. In this talk, we'll explore how to build resilient, self-improving agents. To drive self-improvement effectively, both the agent and the evaluation techniques must simultaneously improve with a continuously iterating feedback loop. Drawing from extensive real-world experiences across numerous productionized use cases, we will demonstrate practical strategies for combining tools from Arize, Databricks MLflow and Mosaic AI to evaluate and improve high-performing agents. /Co-Founder and Chief Product Officer"}
{"session_id": "servicenow-walks-talk-databricks-revolutionizing-go-market-ai", "title": "ServiceNow \u2018Walks the Talk\u2019 With Databricks: Revolutionizing Go-To-Market With AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Senior AI Product Management, ServiceNow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At ServiceNow, we\u2019re not just talking about AI innovation \u2014 we\u2019re delivering it. By harnessing the power of Databricks, we\u2019re reimagining Go-To-Market (GTM) strategies, seamlessly integrating AI at every stage of the deal journey \u2014 from identifying high-value leads to generating hyper-personalized outreach and pitch materials. In this session, learn how we\u2019ve slashed data processing times by over 90%, reducing workflows from an entire day to just 30 minutes with Databricks. This unprecedented speed enables us to deploy AI-driven GTM initiatives faster, empowering our sellers with real-time insights that accelerate deal velocity and drive business growth. As Agentic AI becomes a game-changer in enterprise GTM, ServiceNow and Databricks are leading the charge \u2014 paving the way for a smarter, more efficient future in AI-powered sales. /Senior NLP Data Scientist\nServiceNow /Senior AI Product Management"}
{"session_id": "smart-inbox-cutting-edge-ai-automated-customer-email-classification", "title": "Smart Inbox: A Cutting-Edge AI for Automated Customer Email Classification by ENGIE", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "PYTORCH", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ENGIE"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era of digital communication overload, businesses struggle to efficiently process and categorize vast volumes of customer emails. Smart Inbox transforms this challenge into an opportunity by combining classic AI techniques with state-of-the-art generative AI, delivering a highly accurate and business-impactful email classification system. Built on Databricks\u2019 powerful data and AI ecosystem, Smart Inbox integrates semantic analysis, large language models and distributed computing to enhance classification precision and reduce manual processing efforts. By leveraging both structured and unstructured data insights, this hybrid AI approach ensures not only operational efficiency but also improved customer engagement and faster response times. This presentation will unveil the key innovations behind Smart Inbox, demonstrating how cutting-edge AI can transform customer interaction workflows, enhance operational agility and redefine the future of intelligent email processing. /ENGIE"}
{"session_id": "smart-vehicles-secure-data-recreating-vehicle-environments-privacy", "title": "Smart Vehicles, Secure Data: Recreating Vehicle Environments for Privacy-Preserving Machine Learning", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Senior Data Scientist, Mercedes-Benz R&D"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As connected vehicles generate vast amounts of personal and sensitive data, ensuring privacy and security in machine learning (ML) processes is essential. This session explores how Trusted Execution Environments (TEEs) and Azure Confidential Computing can enable privacy-preserving ML in cloud environments. We\u2019ll present a method to recreate a vehicle environment in the cloud, where sensitive data remains private throughout model training, inference and deployment. Attendees will learn how Mercedes-Benz R&D North America builds secure, privacy-respecting personalized systems for the next generation of connected vehicles. /Senior Data Scientist"}
{"session_id": "solving-health-ais-data-problem", "title": "Solving Health AI\u2019s Data Problem", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["SVP Infrastructure & Platform Eng., Datavant"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI in healthcare has a data problem. Fragmented data remains one of the biggest challenges, and bottlenecks the development and deployment of AI solutions across life sciences, payers, and providers. Legacy paper-driven workflows and fragmented technology perpetuate silos, making it difficult to create a comprehensive, real-time picture of patient health. Datavant is leveraging Databricks and AWS technology to solve this problem at scale. Through our partnership with Databricks, we are centralizing storage of clinical data from what is arguably the largest health data network so that we can transform it into structured, AI-ready data \u2013 and shave off 80 percent of the work of deploying a new AI use case. Learn how we are handling the complexity of this effort while preserving the integrity of source data. We\u2019ll also share early use cases now available to our healthcare customers. /SVP Infrastructure & Platform Eng."}
{"session_id": "sponsored-aws-deploying-genai-agent-using-databricks-mosaic-ai", "title": "Sponsored by: AWS | Deploying a GenAI Agent using Databricks Mosaic AI, Anthropic, LangGraph, and AWS Bedrock", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, you\u2019ll see how to build and deploy a GenAI agent and Model Context Protocol (MCP) with Databricks, Anthropic, Mosaic External AI Gateway, and AWS Bedrock. You will learn the architecture, best-practices of using Databricks Mosaic AI, Anthropic Sonnet 3.7 first-party frontier model, and LangGraph for custom workflow orchestration in Databricks Data Intelligence Platform. You\u2019ll also see how to use Databricks Mosaic AI to provide agent evaluation and monitoring. In addition, you will also see how inline agent will use MCP to provide tools and other resources using Amazon Nova models with AWS Bedrock inline agent for deep research. This approach gives you the flexibility of LangGraph, the powerful managed agents offered by Amazon Bedrock, and Databricks Mosaic AI\u2019s operational support for evaluation and monitoring."}
{"session_id": "sponsored-boomi-lp-pipelines-agents-manage-data-and-ai-one-platform", "title": "Sponsored by: Boomi, LP | From Pipelines to Agents: Manage Data and AI on One Platform for Maximum ROI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the age of agentic AI, competitive advantage lies not only in AI models, but in the quality of the data agents reason on and the agility of the tools that feed them. To fully realize the ROI of agentic AI, organizations need a platform that enables high-quality data pipelines and provides scalable, enterprise-grade tools. In this session, discover how a unified platform for integration, data management, MCP server management, API management, and agent orchestration can help you to bring cohesion and control to how data and agents are used across your organization."}
{"session_id": "sponsored-deloitte-ai-innovation-and-governance-att-and-deloitte", "title": "Sponsored by: Deloitte | AI Innovation and Governance: AT&T and Deloitte Leverage Databricks for Secure AI Expansion", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AT&T and Deloitte are driving AI innovation across their respective enterprises. Both companies are leveraging the Databricks Data Intelligence Platform to enable AI expansion and adoption as well as to address the complexities and risks associated with AI implementation in large, dynamic organizations. This session will explore how AT&T and Deloitte are transforming their businesses with advanced AI and Gen AI solutions. Attendees will also gain insights into how AT&T and Deloitte are managing enterprise risks, security threats, and data governance challenges without stifling innovation. Both companies will discuss how Databricks Data Intelligence Platform is enabling secure and robust AI development in support of AI security and governance requirements."}
{"session_id": "sponsored-ey-unlocking-value-through-ai-takeda-pharmaceuticals", "title": "Sponsored by: EY | Unlocking Value Through AI at Takeda Pharmaceuticals", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, PROFESSIONAL SERVICES", "technologies": ["AI/BI"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["GenAI use cases that enhance operational efficiency across commercial, R&D, manufacturing, and back-office functions, including these capabilities:"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the rapidly evolving landscape of pharmaceuticals, the integration of AI and GenAI is transforming how organizations operate and deliver value. We will explore the profound impact of the AI program at Takeda Pharmaceuticals and the central role of Databricks. We will delve into eight pivotal AI/GenAI use cases that enhance operational efficiency across commercial, R&D, manufacturing, and back-office functions, including these capabilities:"}
{"session_id": "sponsored-hightouch-unleashing-ai-petsmart-using-ai-decisioning-agents", "title": "Sponsored by: Hightouch | Unleashing AI at PetSmart: Using AI Decisioning Agents to Drive Revenue", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DELTA LAKE"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["B testing with AI Decisioning, achieving a +22% incremental lift in bookings. Join Bradley Breuer, VP of Marketing \u2013 Loyalty, Personalization, CRM, and Customer Analytics, to learn how his team reimagined CRM using personalize campaigns dynamically optimize creative, offers, timing for every unique pet parent. Learn: How PetSmart blends human insight creativity deliver that engage convert. they moved beyond batch-and-blast calendars Decisioning Agents sends\u2014while keeping control over brand, messaging, frequency. Databricks as their source truth led surprising learnings better outcomes."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With 75M+ Treats Rewards members, PetSmart knows how to build loyalty with pet parents. But recently, traditional email testing and personalization strategies weren\u2019t delivering the engagement and growth they wanted\u2014especially in the Salon business. This year, they replaced their email calendar and A/B testing with AI Decisioning, achieving a +22% incremental lift in bookings. Join Bradley Breuer, VP of Marketing \u2013 Loyalty, Personalization, CRM, and Customer Analytics, to learn how his team reimagined CRM using AI to personalize campaigns and dynamically optimize creative, offers, and timing for every unique pet parent. Learn: How PetSmart blends human insight and creativity with AI to deliver campaigns that engage and convert. How they moved beyond batch-and-blast calendars with AI Decisioning Agents to optimize sends\u2014while keeping control over brand, messaging, and frequency. How using Databricks as their source of truth led to surprising learnings and better outcomes."}
{"session_id": "sponsored-infosys-ai-driven-growth-expedite-potential-agentic-ai-and", "title": "Sponsored by: Infosys | AI-Driven Growth: Expedite Potential of Agentic AI and Drive Beyond Customer Experience and Operational Efficiency", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Agentic AI has the power to revolutionize mission-critical domains. Yet this journey is not without its challenges \u2013 the inevitable barriers, frustrations, and setbacks that mark all progress. This session dives into how Infosys Topaz helps enterprises to strategically implement AI at scale in as little as two months to personalize customer journeys, optimize operations, and unlock new revenue streams. Learn how different enterprises have architected foundation capabilities such as Agentic AI factory to build & accommodate hundreds or even thousands of intelligent agents, setting up data fingerprinting and data harvesting to make enterprise Data ready for AI and ensuring interoperability among diverse AI systems"}
{"session_id": "sponsored-infosys-beyond-hype-scale-democratize-agentic-ai-across", "title": "Sponsored by: Infosys | Beyond Hype: Scale & Democratize Agentic AI across enterprise to realize business outcomes.", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Agentic AI and multimodal data are the next frontiers for realizing intelligent and autonomous business systems. Learn how Infosys innovates with Databricks for accelerating data to AI agent journey at scale across an enterprise. Hear our pragmatic capability driven approach instead of use case-based approach to bring the data universe, AI foundations, agent management, data and AI governance and collaboration under unified management."}
{"session_id": "sponsored-ltimindtree-4-strategies-maximize-sap-data-value-databricks", "title": "Sponsored by: LTIMindtree | 4 Strategies to Maximize SAP Data Value with Databricks and AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES", "technologies": ["AI/BI"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time", "Scala"], "speakers": ["ML at scale. Discover how SAP Business Data Cloud and Databricks can help you build a unified, future-ready data analytics ecosystem\u2014without compromising on scalability, flexibility, or cost-efficiency."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As enterprises strive to become more data-driven, SAP continues to be central to their operational backbone. However, traditional SAP ecosystems often limit the potential of AI and advanced analytics due to fragmented architectures and legacy tools. In this session, we explore four strategic options for unlocking greater value from SAP data by integrating with Databricks and cloud-native platforms. Whether you're on ECC, S4HANA, or transitioning from BW, learn how to modernize your data landscape, enable real-time insights, and power AI/ML at scale. Discover how SAP Business Data Cloud and SAP Databricks can help you build a unified, future-ready data and analytics ecosystem\u2014without compromising on scalability, flexibility, or cost-efficiency."}
{"session_id": "sponsored-moveworks-unlocking-full-stack-ai-transformation-moveworks", "title": "Sponsored by: Moveworks | Unlocking Full-stack AI Transformation with the Moveworks Platform", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "PARTNER CONNECT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how visionaries from the world\u2019s leading organizations use Moveworks to give employees a single place to find information, automate tasks, and be more productive. See the Moveworks AI Assistant in action and experience how its reasoning-based architecture allows it to be a one-stop-shop for all employee requests (across IT, HR, finance, sales, and more), how Moveworks empowers developers to easily build new AI agents atop this architecture, and how we give stakeholders tools to implement effective AI governance. Finally, experience how customers and partners alike leverage information in Databricks to supplement their employees' AI journeys."}
{"session_id": "sponsored-snorkel-ai-evaluating-and-improving-performance-agentic", "title": "Sponsored by: Snorkel AI | Evaluating and Improving Performance of Agentic Systems", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "GenAI systems are evolving beyond basic information retrieval and question answering, becoming sophisticated agents capable of managing multi-turn dialogues and executing complex, multi-step tasks autonomously. However, reliably evaluating and systematically improving their performance remains challenging. In this session, we'll explore methods for assessing the behavior of LLM-driven agentic systems, highlighting techniques and showcasing actionable insights to identify performance bottlenecks and to creating better-aligned, more reliable agentic AI systems."}
{"session_id": "sponsored-snowplow-snowplow-signals-powering-tomorrows-customer", "title": "Sponsored by: Snowplow | Snowplow Signals: Powering Tomorrow\u2019s Customer Experiences on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["AI/BI", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The web is on the verge of a major shift. Agentic applications will redefine how customers engage with digital experiences\u2014delivering highly personalized, relevant interactions. In this talk, Snowplow CTO Yali Sassoon explores how Snowplow Signals enables agents to perceive users through short- and long-term memory, natively on the Databricks Data Intelligence Platform."}
{"session_id": "state-enterprise-ai-ai-agents-and-beyond", "title": "State of Enterprise AI: AI Agents and Beyond", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture"], "speakers": ["CTO, Neural Networks, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is evolving fast \u2014 from LLMs to intelligent agents and autonomous decision-making \u2014 and organizations are rethinking how they build and deploy data and AI systems. This session, led by Databricks executives, breaks down what\u2019s happening now and what\u2019s coming next. You\u2019ll learn how these technologies are reshaping data architectures, what it takes to support AI systems at scale, and how Databricks is enabling businesses to move from experimentation to real-world impact. What you\u2019ll learn: /AI/ML Product Mgmt\nDatabricks /CTO, Neural Networks"}
{"session_id": "streamlining-dspy-development-track-debug-and-deploy-mlflow", "title": "Streamlining DSPy Development: Track, Debug, and Deploy With MLflow", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Senior Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DSPy is a framework for authoring GenAI applications with automatic prompt optimization, while MLflow provides powerful MLOps tooling to track, monitor, and productize machine learning workflows. In this lightning talk, we demonstrate how to integrate MLflow with DSPy to bring full observability to your DSPy development. We\u2019ll walk through how to track DSPy module calls, evaluations, and optimizers using MLflow\u2019s tracing and autologging capabilities. By the end, you'll see how combining these two tools makes it easier to debug, iterate, and understand your DSPy workflows, then deploy your DSPy program \u2014 end to end. /Senior Software Engineer"}
{"session_id": "talking-all-your-data-building-multi-agent-systems-structured-and", "title": "Talking to All Your Data: Building Multi-Agent Systems for Structured and Unstructured Information", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how to build sophisticated systems that enable natural language interactions with both your structured databases and unstructured document collections. This session explores advanced techniques for creating unified and governed AI systems that can seamlessly interpret questions, retrieve relevant information and generate accurate answers across your entire data ecosystem. Key takeaways include: /Staff Product Manager"}
{"session_id": "taming-llm-wild-west-unified-approach-genai-governance", "title": "Taming the LLM Wild West: A Unified Approach to GenAI Governance", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Whether you're using OpenAI, Anthropic or open-source models like Meta Llama, the Mosaic AI Gateway is the central control plane across any AI model or agent. Learn how you can streamline access controls, enforce guardrails for compliance, ensure an audit trail and monitor costs across providers \u2014 without slowing down innovation. Lastly, we\u2019ll dive even deeper into how AI Gateway works with Unity Catalog to deliver a full governance story for your end-to-end AI agents across models, tools and data. Key takeaways: /Product Manager\nDatabricks /Senior Staff Software Engineer"}
{"session_id": "three-big-unlocks-ai-interoperability-databricks", "title": "Three Big Unlocks to AI Interoperability With Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Vice President, Expedia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The ability for different AI systems to collaborate is more critical than ever. From traditional ML development to fine-tuning GenAI models, Databricks delivers the stability, cost-optimization and productivity Expedia Group (EG) needs. Learn how to unlock the full potential of AI interoperability with Databricks. Join Shiyi Pickrell to understand the future of AI interoperability, how it\u2019s generating business value and driving the next generation of travel AI-powered experiences. /Senior Vice President"}
{"session_id": "traditional-ml-scale-implementing-classical-techniques-databricks", "title": "Traditional ML at Scale: Implementing Classical Techniques With Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Staff Technical Marketing, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling to implement traditional machine learning models that deliver real business value? Join us for a hands-on exploration of classical ML techniques powered by Databricks' Mosaic AI platform. This session focuses on time-tested approaches like regression, classification and clustering \u2014 showing how these foundational methods can solve real business problems when combined with Databricks' scalable infrastructure and MLOps capabilities. Key takeaways: /AI/ML Product Mgmt\nDatabricks /Staff Technical Marketing"}
{"session_id": "transforming-title-insurance-databricks-batch-inference", "title": "Transforming Title Insurance With Databricks Batch Inference", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Sr Director AI, First American"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we explore how First American Data & Analytics, a leading property-centric information provider, revolutionized its data extraction processes using batch inference on the Databricks Platform. Discover how it overcame the challenges of extracting data from millions of historical title policy images and reduced project timelines by 75%. Learn how First American optimized its data processing capabilities, reduced costs by 70% and enhanced the efficiency of its title insurance processes, ultimately improving the home-buying experience for buyers, sellers and lenders. This session will delve into the strategic integration of AI technologies, highlighting the power of collaboration and innovation in transforming complex data challenges into scalable solutions. /VP, Data and AI\nFirst American Financial Corporation /Sr Director AI"}
{"session_id": "unlock-agentic-ai-insurance-deloitte-databricks", "title": "Unlock Agentic AI for Insurance With Deloitte & Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["AI & Data Managing Director, Deloitte"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era where insights-driven decision-making is paramount, the insurance industry stands at the cusp of a major technological revolution. This session will delve into how Agentic AI \u2014 AI agents act autonomously to achieve critical goals \u2014 can be leveraged to transform insurance operation (underwriting, claims, services), enhance customer experiences and drive strategic growth. /AI & Data Managing Director"}
{"session_id": "unlocking-quality-scale-and-cost-efficient-retrieval-mosaic-ai-vector", "title": "Unlocking Quality, Scale and Cost-Efficient Retrieval With Mosaic AI Vector Search", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Mosaic AI Vector Search is powering high-accuracy retrieval systems in production across a wide range of use cases \u2014 including RAG applications, entity resolution, recommendation systems and search. Fully integrated with the Databricks Data Intelligence Platform, it eliminates pipeline maintenance by automatically syncing data from source to index. Over the past year, customers have asked for greater scale, better quality out-of-the-box and cost-efficient performance. This session delivers on those needs \u2014 showcasing best practices for implementing high-quality retrieval systems and revealing major product advancements that improve scalability, efficiency and relevance. What you\u2019ll learn: /Senior Software Engineer\nDatabricks /Product Manager"}
{"session_id": "use-external-models-databricks-connecting-azure-aws-gcp-anthropic-and", "title": "Use External Models in Databricks: Connecting to Azure, AWS, GCP, Anthropic and More", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session you will learn how to leverage a wide set of GenAI models in Databricks, including external connections to cloud vendors and other model providers. We will cover establishing connection to externally served models, via Mosaic AI Gateway. This will showcase connection to Azure, AWS & GCP models, as well as model vendors like Anthropic, Cohere, AI21 Labs and more. You will also discover best practices on model comparison, governance and cost control on those model deployments. /Product Manager"}
{"session_id": "using-ai-runtimes-model-training-and-development-databricks", "title": "Using AI Runtimes for Model Training and Development on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Tired of managing complex GPU infrastructure for AI projects? Join us to discover how Databricks' AI Runtime simplifies GPU-accelerated model development with a serverless experience. Learn how the Machine Learning Runtime provides access to pre-configured GPU environments with popular ML frameworks and optimizations for unmatched performance. Key takeaways: Whether training custom models or fine-tuning foundation models, learn to focus on building AI solutions rather than managing infrastructure. /Sr. Product Manager\nDatabricks /Databricks"}
{"session_id": "when-ai-helps-saving-lives", "title": "When AI Helps Saving Lives", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Laerdal Medical"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is set to revolutionize healthcare by enhancing diagnostic accuracy, optimising treatment protocols and improving response times in time-critical medical emergencies. As a world leader in this field, Laerdal Medical is continuously working to develop innovative solutions for training and equipment enabling healthcare workers and community responders to provide the best possible care. All to support our overall mission of \"Helping save one million more lives every year from 2030\". From the RevivR app, using AI to enable mobile-based and self-paced CPR training, to LiveBorn using ambient intelligence to reduce neonatal mortality and a new set of AI agents that will make sure we can reach our mission goal. In this presentation we will show how Laerdal is using AI and data to develop this new generation of innovative healthcare products, as well as how we work with partners and solutions like Databricks and MosaicML to make the development and operations of these AI tools efficient. /Laerdal Medical"}
