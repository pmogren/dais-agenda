{"session_id": "accelerating-analytics-integrating-bi-tools-databricks-sql", "title": "Accelerating Analytics: Integrating BI Tools to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Did you know that you can integrate with your favorite BI tools directly from Databricks SQL? You don\u2019t even need to stand up an additional warehouse. This session shows the integrations with Microsoft Power Platform, Power BI, Tableau, Sigma and Looker so you can have a seamless integration experience. Directly connect your Databricks workspace with Fabric and Power BI workspaces or Tableau to publish and sync data models, with defined primary and foreign keys, between the two platforms. /Sr. Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "accelerating-data-transformation-best-practices-governance-agility-and", "title": "Accelerating Data Transformation: Best Practices for Governance, Agility and Innovation", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Quality", "ELT", "SQL", "Scala"], "speakers": ["Practice Director, Data & AI, NCS Australia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share NCS\u2019s approach to implementing a Databricks Lakehouse architecture, focusing on key lessons learned and best practices from our recent implementations. By integrating Databricks SQL Warehouse, the DBT Transform framework and our innovative test automation framework, we\u2019ve optimized performance and scalability, while ensuring data quality. We\u2019ll dive into how Unity Catalog enabled robust data governance, empowering business units with self-serve analytical workspaces to create insights while maintaining control. Through the use of solution accelerators, rapid environment deployment and pattern-driven ELT frameworks, we\u2019ve fast-tracked time-to-value and fostered a culture of innovation. Attendees will gain valuable insights into accelerating data transformation, governance and scaling analytics with Databricks. /Practice Director, Data & AI"}
{"session_id": "ai-meets-sql-leverage-genai-scale-enrich-your-data", "title": "AI Meets SQL: Leverage GenAI at Scale to Enrich Your Data", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "SQL"], "speakers": ["swe, databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI into existing data workflows can be challenging, often requiring specialized knowledge and complex infrastructure. In this session, we'll share how SQL users can leverage AI/ML to access large language models (LLMs) and traditional machine learning directly from within SQL, simplifying the process of incorporating AI into data workflows. We will demonstrate how to use Databricks SQL for natural language processing, traditional machine learning, retrieval augmented generation and more. You'll learn about best practices and see examples of solving common use cases such as opinion mining, sentiment analysis, forecasting and other common AI/ML tasks. /Sr. Product Manager\nDatabricks /swe"}
{"session_id": "busting-data-modeling-myths-truths-and-best-practices-data-modeling", "title": "Busting Data Modeling Myths: Truths and Best Practices for Data Modeling in the Lakehouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Modeling", "Data Quality", "SQL", "Scala"], "speakers": ["Practice Lead, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the truth behind data modeling in Databricks. This session will tackle the top 10 myths surrounding relational and dimensional data modeling. Attendees will gain a clear understanding of what Databricks Lakehouse truly supports today, including how to leverage primary and foreign keys, identity columns for surrogate keys, column-level data quality constraints and much more. This session will talk through the lens of medallion architecture, explaining how to implement data models across bronze, silver, and gold tables. Whether you\u2019re migrating from a legacy warehouse or building new analytics solutions, you\u2019ll leave equipped to fully leverage Databricks\u2019 capabilities, and design scalable, high-performance data models for enterprise analytics. /DBSQL Product Specialist\nDatabricks /Practice Lead"}
{"session_id": "comprehensive-data-warehouse-migrations-databricks-sql", "title": "Comprehensive Data Warehouse Migrations to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse", "SQL"], "speakers": ["Lead Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks has a free, comprehensive solution for migrating legacy data warehouses from a wide range of source systems. See how we accelerate migrations from legacy data warehouses to Databricks SQL, achieving 50% faster migration than traditional methods. We'll cover the tool\u2019s automated migration process: This comprehensive approach increases the predictability of migration projects, allowing businesses to plan and execute migrations with greater confidence. /Sr Specialist Solutions Architect\nDatabricks /Lead Specialist Solutions Architect"}
{"session_id": "cooking-sql-ingredients-insights-minimal-prep", "title": "Cooking With SQL: From Ingredients to Insights With Minimal Prep", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Sr. Specialist Solutions Architect DWH, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session we\u2019ll dive into the SQL kitchen and use a combination of SQL staples and nouvelle cuisine such as recursive queries, temporary tables, and stored procedures. We\u2019ll leave you with well-scripted recipes to execute immediately or store for later consumption in your Unity Catalog. Think of this session as building your go-to cookbook of SQL techniques. Bon app\u00e9tit! /Principal Software Engineer\nDatabricks /Sr. Specialist Solutions Architect DWH"}
{"session_id": "cost-effective-data-architecture-and-ai-practice-databricks-funplus", "title": "Cost-Effective Data Architecture and AI Practice With Databricks at FunPlus", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering"], "speakers": ["Data Director, FunPlus"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "FunPlus's journey to building a cost-effective and efficient data platform with Databricks: exploring how FunPlus leveraged Databricks to tackle key challenges, enhance data engineering and ML efficiency, and showcasing best practices and their impact on game development and operations. /Data Director"}
{"session_id": "crafting-business-brilliance-leveraging-databricks-sql-next-gen", "title": "Crafting Business Brilliance: Leveraging Databricks SQL for Next-Gen Applications", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Software Engineering, Haleon"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Haleon, we've leveraged Databricks APIs and serverless compute to develop customer-facing applications for our business. This innovative solution enables us to efficiently deliver SAP invoice and order management data through front-end applications developed and served via our API Gateway. The Databricks lakehouse architecture has been instrumental in eliminating the friction associated with directly accessing SAP data from operational systems, while enhancing our performance capabilities. Our system acheived response times of less than 3 seconds from API call, with ongoing efforts to optimise this performance. This architecture not only streamlines our data and application ecosystem but also paves the way for integrating GenAI capabilities with robust governance measures for our future infrastructure. The implementation of this solution has yielded significant benefits, including a 15% reduction in customer service costs and a 28% increase in productivity for our customer support team. /Senior Solutions Architect\nDatabricks /Director of Software Engineering"}
{"session_id": "data-intelligence-marketing-forum-unlocking-future-marketing-ai", "title": "Data Intelligence for Marketing Forum: Unlocking the Future of Marketing With AI", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "180 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Marketing Solutions GTM, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don\u2019t miss the Data Intelligence for Marketing forum at this year\u2019s Databricks Data + AI Summit \u2014 designed for marketing leaders ready to transform with data and AI. Learn how Databricks unified its marketing data with lakehouse architecture, hear Deloitte\u2019s David Geisinger on the evolving CMO role in the GenAI era, and discover how Reckitt\u2019s Bastien Parizot built a cutting-edge GenAI platform to revolutionize marketing operations. Walk away with actionable strategies, real-world blueprints, and insights from top experts to drive agility, customer insights, and AI-powered marketing success. Prioritize this forum to lead your organization\u2019s data-driven future. /Marketing Solutions GTM"}
{"session_id": "elevate-sql-productivity-power-notebooks-and-sql-editor", "title": "Elevate SQL Productivity: The Power of Notebooks and SQL Editor", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["SQL"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Writing SQL is a core part of any data analyst\u2019s workflow, but small inefficiencies can add up, slowing down analysis and making it harder to iterate quickly. In this session, we\u2019ll explore our powerful features in the Databricks SQL editor and notebook that help you to be more productive when writing SQL on Databricks. We\u2019ll demo the new features and the customer use cases that inspired them. /Senior Product Manager"}
{"session_id": "empowering-healthcare-insights-unified-lakehouse-approach-databricks", "title": "Empowering Healthcare Insights: A Unified Lakehouse Approach With Databricks", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATA MARKETPLACE", "DELTA LAKE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Scala"], "speakers": ["Databricks Solution Architect Champion, BJSS"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "NHS England is revolutionizing healthcare research by enabling secure, seamless access to de-identified patient data through the Federated Data Platform (FDP). Despite vast data resources spread across regional and national systems, analysts struggle with fragmented, inconsistent datasets. Enter Databricks: powering a unified, virtual data lake with Unity Catalog at its core \u2014 integrating diverse NHS systems while ensuring compliance and security. By bridging AWS and Azure environments with a private exchange and leveraging the Iceberg connector to interface with Palantir, analysts gain scalable, reliable and governed access to vital healthcare data. This talk explores how this innovative architecture is driving actionable insights, accelerating research and ultimately improving patient outcomes. /Specialist Solutions Architect\nDatabricks /Databricks Solution Architect Champion"}
{"session_id": "enterprise-cost-management-data-warehousing-databricks-sql", "title": "Enterprise Cost Management for Data Warehousing with Databricks SQL", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Software Engineer\nDatabricks /Product Manager"}
{"session_id": "geo-powering-insights-art-spatial-data-integration-and-visualization", "title": "Geo-Powering Insights: The Art of Spatial Data Integration and Visualization", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we will explore how to leverage Databricks' SQL engine to efficiently ingest and transform geospatial data. We'll demonstrate the seamless process of connecting to external systems such as ArcGIS to retrieve datasets, showcasing the platform's versatility in handling diverse data sources. We'll then delve into the power of Databricks Apps, illustrating how you can create custom geospatial dashboards using various frameworks like Streamlit and Flask, or any framework of your choice. This flexibility allows you to tailor your visualizations to your specific needs and preferences. Furthermore, we'll highlight the Databricks Lakehouse's integration capabilities with popular dashboarding tools such as Tableau and Power BI. This integration enables you to combine the robust data processing power of Databricks with the advanced visualization features of these specialized tools. /Specialist Solutions Architect"}
{"session_id": "geospatial-insights-databricks-sql-techniques-and-applications", "title": "Geospatial Insights With Databricks SQL: Techniques and Applications", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Lead Geospatial Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager\nDatabricks /Lead Geospatial Product Specialist"}
{"session_id": "how-migrate-oracle-databricks-sql", "title": "How to Migrate From Oracle to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Migrating your legacy Oracle data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. In this session, learn the top strategies for completing this data migration. We will cover data type conversion, basic to complex code conversions, validation and reconciliation best practices. Discover the pros and cons of using CSV files to PySpark or using pipelines to Databricks tables. See before-and-after architectures of customers who have migrated, and learn about the benefits they realized. /Sr. Specialist Solutions Architect"}
{"session_id": "how-migrate-snowflake-databricks-sql", "title": "How to Migrate From Snowflake to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse", "ELT", "SQL"], "speakers": ["DBSQL Adoption Lead, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Migrating your Snowflake data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. Though a cloud platform-to-cloud platform migration should be relatively easy, the breadth of the Databricks Platform provides flexibility and hence requires careful planning and execution. In this session, we present the migration methodology, technical approaches, automation tools, product/feature mapping, a technical demo and best practices using real-world case studies for migrating data, ELT pipelines and warehouses from Snowflake to Databricks. /DBSQL Adoption Lead"}
{"session_id": "how-migrate-teradata-databricks-sql", "title": "How to Migrate from Teradata to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": ["Sr. Specialist Solutions Architect DWH, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Storage and processing costs of your legacy Teradata data warehouses impact your ability to deliver. Migrating your legacy Teradata data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. In this session, learn the top strategies for completing this data migration. We will cover data type conversion, basic to complex code conversions, validation and reconciliation best practices. How to use Databricks natively hosted LLMs to assist with migration activities. See before-and-after architectures of customers who have migrated, and learn about the benefits they realized. /SSA\nDatabricks /Sr. Specialist Solutions Architect DWH"}
{"session_id": "hps-data-platform-migration-journey-redshift-lakehouse", "title": "HP's Data Platform Migration Journey: Redshift to Lakehouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ETL", "SQL", "Scala"], "speakers": ["Data Engineer, HP Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "HP Print's data platform team took on a migration from a monolithic, shared resource of AWS Redshift, to a modular and scalable data ecosystem on Databricks lakehouse. The result was 30\u201340% cost savings, scalable and isolated resources for different data consumers and ETL workloads, and performance optimization for a variety of query types. Through this migration, there were technical challenges and learnings relating to the ETL migrations with DBT, new Databricks features like Liquid Clustering, predictive optimization, Photon, SQL serverless warehouses, managing multiple teams on Unity Catalog, and others. This presentation dives into both the business and technical sides of this migration. Come along as we share our key takeaways from this journey. /Data Engineer\nHP Inc. /Data Engineer"}
{"session_id": "improving-user-experience-and-efficiency-using-dbsql", "title": "Improving User Experience and Efficiency Using DBSQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL", "Scala"], "speakers": ["Data Platform Manager, PicPay"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "To scale Databricks SQL to 2,000 users efficiently and cost-effectively, we adopted serverless, ensuring dynamic scalability and resource optimization. During peak times, resources scale up automatically; during low demand, they scale down, preventing waste. Additionally, we implemented a strong content governance model. We created continuous monitoring to assess query and dashboard performance, notifying users about adjustments and ensuring only relevant content remains active. If a query exceeds time or impact limits, access is reviewed and, if necessary, deactivated. This approach brought greater efficiency, cost reduction and an improved user experience, keeping the platform well-organized and high-performing. /Data Platform Manager"}
{"session_id": "introduction-databricks-sql", "title": "Introduction to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Product Management, Databricks, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Technical Product Marketing Engineer\nDatabricks /Director, Product Management, Databricks"}
{"session_id": "japanese-mega-banks-journey-modern-genai-powered-governed-data-platform", "title": "A Japanese Mega-Bank\u2019s Journey to a Modern, GenAI-Powered, Governed Data Platform", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal, Deloitte Consulting LLP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "SMBC-AD, a major Japanese multinational financial services institution, has embarked on an initiative to build a GenAI-powered, modern and well-governed cloud data platform on Azure/Databricks. This initiative aims to build an enterprise data foundation encompassing loans, deposits, securities, derivatives, and other data domains. Its primary goals are: Deloitte and SMBC leveraged the Brickbuilder asset \u201cData as a Service for Banking\u201d to accelerate this highly strategic transformation. /Principal"}
{"session_id": "jll-training-and-upskill-program-our-warehouse-migration-databricks", "title": "The JLL Training and Upskill Program for Our Warehouse Migration to Databricks", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Global Technology Director, JLL"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Odyssey is JLL\u2019s bespoke training program designed to upskill and prepare data professionals for a new world of data lakehouse. Based on the concepts of learn, practice and certify, participants earn points, moving through five levels by completing activities with business application of Databricks key features. Databricks Odyssey facilitates cloud data warehousing migration by providing best practice frameworks, ensuring efficient use of pay-per-compute platforms. JLL/T Insights and Data fosters a data culture through learning programs that develop in-house talent and create career pathways. Databricks Odyssey offers: Benefits include: /Global Technology Director"}
{"session_id": "migrating-legacy-sas-code-databricks-lakehouse-what-we-learned-along", "title": "Migrating Legacy SAS Code to Databricks Lakehouse: What We Learned Along the Way", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Warehouse"], "speakers": ["Senior Data Platforms Developer, PacificSource Health Plans"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In PacificSource Health Plans, a health insurance company in the US, we are on a successful multi-year journey to migrate all of our data and analytics ecosystem to Databricks Enterprise Data Warehouse (lakehouse). A particular obstacle on this journey was a reporting data mart which relied on copious amounts of legacy SAS code that applied sophisticated business logic transformations for membership, claims, premiums and reserves. This core data mart was driving many of our critical reports and analytics. In this session we will share the unique and somewhat unexpected challenges and complexities we encountered in migrating this legacy SAS code. How our partner (T1A) leveraged automation technology (Alchemist) and some unique approaches to reverse engineer (analyze), instrument, translate, migrate, validate and reconcile these jobs; and what lessons we learned and carried from this migration effort. /Principal Architect\nTier One Analytics Inc. /Senior Data Platforms Developer"}
{"session_id": "multi-statement-transactions-how-improve-data-consistency-and", "title": "Multi-Statement Transactions: How to Improve Data Consistency and Performance", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Principal Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Multi-statement transactions bring the atomicity and reliability of traditional databases to modern data warehousing on the lakehouse. In this session, we\u2019ll explore real-world patterns enabled by multi-statement transactions \u2014 including multi-table updates, deduplication pipelines and audit logging \u2014 and show how Databricks ensures atomicity and consistency across complex workflows. We\u2019ll also dive into demos and share tips to getting started and migrations with this feature in Databricks SQL. /Principal Solutions Architect"}
{"session_id": "our-journey-operations-excellence-finops-observability", "title": "Our Journey to Operations Excellence (FinOps, Observability)", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["WBD"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data platforms scale, managing costs and ensuring system reliability become increasingly complex. Achieving operational excellence requires a strategic approach to FinOps (Cloud Cost Optimization) and Observability (End-to-End Monitoring). In this session, we\u2019ll share our journey in: /Sr.Manager, Software Engineering\nWarnerBros Discovery /WBD"}
{"session_id": "performance-best-practices-fast-queries-high-concurrency-and-scaling", "title": "Performance Best Practices for Fast Queries, High Concurrency, and Scaling on Databricks SQL", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data warehousing in enterprise and mission-critical environments needs special consideration for price/performance and security. This session will explain how Databricks SQL addresses the most challenging requirements for high-concurrency, low-latency performance and managing user identity, access and governance at scale. We will also cover the latest advancements in resource-based scheduling, autoscaling and caching enhancements that allow for seamless performance and workload management. /Product Manager\nDatabricks /Principal Software Engineer"}
{"session_id": "pushing-limits-what-your-warehouse-can-do-using-python-and-databricks", "title": "Pushing the Limits of What Your Warehouse Can Do Using Python and Databricks", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager"}
{"session_id": "revolutionizing-banking-data-analytics-and-ai-building-enterprise-data", "title": "Revolutionizing Banking Data, Analytics and AI: Building an Enterprise Data Hub With Databricks", "track": "DATA WAREHOUSING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Chief Information Officer, First Horizon Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore the transformative journey of a regional bank as it modernizes its enterprise data infrastructure amidst the challenges of legacy systems and past mergers and acquisitions. The bank is creating an Enterprise Data Hub using Deloitte's industry experience and the Databricks Data Intelligence Platform to drive growth, efficiency and Large Financial Institution readiness needs. This session will showcase how the new data hub will be a one-stop-shop for LOB and enterprise needs, while unlocking the advanced analytics and GenAI possibilities. Discover how this initiative is going to empower the ambitions of a regional bank to realize their \u201cbig bank muscle, small bank hustle.\u201d /Principal\nDeloitte /Chief Information Officer"}
{"session_id": "revolutionizing-pepsico-bi-capabilities-traditional-bi-next-gen", "title": "Revolutionizing PepsiCo BI Capabilities: From Traditional BI to Next-Gen Analytics Powerhouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Director, Data Services, PepsiCo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will provide an in-depth overview of how PepsiCo, a global leader in food and beverage, transformed its outdated data platform into a modern, unified and centralized data and AI-enabled platform using the Databricks SQL serverless environment. Through three distinct implementations that transpired at PepsiCo in 2024, we will demonstrate how the PepsiCo Data Analytics & AI Group unlocked pivotal capabilities that facilitated the delivery of diverse data-driven insights to the business, reduced operational expenses and enhanced overall performance through the newly implemented platform. /Lead Global D&AI Solution Architect\nPepsiCo Inc. /Director, Data Services"}
{"session_id": "self-service-assortment-and-space-analytics-walmart-scale", "title": "Self-Service Assortment and Space Analytics at Walmart Scale", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Integration"], "speakers": ["Sr. Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Assortment and space analytics optimizes product selection and shelf allocation to boost sales, improve inventory management and enhance customer experience. However, challenges like evolving demand, data accuracy and operational alignment hinder success. Older approaches struggled due to siloed tools, slow performance and poor governance. Databricks unified platform resolved these issues, enabling seamless data integration, high-performance analytics and governed sharing. The innovative AI/BI Genie interface empowered self-service analytics, driving non-technical user adoption. This solution helped Walmart cut time to value by 90% and saved $5.6M annually in FTE hours leading to increased productivity. Looking ahead, AI agents will let store managers and merchants execute decisions via conversational interfaces, streamlining operations and enhancing accessibility. This transformation positions retailers to thrive in a competitive, customer-centric market. /Senior Manager, Assortment & Space\nWalmart /Sr. Delivery Solutions Architect"}
{"session_id": "smashing-silos-shaping-future-data-all-next-gen-ecosystem", "title": "Smashing Silos, Shaping the Future: Data for All in the Next-Gen Ecosystem", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Director, Core Data, Rivian Volkswagen Group Technology"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A successful data strategy requires the right platform and the ability to empower the broader user community by creating simple, scalable and secure patterns that lower the barrier to entry while ensuring robust data practices. Guided by the belief that everyone is a data person, we focus on breaking down silos, democratizing access and enabling distributed teams to contribute through a federated \"data-as-a-product\" model. We\u2019ll share the impact and lessons learned in creating a single source of truth on Unity Catalog, consolidated from diverse sources and cloud platforms. We\u2019ll discuss how we streamlined governance with Databricks Apps, Workflows and native capabilities, ensuring compliance without hindering innovation. We\u2019ll also cover how we maximize the value of that catalog by leveraging semantics to enable trustworthy, AI-driven self-service in AI/BI dashboards and downstream apps. Come learn how we built a next-gen data ecosystem that empowers everyone to be a data person. /Director, Core Data"}
{"session_id": "spark-right-sizing-secret-saving-millions-dollars-linkedin", "title": "Spark Right-Sizing: The Secret to Saving Millions of Dollars at LinkedIn", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Senior Software Engineer, LinkedIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At LinkedIn, we manage over 400,000 daily Spark applications consuming 200+ PBHrs of compute daily. To address the challenges posed by manual configuration of Spark's memory tuning options, which led to low memory utilization and frequent OOM errors, we developed an automated Spark executor memory right-sizing system. Our approach, utilizing a policy-based system with nearline and real-time feedback loops, automates memory tuning, leading to more efficient resource allocation, improved user productivity and increased job reliability. By leveraging historical data and real-time error classification, we dynamically adjust memory, significantly narrowing the gap between allocated and utilized resources while reducing failures. This initiative has achieved a 13% increase in memory utilization and a 90% drop in OOM-related job failures, saving us 1000s of PBHrs of compute every year. /Senior Software Engineer"}
{"session_id": "sponsored-capital-one-software-how-capital-one-balances-lower-cost-and", "title": "Sponsored by: Capital One Software | How Capital One Balances Lower Cost and Peak Performance in Databricks", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Companies need a lot of data to build and deploy AI models\u2014and they want it quickly. To meet this demand, platform teams are quickly scaling their Databricks usage, resulting in excess cost driven by inefficiencies and performance anomalies. Capital One has over 4,000 users leveraging Databricks to power advanced analytics and machine learning capabilities at scale. In this talk, we\u2019ll share lessons learned from optimizing our own Databricks usage while balancing lower cost with peak performance. Attendees will learn how to identify top sources of waste, best practices for cluster management, tips for user governance and methods to keep costs in check."}
{"session_id": "sql-based-etl-options-sql-only-databricks-development", "title": "SQL-Based ETL: Options for SQL-Only Databricks Development", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "ELT", "ETL", "SQL"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Using SQL for data transformation is a powerful way for an analytics team to create their own data pipelines. However, relying on SQL often comes with tradeoffs such as limited functionality, hard-to-maintain stored procedures or skipping best practices like version control and data tests. Databricks supports building high-performing SQL ETL workloads. Attend this session to hear how Databricks supports SQL for data transformation jobs as a core part of your Data Intelligence Platform. In this session we will cover 4 options to use Databricks with SQL syntax to create Delta tables: /Sr. Specialist Solutions Architect"}
{"session_id": "traditional-mdm-dead-how-next-generation-data-products-are-winning", "title": "Traditional MDM is Dead. How Next-Generation Data Products are Winning the Enterprise", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "LAKEFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Global Head of Data Management, Quantexa"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Global Head of Data Management"}
{"session_id": "unifying-data-delivery-using-databricks-your-enterprise-serving-layer", "title": "Unifying Data Delivery: Using Databricks as Your Enterprise Serving Layer", "track": "DATA WAREHOUSING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "ELT", "SQL", "Scala"], "speakers": ["Data Platform Architect, The World Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will take you on our journey of integrating Databricks as the core serving layer in a large enterprise, demonstrating how you can build a unified data platform that meets diverse business needs. We will walk through the steps for constructing a central serving layer by leveraging Databricks\u2019 SQL Warehouse to efficiently deliver data to analytics tools and downstream applications. To tackle low latency requirements, we\u2019ll show you how to incorporate an interim scalable relational database layer that delivers sub-second performance for hot data scenarios. Additionally, we\u2019ll explore how Delta Sharing enables secure and cost-effective data distribution beyond your organization, eliminating silos and unnecessary duplication for a truly end-to-end centralized solution. This session is perfect for data architects, engineers and decision-makers looking to unlock the full potential of Databricks as a centralized serving hub. /Data Platform Architect\nThe World Bank /Data Platform Architect"}
{"session_id": "whats-new-databricks-sql-latest-features-and-live-demos", "title": "What\u2019s New in Databricks SQL: Latest Features and Live Demos", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager\nDatabricks /Sr Staff Product Manager"}
