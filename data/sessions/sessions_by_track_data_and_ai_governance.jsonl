{"session_id": "adobes-security-lakehouse-ocsf-data-efficiency-and-threat-detection", "title": "Adobe\u2019s Security Lakehouse: OCSF, Data Efficiency and Threat Detection at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Security", "Real-time", "Scala"], "speakers": ["AntiMatter"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore how Adobe uses a sophisticated data security architecture built on the Databricks Data Intelligence Platform, along with the Open Cybersecurity Schema Framework (OCSF), to enable scalable, real-time threat detection across more than 10 PB of security data. We\u2019ll compare different approaches to OCSF implementation and demonstrate how Adobe processes massive security datasets efficiently \u2014 reducing query times by 18%, maintaining 99.4% SLA compliance, and supporting 286 security users across 17 teams with over 4,500 daily queries. By using Databricks' Platform for serverless compute, scalable architecture, and LLM-powered recommendations, Adobe has significantly improved processing speed and efficiency, resulting in substantial cost savings. We\u2019ll also highlight how OCSF enables advanced cross-tool analytics and automation, streamlining investigations. Finally, we\u2019ll introduce Databricks\u2019 new open-source OCSF toolkit for scalable security data normalization and invite the community to contribute. /Sr. Manager, Security Software Engineering\nAdobe /AntiMatter"}
{"session_id": "ai-governance-journey-securing-data-models-and-agents", "title": "The AI Governance Journey: Securing Data, Models and Agents", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores how to centrally govern the entire AI lifecycle on Databricks \u2014 from experimenting with models to deploying production-grade agents. It will cover managing access to AI models with Mosaic AI Gateway, applying usage tracking, auditing, rate limits and AI guardrails for safe and compliant usage. It'll also cover how Unity Catalog integrates seamlessly throughout the process, enabling lineage tracking, granular access controls and the secure monitoring of model performance and data quality. /Product Manager"}
{"session_id": "ai-powered-data-discovery-and-curation-unity-catalog", "title": "AI-Powered Data Discovery and Curation With Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data landscape, the challenge isn\u2019t just storing or processing data \u2014 it\u2019s enabling every user, from data stewards to analysts, to find and trust the right data, fast. This session explores how Databricks is reimagining data discovery with the new Discover Page Experience \u2014 an intuitive, curated interface showcasing key data and workspace assets. We\u2019ll dive into AI-assisted governance and AI-powered discovery features like AI-generated metadata, AI-assisted lineage and natural language data exploration in Unity Catalog. Plus, see how new certifications and deprecations bring clarity to complex data environments. Whether you\u2019re a data steward highlighting trusted assets or an analyst navigating data without deep schema knowledge, this session will show how Databricks is making data discovery seamless for everyone. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "ai-powered-marketing-data-management-solving-dirty-data-problem", "title": "AI-Powered Marketing Data Management: Solving the Dirty Data Problem with Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["VP, Cloud Product Management & UX, Acxiom"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing teams struggle with \u2018dirty data\u2019 \u2014 incomplete, inconsistent, and inaccurate information that limits campaign effectiveness and reduces the accuracy of AI agents. Our AI-powered marketing data management platform, built on Databricks, solves this with anomaly detection, ML-driven transformations and the built-in Acxiom Referential Real ID Graph with Data Hygiene. We\u2019ll showcase how Delta Lake, Unity Catalog and DLT power our multi-tenant architecture, enabling secure governance and 75% faster data processing. Our privacy-first design ensures compliance with GDPR, CCPA and HIPAA through role-based access, encryption key management and fine-grained data controls. Join us for a live demo and Q&A, where we\u2019ll share real-world results and lessons learned in building a scalable, AI-driven marketing data solution with Databricks. /Principal Product Manager\nAcxiom /VP, Cloud Product Management & UX"}
{"session_id": "building-reliable-agentic-ai-databricks", "title": "Building Reliable Agentic AI on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["CEO & Co-Founder, Monte Carlo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/CEO & Co-Founder"}
{"session_id": "building-responsible-ai-agents-databricks", "title": "Building Responsible AI Agents on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Delivery Solutions Architect (DSA) FINS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation explores how Databricks' Data Intelligence Platform supports the development and deployment of responsible AI in credit decisioning, ensuring fairness, transparency and regulatory compliance. Key areas include bias and fairness monitoring using Lakehouse Monitoring to track demographic metrics and automated alerts for fairness thresholds. Transparency and explainability are enhanced through the Mosaic AI Agent Framework, SHAP values and LIME for feature importance auditing. Regulatory alignment is achieved via Unity Catalog for data lineage and AIBI dashboards for compliance monitoring. Additionally, LLM reliability and security are ensured through AI guardrails and synthetic datasets to validate model outputs and prevent discriminatory patterns. The platform integrates real-time SME and user feedback via Databricks Apps and AI/BI Genie Space. /Senior Resident Solutions Architect\nDatabricks /Delivery Solutions Architect (DSA) FINS"}
{"session_id": "building-responsible-and-resilient-ai-databricks-ai-governance", "title": "Building Responsible and Resilient AI: The Databricks AI Governance Framework", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Sr. Director, Field Security, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "GenAI & machine learning are reshaping industries, driving innovation and redefining business strategies. As organizations embrace these technologies, they face significant challenges in managing AI initiatives effectively, such as balancing innovation with ethical integrity, operational resilience and regulatory compliance. This presentation introduces the Databricks AI Governance Framework (DAGF), a practical framework designed to empower organizations to navigate the complexities of AI. It provides strategies for building scalable, responsible AI programs that deliver measurable value, foster innovation and achieve long-term success. By examining the framework's five foundational pillars \u2014 AI organization, ethics, legal and regulatory compliance, transparency and interpretability, AI operations and infrastructure and AI security \u2014 this session highlights how AI governance aligns programs with the organization's strategic goals, mitigates risks and builds trust across stakeholders. /Senior Specialist Solution Architect\nDatabricks /Sr. Director, Field Security"}
{"session_id": "building-trustworthy-ai-northwestern-mutual-guardrail-technologies-and", "title": "Building Trustworthy AI at Northwestern Mutual: Guardrail Technologies and Strategies", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science"], "speakers": ["Data Science, Northwestern Mutual"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Science"}
{"session_id": "comprehensive-data-management-and-governance-azure-data-lake-storage", "title": "Comprehensive Data Management and Governance With Azure Data Lake Storage", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake"], "speakers": ["Principal PDM Manager, Microsoft Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Given that data is the new oil, it must be treated as such. Organizations that pursue greater insight into their businesses and their customers must manage and govern the use of the data that drives these insights in an efficient, cost-effective, compliant and auditable manner without degrading access to that data. Data catalogs provide an end-user experience to describe the management and governance activities associated with tables of data. However, without those data catalogs being integrated into the storage service that holds that data, there is a disconnection that undermines an organization's data management and governance posture. Azure Data Lake Storage is excited to announce Project Metalake (name: TBD) that will allow customers to attach their own data catalog (including Unity Catalog) to their ADLS account and have those management and governance policies natively and efficiently implemented by ADLS. /Principal Program Manager\nMicrosoft /Principal PDM Manager"}
{"session_id": "cost-management-foundations-first-100-days-checklist", "title": "Cost Management Foundations: The First 100 Days Checklist", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session you'll learn how to onboard to Databricks in a way that ensures you can effectively measure and manage ROI of using Databricks down the line. We will show you how to set-up workspaces and compute, decide on a tagging strategy and how to utilize policies to enforce best practices and make future you a happy camper. /Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "data-intelligence-cybersecurity-forum-insights-sap-anvilogic-capital", "title": "Data Intelligence for Cybersecurity Forum: Insights From SAP, Anvilogic, Capital One, and Wiz", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "MOSAIC AI", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Delta Lake", "ELT", "Scala", "Streaming"], "speakers": ["VP of Product Marketing, Wiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join cybersecurity leaders from SAP, Anvilogic, Capital One, Wiz, and Databricks to explore how modern data intelligence is transforming security operations. Discover how SAP adopted a modular, AI-powered detection engineering lifecycle using Anvilogic on Databricks. Learn how Capital One built a detection and correlation engine leveraging Delta Lake, Apache Spark Streaming, and Databricks to process millions of cybersecurity events per second. Finally, see how Wiz and Databricks\u2019 partnership enhances cloud security with seamless threat visibility. Through expert insights and live demos, gain strategies to build scalable, efficient cybersecurity powered by data and AI. /Chief Security Architect\nSAP /VP of Product\nAnvilogic /Director, Software Engineering\nCapital one /VP of Product Marketing"}
{"session_id": "data-intelligence-unity-catalog-managed-tables-powered-predictive", "title": "Data Intelligence on Unity Catalog Managed Tables Powered by Predictive Optimization", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "databricks-best-practices-mitigate-ai-security-risks", "title": "Databricks Best Practices to Mitigate AI Security Risks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is transforming industries, enhancing customer experiences and automating decisions. As organizations integrate AI into core operations, robust security is essential. The Databricks Security team collaborated with top cybersecurity researchers from OWASP, Gartner, NIST, HITRUST and Fortune 100 companies to evolve the Databricks AI Security Framework (DASF) to version 2.0. In this session, we\u2019ll cover an AI security architecture using Unity Catalog, MLflow, egress controls, and AI gateway. Learn how security teams, AI practitioners and data engineers can secure AI applications on Databricks. Walk away with:\u2022 A reference architecture for securing AI applications\u2022 A worksheet with AI risks and controls mapped to industry standards like MITRE, OWASP, NIST and HITRUST\u2022 A DASF AI assistant tool to test your AI security /Principal Staff Security Field Engineer\nDatabricks /Senior Staff Product Manager"}
{"session_id": "databricks-observability-using-system-tables-monitor-and-manage-your", "title": "Databricks Observability: Using System Tables to Monitor and Manage Your Databricks Instance", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager, Billing\nDatabricks /Product Manager"}
{"session_id": "dealing-sensitive-data-databricks-natura", "title": "Dealing With Sensitive Data on Databricks at Natura", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Data Architect, Natura"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Architect"}
{"session_id": "demystifying-upgrading-unity-catalog-challenges-design-and-execution", "title": "Demystifying Upgrading to Unity Catalog \u2014 Challenges, Design and Execution", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["CEO & Co-Founder, Celebal Technologies"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Unity Catalog (UC) is the industry\u2019s only unified and open governance solution for data and AI, built into the Databricks Data Intelligence Platform. UC provides a single source of truth for organization\u2019s data and AI, providing open connectivity to any data source, any format, lineage, monitoring and support for open sharing and collaboration. In this session we will discuss the challenges in upgrading to UC from your existing databricks Non-UC set up. We will discuss a few customer use cases and how we overcame difficulties and created a repeatable pattern and reusable assets to replicate the success of upgrading to UC across some of the largest databricks customers. It is co-presented with our partner Celebal Technologies. /Lead Specialist Solutions Architect\nDatabricks /CEO & Co-Founder"}
{"session_id": "deploying-unity-catalog-oss-kubernetes-simplifying-infrastructure", "title": "Deploying Unity Catalog OSS on Kubernetes: Simplifying Infrastructure Management", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Developer, Nebius"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Software Developer"}
{"session_id": "driving-secure-ai-innovation-obsidian-security-databricks-and", "title": "Driving Secure AI Innovation with Obsidian Security, Databricks, and PointGuard AI", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-Founder, PointGuard AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As enterprises adopt AI and Large Language Models (LLMs), securing and governing these models\u2014and the data used to train them\u2014is essential. In this session, learn how the Databricks AI Security Framework helps organizations like PointGuard AI manage AI-specific risks, ensuring security, compliance, and governance across the entire AI lifecycle. Then, discover how Obsidian Security provides a robust approach to AI security, enabling organizations to confidently scale AI applications. /Sr. Specialist Solutions Architect\nDatabricks /CISO\nObsidian Security /Co-Founder"}
{"session_id": "elevating-data-quality-standards-databricks-dqx", "title": "Elevating Data Quality Standards With Databricks DQX", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Batch Processing", "Data Quality", "Python", "Real-time", "Streaming"], "speakers": ["RSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an introductory session on Databricks DQX, a Python-based framework designed to validate the quality of PySpark DataFrames. Discover how DQX can empower you to proactively tackle data quality challenges, enhance pipeline reliability and make more informed business decisions with confidence. Traditional data quality tools often fall short by providing limited, actionable insights, relying heavily on post-factum monitoring, and being restricted to batch processing. DQX overcomes these limitations by enabling real-time quality checks at the point of data entry, supporting both batch and streaming data validation and delivering granular insights at the row and column level. If you\u2019re seeking a simple yet powerful data quality framework that integrates seamlessly with Databricks, this session is for you. /Sr. Resident Solutions Architect\nDatabricks /RSA"}
{"session_id": "enhancing-efficiency-security-how-morgan-stanley-adopting-fully-managed", "title": "Enhancing Efficiency With Security: How Morgan Stanley is Adopting a Fully-Managed Lakehouse", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Distinguished Engineer, Morgan Stanley"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Morgan Stanley, a highly regulated financial institution, needs to meet stringent security and regulatory requirements around data storage and processing. Traditionally, this has necessitated maintaining control over data and compute within their own accounts with the associated management overhead. In this session, we will cover how Morgan Stanley has partnered with Databricks on a fully-managed compute and storage solution that allows them to meet their regulatory obligations with significantly reduced effort. This innovative approach enables rapid onboarding of new projects onto the platform, improving operational efficiency while maintaining the highest levels of security and compliance. /Senior Staff Product Manager\nDatabricks /Distinguished Engineer"}
{"session_id": "fine-grained-access-control-unstructured-data-volume-path-policies", "title": "Fine-Grained Access Control for Unstructured Data With Volume Path Policies", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unstructured data \u2014 images, documents, videos and more \u2014 is growing in importance with AI and ML. Yet managing access control at scale is challenging. Unity Catalog Volumes offer a secure foundation, but access control has remained volume-level \u2014 until now. This session introduces Volume Path Policies, a new feature enabling fine-grained access within volumes. Built on Unity Catalog\u2019s ABAC (Attribute-based Access Control), they let you define rules for users and groups based on path prefixes. We\u2019ll cover the governance model, share examples and demonstrate how to enforce least-privilege access. By the end, you\u2019ll know how to manage file-level access with Unity Catalog\u2019s flexibility and control. /Sr. Staff Product Manager\nDatabricks /Sr. Product Manager"}
{"session_id": "finops-scale-best-practices-cost-efficient-growth-databricks", "title": "FinOps at Scale: Best Practices for Cost-Efficient Growth on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "You\u2019ve seen your usage grow on Databricks, across departments, use cases, product lines and users. What can you do to ensure your end-users (data practitioners) of the platform remain cost-efficient and productive, while staying accountable to your budget? We\u2019ll discuss spend monitoring, chargeback models and developing a culture of cost efficiency by using Databricks tools. /Lead Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "franchise-ip-and-data-governance-krafton-driving-cost-efficiency-and", "title": "Franchise IP and Data Governance at Krafton: Driving Cost Efficiency and Scalability", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Pipeline", "Real-time", "Scala"], "speakers": ["KRAFTON"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we explore how KRAFTON optimized data governance for PUBG IP, enhancing cost efficiency and scalability. With over 300 million downloads in India and 700,000 MAU on PUBG IP, KRAFTON operates a massive data ecosystem, processing tens of terabytes daily. As real-time analytics demands increased, traditional Batch-based processing faced scalability challenges. To address this, we redesigned data pipelines and governance models, improving performance while reducing costs. Learn more: https://www.databricks.com/customers/krafton /KRAFTON"}
{"session_id": "got-metrics-build-metric-store-tour-developing-metrics-through-uc", "title": "Got Metrics? Build a Metric Store \u2014 A Tour of Developing Metrics Through UC Metric Views", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Business Intelligence"], "speakers": ["Sr. Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "I have metrics, you have metrics \u2014 we all have metrics. But the real problem isn\u2019t having metrics, it\u2019s that the numbers never line up, leading to endless cycles of reconciliation and confusion. Join us as we share how our Data Team at Databricks tackled this fundamental challenge in Business Intelligence by building an internal Metric Store \u2014 creating a single source of truth for all business metrics using the newly-launched UC Metric Views. Imagine a world where numbers always align, metric definitions are consistently applied across the organization and every metric comes with built-in ML-based forecasting, AI-powered anomaly detection and automatic explainability. That\u2019s the future we\u2019ve built \u2014 and we\u2019ll show you how you can get started today. /Staff Software Engineer\nDatabricks /Sr. Software Engineer"}
{"session_id": "graph-powered-observability-data-analysis-databricks-credential-vending", "title": "Graph-Powered Observability Data Analysis in Databricks With Credential Vending", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Engineering Manager, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Observability data \u2014 logs, metrics, and traces \u2014 captures the complex interactions within modern distributed systems. A graph query engine on top of Databricks enables complex traversal of massive observability data, helping users trace service dependencies, analyze upstream/downstream impacts, and uncover recurring error patterns, making it easier to diagnose issues and optimize system performance. A critical challenge in handling observability data is managing dynamic RBAC for the sensitive system telemetry. This session explains how Coinbase leverages credential vending, a method for issuing short-lived credentials to enable fine-grained, secure access to observability data stored in Databricks without long-lived secrets. Key takeaways: /Staff Software Engineer\nCoinbase /Senior Engineering Manager"}
{"session_id": "hipaa-without-headache-hinge-health-simple-phi-governance-fine-grain", "title": "HIPAA Without the Headache at Hinge Health: Simple PHI Governance With Fine Grain Access Control", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Manager, Engineering, Hinge Health"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Hinge Health faced challenges in hiring global data teams due to the complexities of PHI (Protected Health Information) governance. Unable to hire without compliant data sharing and unable to scale PHI governance without a larger team, we overcame this chicken or the egg challenge by adopting Unity Catalog's Fine-Grain Access Control. In this session, we will share our journey migrating to Unity Catalog, securing PHI with row filters/column masks, lessons learned and how our efforts surpassed our own expectations. This session equips data teams with strategies for HIPAA compliance without compromising flexibility and collaboration. Hinge Health is the leading digital MSK clinic, serving 11M+ members and 500+ employer health plans offering virtual physical therapy to reduce pain, surgeries and opioid use. /Sr. Solutions Architect\nDatabricks /Manager, Engineering"}
{"session_id": "how-arctic-wolf-modernizes-cloud-security-and-enhances-threat-detection", "title": "How Arctic Wolf Modernizes Cloud Security and Enhances Threat Detection with Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Delivery Solutions Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, you\u2019ll gain actionable insights to modernize your security operations and strengthen cyber resilience. Arctic Wolf will highlight how they eliminated data silos & enhanced their MDR pipeline to investigate suspicious threat actors for customers using Databricks. /Distinguished Data Architect\nArctic Wolf /Sr. Delivery Solutions Engineer"}
{"session_id": "how-corning-harnesses-unity-catalog-enhanced-finops-maturity-and-cost", "title": "How Corning Harnesses Unity Catalog for Enhanced FinOps Maturity and Cost Optimization", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Data Office, Corning"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We will explore how leveraging Databricks' Unity Catalog has accelerated our FinOps maturity, enabling us to optimize platform utilization and achieve significant cost reductions. By implementing Unity Catalog, we've gained comprehensive visibility and governance over our data assets, leading to more informed decision-making and efficient resource allocation. Learn how Corning discovered actionable insights and leveraged best practices on utilizing Unity Catalog to streamline data management, enhance financial operations and drive substantial savings within your organization. /Director, Data Office"}
{"session_id": "how-fedex-achieved-self-serve-analytics-and-data-democratization", "title": "How FedEx Achieved Self-Serve Analytics and Data Democratization on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Real-time"], "speakers": ["Product Manager, Fedex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "FedEx, a global leader in transportation and logistics, faced a common challenge in the era of big data: how to democratize data and foster data-driven decision making with thousands of data practitioners at FedEx wanting to build models, get real-time insights, explore enterprise data, and build enterprise-grade solutions to run the business. This breakout session will highlight how FedEx overcame challenges in data governance and security using Unity Catalog, ensuring that sensitive information remains protected while still allowing appropriate access across the organization. We'll share their approach to building intuitive self-service interfaces, including the use of natural-language processing to enable non-technical users to query data effortlessly. The tangible outcomes of this initiative are numerous, but chiefly: increased data literacy across the company, faster time-to-insight for business decisions, and significant cost-savings through improved operational efficiency. /Product Manager"}
{"session_id": "how-hms-federation-powered-nationwides-seamless-and-efficient-unity", "title": "How HMS Federation Powered Nationwide\u2019s Seamless and Efficient Unity Catalog Migration", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Scala"], "speakers": ["Lead Data Engineer, Nationwide"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk takes you through the Nationwide Security and Infrastructure data team's journey of migrating from HMS to UC. Discover how HMS federation simplified our transition to UC, allowing for an incremental migration that minimized disruption to data consumers while optimizing our data layout. We\u2019ll share the key technical decisions, challenges faced and lessons learned along the way. The migration process wasn\u2019t without its hurdles, so we\u2019ll walk you through our detailed, step-by-step approach covering planning, execution and validation. We will also showcase the benefits realized, such as improved data governance, more efficient data access and enhanced operational performance. Join us to gain practical insights into executing complex data migrations with a focus on security, flexibility and long-term scalability. /Lead Data Engineer"}
{"session_id": "how-navy-federals-enterprise-data-ecosystem-leverages-unity-catalog", "title": "How Navy Federal's Enterprise Data Ecosystem Leverages Unity Catalog for Data + AI Governance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Lake", "Data Pipeline", "Data Warehouse", "Machine Learning"], "speakers": ["AVP, Data Analytics Engineering, NFCU"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Navy Federal Credit Union has 200+ enterprise data sources in the enterprise data lake. These data assets are used for training 100+ machine learning models and hydrating a semantic layer for serving, at an average 4,000 business users daily across the credit union. The only option for extracting data from analytic semantic layer was to allow consuming application to access it via an already-overloaded cloud data warehouse. Visualizing data lineage for 1,000 + data pipelines and associated metadata is impossible and understanding the granular cost for running data pipelines is a challenge. Implementing Unity Catalog opened alternate path for accessing analytic semantic data from lake. It also opened the doors to remove duplicate data assets stored across multiple lakes which will save hundred thousands of dollars in data engineering efforts, compute and storage costs. /Field CTO\nDatabricks /AVP, Data Analytics Engineering"}
{"session_id": "how-nubank-improves-governance-security-and-user-experience-unity", "title": "How Nubank improves Governance, Security and User Experience with Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance"], "speakers": ["Engineering Manager, Nubank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Nubank, we successfully migrated to Unity Catalog, addressing the needs of our large-scale data environment with 3k active users, over 4k notebooks and jobs and 1.1 million tables, including sensitive PII data. Our primary objectives were to enhance data governance, security and user experience.Key points: This migration significantly improved our data governance capabilities, enhanced security measures and provided a more user-friendly experience for our large user base, ultimately leading to better control and utilization of our vast data resources. /Engineering Manager"}
{"session_id": "implementing-greenops-databricks-practical-guide-regulated-environments", "title": "Implementing GreenOps in Databricks: A Practical Guide for Regulated Environments", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Solution Architect, Blue Rocket (ABN Amro)"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us on a technical journey into GreenOps at ABN AMRO Bank using Databricks system tables. We'll explore security, implementation challenges and best-practice verification, with practical examples and actionable reports. Discover how to optimize resource usage, ensure compliance and maintain agility. We'll discuss best practices, potential pitfalls and the nuanced 'it depends' scenarios, offering a comprehensive guide for intermediate to advanced practitioners. /Abn AMRO /Solution Architect"}
{"session_id": "introduction-unity-catalog-metrics-define-your-business-metrics-once", "title": "Introduction to Unity Catalog Metrics: Define Your Business Metrics Once, Trust Everywhere", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Today\u2019s organizations need faster, more reliable insights \u2014 but metric sprawl and inconsistent KPIs make that difficult. In this session, you\u2019ll learn how Unity Catalog Metrics helps unify business semantics across your organization. Define your KPIs once, apply enterprise-grade governance with fine-grained access controls, auditing and lineage, and use them across any Databricks tool \u2014 from AI/BI Dashboards and Genie to notebooks and Lakeflow. You\u2019ll learn how to eliminate metric chaos by centrally defining and governing metrics with Unity Catalog. You\u2019ll walk away with strategies to boost trust through built-in governance and empower every team \u2014 regardless of technical skill \u2014 to work from the same certified metrics. /Staff Software Engineer\nDatabricks /Sr. Staff Product Manager"}
{"session_id": "largest-best-how-we-transformed-databricks-biggest-workspace-unity", "title": "From Largest to Best: How We Transformed Databricks\u2019 Biggest Workspace With Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we unveil how we transformed the largest Databricks workspace into the best-in-class lakehouse through Unity Catalog. Discover how we harnessed lineage and unified access management to build ultimate governance automation. /Platform Lead\nDatabricks /Senior Engineering Manager"}
{"session_id": "last-mile-data-delivery-fast-federated-and-fully-compliant", "title": "Last-Mile Data Delivery: Fast, Federated, and Fully Compliant", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "ELT", "Scala"], "speakers": ["software engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations scale, evolving privacy requirements and decentralized team ownership often lead to fragmented data lakehouses\u2014where data is produced and accessed across separate workspaces. Mission-critical workflows like AI model training or customer support triage frequently span datasets scattered across teams and systems, each with varying sensitivity and access controls. Engineers spend more time discovering and fetching data than delivering outcomes. They're slowed by increasing compliance hurdles\u2014and yet, direct access still exposes organizations to significant risk. To solve this, we built a secure, scalable serving layer on Databricks that brings the right data to the right users at the right time, for just the right amount of time. Powered by Unity Catalog, Delta Sharing, and Databricks Apps, our solution ensures governed, efficient access across the lakehouse\u2014without compromising speed, security, or user experience. /Engineering Manager\nDatabricks /software engineer"}
{"session_id": "leveraging-databricks-unity-catalog-enhanced-data-governance-unipol", "title": "Leveraging Databricks Unity Catalog for Enhanced Data Governance in Unipol", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Engineer, Data Reply"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the contemporary landscape of data management, organizations are increasingly faced with the challenges of data segregation, governance and permission management, particularly when operating within complex structures such as holding companies with multiple subsidiaries. Unipol comprises seven subsidiary companies, each with a diverse array of workgroups, leading to a cumulative total of multiple operational groups. This intricate organizational structure necessitates a meticulous approach to data management, particularly regarding the segregation of data and the assignment of precise read-and-write permissions tailored to each workgroup. The challenge lies in ensuring that sensitive data remains protected while enabling seamless access for authorized users. This speech wants to demonstrate how Unity Catalog emerges as a pivotal tool in the daily use of the data platform, offering a unified governance solution that supports data management across diverse AWS environments. /Data Platform Manager\nUnipol S.p.A. /Senior Data Engineer"}
{"session_id": "managing-databricks-scale", "title": "Managing Databricks at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Delta Lake", "ELT", "Scala"], "speakers": ["Senior Manager, Network Data & AI, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "T-Mobile\u2019s leadership in 5G innovation and its rapid growth in the fixed wireless business have led to an exponential increase in data, reaching 100s of terabytes daily. This session explores how T-Mobile uses Databricks to manage this data efficiently, focusing on scalable architecture with Delta Lake, auto-scaling clusters, performance optimization through data partitioning and caching and comprehensive data governance with Unity Catalog. Additionally, it covers cost management, collaborative tools and AI-driven productivity tools, highlighting how these strategies empower T-Mobile to innovate, streamline operations and maximize data impact across network optimization, supporting the community, energy management and more. /Senior Manager, Network Data & AI"}
{"session_id": "managing-governed-cloud", "title": "Managing the Governed Cloud", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["DATA MARKETPLACE", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Scala"], "speakers": ["Enterprise Data Governance Principle, GM"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly adopt Databricks as a unified platform for analytics and AI, ensuring robust data governance becomes critical for compliance, security, and operational efficiency. This presentation will explore the end-to-end framework for governing the Databricks cloud, covering key use cases, foundational governance principles, and scalable automation strategies. We will discuss best practices for metadata, data access, catalog, classification, quality, and lineage, while leveraging automation to streamline enforcement. Attendees will gain insights into best practices and real-world approaches to building a governed data cloud that balances innovation with control. /Enterprise Data Governance Principle"}
{"session_id": "master-schema-translations-era-open-data-lake", "title": "Master Schema Translations in the Era of Open Data Lake", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "ETL"], "speakers": ["Head of Data Platform, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog puts variety of schemas into a centralized repository, now the developer community wants more productivity and automation for schema inference, translation, evolution and optimization especially for the scenarios of ingestion and reverse-ETL with more code generations.Coinbase Data Platform attempts to pave a path with \"Schemaster\" to interact with data catalog with the (proposed) metadata model to make schema translation and evolution more manageable across some of the popular systems, such as Delta, Iceberg, Snowflake, Kafka, MongoDB, DynamoDB, Postgres...This Lighting Talk covers 4 areas: Takeaway: standardize schema lineage & translation /Head of Data Platform"}
{"session_id": "mastering-data-security-and-compliance-coorsteks-journey-databricks", "title": "Mastering Data Security and Compliance: CoorsTek's Journey With Databricks Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PUBLIC SECTOR", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Security"], "speakers": ["Chief Business Officer, Tredence"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ensuring data security & meeting compliance requirements are critical priorities for businesses operating in regulated industries, where the stakes are high and the standards are stringent. We will showcase how CoorsTek, a global leader in technical ceramics MFG, partnered with Databricks to leverage the power of UC for addressing regulatory challenges while achieving significant operational efficiency gains. We'll dive into the migration journey, highlighting the adoption of key features such as RBAC, comprehensive data lineage tracking and robust auditing capabilities. Attendees will gain practical insights into the strategies and tools used to manage sensitive data, ensure compliance with industry standards and optimize cloud data architectures. Additionally, we\u2019ll share real-world lessons learned, best practices for integrating compliance into a modern data ecosystem and actionable takeaways for leveraging Databricks as a catalyst for secure and compliant data innovation. /Director of Data and Analytics\nCoorsTek /Chief Business Officer"}
{"session_id": "migration-unity-catalog-american-airlines-using-automation", "title": "Migration to Unity Catalog at American Airlines Using Automation", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["Sr Manager, Enterprise Data Platforms, American Airlines"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "American Airlines migrated from Hive Metastore to Unity Catalog using automated processes with Databricks APIs and GitHub Actions. This automation streamlined the migration for many applications within AA, ensuring consistency, efficiency and minimal disruption while enhancing data governance and disaster recovery capabilities. /Sr. Principal Data Architect\nAmerican Airlines /Sr Manager, Enterprise Data Platforms"}
{"session_id": "monitor-data-and-ai-quality-scale-data-intelligence-powered-unity", "title": "Monitor Data and AI Quality at Scale With Data Intelligence Powered by Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delivering trusted, high-impact data and AI requires more than basic checks \u2014 it demands intelligent, automated quality monitoring. In this session, discover how quality monitoring scales with Unity Catalog and brings data intelligence to all your assets. Learn how Lakehouse Monitoring, anomaly detection, data classification, and Lakeflow come together to provide end-to-end visibility into the health of your data and AI pipelines. Discover how you can shift from reactive firefighting to proactive, scalable quality monitoring across your entire data estate. /Product Manager"}
{"session_id": "platform-strategies-ai-models-cross-region-migration-and-dr", "title": "Platform Strategies for AI Models\u2014Cross-Region Migration and DR", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL"], "speakers": ["Sr Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI for enterprises, particularly in the era of GenAI, requires rapid experimentation and the ability to productionize models and agents quickly. This means that the availability of the entire Lakehouse- from raw data to ETL pipelines to notebooks and ultimately the serving layer- is more important than ever. A strong Disaster Recovery strategy has become a key part of a strong AI practice; as cloud providers struggle with rising demand for GPUs in environments, VM shortages have become commonplace, and add to the pressure of general cloud outages. Enterprises that can quickly leverage GPU capacity in other cloud regions will be better equipped to capitalize on the promise of AI. Leveraging Unity Catalog Models is a strong recommendation going forward for Disaster Recovery readiness and good AI governance practices. In this presentation we will show an end-to-end example of how UC enables DR for fully-governed AI models. Co-presenters: Tony Farias and Greg Wood /Lead SSA\nDatabricks /Sr Solutions Architect"}
{"session_id": "powering-secure-and-scalable-data-governance-pepsico-unity-catalog-open", "title": "Powering Secure and Scalable Data Governance at PepsiCo With Unity Catalog Open APIs", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Enterprise Data Operations Director, PepsiCo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PepsiCo, given its scale, has numerous teams leveraging different tools and engines to access data and perform analytics and AI. To streamline governance across this diverse ecosystem, PepsiCo unifies its data and AI assets under an open and enterprise-grade governance framework with Unity Catalog. In this session, we'll explore real-world examples of how PepsiCo extends Unity Catalog\u2019s governance to all its data and AI assets, enabling secure collaboration even for teams outside Databricks. Learn how PepsiCo architects permissions using service principals and service accounts to authenticate with Unity Catalog, building a multi-engine architecture with seamless and open governance. Attendees will gain practical insights into designing a scalable, flexible data platform that unifies governance across all teams while embracing openness and interoperability. /Lead Specialist Solutions Architect\nDatabricks /Enterprise Data Operations Director"}
{"session_id": "reimagining-data-governance-and-access-atlassian", "title": "Reimagining Data Governance and Access at Atlassian", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Scala"], "speakers": ["Senior Software Engineer, Atlassian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Atlassian is rebuilding its central lakehouse from the ground up to deliver a more secure, flexible and scalable data environment. In this session, we\u2019ll share how we leverage Unity Catalog for fine-grained governance and supplement it with Immuta for dynamic policy management, enabling row and column level security at scale. By shifting away from broad, monolithic access controls toward a modern, agile solution, we\u2019re empowering teams to securely collaborate on sensitive data without sacrificing performance or usability. Join us for an inside look at our end-to-end policy architecture, from how data owners declare metadata and author policies to the seamless application of access rules across the platform. We\u2019ll also discuss lessons learned on streamlining data governance, ensuring compliance, and improving user adoption. Whether you\u2019re a data architect, engineer or leader, walk away with actionable strategies to simplify and strengthen your own governance and access practices. /Senior Software Engineer"}
{"session_id": "scaling-data-governance-how-unity-catalog-empowering-picpays-data", "title": "Scaling Data Governance: How Unity Catalog is Empowering Picpay's Data Governance Strategy", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["LLAMA", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["Picpay"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With massive data volume and complexity, scaling data governance became a significant challenge. Centralizing metadata management, ensuring regulatory compliance and controlling data access across multiple platforms turned to be critical to maintaining efficiency and trust. /Data Manager\nPicPay /Picpay"}
{"session_id": "scaling-data-intelligence-nab-balancing-innovation-enterprise-grade", "title": "Scaling Data Intelligence at NAB: Balancing Innovation with Enterprise-Grade Governance", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Distinguished Engineer, National Australia Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, discover how National Australia Bank (NAB) is reshaping its data and AI strategy by positioning data as a strategic enabler. Driven by a vision to unlock data like electricity\u2014continuous and reliable\u2014NAB has established a scalable foundation for data intelligence that balances agility with enterprise-grade control. We'll delve into the key architectural, security, and governance capabilities underpinning this transformation, including Unity Catalog, Serverless, Lakeflow and GenAI. The session will highlight NAB's adoption of Databricks Serverless, platform security controls like private link, and persona-based data access patterns. Attendees will walk away with practical insights into building secure, scalable, and cost-efficient data platforms that fuel innovation while meeting the demands of compliance in highly regulated environments. /Senior Solutions Architect\nDatabricks /Distinguished Engineer"}
{"session_id": "schiphol-groups-transformation-unity-catalog", "title": "Schiphol Group\u2019s Transformation to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Europe\u2019s third-busiest airport, Schiphol Group, is elevating its data operations by transitioning from a standard Databricks setup to the advanced capabilities of Unity Catalog. In this session, we will share the motivations, obstacles and strategic decisions behind executing a seamless migration in a large-scale environment \u2014 one that spans hundreds of workspaces and demands continuous availability. Gain insights into planning and governance, learn how to safeguard data integrity and maintain operational flow, and understand the process of integrating Unity Catalog\u2019s enhanced security and governance features. Attendees will leave with practical lessons from our hands-on experience, proven methods for similar migrations, and a clear perspective on the benefits this transition offers for complex, rapidly evolving organizations. /Manager, Field Engineering\nDatabricks /Solutions Architect"}
{"session_id": "semiconductor-ai-success-marvells-data-ai-governance", "title": "Semiconductor AI Success: Marvell\u2019s Data + AI Governance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Security"], "speakers": ["Marvell Semiconductors Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marvell\u2019s AI-driven solutions, powered by Databricks\u2019 Data Intelligence Platform, provide a robust framework for secure, compliant and transparent Data and AI workflows leveraging Data & AI Governance through Unity Catalog. Marvell ensures centralized management of data and AI assets with quality, security, lineage and governance guardrails. With Databricks Unity Catalog, Marvell achieves comprehensive oversight of structured and unstructured data, AI models and notebooks. Automated governance policies, fine-grained access controls and lineage tracking help enforce regulatory compliance while streamlining AI development. This governance framework enhances trust and reliability in AI-powered decision-making, enabling Marvell to scale AI innovation efficiently while minimizing risks. By integrating data security, auditability and compliance standards, Marvell is driving the future of responsible AI adoption with Databricks. /Solutions Architect\nDatabricks /Head of Data\nMarvell Technology, Inc. /Marvell Semiconductors Inc"}
{"session_id": "simplifying-migration-experience-unity-catalog", "title": "Simplifying the Migration Experience to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "solving-exclusive-data-access-role-based-access-control", "title": "Solving Exclusive Data Access With Role-Based Access Control", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Do you have users that wear multiple hats over a day? Like working with data from various customers and hoping they don\u2019t inadvertently aggregate data? Or are they working on sensitive datasets such as clinical trials that should not be combined, or are data sets that are subject to regulations? We have a solution! In this session, we will present a new capability that allows users wearing multiple hats to switch roles in the Databricks workspace to work exclusively on a dedicated project, data of a particular client or clinical trial. When switching to a particular role, the workspace adapts in such a way that only workspace objects and UC data of that particular role are accessible. We will also showcase the administrative experience of setting up exclusive access using groups and UC permissions. /Director, Product Management\nDatabricks /Product Management"}
{"session_id": "sponsored-actian-beyond-lakehouse-unlocking-enterprise-wide-ai-ready", "title": "Sponsored by: Actian | Beyond the Lakehouse: Unlocking Enterprise-Wide AI-Ready Data with Unified Metadata Intelligence", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations scale AI initiatives on platforms like Databricks, one challenge remains: bridging the gap between the data in the lakehouse and the vast, distributed data that lives elsewhere. Turning massive volumes of technical metadata into trusted, business-ready insight requires more than cataloging what's inside the lakehouse\u2014it demands true enterprise-wide intelligence. Actian CTO Emma McGrattan will explore how combining Databricks Unity Catalog with the Actian Data Platform extends visibility, governance, and trust beyond the lakehouse. Learn how leading enterprises are:"}
{"session_id": "sponsored-alation-better-together-enterprise-catalog-databricks-alation", "title": "Sponsored by: Alation | Better Together: Enterprise Catalog with Databricks & Alation at American Airlines", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the era of data-driven enterprises, true democratization requires more than just access\u2013it demands context, trust, and governance at scale. In this session, discover how to seamlessly integrate Databricks Unity Catalog with Alation\u2019s Enterprise Data Catalog to deliver:"}
{"session_id": "sponsored-atlan-how-fox-atlan-are-partnering-make-metadata-common", "title": "Sponsored by: Atlan | How Fox & Atlan are Partnering to Make Metadata a Common System of Trust, Context, and Governance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE ICEBERG", "UNITY CATALOG"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With hundreds of millions viewing broadcasts from news to sports, Fox relies on a sophisticated and trusted architecture ingesting 100+ data sources, carefully governed to improve UX across products, drive sales and marketing, and ensure KPI tracking. Join Oliver Gomes, VP of Enterprise and Data Platform at Fox, and Prukalpa Sankar of Atlan to learn how true partnership helps their team navigate opportunities from Governance to AI. To govern and democratize their multi-cloud data platform, Fox chose Atlan to make data accessible and understandable for more users than ever before. Their team then used a data product approach to create a shared language using context from sources like Unity Catalog at a single point of access, no matter the underlying technology. Now, Fox is defining an ambitious future for Metadata. With Atlan and Iceberg driving interoperability, their team prepares to build a \u201ccontrol plane\u201d, creating a common system of trust and governance."}
{"session_id": "sponsored-capital-one-software-how-manage-high-quality-secure-data-and", "title": "Sponsored by: Capital One Software | How to Manage High-Quality, Secure Data and Cost Visibility for AI", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Companies need robust data management capabilities to build and deploy AI. Data needs to be easy to find, understandable, and trustworthy. And it\u2019s even more important to secure data properly from the beginning of its lifecycle, otherwise it can be at risk of exposure during training or inference. Tokenization is a highly efficient method for securing data without compromising performance. In this session, we\u2019ll share tips for managing high-quality, well-protected data at scale that are key for accelerating AI. In addition, we\u2019ll discuss how to integrate visibility and optimization into your compute environment to manage the hidden cost of AI \u2014 your data."}
{"session_id": "sponsored-deloitte-transforming-nestle-usas-nusa-data-platform-unlock", "title": "Sponsored by: Deloitte | Transforming Nestl\u00e9 USA\u2019s (NUSA) data platform to unlock new analytics and GenAI capabilities", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Nestl\u00e9 USA, a division of the world\u2019s largest food and beverage company, Nestl\u00e9 S.A., has embarked on a transformative journey to unlock GenAI capabilities on their data platform. Deloitte, Databricks, and Nestl\u00e9 have collaborated on a data platform modernization program to address gaps associated with Nestl\u00e9\u2019s existing data platform. This joint effort introduces new possibilities and capabilities, ranging from development of advanced machine learning models, implementing Unity Catalog, and adopting Lakehouse Federation, all while adhering to confidentiality protocols. With help from Deloitte and Databricks, Nestl\u00e9 USA is now able to meet its advanced enterprise analytics and AI needs with the Databricks Data Intelligence Platform."}
{"session_id": "sponsored-informatica-extending-unity-catalog-govern-data-estate", "title": "Sponsored by: Informatica | Extending Unity Catalog to Govern the Data Estate With Informatica Cloud Data Governance & Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this 20-minute session to learn how Informatica CDGC integrates with and leverages Unity Catalog metadata to provide end-to-end governance and security across an enterprise data landscape. Topics covered will include:"}
{"session_id": "sponsored-infosys-data-ai-governance-action-policy-practice", "title": "Sponsored by: Infosys | Data & AI Governance in Action: From Policy to Practice", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session focuses on the practical aspects of implementing AI governance within organizations. Learn how to move beyond policy documents to establish effective governance structures, integrate ethical AI practices into daily operations, and utilize tools and frameworks to ensure responsible AI development. Gain insights on transforming AI governance from theory to practice"}
{"session_id": "sponsored-kpmg-enhancing-regulatory-compliance-through-data-quality-and", "title": "Sponsored by: KPMG | Enhancing Regulatory Compliance through Data Quality and Traceability", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DELTA LAKE", "DELTA SHARING"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Data Quality"], "speakers": ["ML-driven methods. This fosters complete visibility across IT, data management, and business operations, facilitating rapid issue resolution continuous quality enhancement. The outcome is quicker, more accurate, transparent financial reporting. We will detail a framework for observability offer practical examples of implementing checks throughout the lifecycle, specifically focusing on creating pipelines regulatory"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In highly regulated industries like financial services, maintaining data quality is an ongoing challenge. Reactive measures often fail to prevent regulatory penalties, causing inaccuracies in reporting and inefficiencies due to poor data visibility. Regulators closely examine the origins and accuracy of reporting calculations to ensure compliance. A robust system for data quality and lineage is crucial. Organizations are utilizing Databricks to proactively improve data quality through rules-based and AI/ML-driven methods. This fosters complete visibility across IT, data management, and business operations, facilitating rapid issue resolution and continuous data quality enhancement. The outcome is quicker, more accurate, transparent financial reporting. We will detail a framework for data observability and offer practical examples of implementing quality checks throughout the data lifecycle, specifically focusing on creating data pipelines for regulatory reporting."}
{"session_id": "sponsored-prophecy-how-marks-spencer-building-platform-future-deliver", "title": "Sponsored by: Prophecy | How Marks & Spencer is Building the Platform of the Future to Deliver Premium Retail Experiences", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As one of Britain\u2019s most iconic brands, Marks & Spencer (M&S) delivers premium food, clothing, and homeware to millions of customers. Behind customer experiences is data that must adapt to demand changes, supply chain volatility, and shifting digital consumers. To meet these challenges, the M&S Data + AI team is building the Platform for the Future. With Databricks and Prophecy, the platform will simplify the stack, democratize access, and deliver insights faster. M&S is making a bold shift to AI supercharge pipeline creation, moving from a code and engineer-heavy process to self-service with Prophecy\u2019s visual and AI development. With governance to reliably use data and manage costs. The result? People from across the business are participating, bringing the cost and time of food supply chain analysis down by a third and speeding pipeline creation by 30-60%. Join this session to learn how M&S is delivering AI-powered self-service and charting a roadmap for real transformation."}
{"session_id": "sponsored-tealium-personalizing-experiences-and-improving-engagement", "title": "Sponsored by: Tealium | Personalizing Experiences and Improving Engagement with a Modernized Data Infrastructure", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION", "technologies": ["DATABRICKS WORKFLOWS", "PARTNER CONNECT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this session to hear how Western Governors University leverages Tealium & Databricks to power their data activation strategy with real-time customer data collection, activation and advanced analytics."}
{"session_id": "state-street-uses-databricks-cybersecurity-lakehouse-threat", "title": "State Street Uses Databricks as a Cybersecurity Lakehouse for Threat Intelligence & Real-Time Alerts", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Organizations face the challenge of managing vast amounts of data to combat emerging threats. The Databricks Data Intelligence platform represents a paradigm shift in cybersecurity at State Street, providing a comprehensive solution for managing and analyzing diverse security data. Through its partnership with Databricks, State Street has created a capability to: Efficiently manage structured and unstructured data. Scale up to analyze 50 petabytes of data in real-time. Ingest and parse data for critical security data streams. Build advanced cybersecurity data products and use automation & orchestration to streamline cybersecurity operations. By leveraging these capabilities, State Street has positioned itself as a leader in the financial services industry when it comes to cybersecurity. /Managing Director, Sec Arch & Eng\nState Street /Senior Solutions Architect"}
{"session_id": "story-unity-catalog-uc-migration-using-ucx-7-eleven-reorient-complex-uc", "title": "Story of a Unity Catalog (UC) Migration: Using UCX at 7-Eleven to Reorient a Complex UC Migration", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Director, Architecture and Governance, 7-Eleven"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog (UC) enables governance and security for all data and AI assets within an enterprise\u2019s data lake and is necessary to unlock the full potential of Databricks as a true Data Intelligence Platform. Unfortunately, UC migrations are non-trivial; especially for enterprises that have been using Databricks for more than five years, i.e., 7-Eleven. System Integrators (SIs) offer accelerators, guides, and services to support UC migrations; however, cloud infrastructure changes, anti-patterns within code, and data sprawl can significantly complicate UC migrations. There is no \u201cshortcut\u201d to success when planning and executing a complex UC migration. In this session, we will share how UCX by Databricks Labs, a UC Migration Assistant, allowed 7-Eleven to reorient their UC migration by leveraging assessments and workflows, etc., to assess, characterize, and ultimately plan a tenable approach for their UC migration. /Delivery Solutions Architect\nDatabricks /Director, Architecture and Governance"}
{"session_id": "test", "title": "test", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, PUBLIC SECTOR, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "APACHE ICEBERG", "DLT"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Testing Email Allocations"}
{"session_id": "tracking-data-and-ai-lineage-ensuring-transparency-and-compliance", "title": "Tracking Data and AI Lineage: Ensuring Transparency and Compliance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As AI becomes more deeply integrated into data platforms, understanding where data comes from \u2014 and where it goes \u2014 is essential for ensuring transparency, compliance and trust. In this session, we\u2019ll explore the newest advancements in data and AI lineage across the Databricks Platform, including during model training, evaluation and inference. You\u2019ll also learn how lineage system tables can be used for impact analysis and to gain usage insights across your data estate. We\u2019ll cover newly released capabilities \u2014 such as Bring Your Own Lineage \u2014 that enable an end-to-end view of your data and AI assets in Unity Catalog. Plus, get a sneak peek at what\u2019s coming next on the lineage roadmap! /SWE\nDatabricks /Product Manager"}
{"session_id": "transforming-credit-analytics-compliant-lakehouse-rabobank", "title": "Transforming Credit Analytics With a Compliant Lakehouse at Rabobank", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture"], "speakers": ["Product Manager, Rabobank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation outlines Rabobank Credit analytics transition to a secure, audit-ready data architecture using Unity Catalog (UC), addressing critical regulatory challenges in credit analytics for IRB and IFRS9 regulatory modelling. Key technical challenges: Details: Outcomes: Next: /Sr. Resident Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "transforming-data-governance-multimodal-data-amgen-databricks", "title": "Transforming Data Governance for Multimodal Data at Amgen With Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Fabric", "Data Governance"], "speakers": ["Senior Manager Data & Analytics, Amgen"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Amgen is advancing its Enterprise Data Fabric to securely manage sensitive multimodal data, such as imaging and research data, across formats.Databricks is already the de facto standard for governance on structured data, and Amgen seeks to extend it for unstructured multi modal data too. This approach will also allow Amgen to standardize its GenAI projects on Databricks. Key priorities include: Learn strategies for implementing a comprehensive multimodal data governance framework using Databricks, as we share our experience on standardizing data governance for GenAI use cases. /Sr Data Engineer\nAMGEN /Senior Manager Data & Analytics"}
{"session_id": "trust-you-can-measure-data-quality-standards-lakehouse", "title": "Trust You Can Measure: Data Quality Standards in The Lakehouse", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Data Quality"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Do you trust your data? If you\u2019ve ever struggled to figure out which datasets are reliable, well-governed, or safe to use, you\u2019re not alone. At Databricks, our own internal lakehouse faced the same challenge\u2014hundreds of thousands of tables, but no easy way to tell which data met quality standards. In this talk, the Databricks Data Platform team shares how we tackled this problem by building the Data Governance Score\u2014a way to systematically measure and surface trust signals across the entire lakehouse. You\u2019ll learn how we leverage Unity Catalog, governed tags, and enforcement to drive better data decisions at scale. Whether you're a data engineer, platform owner, or business leader, you\u2019ll leave with practical ideas on how to raise the bar for data quality and trust in your own data ecosystem. /Staff Software Engineer\nDatabricks /Staff Software Engineer"}
{"session_id": "unity-catalog-deep-dive-practitioners-guide-best-practices-and-patterns", "title": "Unity Catalog Deep Dive: Practitioner's Guide to Best Practices and Patterns", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Pipeline", "Machine Learning"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this deep dive session for practitioners on Unity Catalog, Databricks\u2019 unified data governance solution, to explore its capabilities for managing data and AI assets across workflows. Unity Catalog provides fine-grained access control, automated lineage tracking, quality monitoring and policy enforcement and observability at scale. Whether your focus is data pipelines, analytics or machine learning and generative AI workflows, this session offers actionable insights on leveraging Unity Catalog\u2019s open interoperability across tools and platforms to boost productivity and drive innovation. Learn governance best practices, including catalog configurations, access strategies for collaboration and controls for securing sensitive data. Additionally, discover how to design effective multi-cloud and multi-region deployments to ensure global compliance. /Sr. Solutions Architect\nDatabricks /Solutions Architect"}
{"session_id": "unity-catalog-implementation-evolution-edward-jones", "title": "Unity Catalog Implementation & Evolution at Edward Jones", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics"], "speakers": ["Technical Architect, Edward Jones"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Analytics Hub is a product which serves as the central repository for analytics and reporting data across Edward Jones. Analytics Hub followed the UC design aligning to the Medallion architecture: We have good amount of data that are migrated to Analytics Hub and our user base is growing exponentially. This comes with next level of challenge of getting ready for DR. In a nutshell, we will talk about: /Technical Architect"}
{"session_id": "unity-catalog-lakeguard-secure-and-efficient-compute-your-enterprise", "title": "Unity Catalog Lakeguard: Secure and Efficient Compute for Your Enterprise", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "SQL"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Modern data workloads span multiple sources \u2014 data lakes, databases, apps like Salesforce and services like cloud functions. But as teams scale, secure data access and governance across shared compute becomes critical. In this session, learn how to confidently integrate external data and services into your workloads using Spark and Unity Catalog on Databricks. We'll explore compute options like serverless, clusters, workflows and SQL warehouses, and show how Unity Catalog\u2019s Lakeguard enforces fine-grained governance \u2014 even when concurrently sharing compute by multiple users. Walk away ready to choose the right compute model for your team\u2019s needs \u2014 without sacrificing security or efficiency. /Staff Product Manager\nDatabricks /Sr. Staff Product Manager"}
{"session_id": "unity-catalog-upgrades-made-easy-step-step-guide-databricks-labs-ucx", "title": "Unity Catalog Upgrades Made Easy. Step-by-Step Guide for Databricks Labs UCX", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks labs project UCX aims to optimize the Unity Catalog (UC) upgrade process, ensuring a seamless transition for businesses. This session will delve into various aspects of the UCX project including the installation and configuration of UCX, the use of the UCX Assessment Dashboard to reduce upgrade risks and prepare effectively for a UC upgrade, and the automation of key components such as group, table and code migration. Attendees will gain comprehensive insights into leveraging UCX and Lakehouse Federation for a streamlined and efficient upgrade process. This session is aimed at customers new to UCX as well as veterans. /Lead Solutions Architect\nDatabricks /Lead Product Specialist"}
{"session_id": "unleash-power-automated-data-governance-classify-tag-and-protect-your", "title": "Unleash the Power of Automated Data Governance: Classify, Tag and Protect Your Data \u2014 Effortlessly", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling to keep up with data governance at scale? Join us to explore how automated data classification, tag policies and ABAC streamline access control while enhancing security and compliance. Get an exclusive look at the new Governance Hub, built to give your teams deeper visibility into data usage, access patterns and metadata \u2014 all in one place. Whether you're managing thousands or millions of assets, discover how to classify, tag and protect your data estate effortlessly with the latest advancements in Unity Catalog. /Product Manager\nDatabricks /Senior Product Manager"}
{"session_id": "unleashing-data-governance-ifoodharnessing-system-tables-and-lineage", "title": "Unleashing Data Governance at iFood:Harnessing System Tables and Lineage for Dynamic Tag Propagation", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Scala"], "speakers": ["Staff Data Engineer, iFood"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With regulations like LGPD (Brazil's General Data Protection Law) and GDPR, managing sensitive data access is critical. This session demonstrates how to leverage Databricks Unity Catalog system tables and data lineage to dynamically propagate classification tags, empowering organizations to monitor governance and ensure compliance. The presentation covers practical steps, including system table usage, data normalization, ingestion with DLT pipelines and classification tag propagation to downstream tables. It also explores permission monitoring with alerts to proactively address governance risks. Designed for advanced audiences, this session offers actionable strategies to strengthen data governance, prevent breaches and avoid regulatory fines while building scalable frameworks for sensitive data management. /Head of Data Platform\niFood /Staff Data Engineer"}
{"session_id": "unlocking-access-simplifying-identity-management-scale-databricks", "title": "Unlocking Access: Simplifying Identity Management at Scale With Databricks", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Effective Identity and Access Management (IAM) is essential for securing enterprise environments while enabling innovation and collaboration. As companies scale, ensuring users have the right access without adding administrative overhead is critical. In this session, we\u2019ll explore how Databricks is simplifying identity management by integrating with customers\u2019 Identity Providers (IDPs). Learn about Automatic Identity Management in Azure Databricks, which eliminates SCIM for Entra ID users and ensures scalable identity provisioning for other IDPs. We'll also cover externally managed groups, PIM integration and upcoming enhancements like a bring-your-own-IDP model for GCP. Through a customer success story and live demo, see how Databricks is making IAM more scalable, secure and user-friendly. /Specialist Solutions Architect\nDatabricks /Staff Product Manager"}
{"session_id": "unlocking-cross-organizational-collaboration-protect-environment", "title": "Unlocking Cross-Organizational Collaboration to Protect the Environment With Databricks at DEFRA", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR", "technologies": ["AI/BI", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Scala"], "speakers": ["Head of Data Analytics and Science Hub, Defra"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to learn how the UK's Department for Environment, Food & Rural Affairs (DEFRA) transformed data use with Databricks\u2019 Unity Catalog, enabling nationwide projects through secure, scalable analytics. DEFRA safeguards the UK's natural environment. Historical fragmentation of data, talent and tools across siloed platforms and organizations, made it difficult to fully exploit the department\u2019s rich data. DEFRA launched its Data Analytics & Science Hub (DASH), powered by the Databricks Data Intelligence Platform, to unify its data ecosystem. DASH enables hundreds of users to access and share datasets securely. A flagship example demonstrates its power, using Databricks to process aerial photography and satellite data to identify peatlands in need of restoration \u2014 a complex task made possible through unified data governance, scalable compute and AI. Attendees will hear about DEFRA\u2019s journey, learn valuable lessons about building a platform crossing organizational boundaries. /Head of Data Exploitation\nDefra /Head of Data Analytics and Science Hub"}
{"session_id": "unlocking-data-intelligence-beginners-guide-unity-catalog", "title": "Unlocking Data Intelligence: A Beginner\u2019s Guide to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Product Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Getting started with data and AI governance in the modern data stack? Unity Catalog is your gateway to secure, discoverable and well-governed data and AI assets. In this session, we\u2019ll break down what Unity Catalog is, why it matters and how it simplifies access control, lineage, discovery, auditing, business semantics and secure, open collaboration \u2014 all from a single place. We\u2019ll explore how it enables open interoperability across formats, tools and platforms, helping you avoid lock-in and build on open standards. Most importantly, you\u2019ll learn how Unity Catalog lays the foundation for data intelligence \u2014 by unifying governance across data and AI, enabling AI tuned to your business. It helps build a deep understanding of your data and delivers contextual, domain-specific insights that boost productivity for both technical and business users across any workload. /Principal Product Marketing Manager\nDatabricks /Lead Product Architect"}
{"session_id": "using-catalogs-well-governed-and-efficient-data-ecosystem", "title": "Using Catalogs for a Well-Governed and Efficient Data Ecosystem", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "SQL"], "speakers": ["Distinguished Engineer, Capital One"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The ability to enforce data management controls at scale and reduce the effort required to manage data pipelines is critical to operating efficiently. Capital One has scaled its data management capabilities and invested in platforms to help address this need. In the past couple of years, the role of \u201cthe catalog\u201d in a data platform architecture has transitioned from just providing SQL to providing a full suite of capabilities that can help solve this problem at scale. This talk will give insight into how Capital One is thinking about leveraging Databricks Unity Catalog to help tackle these challenges. /Distinguished Engineer"}
{"session_id": "using-identity-security-unity-catalog-faster-safer-data-access", "title": "Using Identity Security With Unity Catalog for Faster, Safer Data Access", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Product Marketing Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Managing authentication effectively is key to securing your data platform. In this session, we\u2019ll explore best practices from Databricks for overcoming authentication challenges, including token visibility, MFA/SSO, CI/CD token federation and risk containment. Discover how to map your authentication maturity journey while maximizing security ROI. We'll showcase new capabilities like access token reports for improved visibility, streamlined MFA implementation and secure SSO with token federation. Learn strategies to minimize token risk through TTL limits, scoped tokens and network policies. You'll walk away with actionable insights to enhance your authentication practices and strengthen platform security on Databricks. /Product Management\nDatabricks /Senior Product Marketing Manager"}
{"session_id": "viewshift-dynamic-policy-enforcement-spark-and-sql-views", "title": "ViewShift: Dynamic Policy Enforcement With Spark and SQL Views", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "SQL"], "speakers": ["Senior Staff Software Engineer, LinkedIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dynamic policy enforcement is increasingly critical in today's landscape, where data compliance is a top priorities for companies, individuals, and regulators alike. In this talk, Walaa and Khai explore how LinkedIn has implemented a robust dynamic policy enforcement engine, ViewShift, and integrated it within its data lake. They will demystify LinkedIn's query engine stack by demonstrating how catalogs can automatically route table resolutions to compliance-enforcing SQL views. These SQL views possess several noteworthy properties: Auto-Generated: Created automatically from declarative data annotations. User-Centric: They honor user-level consent and preferences. Context-Aware: They apply different transformations tailored to specific use cases. Portable: Despite the SQL logic being implemented in a single dialect, it remains accessible across all engines. Join this session to learn how ViewShift helps ensure that compliance is seamlessly integrated into data processing workflows. /Senior Staff Software Engineer"}
{"session_id": "what-i-wish-i-had-known-my-last-soc-confessions-cybersecurity-executive", "title": "\u201cWhat I Wish I Had Known in My Last SOC.\u201d Confessions of a Cybersecurity Executive", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Janitor, Ziggiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In Bruce\u2019s career in cyber warfare and enterprise cybersecurity, he worked on many of the highest profile botnet and nation state takedowns in history. He also helped build the tech in one of the world\u2019s most advanced SOCs. Bruce will explain what he learned from that experience and why it prompted him to leave early retirement, sell his beloved sports car and co-found ziggiz. We all know there\u2019s more data than ever. Anyone close to cybersecurity also knows that SIEMs, typically at the center of enterprise cybersecurity operations, have become too expensive even at the highest levels of government and Fortune 100s. /Janitor"}
{"session_id": "whats-new-security-and-compliance-databricks-data-intelligence-platform", "title": "What\u2019s New in Security and Compliance on the Databricks Data Intelligence Platform", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Senior Director, Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we\u2019ll walk through the latest advancements in platform security and compliance on Databricks \u2014 from networking updates to encryption, serverless security and new compliance certifications across AWS, Azure and GCP. We\u2019ll also share our roadmap and best practices for how to securely configure workloads on Databricks SQL Serverless, Unity Catalog, Mosaic AI and more \u2014 at scale. If you're building on Databricks and want to stay ahead of evolving risk and regulatory demands, this session is your guide. /Staff Product Manager\nDatabricks /Senior Director, Product Management"}
{"session_id": "whats-new-unity-catalog-live-demos", "title": "What\u2019s New in Unity Catalog With Live Demos", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["Data Governance"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join the Unity Catalog product team for an exclusive deep dive into the latest innovations and upcoming features of Unity Catalog! Explore cutting-edge advancements in access control, discovery, lineage and monitoring \u2014 plus get a sneak peek at what\u2019s coming next. Packed with live demos, expert insights and best practices from thousands of customers running Unity Catalog in production, this session is also your chance to engage directly with product experts and get answers to your most pressing questions. Don\u2019t miss this opportunity to stay ahead of the curve and elevate your data governance strategy! /Staff Product Manager\nDatabricks /Product Manager"}
