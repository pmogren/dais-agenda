{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "accelerate-end-end-multi-agents-databricks-and-dspy", "title": "Accelerate End-to-End Multi-Agents on Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A production-ready GenAI application is more than the framework itself. Like ML, you need a unified platform to create an end-to-end workflow for production quality applications. Below is an example of how this works on Databricks: In this session, learn how to build agents to access all your data and models through function calling. Then, learn how DSPy enables agent interaction with each other to ensure the question is answered correctly. We will demonstrate a chatbot, powered by multiple agents, to be able to answer questions and reason answers the base LLM does not know and very specialized topics. /Delivery Solutions Architect"}
{"session_id": "accelerating-analytics-integrating-bi-tools-databricks-sql", "title": "Accelerating Analytics: Integrating BI Tools to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Did you know that you can integrate with your favorite BI tools directly from Databricks SQL? You don\u2019t even need to stand up an additional warehouse. This session shows the integrations with Microsoft Power Platform, Power BI, Tableau, Sigma and Looker so you can have a seamless integration experience. Directly connect your Databricks workspace with Fabric and Power BI workspaces or Tableau to publish and sync data models, with defined primary and foreign keys, between the two platforms. /Sr. Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "accelerating-data-ingestion-new-innovations-auto-loaders-performance", "title": "Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala", "Streaming"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including: You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect. /Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "accelerating-data-transformation-best-practices-governance-agility-and", "title": "Accelerating Data Transformation: Best Practices for Governance, Agility and Innovation", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Quality", "ELT", "SQL", "Scala"], "speakers": ["Practice Director, Data & AI, NCS Australia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share NCS\u2019s approach to implementing a Databricks Lakehouse architecture, focusing on key lessons learned and best practices from our recent implementations. By integrating Databricks SQL Warehouse, the DBT Transform framework and our innovative test automation framework, we\u2019ve optimized performance and scalability, while ensuring data quality. We\u2019ll dive into how Unity Catalog enabled robust data governance, empowering business units with self-serve analytical workspaces to create insights while maintaining control. Through the use of solution accelerators, rapid environment deployment and pattern-driven ELT frameworks, we\u2019ve fast-tracked time-to-value and fostered a culture of innovation. Attendees will gain valuable insights into accelerating data transformation, governance and scaling analytics with Databricks. /Practice Director, Data & AI"}
{"session_id": "accelerating-growth-capital-markets-data-driven-strategies-success", "title": "Accelerating Growth in Capital Markets: Data-Driven Strategies for Success", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Software Engineering Manager, B3 - Bolsa, Brasil e Balc\u00e3o"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Growth in capital markets thrives on innovation, agility and real-time insights. This session highlights how leading firms use Databricks\u2019 Data Intelligence Platform to uncover opportunities, optimize trading strategies and deliver personalized client experiences. Learn how advanced analytics and AI help organizations expand their reach, improve decision-making and unlock new revenue streams. Industry leaders share how unified data platforms break down silos, deepen insights and drive success in a fast-changing market. Key takeaways: Discover how data intelligence empowers capital markets firms to thrive in today\u2019s competitive landscape! /Financial Services Industry Director\nDatabricks /CDO - Wealth Management\nNorthern Trust /Software Engineering Manager"}
{"session_id": "accelerating-innovation-across-life-sciences-value-chain-data-ai", "title": "Accelerating Innovation Across the Life Sciences Value Chain With a Data + AI Foundation", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Quality"], "speakers": ["Industry Marketing Lead, HLS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Artificial intelligence and robust data strategies are revolutionizing life sciences, delivering measurable value across drug discovery, commercial operations, supply chain, manufacturing and commercialization. Foundational data governance ensures data quality, compliance and interoperability, enabling organizations to extract actionable insights from vast, complex datasets. AI accelerates drug discovery by identifying novel targets and optimizing compounds, while data intelligence and sharing streamline clinical trials, regulatory submissions and market positioning. In manufacturing and supply chain, advanced analytics and AI-driven tools optimize workflows, forecast disruptions and enhance operational efficiency. Together, these capabilities empower life sciences organizations to innovate faster, reduce costs and bring therapies to market more efficiently, driving growth and improving patient outcomes. /Industry Marketing Lead, HLS"}
{"session_id": "accelerating-model-development-and-fine-tuning-databricks-twelvelabs", "title": "Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Scaling large language models (LLMs) and multimodal architectures requires efficient data management and computational power. NVIDIA NeMo Framework Megatron-LM on Databricks is an open source solution that integrates GPU acceleration and advanced parallelism with Databricks Delta Lakehouse, streamlining workflows for pre-training and fine-tuning models at scale. This session highlights context parallelism, a unique NeMo capability for parallelizing over sequence lengths, making it ideal for video datasets with large embeddings. Through the case study of TwelveLabs\u2019 Pegasus-1 model, learn how NeMo empowers scalable multimodal AI development, from text to video processing, setting a new standard for LLM workflows. /Chief Technology Officer & Co-Founder\nTwelve Labs, Inc /Solutions Architect - NVIDIA"}
{"session_id": "achieve-your-mission-ai-driven-decisions", "title": "Achieve Your Mission With AI-Driven Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Government leaders overwhelmingly recognize the potential benefits of AI as critical to long-term strategic goals of efficiency, but implementation challenges and security concerns could be obstacles to success. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "achieving-precision-ai-retrieving-right-data-using-ai-agents", "title": "Achieving Precision in AI: Retrieving the Right Data Using AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director"}
{"session_id": "adobes-security-lakehouse-ocsf-data-efficiency-and-threat-detection", "title": "Adobe\u2019s Security Lakehouse: OCSF, Data Efficiency and Threat Detection at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Security", "Real-time", "Scala"], "speakers": ["AntiMatter"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore how Adobe uses a sophisticated data security architecture built on the Databricks Data Intelligence Platform, along with the Open Cybersecurity Schema Framework (OCSF), to enable scalable, real-time threat detection across more than 10 PB of security data. We\u2019ll compare different approaches to OCSF implementation and demonstrate how Adobe processes massive security datasets efficiently \u2014 reducing query times by 18%, maintaining 99.4% SLA compliance, and supporting 286 security users across 17 teams with over 4,500 daily queries. By using Databricks' Platform for serverless compute, scalable architecture, and LLM-powered recommendations, Adobe has significantly improved processing speed and efficiency, resulting in substantial cost savings. We\u2019ll also highlight how OCSF enables advanced cross-tool analytics and automation, streamlining investigations. Finally, we\u2019ll introduce Databricks\u2019 new open-source OCSF toolkit for scalable security data normalization and invite the community to contribute. /Sr. Manager, Security Software Engineering\nAdobe /AntiMatter"}
{"session_id": "advanced-governance-and-auth-databricks-apps", "title": "Advanced Governance and Auth With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore advanced governance and authentication patterns for building secure, enterprise-grade apps with Databricks Apps. Learn how to configure complex permissions and manage access control using Unity Catalog. We\u2019ll dive into \u201con-behalf-of-user\u201d authentication \u2014 allowing agents to enforce user-specific access controls \u2014 and cover API-based authentication, including PATs and OAuth flows for external integrations. We\u2019ll also highlight how Addepar uses these capabilities to securely build and scale applications that handle sensitive financial data. Whether you're building internal tools or customer-facing apps, this session will equip you with the patterns and tools to ensure robust, secure access in your Databricks apps. /Staff Software Engineer"}
{"session_id": "advanced-machine-learning-operations", "title": "Advanced Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Python"], "speakers": ["CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course is designed to cover advanced concepts and workflows in machine learning operations. It starts by introducing participants to continuous integration (CI) and continuous development (CD) workflows within machine learning projects, guiding them through the deployment of a sample CI/CD workflow using Databricks in the first section. Moving on to the second part, participants delve into data and model testing, where they actively create tests and automate CI/CD workflows. Finally, the course concludes with an exploration of model monitoring concepts, demonstrating the use of Lakehouse Monitoring to oversee machine learning models in production settings. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. intermediate-level knowledge of traditional ML concepts, development with CI/CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "advanced-rag-overview-thawing-your-frozen-rag-pipeline", "title": "Advanced RAG Overview \u2014 Thawing Your Frozen RAG Pipeline", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ML Innovation, Experian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The most common RAG systems rely on a frozen RAG system \u2014 one where there\u2019s a single embedding model and single vector index. We\u2019ve achieved a modicum of success with that, but when it comes to increasing accuracy for production systems there is only so much this approach solves. In this session we will explore how to move from the frozen systems to adaptive RAG systems which produce more tailored outputs with higher accuracy. Databricks services: Lakehouse, Unity Catalog, Mosaic, Sweeps, Vector Search, Agent Evaluation, Managed Evaluation, Inference Tables /Head of AI/ML Innovation"}
{"session_id": "agentic-architectures-create-realistic-conversations-using-genai-teach", "title": "Agentic Architectures to Create Realistic Conversations: Using GenAI to Teach Empathy in Healthcare", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Providence Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Medical providers often receive less than 15 minutes of instruction in how to interact with patients during emotionally charged end of life interactions. Continuing education for clinicians is critical to hone these skills but is difficult to scale traditional approaches that require professional patients and instructors. Here, we describe a custom chatbot that plays the role of patient and coach to provide a scaling learning experience. A critical challenge was how to mitigate the persistently cheerful and helpful tone which results from standard pretraining in the Patient Persona AI. We accomplished this by implementing a multi-agent architecture based upon a graphical model of the conversation. System prompts reflecting the patient\u2019s cognitive state are dynamically updated as the conversation progresses. Future extensions of the work are intended to focus on additional custom model fine-tuning in the Mosaic AI platform to further improve the realism of the conversation. /Head of Data Science\nProvidence Health /Senior Data Scientist\nTegria Consulting/Providence Healthcare"}
{"session_id": "ai-agents-action-structuring-unstructured-data-demand-databricks-and", "title": "AI Agents in Action: Structuring Unstructured Data on Demand with Databricks and Unstructured", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Head of Product and Engineering, Unstructured"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Product and Engineering"}
{"session_id": "ai-agents-hackathon", "title": "AI Agents Hackathon", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "540 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "ai-agents-marketing-leveraging-mosaic-ai-create-multi-purpose-agentic", "title": "AI Agents for Marketing: Leveraging Mosaic AI to Create a Multi-Purpose Agentic Marketing Assistant", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Head of AI Center Excellence, 7-Eleven Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing professionals build campaigns, create content and use effective copywriting to tell a good story to promote a product/offer. All of this requires a thorough and meticulous process for every individual campaign. In order to assist marketing professionals at 7-Eleven, we built a multi-purpose assistant that could: We will walk you through how we created multiple agents as different personas with LangGraph and Mosaic AI to create a chat assistant that assumes a different persona based on the user query. We will also explain our evaluation methodology in choosing models and prompts and how we implemented guardrails for high reliability with sensitive marketing content. This assistant by 7-Eleven was showcased at the Databricks booth at NRF earlier this year. /Head of AI Center of Excellence"}
{"session_id": "ai-and-genie-analyzing-healthcare-improvement-opportunities", "title": "AI and Genie: Analyzing Healthcare Improvement Opportunities", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Software Architect, Premier Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Improving healthcare impacts us all. We highlight how Premier Inc. took risk-adjusted patient data from more than 1,300 member hospitals across America, applying a natural language interface using AI/BI Genie, allowing our users to discover new insights. The stakes are high, new insights surfaced represent potential care improvement and lives positively impacted. Using Genie and our AI-ready data in Unity Catalog, our team was able to stand up a Genie instance in three short days, bypassing costs and time of custom modeling and application development. Additionally, Genie allowed our internal teams to generate complex SQL, as much as 10 times faster than writing it by hand. As Genie and lakehouse apps continue to advance rapidly, we are excited to leverage these features by introducing Genie to as many as 20,000 users across hundreds of hospitals. This will support our members\u2019 ongoing mission to enhance the care they provide to the communities they serve. /Senior Director, Analytics\nPremier Inc /Software Architect"}
{"session_id": "ai-assisted-bi-everything-you-need-know", "title": "AI-Assisted BI: Everything You Need to Know", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how AI is transforming business intelligence and data analytics across the Databricks Platform. This session provides a comprehensive overview of AI-assisted capabilities \u2014 from auto-generating dashboards and visualizations to enabling conversational analytics with Genie. We\u2019ll also cover AI-powered notebooks and SQL editors that accelerate code generation, streamline exploratory analysis and simplify workflows. Whether you\u2019re a data engineer, analyst or BI developer, you\u2019ll walk away ready to harness AI to drive faster, smarter decisions across your organization. /Sr. Product Manager\nDatabricks /Databricks"}
{"session_id": "ai-driven-drug-discovery-accelerating-molecular-insights-nvidia-and", "title": "AI-Driven Drug Discovery: Accelerating Molecular Insights With NVIDIA and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the race to revolutionize healthcare and drug discovery, biopharma companies are turning to AI to streamline workflows and unlock new scientific insights. This session, we will explore how NVIDIA BioNeMo, combined with Databricks Delta Lakehouse, can be used for advancing drug discovery for critical applications like molecular structure modeling, protein folding and diagnostics. We\u2019ll demonstrate how BioNeMo pre-trained models can run inference on data securely stored in Delta Lake, delivering actionable insights. By leveraging containerized solutions on Databricks\u2019 ML Runtime with GPU acceleration, users can achieve significant performance gains compared to traditional CPU-based computation. /Solutions Architect - NVIDIA"}
{"session_id": "ai-meets-sql-leverage-genai-scale-enrich-your-data", "title": "AI Meets SQL: Leverage GenAI at Scale to Enrich Your Data", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "SQL"], "speakers": ["swe, databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI into existing data workflows can be challenging, often requiring specialized knowledge and complex infrastructure. In this session, we'll share how SQL users can leverage AI/ML to access large language models (LLMs) and traditional machine learning directly from within SQL, simplifying the process of incorporating AI into data workflows. We will demonstrate how to use Databricks SQL for natural language processing, traditional machine learning, retrieval augmented generation and more. You'll learn about best practices and see examples of solving common use cases such as opinion mining, sentiment analysis, forecasting and other common AI/ML tasks. /Sr. Product Manager\nDatabricks /swe"}
{"session_id": "ai-motion-build-roadmap-impact-just-30-minutes", "title": "AI in Motion: Build a Roadmap for Impact in Just 30 Minutes", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": ["Lead Data & AI Strategist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This high-velocity workshop is designed for data and AI leaders seeking to rapidly develop a comprehensive AI strategy tailored to their organization's needs. In just 30 minutes, participants will engage in a focused, interactive session that delivers actionable insights and a strategic framework for AI implementation. Key components of the workshop include: By the end of this intensive session, you will have the foundation of a robust AI strategy and guidance on roadmap execution. /Lead Data & AI Strategist"}
{"session_id": "ai-powered-data-discovery-and-curation-unity-catalog", "title": "AI-Powered Data Discovery and Curation With Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data landscape, the challenge isn\u2019t just storing or processing data \u2014 it\u2019s enabling every user, from data stewards to analysts, to find and trust the right data, fast. This session explores how Databricks is reimagining data discovery with the new Discover Page Experience \u2014 an intuitive, curated interface showcasing key data and workspace assets. We\u2019ll dive into AI-assisted governance and AI-powered discovery features like AI-generated metadata, AI-assisted lineage and natural language data exploration in Unity Catalog. Plus, see how new certifications and deprecations bring clarity to complex data environments. Whether you\u2019re a data steward highlighting trusted assets or an analyst navigating data without deep schema knowledge, this session will show how Databricks is making data discovery seamless for everyone. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "ai-powered-marketing-data-management-solving-dirty-data-problem", "title": "AI-Powered Marketing Data Management: Solving the Dirty Data Problem with Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["VP, Cloud Product Management & UX, Acxiom"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing teams struggle with \u2018dirty data\u2019 \u2014 incomplete, inconsistent, and inaccurate information that limits campaign effectiveness and reduces the accuracy of AI agents. Our AI-powered marketing data management platform, built on Databricks, solves this with anomaly detection, ML-driven transformations and the built-in Acxiom Referential Real ID Graph with Data Hygiene. We\u2019ll showcase how Delta Lake, Unity Catalog and DLT power our multi-tenant architecture, enabling secure governance and 75% faster data processing. Our privacy-first design ensures compliance with GDPR, CCPA and HIPAA through role-based access, encryption key management and fine-grained data controls. Join us for a live demo and Q&A, where we\u2019ll share real-world results and lessons learned in building a scalable, AI-driven marketing data solution with Databricks. /Principal Product Manager\nAcxiom /VP, Cloud Product Management & UX"}
{"session_id": "ai-powered-profits-smarter-order-and-inventory-management", "title": "AI-Powered Profits: Smarter Order and Inventory Management", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Data and Enterprise Digital, Xylem"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Solutions Architect\nDatabricks /VP, Data and Enterprise Digital"}
{"session_id": "ai-powering-epsilons-identity-strategy-unified-marketing-platform", "title": "AI Powering Epsilon's Identity Strategy: Unified Marketing Platform on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Data Warehouse", "Delta Lake", "ELT"], "speakers": ["Vice President, Decision Sciences, Epsilon Data Management"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to hear about how Epsilon Data Management migrated Epsilon\u2019s unique, AI-powered marketing identity solution from multi-petabyte on-prem Hadoop and data warehouse systems to a unified Databricks Lakehouse platform. This transition enabled Epsilon to further scale its Decision Sciences solution and enable new cloud-based AI research capabilities on time and within budget, without being bottlenecked by the resource constraints of on-prem systems. Learn how Delta Lake, Unity Catalog, MLflow and LLM endpoints powered massive data volume, reduced data duplication, improved lineage visibility, accelerated Data Science and AI, and enabled new data to be immediately available for consumption by the entire Epsilon platform in a privacy-safe way. Using the Databricks platform as the base for AI and Data Science at global internet scale, Epsilon deploys marketing solutions across multiple cloud providers and multiple regions for many customers. /Vice President , Database\nEpsilon Data Management /Vice President, Decision Sciences"}
{"session_id": "aibi-dashboards-and-aibi-genie-dashboards-and-last-mile-analytics-made", "title": "AI/BI Dashboards and AI/BI Genie: Dashboards and Last-Mile Analytics Made Simple", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["VP of Data + AI Architecture, JosueBogran.com & zeb.co"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks announced two new features in 2024: AI/BI Dashboards and AI/BI Genie. Dashboards is a redesigned dashboarding experience for your regular reporting needs, while Genie provides a natural language experience for your last-mile analytics. In this session, Databricks Solutions Architect and content creator Youssef Mrini will present alongside Databricks MVP and content creator Josue A. Bogran on how you can get the most value from these tools for your organization. Content covered includes: Fluff-free, full of practical tips, and geared to help you deliver immediate impact with these new Databricks capabilities. /Solutions Architect\nDatabricks /VP of Data + AI Architecture"}
{"session_id": "aibi-data-analysts", "title": "AI/BI for Data Analysts", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence"], "speakers": ["abilities: Labs: Yes"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to use the features Databricks provides for business intelligence needs: AI/BI Dashboards and AI/BI Genie. As a Databricks Data Analyst, you will be tasked with creating AI/BI Dashboards and AI/BI Genie Spaces within the platform, managing the access to these assets by stakeholders and necessary parties, and maintaining these assets as they are edited, refreshed, or decommissioned over the course of their lifespan. This course intends to instruct participants on how to design dashboards for business insights, share those with collaborators and stakeholders, and maintain those assets within the platform. Participants will also learn how to utilize AI/BI Genie Spaces to support self-service analytics through the creation and maintenance of these environments powered by the Databricks Data Intelligence Engine. Pre-requisites: The content was developed for participants with these skills/knowledge/abilities: Labs: Yes"}
{"session_id": "aibi-driving-speed-value-supply-chain", "title": "AI/BI Driving Speed to Value in Supply Chain", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science"], "speakers": ["Data Science Manager, Conagra Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Conagra is a global food manufacturer with $12.2B in revenue, 18K+ employees, 45+ plants in US, Canada and Mexico. Conagra's Supply Chain organization is heavily focused on delivering results in productivity, waste reduction, inventory rationalization, safety and customer service levels. By migrating the Supply Chain reporting suite to Databricks over the past 2 years, Conagra's Supply Chain Analytics & Data Science team has been able to deliver new AI solutions which complement traditional BI platforms and lay the foundation for additional AI/ML applications in the future. With Databricks Genie integrated within traditional BI reports, Conagra Supply Chain users can now go from insight to action faster and with fewer clicks, enabling speed to value in a complex Supply Chain. The Databricks platform also allows the team to curate data products to be consumed by traditional BI applications today as well as the ability to rapidly scale for the AI/ML applications of tomorrow. /Head of SC Adv Analytics & Data Science\nConagra Brands /Data Science Manager"}
{"session_id": "aibi-genie-look-under-hood-everyones-friendly-neighborhood-genai", "title": "AI/BI Genie: A Look Under the Hood of Everyone's Friendly, Neighborhood GenAI Product", "track": "ANALYTICS AND BI", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["swe, databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Go beyond the user interface and explore the cutting-edge technology driving AI/BI Genie. This session breaks down the AI/BI Genie architecture, showcasing how LLMs, retrieval-augmented generation (RAG) and finely tuned knowledge bases work together to deliver fast, accurate responses. We\u2019ll also explore how AI agents orchestrate workflows, optimize query performance and continuously refine their understanding. Ideal for those who want to geek out about the tech stack behind Genie, this session offers a rare look at the magic under the hood. /Software Engineer\nDatabricks /swe"}
{"session_id": "aibi-self-service-analytics", "title": "AI/BI for Self-Service Analytics", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you will learn how to self-serve business insights from your company\u2019s Databricks Data Intelligence Platform using AI/BI. After a tour of the fundamental components of the platform, you\u2019ll learn how to interact with pre-created AI/BI Dashboards to explore your company\u2019s data through existing charts and visualizations. You\u2019ll also learn how to use AI/BI Genie to go beyond dashboards by asking follow-up questions in natural language to self-serve new insights, create visualizations, and share them with your colleagues. Pre-requisites: A working understanding of your organization\u2019s business and key performance indicators. Labs: No Certification Path: N/A"}
{"session_id": "amplifying-human-human-connection-face-mental-health-crisis-using", "title": "Amplifying Human-to-Human Connection in the Face of Mental Health Crisis Using Agentic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "LLAMA", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Data Scientist, Crisis Text Line"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Crisis Text Line has been innovating for ten years in text-based mental health crisis intervention and is now leading the next wave of GenAI use cases in the space. With over 300 million messages exchanged since 2013 and a decade of expertise, Crisis Text Line is unlocking the potential of AI to amplify human connection at a global scale.We will discuss how we leveraged our bedrock application to co-navigate crisis care through a set of early AI agent workflows. First, a simulator that reproduces texter behavior to train responders in taking conversations ranging in difficulty where the texter is in imminent risk of suicide or self-harm. Second, a tool that automatically monitors clinical quality of conversations. Third, predicted summarization to capture key context before conversations are transferred. Through the power of suggestion, this compound system aims to reduce burden and drive efficiency, such that our responders can focus on what they do best \u2014 support people in need. /Principal Product Manager\nCrisis Text Line /Lead Data Scientist"}
{"session_id": "analyst-roadmap-databricks-sql-end-end-bi", "title": "Analyst Roadmap to Databricks: From SQL to End-to-End BI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "SQL"], "speakers": ["Senior Business Analyst, Spencer Gifts"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Analysts often begin their Databricks journey by running familiar SQL queries in the SQL Editor, but that\u2019s just the start. In this session, I\u2019ll share the roadmap I followed to expand beyond ad-hoc querying into SQL Editor/notebook-driven development to scheduled data pipelines producing interactive dashboards \u2014 all powered by Databricks SQL and Unity Catalog. You\u2019ll learn how to organize tables with primary-key/foreign-key relationships along with creating table and column comments to form the semantic model, utilizing DBSQL features like RELY constraints. I\u2019ll also show how parameterized dashboards can be set up to empower self-service analytics and feed into Genie Spaces. Attendees will walk away with best practices for starting out with building a robust BI platform on Databricks, including tips for table design and metadata enrichment. Whether you\u2019re a data analyst or BI developer, this talk will help you unlock powerful, AI-enhanced analytics workflows. /Senior Business Analyst"}
{"session_id": "anomaly-detection-apple-large-scale-data-using-apache-spark-and-flink", "title": "Anomaly Detection at Apple for Large-Scale Data Using Apache Spark and Flink", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Software Engineer, Apple Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Anomaly detection in time series data is crucial for identifying unusual patterns and trends, enabling better alerting and action when data deviates from normal. Most anomaly detection algorithms perform adequately on a single node machine with public datasets, but do not scale well with distributed processing frameworks used in modern big data environments. This talk will focus on how we scaled anomaly detection for large-scale datasets using Apache Spark and Flink for both batch and near real-time use cases. We will also discuss how we leveraged Apache Spark to parallelize and scale common anomaly detection algorithms, enabling support for large-scale data processing. We'll highlight some of the challenges faced and how we resolved them to make it useful for massive datasets with varying degree of anomalies. Finally, we will demonstrate how our anomaly detection framework works in batch for petabytes of data and in streaming mode for hundreds of thousands of transactions per second. /Senior Machine Learning Engineer\nApple Inc /Principal Software Engineer"}
{"session_id": "apache-iceberg-unity-catalog-hellofresh", "title": "Apache Iceberg with Unity Catalog at HelloFresh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["Senior Staff Data Engineer, HelloFresh"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Table formats like Delta Lake and Iceberg have been game changers for pushing lakehouse architecture into modern Enterprises. The acquisition of Tabular added Iceberg to the Databricks ecosystem, an open format that was already well supported by processing engines across the industry. At HelloFresh we are building a lakehouse architecture that integrates many touchpoints and technologies all across the organization. As such we chose Iceberg as the table format to bridge the gaps in our decentralized managed tech landscape. We are leveraging Unity Catalog as the Iceberg REST catalog of choice for storing metadata and managing tables. In this talk we will outline our architectural setup between Databricks, Spark, Flink and Snowflake and will explain the native Unity Iceberg REST catalog, as well as catalog federation towards connected engines. We will highlight the impact on our business and discuss the advantages and lessons learned from our early adopter experience. /Director of Data Engineering\nHelloFresh /Senior Staff Data Engineer"}
{"session_id": "apache-spark-ask-us-anything", "title": "Apache Spark \u2014 Ask Us Anything", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Staff Developer Advocate:Technical, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an interactive Ask Me Anything (AMA) session on the latest advancements in Apache Spark 4, including Spark Connect\u2014the new client-server architecture enabling seamless integration with IDEs, notebooks and custom applications. Learn about performance improvements, enhanced APIs and best practices for leveraging Spark\u2019s next-generation features. Whether you're a data engineer, Spark developer or big data enthusiast, bring your questions on architecture, real-world use cases and how these innovations can optimize your workflows. Don\u2019t miss this chance to dive deep into the future of distributed computing with Spark! /Staff Software Engineer\nDatabricks /Senior Engineering Manager\nDatabricks /Staff Developer Advocate:Technical Staff"}
{"session_id": "att-autoclassify-unified-multi-head-binary-classification-unlabeled", "title": "AT&T AutoClassify: Unified Multi-Head Binary Classification From Unlabeled Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, AT&T"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present AT&T AutoClassify, built jointly between AT&T's Chief Data Office (CDO) and Databricks professional services, a novel end-to-end system for automatic multi-head binary classifications from unlabeled text data. Our approach automates the challenge of creating labeled datasets and training multi-head binary classifiers with minimal human intervention. Starting only from a corpus of unlabeled text and a list of desired labels, AT&T AutoClassify leverages advanced natural language processing techniques to automatically mine relevant examples from raw text, fine-tune embedding models and train individual classifier heads for multiple true/false labels. This solution can reduce LLM classification costs by 1,000x, making it an efficient solution in operational costs. The end result is a highly optimized and low-cost model servable in Databricks capable of taking raw text and producing multiple binary classifications. An example use case using call transcripts will be examined. /Staff Data Scientist\nDatabricks /Senior Data Scientist"}
{"session_id": "authoring-data-pipelines-new-dlt-editor", "title": "Authoring Data Pipelines With the New DLT Editor", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re introducing a new developer experience for DLT designed for data practitioners who prefer a code-first approach and expect robust developer tooling. The new multi-file editor brings an IDE-like environment to declarative pipeline development, making it easy to structure transformation logic, configure pipelines throughout the development lifecycle and iterate efficiently. Features like contextual data previews and selective table updates enable step-by-step development. UI-driven tools, such as DAG previews and DAG-based actions, enhance productivity for experienced users and provide a bridge for those transitioning to declarative workflows. In this session, we\u2019ll showcase the new editor in action, highlighting how these enhancements simplify declarative coding and improve development for production-ready data pipelines. Whether you\u2019re an experienced developer or new to declarative data engineering, join us to see how DLT can enhance your data practice. /Sr. Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "automated-deployment-databricks-asset-bundles", "title": "Automated Deployment with Databricks Asset Bundles", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate project deployments Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge platform, including experience Workspaces, Apache Spark, Delta Lake, Medallion Architecture, Unity Catalog, Live Tables, and Workflows. In particular, leveraging Expectations DLTs. Labs: Yes Certification Path: Certified Data Engineer Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides a comprehensive review of DevOps principles and their application to Databricks projects. It begins with an overview of core DevOps, DataOps, continuous integration (CI), continuous deployment (CD), and testing, and explores how these principles can be applied to data engineering pipelines. The course then focuses on continuous deployment within the CI/CD process, examining tools like the Databricks REST API, SDK, and CLI for project deployment. You will learn about Databricks Asset Bundles (DABs) and how they fit into the CI/CD process. You\u2019ll dive into their key components, folder structure, and how they streamline deployment across various target environments in Databricks. You will also learn how to add variables, modify, validate, deploy, and execute Databricks Asset Bundles for multiple environments with different configurations using the Databricks CLI. Finally, the course introduces Visual Studio Code as an Interactive Development Environment (IDE) for building, testing, and deploying Databricks Asset Bundles locally, optimizing your development process. The course concludes with an introduction to automating deployment pipelines using GitHub Actions to enhance the CI/CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate Databricks project deployments with Databricks Asset Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge of the Databricks platform, including experience with Databricks Workspaces, Apache Spark, Delta Lake, the Medallion Architecture, Unity Catalog, Delta Live Tables, and Workflows. In particular, knowledge of leveraging Expectations with DLTs. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "automating-engineering-ai-llms-metadata-driven-frameworks", "title": "Automating Engineering with AI - LLMs in Metadata Driven Frameworks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Quality"], "speakers": ["CTO, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for data engineering keeps growing, but data teams are bored by repetitive tasks, stumped by growing complexity and endlessly harassed by an unrelenting need for speed. What if AI could take the heavy lifting off your hands? What if we make the move away from code-generation and into config-generation \u2014 how much more could we achieve? In this session, we\u2019ll explore how AI is revolutionizing data engineering, turning pain points into innovation. Whether you\u2019re grappling with manual schema generation or struggling to ensure data quality, this session offers practical solutions to help you work smarter, not harder. You\u2019ll walk away with a good idea of where AI is going to disrupt the data engineering workload, some good tips around how to accelerate your own workflows and an impending sense of doom around the future of the industry! /CTO"}
{"session_id": "automating-taxonomy-generation-compound-ai-databricks", "title": "Automating Taxonomy Generation With Compound AI on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Consultant, Data & AI, Lovelytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Taxonomy generation is a challenge across industries such as retail, manufacturing and e-commerce. Incomplete or inconsistent taxonomies can lead to fragmented data insights, missed monetization opportunities and stalled revenue growth. In this session, we will explore a modern approach to solving this problem by leveraging Databricks platform to build a scalable compound AI architecture for automated taxonomy generation. The first half of the session will walk you through the business significance and implications of taxonomy, followed by a technical deep dive in building an architecture for taxonomy implementation on the Databricks platform using a compound AI architecture. We will walk attendees through the anatomy of taxonomy generation, showcasing an innovative solution that combines multimodal and text-based LLMs, internal data sources and external API calls. This ensemble approach ensures more accurate, comprehensive and adaptable taxonomies that align with business needs. /Managing Director, GenAI\nLovelytics /Lead Consultant, Data & AI"}
{"session_id": "autonomous-ai-agents-ai-infrastructure", "title": "Autonomous AI Agents in AI Infrastructure", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Real-time", "Scala"], "speakers": ["Principal Software Engineer, Walmart Global Tech"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Autonomous AI agents are transforming industries by enabling systems to perform tasks, make decisions and adapt in real time without human intervention. In this talk, I will delve into the architecture and design principles required to build these agents within scalable AI infrastructure. Key topics will include constructing modular, reusable frameworks, optimizing resource allocation and enabling interoperability between agents and data pipelines. I will discuss practical use cases in which attendees will learn how to leverage containerization and orchestration techniques to enhance the flexibility and performance of these agents while ensuring low-latency decision-making. This session will also highlight challenges like ensuring robustness, ethical considerations and strategies for real-time feedback loops. Participants will gain actionable insights into building autonomous AI agents that drive efficiency, scalability and innovation in modern AI ecosystems. /Principal Software Engineer"}
{"session_id": "bayadas-snowflake-databricks-migration-transforming-data-speed", "title": "Bayada\u2019s Snowflake-to-Databricks Migration: Transforming Data for Speed & Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Integration", "Machine Learning", "Real-time", "SQL", "Scala"], "speakers": ["CDAO, BAYADA Home Health Care"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayada is transforming its data ecosystem by consolidating Matillion+Snowflake and SSIS+SQL Server into a unified Enterprise Data Platform powered by Databricks. Using Databricks' Medallion architecture, this platform enables seamless data integration, advanced analytics and machine learning across critical domains like general ledger, recruitment and activity-based costing. Databricks was selected for its scalability, real-time analytics and ability to handle both structured and unstructured data, positioning Bayada for future growth. The migration aims to reduce data processing times by 35%, improve reporting accuracy and cut reconciliation efforts by 40%. Operational costs are projected to decrease by 20%, while real-time analytics is expected to boost efficiency by 15%. Join this session to learn how Bayada is leveraging Databricks to build a high-performance data platform that accelerates insights, drives efficiency and fosters innovation organization-wide. /Head of Data Architecture & Governance\nBAYADA Home Health Care /Sr. Director - HLS\nTredence Inc /CDAO"}
{"session_id": "best-practices-building-user-facing-ai-systems-databricks", "title": "Best Practices for Building User-Facing AI Systems on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI agents into business systems requires tailored approaches for different maturity levels (crawl-walk-run) that balance scalability, accuracy and usability. This session addresses the critical challenge of making AI agents accessible to business users. We will explore four key integration methods: We'll compare these approaches, discussing their strengths, challenges and ideal use cases to help businesses select the most suitable integration strategy for their specific needs. /Senior Specialist Solutions Architect\nDatabricks /Senior Solutions Architect"}
{"session_id": "better-together-change-data-feed-streaming-data-flow", "title": "Better Together: Change Data Feed in a Streaming Data Flow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Streaming"], "speakers": ["Data Engineer & Architect, 84.51 LLC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditional streaming works great when your data source is append-only, but what if your data source includes updates and deletes? At 84.51 we used DLT and Delta Lake to build a streaming data flow that consumes inserts, updates and deletes while still taking advantage of streaming checkpoints. We combined this flow with a materialized view and Enzyme incremental refresh for a low-code, efficient and robust end-to-end data flow. We process around 8 million sales transactions each day with 80 million items purchased. This flow not only handles new transactions but also handles updates to previous transactions. Join us to learn how 84.51 combined change data feed, data streaming and materialized views to deliver a \u201cbetter together\u201d solution. 84.51 is a retail insights, media & marketing company. We use first-party retail data from 60 million households sourced through a loyalty card program to drive Kroger\u2019s customer-centric journey. /Lead Data Engineer\n84.51\u02da /Data Engineer & Architect"}
{"session_id": "beyond-ai-accuracy-building-trustworthy-and-responsible-ai-application", "title": "Beyond AI Accuracy: Building Trustworthy and Responsible AI Application Through Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Generic LLM metrics are useless until it meets your business needs.In this session we will dive deep into creating bespoke custom state-of-the-art AI metrics that matters to you. Discuss best practices on LLM evaluation strategies, when to use LLM judge vs. statistical metrics and many more. Through a live demo using Mosaic AI Framework, we will showcase: By the end of this session, you'll be equipped to create AI solutions that are not only powerful but also relevant to your organizations needs. Join us to transform your AI strategy and make a tangible impact on your business! /Specialist Solution Architect"}
{"session_id": "beyond-chatbots-building-autonomous-insurance-applications-agentic-ai", "title": "Beyond Chatbots: Building Autonomous Insurance Applications With Agentic AI Framework", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["VP, Software Engineering, Travelers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The insurance industry is at the crossroads of digital transformation, facing challenges from market competition and customer expectations. While conventional ML applications have historically provided capabilities in this domain, the emergence of Agentic AI frameworks presents a revolutionary opportunity to build truly autonomous insurance applications. We will address issues related to data governance and quality while discussing how to monitor/evaluate fine-tune models. We'll demonstrate the application of the agentic framework in the insurance context and how these autonomous agents can work collaboratively to handle complex insurance workflows \u2014 from submission intake and risk evaluation to expedited quote generation. This session demonstrates how to architect intelligent insurance solutions using Databricks Mosaic AI agentic core components including Unity Catalog, Playground, model evaluation/guardrails, privacy filters, AI functions and AI/BI Genie. /Sr. Delivery Solutions Architect\nDatabricks /VP, Software Engineering"}
{"session_id": "beyond-privacy-utility-tradeoff-differential-privacy-tabular-data", "title": "Beyond the Privacy-Utility Tradeoff: Differential Privacy in Tabular Data Synthesis", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Scientist, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly leverage sensitive data for AI applications, generating high quality synthetic data with mathematical guarantees of privacy has become crucial. This talk explores the use of Gretel Navigator to generate differentially private synthetic data that maintains high fidelity to the source data and high utility on downstream tasks across heterogeneous datasets. Our analysis covers a framework for privacy-preserving synthetic data generation with two use cases: patient events and e-commerce reviews. We reveal nuanced strategies for: calibrating privacy parameters \u03b5 and \u03b4 for mixed-modal data, leveraging both record-level and user-level differential privacy depending on which entity in the dataset requires protection, maintaining statistical properties and high utility on downstream classification tasks under stringent privacy constraints (e.g., <0.05 difference in AUC when using DP), and quantifying resilience to membership inference and attribute inference attacks. /Research Scientist"}
{"session_id": "boosting-data-science-and-ai-productivity-databricks-notebooks", "title": "Boosting Data Science and AI Productivity With Databricks Notebooks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to accelerate your team's data science workflow? This session reveals how Databricks Notebooks can transform your productivity through an optimized environment designed specifically for data science and AI work. Discover how notebooks serve as a central collaboration hub where code, visualizations, documentation and results coexist seamlessly, enabling faster iteration and development. Key takeaways: You'll leave with practical techniques to enhance your notebook-based workflow and deliver AI projects faster with higher-quality results. /Databricks"}
{"session_id": "breaking-barriers-building-custom-spark-40-data-connectors-python", "title": "Breaking Barriers: Building Custom Spark 4.0 Data Connectors with Python", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python", "Scala", "Streaming"], "speakers": ["Resident Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building a custom Spark data source connector once required Java or Scala expertise, making it complex and limiting. This left many proprietary data sources without public SDKs disconnected from Spark. Additionally, data sources with Python SDKs couldn't harness Spark\u2019s distributed power. Spark 4.0 changes this with a new Python API for data source connectors, allowing developers to build fully functional connectors without Java or Scala. This unlocks new possibilities, from integrating proprietary systems to leveraging untapped data sources. Supporting both batch and streaming, this API makes data ingestion more flexible than ever. In this talk, we\u2019ll demonstrate how to build a Spark connector for Excel using Python, showcasing schema inference, data reads/writes and streaming support. Whether you're a data engineer or Spark enthusiast, you\u2019ll gain the knowledge to integrate Spark with any data source \u2014 entirely in Python. /Senior Resident Solutions Architect\nDatabricks /Resident Solutions Architect"}
{"session_id": "breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse", "title": "Breaking the Ice(berg): Riskified\u2019s Journey to its Next-Generation Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Warehouse"], "speakers": ["Staff Data Platform Engineer, Riskified"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How many of you manage multiple data stores in your organization, wrestling with different data formats that led to data duplication, infrastructure redundancy, and uncontrolled costs? Imagine reducing these costs by 25% while maintaining your existing SLAs \u2014 this is exactly what we achieved with our next-generation architecture. In this session, we'll show you how we built Riskified's next-generation lakehouse by leveraging Databricks' native Apache Iceberg support and Unity Catalog. We'll share our innovative approach to cross-platform querying without data duplication, and how we transformed our data warehouse into a modern lakehouse architecture. Throughout the session, we'll explore the technical challenges we conquered, from data migration to performance optimization. The result? A simplified world where everything is identical across engines, leaving users with just one choice \u2014 which query engine best suits their use case. /Data Platform Architect\nRiskified /Staff Data Platform Engineer"}
{"session_id": "breaking-silos-cignas-journey-seamless-data-sharing-delta-sharing", "title": "Breaking Silos: Cigna\u2019s Journey to Seamless Data Sharing with Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Real-time"], "speakers": ["Senior Director, Evernorth Health Services"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow increasingly complex, the ability to share data securely, seamlessly, and in real time has become a strategic differentiator. In this session, Cigna will showcase how Delta Sharing on Databricks has enabled them to modernize data delivery, reduce operational overhead, and unlock new market opportunities. Learn how Cigna achieved significant savings by streamlining operations, compute, and platform overhead for just one use case. Explore how decentralizing data ownership\u2014transitioning from hyper-centralized teams to empowered product owners\u2014has simplified delivery and accelerated innovation. Most importantly, see how this modern open data-sharing framework has positioned Cigna to win contracts they previously couldn\u2019t, by enabling real-time, cross-organizational data collaboration with external partners. Join us to hear how Cigna is using Delta Sharing not just as a technical enabler, but as a business catalyst. /Senior Director"}
{"session_id": "breaking-silos-enabling-databricks-snowflake-interoperability-iceberg", "title": "Breaking Silos: Enabling Databricks-Snowflake Interoperability With Iceberg and Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT"], "speakers": ["Member of Technical Staff, Sol Arch, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow more complex, organizations often struggle with siloed platforms and fragmented governance. In this session, we\u2019ll explore how our team made Databricks the central hub for cross-platform interoperability, enabling seamless Snowflake integration through Unity Catalog and the Iceberg REST API. We\u2019ll cover: By leveraging Uniform, Delta, and Iceberg, we created a flexible, vendor-agnostic architecture that bridges Databricks and Snowflake without compromising performance or security. /Director of Data Engineering\nT-Mobile /Member of Technical Staff, Sol Arch"}
{"session_id": "breaking-silos-using-sap-delta-sharing-connector-seamless-access", "title": "Breaking Silos: Using the SAP Delta Sharing Connector for Seamless Access in Databricks", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT", "ETL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re excited to announce the General Availability of the SAP Delta Sharing Connector, enabling SAP Business Data Cloud (BDC) customers to securely and seamlessly share data with Databricks \u2014 no complex ETL or data duplication required. This connector enables organizations to securely share SAP data for analytics and AI in Databricks, while also supporting bidirectional data sharing back to SAP. In this session, we\u2019ll walk through a live demo of the SAP Delta Sharing Connector in action, followed by a real-world customer story showcasing how this integration is driving value across analytics and AI use cases. You\u2019ll also leave with practical best practices to help you implement the connector in your own environment. Whether you\u2019re looking to bring SAP data into Databricks for advanced analytics or build AI models on top of trusted SAP datasets, this session will show you how to get started \u2014 securely and efficiently. /Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "breaking-spark-versions-better-way-manage-workload-compatibility", "title": "Breaking With Spark Versions: A Better Way to Manage Workload Compatibility + Dependency Management", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Product Manager - serverless jedi, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explains how we\u2019ve made Apache Spark\u2122 versionless for end users by introducing a stable client API, environment versioning and automatic remediation. These capabilities have enabled auto-upgrade of hundreds of millions of workloads with minimal disruption. We\u2019ll also introduce a new approach to dependency management using environments. Admins will learn how to speed up package installation with Default Base Environments, and users will see how to manage custom environments for their own workloads. /Staff Product Manager - serverless jedi"}
{"session_id": "bridging-bi-tools-deep-dive-aibi-dashboards-power-bi-practitioners", "title": "Bridging BI Tools: Deep Dive Into AI/BI Dashboards for Power BI Practitioners", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Snr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the rapidly-evolving field of data analytics, (AI/BI) dashboards and Power BI stand out as two formidable approaches, each offering unique strengths and catering to specific use cases. Power BI has earned its reputation for delivering user-friendly, highly customisable visualisations and reports for data analysis. On the other hand, AI/BI dashboards have gained good traction due to their seamless integration with the Databricks platform, making them an attractive option for data practitioners. This session will provide a comparison of these two tools, highlighting their respective features, strengths and potential limitations. Understanding the nuances between these tools is crucial for organizations aiming to make informed decisions about their data analytics strategy. This session will equip participants with the knowledge needed to select the most appropriate tool or combination of tools to meet their data analysis requirements and drive data-informed decision-making processes. /Senior Solutions Architect\nDatabricks /Snr. Solutions Architect"}
{"session_id": "bridging-big-data-and-ai-empowering-pyspark-lance-format-multi-modal-ai", "title": "Bridging Big Data and AI: Empowering PySpark With Lance Format for Multi-Modal AI Data Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Machine Learning", "Python", "SQL"], "speakers": ["Database Engineer, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark has long been a cornerstone of big data processing, excelling in data preparation, analytics and machine learning tasks within traditional data lakes. However, the rise of multimodal AI and vector search introduces challenges beyond its capabilities. Spark\u2019s new Python data source API enables integration with emerging AI data lakes built on the multi-modal Lance format. Lance delivers unparalleled value with its zero-copy schema evolution capability and robust support for large record-size data (e.g., images, tensors, embeddings, etc), simplifying multimodal data storage. Its advanced indexing for semantic and full-text search, combined with rapid random access, enables high-performance AI data analytics to the level of SQL. By unifying PySpark's robust processing capabilities with Lance's AI-optimized storage, data engineers and scientists can efficiently manage and analyze the diverse data types required for cutting-edge AI applications within a familiar big data framework. /Staff Software Engineer\nDatabricks /Database Engineer"}
{"session_id": "bringing-ai-your-data-using-sql-functions-databricks", "title": "Bringing AI to Your Data: Using SQL Functions in Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks AI Functions make it easy for analysts and data engineers to integrate advanced AI capabilities into data workflows \u2014 no ML expertise is required. These built-in SQL functions let you apply tasks like sentiment analysis, text summarization and language translation directly to data in Databricks, whether you're working in SQL queries, notebooks, DLT or Jobs. This session will walk through practical applications of both general-purpose and task-specific AI Functions, showing how to analyze text data at scale using simple SQL. You'll learn how to convert unstructured content into structured insights, and how to embed AI directly into batch pipelines and analytics processes. What you\u2019ll learn: /Databricks"}
{"session_id": "build-ai-powered-applications-natively-databricks", "title": "Build AI-Powered Applications Natively on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI", "Analytics", "Data Quality", "Scala"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how to build and deploy AI-powered applications natively on the Databricks Data Intelligence Platform. This session introduces best practices and a standard reference architecture for developing production-ready apps using popular frameworks like Dash, Shiny, Gradio, Streamlit and Flask. Learn how to leverage agents for orchestration and explore primary use cases supported by Databricks Apps, including data visualization, AI applications, self-service analytics and data quality monitoring. With serverless deployment and built-in governance through Unity Catalog, Databricks Apps enables seamless integration with your data and AI models, allowing you to focus on delivering impactful solutions without the complexities of infrastructure management. Whether you're a data engineer or an app developer, this session will equip you with the knowledge to create secure, scalable and efficient applications within a Databricks environment. /Staff Software Engineer"}
{"session_id": "build-data-pipelines-dlt", "title": "Build Data Pipelines with DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to define and schedule data pipelines that incrementally ingest and process data through multiple tables on the Data Intelligence Platform, using DLT in Spark SQL and Python. We\u2019ll cover topics like how to get started with DLT, how DLT tracks data dependencies in data pipelines, how to configure and run data pipelines using the DLT. UI, how to use Python or Spark SQL to define data pipelines that ingest and process data through multiple tables on the Data Intelligence Platform, using Auto Loader and DLT, how to use APPLY CHANGES INTO syntax to process Change Data Capture feeds, and how to review event logs and data artifacts created by pipelines and troubleshoot syntax. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "build-your-data-and-ai-culture", "title": "Build Your Data and AI Culture", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Learning & Enablement APJ, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Many studies have indicated that having a strong Data & AI culture helps our businesses be more successful. This can lead to better business performance, becoming more profitable and being more competitive compared to your peer companies as well as attaining and retaining top talent. What does it mean to have a Data & AI culture? It\u2019s the ability for an organization to make data-driven decisions. It means using insights to improve your business results and using data ultimately allows you to enable AI. It tends to be the people that get in the way of having and sustaining an effective Data & AI culture. Do you have people already in your teams that can help you build your Data & AI culture? Can you attract and retain that talent to your organization? Can you help integrate that great talent into your organization to promote a Data & AI culture? It\u2019s also ensuring that you fundamentally change the way you/your teams/organizations work. /Sr.Director Customer Enablement\nDatabricks /Director, Learning & Enablement APJ"}
{"session_id": "building-ai-models-health-care-using-semi-synthetic-data", "title": "Building AI Models In Health Care Using Semi-Synthetic Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Co-founder, Fight Health Insurance INC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Regulated or restricted fields like Health Care make collecting training data complicated. We all want to do the right thing, but how? This talk will look at how Fight Health Insurance used de-identified public and proprietary information to create a semi-synthetic training set for use in fine-tuning machine learning models to power Fight Paperwork. We'll explore how to incorporate the latest \"reasoning\" techniques in fine tuning as well as how to make models that you can afford to serve \u2014 think single GPU inference instead of a cluster of A100s. In addition to the talk we have the code used in a public GitHub repo \u2014 although it is a little rough, so you might want to use it more as a source of inspiration rather than directly forking it. /Co-founder"}
{"session_id": "building-and-scaling-production-ai-systems-mosaic-ai", "title": "Building and Scaling Production AI Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP of Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ready to go beyond the basics of Mosaic AI? This session will walk you through how to architect and scale production-grade AI systems on the Databricks Data Intelligence Platform. We\u2019ll cover practical techniques for building end-to-end AI pipelines \u2014 from processing structured and unstructured data to applying Mosaic AI tools and functions for model development, deployment and monitoring. You\u2019ll learn how to integrate experiment tracking with MLflow, apply performance tuning and use built-in frameworks to manage the full AI lifecycle. By the end, you\u2019ll be equipped to design, deploy and maintain AI systems that deliver measurable outcomes at enterprise scale. /CTO, Neural Networks\nDatabricks /VP of Engineering"}
{"session_id": "building-dashboards-production-grade-data-product", "title": "Building Dashboards as a Production-Grade Data Product", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Software Development Engineer, Zillow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Zillow, we have accelerated the volume and quality of our dashboards by leveraging a modern SDLC with version control and CI/CD. In the past three months, we have released 32 production-grade dashboards and shared them securely across the organization while cutting error rates in half over that span. In this session, we will provide an overview of how we utilize Databricks asset bundles and GitLab CI/CD to create performant dashboards that can be confidently used for mission-critical operations. As a concrete example, we'll then explore how Zillow's Data Platform team used this approach to automate our on-call support analysis, leveraging our dashboard development strategy alongside Databricks LLM offerings to create a comprehensive view that provides actionable performance metrics alongside AI-generated insights and action items from the hundreds of requests that make up our support workload. /Senior Software Development Engineer"}
{"session_id": "building-intelligent-ai-agents-claude-models-and-databricks-mosaic-ai", "title": "Building Intelligent AI Agents With Claude Models and Databricks Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Technical Staff, Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how Anthropic's frontier models power AI agents in Databricks Mosaic AI Agent Framework. Learn to leverage Claude's state-of-the-art capabilities for complex agentic workflows while benefiting from Databricks unified governance, credential management and evaluation tools. We'll demonstrate how Anthropic's models integrate seamlessly to create production-ready applications that combine Claude's reasoning with Databricks data intelligence capabilities. /Technical Staff"}
{"session_id": "building-knowledge-agents-automate-document-workflows", "title": "Building Knowledge Agents to Automate Document Workflows", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-founder and CEO, LlamaIndex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "One of the biggest promises for LLM agents is automating all knowledge work over unstructured data \u2014 we call these \"knowledge agents\". To date, while there are fragmented tools around data connectors, storage and agent orchestration, AI engineers have trouble building and shipping production-grade agents beyond basic chatbots. In this session, we first outline the highest-value knowledge agent use cases we see being built and deployed at various enterprises. These are: We then define the core architectural components around knowledge management and agent orchestration required to build these use cases. By the end you'll not only have an understanding of the core technical concepts, but also an appreciation of the ROI you can generate for end-users by shipping these use cases to production. /Co-founder and CEO"}
{"session_id": "building-real-time-sport-model-insights-spark-structured-streaming", "title": "Building Real-Time Sport Model Insights with Spark Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Lead Data Science Engineer, Draftkings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the dynamic world of sports betting, precision and adaptability are key. Sports traders must navigate risk management, limitations of data feeds, and much more to prevent small model miscalculations from causing significant losses. To ensure accurate real-time pricing of hundreds of interdependent markets, traders provide key inputs such as player skill-level adjustments, whilst maintaining precise correlations. Black-box models aren\u2019t enough\u2014 constant feedback loops drive informed, accurate decisions. Join DraftKings as we showcase how we expose real-time metrics from our simulation engine, to empower traders with deeper insights into how their inputs shape the model. Using Spark Structured Streaming, Kafka, and Databricks dashboards, we transform raw simulation outputs into actionable data. This transparency into our engines enables fine-grained control over pricing\u2015 leading to more accurate odds, a more efficient sportsbook, and an elevated customer experience. /Lead Machine Learning Engineer\nDraftkings /Lead Data Science Engineer"}
{"session_id": "building-real-time-trading-dashboards-dlt-and-databricks-apps", "title": "Building Real-Time Trading Dashboards With DLT and Databricks Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Barclays Post Trade real-time trade monitoring platform was historically built on a complex set of legacy technologies including Java, Solace, and custom micro-services.This ssession will demonstrate how the power of DLT new real-time mode, in conjunction with the foreach_batch_sink, can enable simple, cost-effective streaming pipelines that can load high volumes of data into our OLTP database with very low latency. Once in our OLTP database, this can be used to update real-time trading dashboards, securely hosted in Databricks Apps, with the latest stock trades - enabling better, more responsive decision-making and alerting.The session will walk-through the architecture, and demonstrate how simple it is to create and manage the pipelines and apps within the Databricks environment. /Senior Specialist Solution Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "building-reliable-agentic-ai-databricks", "title": "Building Reliable Agentic AI on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["CEO & Co-Founder, Monte Carlo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/CEO & Co-Founder"}
{"session_id": "building-responsible-ai-agents-databricks", "title": "Building Responsible AI Agents on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Delivery Solutions Architect (DSA) FINS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation explores how Databricks' Data Intelligence Platform supports the development and deployment of responsible AI in credit decisioning, ensuring fairness, transparency and regulatory compliance. Key areas include bias and fairness monitoring using Lakehouse Monitoring to track demographic metrics and automated alerts for fairness thresholds. Transparency and explainability are enhanced through the Mosaic AI Agent Framework, SHAP values and LIME for feature importance auditing. Regulatory alignment is achieved via Unity Catalog for data lineage and AIBI dashboards for compliance monitoring. Additionally, LLM reliability and security are ensured through AI guardrails and synthetic datasets to validate model outputs and prevent discriminatory patterns. The platform integrates real-time SME and user feedback via Databricks Apps and AI/BI Genie Space. /Senior Resident Solutions Architect\nDatabricks /Delivery Solutions Architect (DSA) FINS"}
{"session_id": "building-responsible-and-resilient-ai-databricks-ai-governance", "title": "Building Responsible and Resilient AI: The Databricks AI Governance Framework", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Sr. Director, Field Security, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "GenAI & machine learning are reshaping industries, driving innovation and redefining business strategies. As organizations embrace these technologies, they face significant challenges in managing AI initiatives effectively, such as balancing innovation with ethical integrity, operational resilience and regulatory compliance. This presentation introduces the Databricks AI Governance Framework (DAGF), a practical framework designed to empower organizations to navigate the complexities of AI. It provides strategies for building scalable, responsible AI programs that deliver measurable value, foster innovation and achieve long-term success. By examining the framework's five foundational pillars \u2014 AI organization, ethics, legal and regulatory compliance, transparency and interpretability, AI operations and infrastructure and AI security \u2014 this session highlights how AI governance aligns programs with the organization's strategic goals, mitigates risks and builds trust across stakeholders. /Senior Specialist Solution Architect\nDatabricks /Sr. Director, Field Security"}
{"session_id": "building-scalable-real-time-concurrency-prediction-service", "title": "Building a Scalable, Real-Time Concurrency Prediction Service", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science", "Machine Learning", "Real-time"], "speakers": ["AVP Data Science & Machine Learning, Dream 11"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dream11's rapid growth has posed critical challenges in scaling infrastructure to handle millions of concurrent users during high-traffic events. Concurrency Prediction Service provides real-time forecasts of peak user activity in 30-minute intervals to optimize resource allocation by the Scaler Service. This presentation covers the critical aspects of building and optimizing the Concurrency Prediction Service, including: /AVP Data Science & Machine Learning"}
{"session_id": "building-seamless-multi-cloud-platform-secure-portable-workloads", "title": "Building a Seamless Multi-Cloud Platform for Secure Portable Workloads", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many challenges to making a data platform actually a platform, something that hides complexity. Data engineers and scientists are looking for a simple and intuitive abstraction to focus on their work, not where it runs to maintain compliance, what credentials it uses to access data or how it generates operational telemetry. At Databricks we\u2019ve developed a data-centric approach to workload development and deployment that enables data workers to stop doing migrations and instead develop with confidence. Attend this session to learn how to run simple, secure and compliant global multi-cloud workloads at scale on Databricks. /Staff Software Engineer"}
{"session_id": "building-self-service-data-platform-small-data-team", "title": "Building a Self-Service Data Platform With a Small Data Team", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Head of Architecture, Dodo Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Dodo Brands, a global pizza and coffee business with over 1,200 retail locations and 40k employees, revolutionized their analytics infrastructure by creating a self-service data platform. This session explores the approach to empowering analysts, data scientists and ML engineers to independently build analytical pipelines with minimal involvement from data engineers. By leveraging Databricks as the backbone of their platform, the team developed automated tools like a \"job-generator\" that uses Jinja templates to streamline the creation of data jobs. This approach minimized manual coding and enabled non-data engineers to create over 1,420 data jobs \u2014 90% of which were auto-generated by user configurations. Supporting thousands of weekly active users via tools like Apache Superset. This session provides actionable insights for organizations seeking to scale their analytics capabilities efficiently without expanding their data engineering teams. /Senior Data Engineer\nDodo Brands /Head of Architecture"}
{"session_id": "building-tool-calling-agents-databricks-agent-framework-and-mcp", "title": "Building Tool-Calling Agents With Databricks Agent Framework and MCP", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to create AI agents that can do more than just generate text? Join us to explore how combining Databricks' Mosaic AI Agent Framework with the Model Context Protocol (MCP) unlocks powerful tool-calling capabilities. We'll show you how MCP provides a standardized way for AI agents to interact with external tools, data and APIs, solving the headache of fragmented integration approaches. Learn to build agents that can retrieve both structured and unstructured data, execute custom code and tackle real enterprise challenges. Key takeaways: Whether you're building customer service bots or data analysis assistants, you'll leave with practical know-how to create powerful, governed AI agents. /Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "building-trustworthy-ai-northwestern-mutual-guardrail-technologies-and", "title": "Building Trustworthy AI at Northwestern Mutual: Guardrail Technologies and Strategies", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science"], "speakers": ["Data Science, Northwestern Mutual"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Science"}
{"session_id": "building-your-talent-pipeline-databricks-university-alliance", "title": "Building Your Talent Pipeline With the Databricks University Alliance", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Program Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks University Alliance represents more than 1,000 universities across the globe. Join us to see how you can leverage this network of faculty and students to broaden your talent pipeline to students learning with the latest tools in data and AI from Databricks. /Program Manager"}
{"session_id": "busting-data-modeling-myths-truths-and-best-practices-data-modeling", "title": "Busting Data Modeling Myths: Truths and Best Practices for Data Modeling in the Lakehouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Modeling", "Data Quality", "SQL", "Scala"], "speakers": ["Practice Lead, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the truth behind data modeling in Databricks. This session will tackle the top 10 myths surrounding relational and dimensional data modeling. Attendees will gain a clear understanding of what Databricks Lakehouse truly supports today, including how to leverage primary and foreign keys, identity columns for surrogate keys, column-level data quality constraints and much more. This session will talk through the lens of medallion architecture, explaining how to implement data models across bronze, silver, and gold tables. Whether you\u2019re migrating from a legacy warehouse or building new analytics solutions, you\u2019ll leave equipped to fully leverage Databricks\u2019 capabilities, and design scalable, high-performance data models for enterprise analytics. /DBSQL Product Specialist\nDatabricks /Practice Lead"}
{"session_id": "capitalizing-alternatives-data-addepar-platform-private-markets", "title": "Capitalizing Alternatives Data on the Addepar Platform: Private Markets Benchmarking", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Pipeline", "ELT", "Scala"], "speakers": ["Addepar"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Addepar possesses an enormous private investment data set with 40% of the $7T assets on the platform allocated to alternatives. Leveraging the Addepar Data Lakehouse (ADL), built on Databricks, we have built a scalable data pipeline that assesses millions of private fund investment cash flows and translates it to a private fund benchmarks data offering. Investors on the Addepar platform can leverage this data seamlessly integrated against their portfolio investments and obtain actionable investment insights. At a high-level, this data offering consists of an extensive data aggregation, filtering, and construction logic that dynamically updates for clients through the Databricks job workflows. This derived dataset has gone through several iterations with investment strategists and academics that leveraged delta shared tables. Irrespective of the data source, the data pipeline coalesces all relevant cash flow activity against a unique identifier before constructing the benchmarks. /Addepar"}
{"session_id": "caring-our-data-how-chick-fil-enables-data-driven-decision-making", "title": "Caring for Our Data: How Chick-fil-A Enables Data-Driven Decision Making with Databricks", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Data architect, Chick-fil-A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Chick-fil-A\u2019s adoption of Databricks has been rewarding, providing us with a modern, scalable, efficient data platform that meets our evolving needs. With lakehouse, Chick-fil-A can unlock new insights, drive business success and continue \u201cWinning Hearts Every Day.\u201d Our top priority is ensuring our customers have an exceptional experience. We have unrivaled discipline in store operations and guest interaction, but our data systems were lagging. Owner/operators were spending too much time sitting in front of screens getting data to manage their stores. That data was often slow and untrusted. This is why we\u2019re enabling AI that enables them to ask questions to get accurate responses, allowing them to spend more time in store working with team members/delighting customers. Databricks also provided BI performance improvement that has addressed major challenges to store operators, helping them to make better decisions with sales forecasting, labor optimizations and inventory management. /Enterprise Architect\nChick-fil-A, Inc /Data architect"}
{"session_id": "cicd-databricks-advanced-asset-bundles-and-github-actions", "title": "CI/CD for Databricks: Advanced Asset Bundles and GitHub Actions", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Asset Bundles (DABs) provide a way to use the command line to deploy and run a set of Databricks assets \u2014 like notebooks, Python code, DLT pipelines and workflows. To automate deployments, you create a deployment pipeline that uses the power of DABs along with other validation steps to ensure high quality deployments. In this session you will learn how to automate CI/CD processes for Databricks while following best practices to keep deployments easy to scale and maintain. After a brief explanation of why Databricks Asset Bundles are a good option for CI/CD, we will walk through a working project including advanced variables, target-specific overrides, linting, integration testing and automatic deployment upon code review approval. You will leave the session clear on how to build your first GitHub Action using DABs. /Sr. Specialist Solutions Architect"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Real-time", "Streaming"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Databricks"}
{"session_id": "japanese-mega-banks-journey-modern-genai-powered-governed-data-platform", "title": "A Japanese Mega-Bank\u2019s Journey to a Modern, GenAI-Powered, Governed Data Platform", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal, Deloitte Consulting LLP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "SMBC-AD, a major Japanese multinational financial services institution, has embarked on an initiative to build a GenAI-powered, modern and well-governed cloud data platform on Azure/Databricks. This initiative aims to build an enterprise data foundation encompassing loans, deposits, securities, derivatives, and other data domains. Its primary goals are: Deloitte and SMBC leveraged the Brickbuilder asset \u201cData as a Service for Banking\u201d to accelerate this highly strategic transformation. /Principal"}
{"session_id": "no-code-ml-forecasting-platform-retail-and-cpg-companies", "title": "A No-Code ML Forecasting Platform for Retail and CPG companies", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Director, Antuit - A Zebra Technologies company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Director"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /"}
{"session_id": "practitioners-guide-databricks-serverless", "title": "A Practitioner\u2019s Guide to Databricks Serverless", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Engineering", "Data Pipeline"], "speakers": ["Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows. /Product Specialist"}
{"session_id": "prescription-success-leveraging-dabs-faster-deployment-and-better", "title": "A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Principal Data Engineer, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Health Catalyst (HCAT) transformed its CI/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements. HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making. /Sr. Solutions Architect\nDatabricks /Principal Data Engineer"}
{"session_id": "unified-solution-data-management-and-model-training-apache-iceberg-and", "title": "A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, PUBLIC SECTOR", "technologies": ["APACHE ICEBERG", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala", "Streaming"], "speakers": ["machine learning system engineer, ByteDance"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming). Magnus uses Iceberg\u2019s branch/tag to manage massive datasets/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability. When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility. In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training. /Infrastructure Engineer\nByteDance /machine learning system engineer"}
{"session_id": "what-i-wish-i-had-known-my-last-soc-confessions-cybersecurity-executive", "title": "\u201cWhat I Wish I Had Known in My Last SOC.\u201d Confessions of a Cybersecurity Executive.", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Janitor, Ziggiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Janitor"}
