{"session_id": "10-hours-10-minutes-unleashing-power-dlt", "title": "From 10 Hours to 10 Minutes: Unleashing the Power of DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Batch Processing", "Data Integration", "Data Pipeline", "Real-time", "SQL"], "speakers": ["Data Engineer, Accenture"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How do you transform a data pipeline from sluggish 10-hour batch processing into a real-time powerhouse that delivers insights in just 10 minutes? This was the challenge we tackled at one of France's largest manufacturing companies, where data integration and analytics were mission-critical for supply chain optimization. Power BI dashboards needed to refresh every 15 minutes. Our team struggled with legacy Azure Data Factory batch pipelines. These outdated processes couldn\u2019t keep up, delaying insights and generating up to three daily incident tickets. We identified DLTs and Databricks SQL as the game-changing solution to modernize our workflow, implement quality checks, and reduce processing times.In this session, we\u2019ll dive into the key factors behind our success: /Senior Data Engineer\nAccenture /Data Engineer"}
{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "accelerate-end-end-multi-agents-databricks-and-dspy", "title": "Accelerate End-to-End Multi-Agents on Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A production-ready GenAI application is more than the framework itself. Like ML, you need a unified platform to create an end-to-end workflow for production quality applications. Below is an example of how this works on Databricks: In this session, learn how to build agents to access all your data and models through function calling. Then, learn how DSPy enables agent interaction with each other to ensure the question is answered correctly. We will demonstrate a chatbot, powered by multiple agents, to be able to answer questions and reason answers the base LLM does not know and very specialized topics. /Delivery Solutions Architect"}
{"session_id": "accelerating-analytics-integrating-bi-tools-databricks-sql", "title": "Accelerating Analytics: Integrating BI Tools to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Did you know that you can integrate with your favorite BI tools directly from Databricks SQL? You don\u2019t even need to stand up an additional warehouse. This session shows the integrations with Microsoft Power Platform, Power BI, Tableau, Sigma and Looker so you can have a seamless integration experience. Directly connect your Databricks workspace with Fabric and Power BI workspaces or Tableau to publish and sync data models, with defined primary and foreign keys, between the two platforms. /Sr. Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "accelerating-data-ingestion-new-innovations-auto-loaders-performance", "title": "Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala", "Streaming"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including: You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect. /Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "accelerating-data-transformation-best-practices-governance-agility-and", "title": "Accelerating Data Transformation: Best Practices for Governance, Agility and Innovation", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Quality", "ELT", "SQL", "Scala"], "speakers": ["Practice Director, Data & AI, NCS Australia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share NCS\u2019s approach to implementing a Databricks Lakehouse architecture, focusing on key lessons learned and best practices from our recent implementations. By integrating Databricks SQL Warehouse, the DBT Transform framework and our innovative test automation framework, we\u2019ve optimized performance and scalability, while ensuring data quality. We\u2019ll dive into how Unity Catalog enabled robust data governance, empowering business units with self-serve analytical workspaces to create insights while maintaining control. Through the use of solution accelerators, rapid environment deployment and pattern-driven ELT frameworks, we\u2019ve fast-tracked time-to-value and fostered a culture of innovation. Attendees will gain valuable insights into accelerating data transformation, governance and scaling analytics with Databricks. /Practice Director, Data & AI"}
{"session_id": "accelerating-growth-capital-markets-data-driven-strategies-success", "title": "Accelerating Growth in Capital Markets: Data-Driven Strategies for Success", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Software Engineering Manager, B3 - Bolsa, Brasil e Balc\u00e3o"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Growth in capital markets thrives on innovation, agility and real-time insights. This session highlights how leading firms use Databricks\u2019 Data Intelligence Platform to uncover opportunities, optimize trading strategies and deliver personalized client experiences. Learn how advanced analytics and AI help organizations expand their reach, improve decision-making and unlock new revenue streams. Industry leaders share how unified data platforms break down silos, deepen insights and drive success in a fast-changing market. Key takeaways: Discover how data intelligence empowers capital markets firms to thrive in today\u2019s competitive landscape! /Financial Services Industry Director\nDatabricks /CDO - Wealth Management\nNorthern Trust /Software Engineering Manager"}
{"session_id": "accelerating-innovation-across-life-sciences-value-chain-data-ai", "title": "Accelerating Innovation Across the Life Sciences Value Chain With a Data + AI Foundation", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Quality"], "speakers": ["Industry Marketing Lead, HLS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Artificial intelligence and robust data strategies are revolutionizing life sciences, delivering measurable value across drug discovery, commercial operations, supply chain, manufacturing and commercialization. Foundational data governance ensures data quality, compliance and interoperability, enabling organizations to extract actionable insights from vast, complex datasets. AI accelerates drug discovery by identifying novel targets and optimizing compounds, while data intelligence and sharing streamline clinical trials, regulatory submissions and market positioning. In manufacturing and supply chain, advanced analytics and AI-driven tools optimize workflows, forecast disruptions and enhance operational efficiency. Together, these capabilities empower life sciences organizations to innovate faster, reduce costs and bring therapies to market more efficiently, driving growth and improving patient outcomes. /Industry Marketing Lead, HLS"}
{"session_id": "accelerating-model-development-and-fine-tuning-databricks-twelvelabs", "title": "Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Scaling large language models (LLMs) and multimodal architectures requires efficient data management and computational power. NVIDIA NeMo Framework Megatron-LM on Databricks is an open source solution that integrates GPU acceleration and advanced parallelism with Databricks Delta Lakehouse, streamlining workflows for pre-training and fine-tuning models at scale. This session highlights context parallelism, a unique NeMo capability for parallelizing over sequence lengths, making it ideal for video datasets with large embeddings. Through the case study of TwelveLabs\u2019 Pegasus-1 model, learn how NeMo empowers scalable multimodal AI development, from text to video processing, setting a new standard for LLM workflows. /Chief Technology Officer & Co-Founder\nTwelve Labs, Inc /Solutions Architect - NVIDIA"}
{"session_id": "achieve-your-mission-ai-driven-decisions", "title": "Achieve Your Mission With AI-Driven Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Government leaders overwhelmingly recognize the potential benefits of AI as critical to long-term strategic goals of efficiency, but implementation challenges and security concerns could be obstacles to success. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "achieving-precision-ai-retrieving-right-data-using-ai-agents", "title": "Achieving Precision in AI: Retrieving the Right Data Using AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director"}
{"session_id": "adobes-security-lakehouse-ocsf-data-efficiency-and-threat-detection", "title": "Adobe\u2019s Security Lakehouse: OCSF, Data Efficiency and Threat Detection at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Security", "Real-time", "Scala"], "speakers": ["AntiMatter"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore how Adobe uses a sophisticated data security architecture built on the Databricks Data Intelligence Platform, along with the Open Cybersecurity Schema Framework (OCSF), to enable scalable, real-time threat detection across more than 10 PB of security data. We\u2019ll compare different approaches to OCSF implementation and demonstrate how Adobe processes massive security datasets efficiently \u2014 reducing query times by 18%, maintaining 99.4% SLA compliance, and supporting 286 security users across 17 teams with over 4,500 daily queries. By using Databricks' Platform for serverless compute, scalable architecture, and LLM-powered recommendations, Adobe has significantly improved processing speed and efficiency, resulting in substantial cost savings. We\u2019ll also highlight how OCSF enables advanced cross-tool analytics and automation, streamlining investigations. Finally, we\u2019ll introduce Databricks\u2019 new open-source OCSF toolkit for scalable security data normalization and invite the community to contribute. /Sr. Manager, Security Software Engineering\nAdobe /AntiMatter"}
{"session_id": "advanced-governance-and-auth-databricks-apps", "title": "Advanced Governance and Auth With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore advanced governance and authentication patterns for building secure, enterprise-grade apps with Databricks Apps. Learn how to configure complex permissions and manage access control using Unity Catalog. We\u2019ll dive into \u201con-behalf-of-user\u201d authentication \u2014 allowing agents to enforce user-specific access controls \u2014 and cover API-based authentication, including PATs and OAuth flows for external integrations. We\u2019ll also highlight how Addepar uses these capabilities to securely build and scale applications that handle sensitive financial data. Whether you're building internal tools or customer-facing apps, this session will equip you with the patterns and tools to ensure robust, secure access in your Databricks apps. /Staff Software Engineer"}
{"session_id": "advanced-machine-learning-operations", "title": "Advanced Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Python"], "speakers": ["CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course is designed to cover advanced concepts and workflows in machine learning operations. It starts by introducing participants to continuous integration (CI) and continuous development (CD) workflows within machine learning projects, guiding them through the deployment of a sample CI/CD workflow using Databricks in the first section. Moving on to the second part, participants delve into data and model testing, where they actively create tests and automate CI/CD workflows. Finally, the course concludes with an exploration of model monitoring concepts, demonstrating the use of Lakehouse Monitoring to oversee machine learning models in production settings. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. intermediate-level knowledge of traditional ML concepts, development with CI/CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "advanced-rag-overview-thawing-your-frozen-rag-pipeline", "title": "Advanced RAG Overview \u2014 Thawing Your Frozen RAG Pipeline", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ML Innovation, Experian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The most common RAG systems rely on a frozen RAG system \u2014 one where there\u2019s a single embedding model and single vector index. We\u2019ve achieved a modicum of success with that, but when it comes to increasing accuracy for production systems there is only so much this approach solves. In this session we will explore how to move from the frozen systems to adaptive RAG systems which produce more tailored outputs with higher accuracy. Databricks services: Lakehouse, Unity Catalog, Mosaic, Sweeps, Vector Search, Agent Evaluation, Managed Evaluation, Inference Tables /Head of AI/ML Innovation"}
{"session_id": "agentic-architectures-create-realistic-conversations-using-genai-teach", "title": "Agentic Architectures to Create Realistic Conversations: Using GenAI to Teach Empathy in Healthcare", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Providence Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Medical providers often receive less than 15 minutes of instruction in how to interact with patients during emotionally charged end of life interactions. Continuing education for clinicians is critical to hone these skills but is difficult to scale traditional approaches that require professional patients and instructors. Here, we describe a custom chatbot that plays the role of patient and coach to provide a scaling learning experience. A critical challenge was how to mitigate the persistently cheerful and helpful tone which results from standard pretraining in the Patient Persona AI. We accomplished this by implementing a multi-agent architecture based upon a graphical model of the conversation. System prompts reflecting the patient\u2019s cognitive state are dynamically updated as the conversation progresses. Future extensions of the work are intended to focus on additional custom model fine-tuning in the Mosaic AI platform to further improve the realism of the conversation. /Head of Data Science\nProvidence Health /Senior Data Scientist\nTegria Consulting/Providence Healthcare"}
{"session_id": "ai-agents-action-structuring-unstructured-data-demand-databricks-and", "title": "AI Agents in Action: Structuring Unstructured Data on Demand with Databricks and Unstructured", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Head of Product and Engineering, Unstructured"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Product and Engineering"}
{"session_id": "ai-agents-hackathon", "title": "AI Agents Hackathon", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "540 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "ai-agents-marketing-leveraging-mosaic-ai-create-multi-purpose-agentic", "title": "AI Agents for Marketing: Leveraging Mosaic AI to Create a Multi-Purpose Agentic Marketing Assistant", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Head of AI Center Excellence, 7-Eleven Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing professionals build campaigns, create content and use effective copywriting to tell a good story to promote a product/offer. All of this requires a thorough and meticulous process for every individual campaign. In order to assist marketing professionals at 7-Eleven, we built a multi-purpose assistant that could: We will walk you through how we created multiple agents as different personas with LangGraph and Mosaic AI to create a chat assistant that assumes a different persona based on the user query. We will also explain our evaluation methodology in choosing models and prompts and how we implemented guardrails for high reliability with sensitive marketing content. This assistant by 7-Eleven was showcased at the Databricks booth at NRF earlier this year. /Head of AI Center of Excellence"}
{"session_id": "ai-and-genie-analyzing-healthcare-improvement-opportunities", "title": "AI and Genie: Analyzing Healthcare Improvement Opportunities", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Software Architect, Premier Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Improving healthcare impacts us all. We highlight how Premier Inc. took risk-adjusted patient data from more than 1,300 member hospitals across America, applying a natural language interface using AI/BI Genie, allowing our users to discover new insights. The stakes are high, new insights surfaced represent potential care improvement and lives positively impacted. Using Genie and our AI-ready data in Unity Catalog, our team was able to stand up a Genie instance in three short days, bypassing costs and time of custom modeling and application development. Additionally, Genie allowed our internal teams to generate complex SQL, as much as 10 times faster than writing it by hand. As Genie and lakehouse apps continue to advance rapidly, we are excited to leverage these features by introducing Genie to as many as 20,000 users across hundreds of hospitals. This will support our members\u2019 ongoing mission to enhance the care they provide to the communities they serve. /Senior Director, Analytics\nPremier Inc /Software Architect"}
{"session_id": "ai-assisted-bi-everything-you-need-know", "title": "AI-Assisted BI: Everything You Need to Know", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how AI is transforming business intelligence and data analytics across the Databricks Platform. This session provides a comprehensive overview of AI-assisted capabilities \u2014 from auto-generating dashboards and visualizations to enabling conversational analytics with Genie. We\u2019ll also cover AI-powered notebooks and SQL editors that accelerate code generation, streamline exploratory analysis and simplify workflows. Whether you\u2019re a data engineer, analyst or BI developer, you\u2019ll walk away ready to harness AI to drive faster, smarter decisions across your organization. /Sr. Product Manager\nDatabricks /Databricks"}
{"session_id": "ai-driven-drug-discovery-accelerating-molecular-insights-nvidia-and", "title": "AI-Driven Drug Discovery: Accelerating Molecular Insights With NVIDIA and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the race to revolutionize healthcare and drug discovery, biopharma companies are turning to AI to streamline workflows and unlock new scientific insights. This session, we will explore how NVIDIA BioNeMo, combined with Databricks Delta Lakehouse, can be used for advancing drug discovery for critical applications like molecular structure modeling, protein folding and diagnostics. We\u2019ll demonstrate how BioNeMo pre-trained models can run inference on data securely stored in Delta Lake, delivering actionable insights. By leveraging containerized solutions on Databricks\u2019 ML Runtime with GPU acceleration, users can achieve significant performance gains compared to traditional CPU-based computation. /Solutions Architect - NVIDIA"}
{"session_id": "ai-governance-journey-securing-data-models-and-agents", "title": "The AI Governance Journey: Securing Data, Models and Agents", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores how to centrally govern the entire AI lifecycle on Databricks \u2014 from experimenting with models to deploying production-grade agents. It will cover managing access to AI models with Mosaic AI Gateway, applying usage tracking, auditing, rate limits and AI guardrails for safe and compliant usage. It'll also cover how Unity Catalog integrates seamlessly throughout the process, enabling lineage tracking, granular access controls and the secure monitoring of model performance and data quality. /Product Manager"}
{"session_id": "ai-meets-sql-leverage-genai-scale-enrich-your-data", "title": "AI Meets SQL: Leverage GenAI at Scale to Enrich Your Data", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "SQL"], "speakers": ["swe, databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI into existing data workflows can be challenging, often requiring specialized knowledge and complex infrastructure. In this session, we'll share how SQL users can leverage AI/ML to access large language models (LLMs) and traditional machine learning directly from within SQL, simplifying the process of incorporating AI into data workflows. We will demonstrate how to use Databricks SQL for natural language processing, traditional machine learning, retrieval augmented generation and more. You'll learn about best practices and see examples of solving common use cases such as opinion mining, sentiment analysis, forecasting and other common AI/ML tasks. /Sr. Product Manager\nDatabricks /swe"}
{"session_id": "ai-motion-build-roadmap-impact-just-30-minutes", "title": "AI in Motion: Build a Roadmap for Impact in Just 30 Minutes", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": ["Lead Data & AI Strategist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This high-velocity workshop is designed for data and AI leaders seeking to rapidly develop a comprehensive AI strategy tailored to their organization's needs. In just 30 minutes, participants will engage in a focused, interactive session that delivers actionable insights and a strategic framework for AI implementation. Key components of the workshop include: By the end of this intensive session, you will have the foundation of a robust AI strategy and guidance on roadmap execution. /Lead Data & AI Strategist"}
{"session_id": "ai-powered-data-discovery-and-curation-unity-catalog", "title": "AI-Powered Data Discovery and Curation With Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data landscape, the challenge isn\u2019t just storing or processing data \u2014 it\u2019s enabling every user, from data stewards to analysts, to find and trust the right data, fast. This session explores how Databricks is reimagining data discovery with the new Discover Page Experience \u2014 an intuitive, curated interface showcasing key data and workspace assets. We\u2019ll dive into AI-assisted governance and AI-powered discovery features like AI-generated metadata, AI-assisted lineage and natural language data exploration in Unity Catalog. Plus, see how new certifications and deprecations bring clarity to complex data environments. Whether you\u2019re a data steward highlighting trusted assets or an analyst navigating data without deep schema knowledge, this session will show how Databricks is making data discovery seamless for everyone. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "ai-powered-marketing-data-management-solving-dirty-data-problem", "title": "AI-Powered Marketing Data Management: Solving the Dirty Data Problem with Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Scala"], "speakers": ["VP, Cloud Product Management & UX, Acxiom"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marketing teams struggle with \u2018dirty data\u2019 \u2014 incomplete, inconsistent, and inaccurate information that limits campaign effectiveness and reduces the accuracy of AI agents. Our AI-powered marketing data management platform, built on Databricks, solves this with anomaly detection, ML-driven transformations and the built-in Acxiom Referential Real ID Graph with Data Hygiene. We\u2019ll showcase how Delta Lake, Unity Catalog and DLT power our multi-tenant architecture, enabling secure governance and 75% faster data processing. Our privacy-first design ensures compliance with GDPR, CCPA and HIPAA through role-based access, encryption key management and fine-grained data controls. Join us for a live demo and Q&A, where we\u2019ll share real-world results and lessons learned in building a scalable, AI-driven marketing data solution with Databricks. /Principal Product Manager\nAcxiom /VP, Cloud Product Management & UX"}
{"session_id": "ai-powered-profits-smarter-order-and-inventory-management", "title": "AI-Powered Profits: Smarter Order and Inventory Management", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Data and Enterprise Digital, Xylem"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Solutions Architect\nDatabricks /VP, Data and Enterprise Digital"}
{"session_id": "ai-powering-epsilons-identity-strategy-unified-marketing-platform", "title": "AI Powering Epsilon's Identity Strategy: Unified Marketing Platform on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Data Warehouse", "Delta Lake", "ELT"], "speakers": ["Vice President, Decision Sciences, Epsilon Data Management"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to hear about how Epsilon Data Management migrated Epsilon\u2019s unique, AI-powered marketing identity solution from multi-petabyte on-prem Hadoop and data warehouse systems to a unified Databricks Lakehouse platform. This transition enabled Epsilon to further scale its Decision Sciences solution and enable new cloud-based AI research capabilities on time and within budget, without being bottlenecked by the resource constraints of on-prem systems. Learn how Delta Lake, Unity Catalog, MLflow and LLM endpoints powered massive data volume, reduced data duplication, improved lineage visibility, accelerated Data Science and AI, and enabled new data to be immediately available for consumption by the entire Epsilon platform in a privacy-safe way. Using the Databricks platform as the base for AI and Data Science at global internet scale, Epsilon deploys marketing solutions across multiple cloud providers and multiple regions for many customers. /Vice President , Database\nEpsilon Data Management /Vice President, Decision Sciences"}
{"session_id": "ai-regulation-dilemma-spur-innovation-or-guardrails-where-are-we-and", "title": "The AI Regulation Dilemma: Spur Innovation, or Guardrails? \u2014 Where Are We and the Impact of Trump 2", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Chief Public Affairs Officer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Trump 2 AI agenda prioritizes US leadership by opposing regulation on bias and frontier AI risks, favoring innovation and AI expansion. With comprehensive federal AI regulation unlikely, states are advancing AI laws on bias, harmful content and transparency (e.g., Colorado). Meanwhile, the EU AI Act imposes global obligations. The emerging patchwork of state rules will burden US companies more than a unified federal approach, undermining Trump\u2019s deregulatory goals. Ironically, the Trump agenda may accelerate state-level regulation and impede innovation. A light federal AI law preempting state rules is politically unlikely, leaving US companies with a fragmented landscape similar to privacy regulation where the EU AI Act \u2014 in the role of GDPR \u2014 has set the stage, and the states are asserting themselves with various incremental requirements. Important developments to then cover: EU GPAI Code of Practice (effective 8/2/25), newly enacted state laws, Korean AI law, Japan \u2014 if final. /Chief Public Affairs Officer"}
{"session_id": "aibi-dashboards-and-aibi-genie-dashboards-and-last-mile-analytics-made", "title": "AI/BI Dashboards and AI/BI Genie: Dashboards and Last-Mile Analytics Made Simple", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["VP of Data + AI Architecture, JosueBogran.com & zeb.co"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks announced two new features in 2024: AI/BI Dashboards and AI/BI Genie. Dashboards is a redesigned dashboarding experience for your regular reporting needs, while Genie provides a natural language experience for your last-mile analytics. In this session, Databricks Solutions Architect and content creator Youssef Mrini will present alongside Databricks MVP and content creator Josue A. Bogran on how you can get the most value from these tools for your organization. Content covered includes: Fluff-free, full of practical tips, and geared to help you deliver immediate impact with these new Databricks capabilities. /Solutions Architect\nDatabricks /VP of Data + AI Architecture"}
{"session_id": "aibi-data-analysts", "title": "AI/BI for Data Analysts", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence"], "speakers": ["abilities: Labs: Yes"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to use the features Databricks provides for business intelligence needs: AI/BI Dashboards and AI/BI Genie. As a Databricks Data Analyst, you will be tasked with creating AI/BI Dashboards and AI/BI Genie Spaces within the platform, managing the access to these assets by stakeholders and necessary parties, and maintaining these assets as they are edited, refreshed, or decommissioned over the course of their lifespan. This course intends to instruct participants on how to design dashboards for business insights, share those with collaborators and stakeholders, and maintain those assets within the platform. Participants will also learn how to utilize AI/BI Genie Spaces to support self-service analytics through the creation and maintenance of these environments powered by the Databricks Data Intelligence Engine. Pre-requisites: The content was developed for participants with these skills/knowledge/abilities: Labs: Yes"}
{"session_id": "aibi-driving-speed-value-supply-chain", "title": "AI/BI Driving Speed to Value in Supply Chain", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science"], "speakers": ["Data Science Manager, Conagra Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Conagra is a global food manufacturer with $12.2B in revenue, 18K+ employees, 45+ plants in US, Canada and Mexico. Conagra's Supply Chain organization is heavily focused on delivering results in productivity, waste reduction, inventory rationalization, safety and customer service levels. By migrating the Supply Chain reporting suite to Databricks over the past 2 years, Conagra's Supply Chain Analytics & Data Science team has been able to deliver new AI solutions which complement traditional BI platforms and lay the foundation for additional AI/ML applications in the future. With Databricks Genie integrated within traditional BI reports, Conagra Supply Chain users can now go from insight to action faster and with fewer clicks, enabling speed to value in a complex Supply Chain. The Databricks platform also allows the team to curate data products to be consumed by traditional BI applications today as well as the ability to rapidly scale for the AI/ML applications of tomorrow. /Head of SC Adv Analytics & Data Science\nConagra Brands /Data Science Manager"}
{"session_id": "aibi-genie-look-under-hood-everyones-friendly-neighborhood-genai", "title": "AI/BI Genie: A Look Under the Hood of Everyone's Friendly, Neighborhood GenAI Product", "track": "ANALYTICS AND BI", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["swe, databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Go beyond the user interface and explore the cutting-edge technology driving AI/BI Genie. This session breaks down the AI/BI Genie architecture, showcasing how LLMs, retrieval-augmented generation (RAG) and finely tuned knowledge bases work together to deliver fast, accurate responses. We\u2019ll also explore how AI agents orchestrate workflows, optimize query performance and continuously refine their understanding. Ideal for those who want to geek out about the tech stack behind Genie, this session offers a rare look at the magic under the hood. /Software Engineer\nDatabricks /swe"}
{"session_id": "aibi-self-service-analytics", "title": "AI/BI for Self-Service Analytics", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you will learn how to self-serve business insights from your company\u2019s Databricks Data Intelligence Platform using AI/BI. After a tour of the fundamental components of the platform, you\u2019ll learn how to interact with pre-created AI/BI Dashboards to explore your company\u2019s data through existing charts and visualizations. You\u2019ll also learn how to use AI/BI Genie to go beyond dashboards by asking follow-up questions in natural language to self-serve new insights, create visualizations, and share them with your colleagues. Pre-requisites: A working understanding of your organization\u2019s business and key performance indicators. Labs: No Certification Path: N/A"}
{"session_id": "amplifying-human-human-connection-face-mental-health-crisis-using", "title": "Amplifying Human-to-Human Connection in the Face of Mental Health Crisis Using Agentic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "LLAMA", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Data Scientist, Crisis Text Line"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Crisis Text Line has been innovating for ten years in text-based mental health crisis intervention and is now leading the next wave of GenAI use cases in the space. With over 300 million messages exchanged since 2013 and a decade of expertise, Crisis Text Line is unlocking the potential of AI to amplify human connection at a global scale.We will discuss how we leveraged our bedrock application to co-navigate crisis care through a set of early AI agent workflows. First, a simulator that reproduces texter behavior to train responders in taking conversations ranging in difficulty where the texter is in imminent risk of suicide or self-harm. Second, a tool that automatically monitors clinical quality of conversations. Third, predicted summarization to capture key context before conversations are transferred. Through the power of suggestion, this compound system aims to reduce burden and drive efficiency, such that our responders can focus on what they do best \u2014 support people in need. /Principal Product Manager\nCrisis Text Line /Lead Data Scientist"}
{"session_id": "analyst-roadmap-databricks-sql-end-end-bi", "title": "Analyst Roadmap to Databricks: From SQL to End-to-End BI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "SQL"], "speakers": ["Senior Business Analyst, Spencer Gifts"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Analysts often begin their Databricks journey by running familiar SQL queries in the SQL Editor, but that\u2019s just the start. In this session, I\u2019ll share the roadmap I followed to expand beyond ad-hoc querying into SQL Editor/notebook-driven development to scheduled data pipelines producing interactive dashboards \u2014 all powered by Databricks SQL and Unity Catalog. You\u2019ll learn how to organize tables with primary-key/foreign-key relationships along with creating table and column comments to form the semantic model, utilizing DBSQL features like RELY constraints. I\u2019ll also show how parameterized dashboards can be set up to empower self-service analytics and feed into Genie Spaces. Attendees will walk away with best practices for starting out with building a robust BI platform on Databricks, including tips for table design and metadata enrichment. Whether you\u2019re a data analyst or BI developer, this talk will help you unlock powerful, AI-enhanced analytics workflows. /Senior Business Analyst"}
{"session_id": "anomaly-detection-apple-large-scale-data-using-apache-spark-and-flink", "title": "Anomaly Detection at Apple for Large-Scale Data Using Apache Spark and Flink", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Software Engineer, Apple Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Anomaly detection in time series data is crucial for identifying unusual patterns and trends, enabling better alerting and action when data deviates from normal. Most anomaly detection algorithms perform adequately on a single node machine with public datasets, but do not scale well with distributed processing frameworks used in modern big data environments. This talk will focus on how we scaled anomaly detection for large-scale datasets using Apache Spark and Flink for both batch and near real-time use cases. We will also discuss how we leveraged Apache Spark to parallelize and scale common anomaly detection algorithms, enabling support for large-scale data processing. We'll highlight some of the challenges faced and how we resolved them to make it useful for massive datasets with varying degree of anomalies. Finally, we will demonstrate how our anomaly detection framework works in batch for petabytes of data and in streaming mode for hundreds of thousands of transactions per second. /Senior Machine Learning Engineer\nApple Inc /Principal Software Engineer"}
{"session_id": "apache-airflow-lakeflow-jobs-guide-workflow-modernization", "title": "From Apache Airflow to Lakeflow Jobs: A Guide for Workflow Modernization", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "Scala"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This is an overview of migrating from Apache Airflow to Lakeflow Jobs for modern data orchestration. It covers key differences, best practices and practical examples of transitioning from traditional Airflow DAGs orchestrating legacy systems to declarative, incremental ETL pipelines with Lakeflow. Attendees will gain actionable tips on how to improve efficiency, scalability and maintainability in their workflows. /Product Manager"}
{"session_id": "apache-iceberg-unity-catalog-hellofresh", "title": "Apache Iceberg with Unity Catalog at HelloFresh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["Senior Staff Data Engineer, HelloFresh"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Table formats like Delta Lake and Iceberg have been game changers for pushing lakehouse architecture into modern Enterprises. The acquisition of Tabular added Iceberg to the Databricks ecosystem, an open format that was already well supported by processing engines across the industry. At HelloFresh we are building a lakehouse architecture that integrates many touchpoints and technologies all across the organization. As such we chose Iceberg as the table format to bridge the gaps in our decentralized managed tech landscape. We are leveraging Unity Catalog as the Iceberg REST catalog of choice for storing metadata and managing tables. In this talk we will outline our architectural setup between Databricks, Spark, Flink and Snowflake and will explain the native Unity Iceberg REST catalog, as well as catalog federation towards connected engines. We will highlight the impact on our business and discuss the advantages and lessons learned from our early adopter experience. /Director of Data Engineering\nHelloFresh /Senior Staff Data Engineer"}
{"session_id": "apache-spark-ask-us-anything", "title": "Apache Spark \u2014 Ask Us Anything", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Staff Developer Advocate:Technical, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an interactive Ask Me Anything (AMA) session on the latest advancements in Apache Spark 4, including Spark Connect\u2014the new client-server architecture enabling seamless integration with IDEs, notebooks and custom applications. Learn about performance improvements, enhanced APIs and best practices for leveraging Spark\u2019s next-generation features. Whether you're a data engineer, Spark developer or big data enthusiast, bring your questions on architecture, real-world use cases and how these innovations can optimize your workflows. Don\u2019t miss this chance to dive deep into the future of distributed computing with Spark! /Staff Software Engineer\nDatabricks /Senior Engineering Manager\nDatabricks /Staff Developer Advocate:Technical Staff"}
{"session_id": "att-autoclassify-unified-multi-head-binary-classification-unlabeled", "title": "AT&T AutoClassify: Unified Multi-Head Binary Classification From Unlabeled Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, AT&T"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present AT&T AutoClassify, built jointly between AT&T's Chief Data Office (CDO) and Databricks professional services, a novel end-to-end system for automatic multi-head binary classifications from unlabeled text data. Our approach automates the challenge of creating labeled datasets and training multi-head binary classifiers with minimal human intervention. Starting only from a corpus of unlabeled text and a list of desired labels, AT&T AutoClassify leverages advanced natural language processing techniques to automatically mine relevant examples from raw text, fine-tune embedding models and train individual classifier heads for multiple true/false labels. This solution can reduce LLM classification costs by 1,000x, making it an efficient solution in operational costs. The end result is a highly optimized and low-cost model servable in Databricks capable of taking raw text and producing multiple binary classifications. An example use case using call transcripts will be examined. /Staff Data Scientist\nDatabricks /Senior Data Scientist"}
{"session_id": "authoring-data-pipelines-new-dlt-editor", "title": "Authoring Data Pipelines With the New DLT Editor", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re introducing a new developer experience for DLT designed for data practitioners who prefer a code-first approach and expect robust developer tooling. The new multi-file editor brings an IDE-like environment to declarative pipeline development, making it easy to structure transformation logic, configure pipelines throughout the development lifecycle and iterate efficiently. Features like contextual data previews and selective table updates enable step-by-step development. UI-driven tools, such as DAG previews and DAG-based actions, enhance productivity for experienced users and provide a bridge for those transitioning to declarative workflows. In this session, we\u2019ll showcase the new editor in action, highlighting how these enhancements simplify declarative coding and improve development for production-ready data pipelines. Whether you\u2019re an experienced developer or new to declarative data engineering, join us to see how DLT can enhance your data practice. /Sr. Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "automated-deployment-databricks-asset-bundles", "title": "Automated Deployment with Databricks Asset Bundles", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate project deployments Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge platform, including experience Workspaces, Apache Spark, Delta Lake, Medallion Architecture, Unity Catalog, Live Tables, and Workflows. In particular, leveraging Expectations DLTs. Labs: Yes Certification Path: Certified Data Engineer Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides a comprehensive review of DevOps principles and their application to Databricks projects. It begins with an overview of core DevOps, DataOps, continuous integration (CI), continuous deployment (CD), and testing, and explores how these principles can be applied to data engineering pipelines. The course then focuses on continuous deployment within the CI/CD process, examining tools like the Databricks REST API, SDK, and CLI for project deployment. You will learn about Databricks Asset Bundles (DABs) and how they fit into the CI/CD process. You\u2019ll dive into their key components, folder structure, and how they streamline deployment across various target environments in Databricks. You will also learn how to add variables, modify, validate, deploy, and execute Databricks Asset Bundles for multiple environments with different configurations using the Databricks CLI. Finally, the course introduces Visual Studio Code as an Interactive Development Environment (IDE) for building, testing, and deploying Databricks Asset Bundles locally, optimizing your development process. The course concludes with an introduction to automating deployment pipelines using GitHub Actions to enhance the CI/CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate Databricks project deployments with Databricks Asset Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge of the Databricks platform, including experience with Databricks Workspaces, Apache Spark, Delta Lake, the Medallion Architecture, Unity Catalog, Delta Live Tables, and Workflows. In particular, knowledge of leveraging Expectations with DLTs. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "automating-engineering-ai-llms-metadata-driven-frameworks", "title": "Automating Engineering with AI - LLMs in Metadata Driven Frameworks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Quality"], "speakers": ["CTO, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for data engineering keeps growing, but data teams are bored by repetitive tasks, stumped by growing complexity and endlessly harassed by an unrelenting need for speed. What if AI could take the heavy lifting off your hands? What if we make the move away from code-generation and into config-generation \u2014 how much more could we achieve? In this session, we\u2019ll explore how AI is revolutionizing data engineering, turning pain points into innovation. Whether you\u2019re grappling with manual schema generation or struggling to ensure data quality, this session offers practical solutions to help you work smarter, not harder. You\u2019ll walk away with a good idea of where AI is going to disrupt the data engineering workload, some good tips around how to accelerate your own workflows and an impending sense of doom around the future of the industry! /CTO"}
{"session_id": "automating-taxonomy-generation-compound-ai-databricks", "title": "Automating Taxonomy Generation With Compound AI on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Consultant, Data & AI, Lovelytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Taxonomy generation is a challenge across industries such as retail, manufacturing and e-commerce. Incomplete or inconsistent taxonomies can lead to fragmented data insights, missed monetization opportunities and stalled revenue growth. In this session, we will explore a modern approach to solving this problem by leveraging Databricks platform to build a scalable compound AI architecture for automated taxonomy generation. The first half of the session will walk you through the business significance and implications of taxonomy, followed by a technical deep dive in building an architecture for taxonomy implementation on the Databricks platform using a compound AI architecture. We will walk attendees through the anatomy of taxonomy generation, showcasing an innovative solution that combines multimodal and text-based LLMs, internal data sources and external API calls. This ensemble approach ensures more accurate, comprehensive and adaptable taxonomies that align with business needs. /Managing Director, GenAI\nLovelytics /Lead Consultant, Data & AI"}
{"session_id": "autonomous-ai-agents-ai-infrastructure", "title": "Autonomous AI Agents in AI Infrastructure", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Real-time", "Scala"], "speakers": ["Principal Software Engineer, Walmart Global Tech"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Autonomous AI agents are transforming industries by enabling systems to perform tasks, make decisions and adapt in real time without human intervention. In this talk, I will delve into the architecture and design principles required to build these agents within scalable AI infrastructure. Key topics will include constructing modular, reusable frameworks, optimizing resource allocation and enabling interoperability between agents and data pipelines. I will discuss practical use cases in which attendees will learn how to leverage containerization and orchestration techniques to enhance the flexibility and performance of these agents while ensuring low-latency decision-making. This session will also highlight challenges like ensuring robustness, ethical considerations and strategies for real-time feedback loops. Participants will gain actionable insights into building autonomous AI agents that drive efficiency, scalability and innovation in modern AI ecosystems. /Principal Software Engineer"}
{"session_id": "bayadas-snowflake-databricks-migration-transforming-data-speed", "title": "Bayada\u2019s Snowflake-to-Databricks Migration: Transforming Data for Speed & Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Integration", "Machine Learning", "Real-time", "SQL", "Scala"], "speakers": ["CDAO, BAYADA Home Health Care"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayada is transforming its data ecosystem by consolidating Matillion+Snowflake and SSIS+SQL Server into a unified Enterprise Data Platform powered by Databricks. Using Databricks' Medallion architecture, this platform enables seamless data integration, advanced analytics and machine learning across critical domains like general ledger, recruitment and activity-based costing. Databricks was selected for its scalability, real-time analytics and ability to handle both structured and unstructured data, positioning Bayada for future growth. The migration aims to reduce data processing times by 35%, improve reporting accuracy and cut reconciliation efforts by 40%. Operational costs are projected to decrease by 20%, while real-time analytics is expected to boost efficiency by 15%. Join this session to learn how Bayada is leveraging Databricks to build a high-performance data platform that accelerates insights, drives efficiency and fosters innovation organization-wide. /Head of Data Architecture & Governance\nBAYADA Home Health Care /Sr. Director - HLS\nTredence Inc /CDAO"}
{"session_id": "best-practices-building-user-facing-ai-systems-databricks", "title": "Best Practices for Building User-Facing AI Systems on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating AI agents into business systems requires tailored approaches for different maturity levels (crawl-walk-run) that balance scalability, accuracy and usability. This session addresses the critical challenge of making AI agents accessible to business users. We will explore four key integration methods: We'll compare these approaches, discussing their strengths, challenges and ideal use cases to help businesses select the most suitable integration strategy for their specific needs. /Senior Specialist Solutions Architect\nDatabricks /Senior Solutions Architect"}
{"session_id": "better-together-change-data-feed-streaming-data-flow", "title": "Better Together: Change Data Feed in a Streaming Data Flow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Streaming"], "speakers": ["Data Engineer & Architect, 84.51 LLC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditional streaming works great when your data source is append-only, but what if your data source includes updates and deletes? At 84.51 we used DLT and Delta Lake to build a streaming data flow that consumes inserts, updates and deletes while still taking advantage of streaming checkpoints. We combined this flow with a materialized view and Enzyme incremental refresh for a low-code, efficient and robust end-to-end data flow. We process around 8 million sales transactions each day with 80 million items purchased. This flow not only handles new transactions but also handles updates to previous transactions. Join us to learn how 84.51 combined change data feed, data streaming and materialized views to deliver a \u201cbetter together\u201d solution. 84.51 is a retail insights, media & marketing company. We use first-party retail data from 60 million households sourced through a loyalty card program to drive Kroger\u2019s customer-centric journey. /Lead Data Engineer\n84.51\u02da /Data Engineer & Architect"}
{"session_id": "beyond-ai-accuracy-building-trustworthy-and-responsible-ai-application", "title": "Beyond AI Accuracy: Building Trustworthy and Responsible AI Application Through Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Generic LLM metrics are useless until it meets your business needs.In this session we will dive deep into creating bespoke custom state-of-the-art AI metrics that matters to you. Discuss best practices on LLM evaluation strategies, when to use LLM judge vs. statistical metrics and many more. Through a live demo using Mosaic AI Framework, we will showcase: By the end of this session, you'll be equipped to create AI solutions that are not only powerful but also relevant to your organizations needs. Join us to transform your AI strategy and make a tangible impact on your business! /Specialist Solution Architect"}
{"session_id": "beyond-chatbots-building-autonomous-insurance-applications-agentic-ai", "title": "Beyond Chatbots: Building Autonomous Insurance Applications With Agentic AI Framework", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["VP, Software Engineering, Travelers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The insurance industry is at the crossroads of digital transformation, facing challenges from market competition and customer expectations. While conventional ML applications have historically provided capabilities in this domain, the emergence of Agentic AI frameworks presents a revolutionary opportunity to build truly autonomous insurance applications. We will address issues related to data governance and quality while discussing how to monitor/evaluate fine-tune models. We'll demonstrate the application of the agentic framework in the insurance context and how these autonomous agents can work collaboratively to handle complex insurance workflows \u2014 from submission intake and risk evaluation to expedited quote generation. This session demonstrates how to architect intelligent insurance solutions using Databricks Mosaic AI agentic core components including Unity Catalog, Playground, model evaluation/guardrails, privacy filters, AI functions and AI/BI Genie. /Sr. Delivery Solutions Architect\nDatabricks /VP, Software Engineering"}
{"session_id": "beyond-privacy-utility-tradeoff-differential-privacy-tabular-data", "title": "Beyond the Privacy-Utility Tradeoff: Differential Privacy in Tabular Data Synthesis", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Scientist, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly leverage sensitive data for AI applications, generating high quality synthetic data with mathematical guarantees of privacy has become crucial. This talk explores the use of Gretel Navigator to generate differentially private synthetic data that maintains high fidelity to the source data and high utility on downstream tasks across heterogeneous datasets. Our analysis covers a framework for privacy-preserving synthetic data generation with two use cases: patient events and e-commerce reviews. We reveal nuanced strategies for: calibrating privacy parameters \u03b5 and \u03b4 for mixed-modal data, leveraging both record-level and user-level differential privacy depending on which entity in the dataset requires protection, maintaining statistical properties and high utility on downstream classification tasks under stringent privacy constraints (e.g., <0.05 difference in AUC when using DP), and quantifying resilience to membership inference and attribute inference attacks. /Research Scientist"}
{"session_id": "boosting-data-science-and-ai-productivity-databricks-notebooks", "title": "Boosting Data Science and AI Productivity With Databricks Notebooks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to accelerate your team's data science workflow? This session reveals how Databricks Notebooks can transform your productivity through an optimized environment designed specifically for data science and AI work. Discover how notebooks serve as a central collaboration hub where code, visualizations, documentation and results coexist seamlessly, enabling faster iteration and development. Key takeaways: You'll leave with practical techniques to enhance your notebook-based workflow and deliver AI projects faster with higher-quality results. /Databricks"}
{"session_id": "breaking-barriers-building-custom-spark-40-data-connectors-python", "title": "Breaking Barriers: Building Custom Spark 4.0 Data Connectors with Python", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python", "Scala", "Streaming"], "speakers": ["Resident Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building a custom Spark data source connector once required Java or Scala expertise, making it complex and limiting. This left many proprietary data sources without public SDKs disconnected from Spark. Additionally, data sources with Python SDKs couldn't harness Spark\u2019s distributed power. Spark 4.0 changes this with a new Python API for data source connectors, allowing developers to build fully functional connectors without Java or Scala. This unlocks new possibilities, from integrating proprietary systems to leveraging untapped data sources. Supporting both batch and streaming, this API makes data ingestion more flexible than ever. In this talk, we\u2019ll demonstrate how to build a Spark connector for Excel using Python, showcasing schema inference, data reads/writes and streaming support. Whether you're a data engineer or Spark enthusiast, you\u2019ll gain the knowledge to integrate Spark with any data source \u2014 entirely in Python. /Senior Resident Solutions Architect\nDatabricks /Resident Solutions Architect"}
{"session_id": "breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse", "title": "Breaking the Ice(berg): Riskified\u2019s Journey to its Next-Generation Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Warehouse"], "speakers": ["Staff Data Platform Engineer, Riskified"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How many of you manage multiple data stores in your organization, wrestling with different data formats that led to data duplication, infrastructure redundancy, and uncontrolled costs? Imagine reducing these costs by 25% while maintaining your existing SLAs \u2014 this is exactly what we achieved with our next-generation architecture. In this session, we'll show you how we built Riskified's next-generation lakehouse by leveraging Databricks' native Apache Iceberg support and Unity Catalog. We'll share our innovative approach to cross-platform querying without data duplication, and how we transformed our data warehouse into a modern lakehouse architecture. Throughout the session, we'll explore the technical challenges we conquered, from data migration to performance optimization. The result? A simplified world where everything is identical across engines, leaving users with just one choice \u2014 which query engine best suits their use case. /Data Platform Architect\nRiskified /Staff Data Platform Engineer"}
{"session_id": "breaking-silos-cignas-journey-seamless-data-sharing-delta-sharing", "title": "Breaking Silos: Cigna\u2019s Journey to Seamless Data Sharing with Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Real-time"], "speakers": ["Senior Director, Evernorth Health Services"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow increasingly complex, the ability to share data securely, seamlessly, and in real time has become a strategic differentiator. In this session, Cigna will showcase how Delta Sharing on Databricks has enabled them to modernize data delivery, reduce operational overhead, and unlock new market opportunities. Learn how Cigna achieved significant savings by streamlining operations, compute, and platform overhead for just one use case. Explore how decentralizing data ownership\u2014transitioning from hyper-centralized teams to empowered product owners\u2014has simplified delivery and accelerated innovation. Most importantly, see how this modern open data-sharing framework has positioned Cigna to win contracts they previously couldn\u2019t, by enabling real-time, cross-organizational data collaboration with external partners. Join us to hear how Cigna is using Delta Sharing not just as a technical enabler, but as a business catalyst. /Senior Director"}
{"session_id": "breaking-silos-enabling-databricks-snowflake-interoperability-iceberg", "title": "Breaking Silos: Enabling Databricks-Snowflake Interoperability With Iceberg and Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT"], "speakers": ["Member of Technical Staff, Sol Arch, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow more complex, organizations often struggle with siloed platforms and fragmented governance. In this session, we\u2019ll explore how our team made Databricks the central hub for cross-platform interoperability, enabling seamless Snowflake integration through Unity Catalog and the Iceberg REST API. We\u2019ll cover: By leveraging Uniform, Delta, and Iceberg, we created a flexible, vendor-agnostic architecture that bridges Databricks and Snowflake without compromising performance or security. /Director of Data Engineering\nT-Mobile /Member of Technical Staff, Sol Arch"}
{"session_id": "breaking-silos-using-sap-delta-sharing-connector-seamless-access", "title": "Breaking Silos: Using the SAP Delta Sharing Connector for Seamless Access in Databricks", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT", "ETL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019re excited to announce the General Availability of the SAP Delta Sharing Connector, enabling SAP Business Data Cloud (BDC) customers to securely and seamlessly share data with Databricks \u2014 no complex ETL or data duplication required. This connector enables organizations to securely share SAP data for analytics and AI in Databricks, while also supporting bidirectional data sharing back to SAP. In this session, we\u2019ll walk through a live demo of the SAP Delta Sharing Connector in action, followed by a real-world customer story showcasing how this integration is driving value across analytics and AI use cases. You\u2019ll also leave with practical best practices to help you implement the connector in your own environment. Whether you\u2019re looking to bring SAP data into Databricks for advanced analytics or build AI models on top of trusted SAP datasets, this session will show you how to get started \u2014 securely and efficiently. /Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "breaking-spark-versions-better-way-manage-workload-compatibility", "title": "Breaking With Spark Versions: A Better Way to Manage Workload Compatibility + Dependency Management", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Product Manager - serverless jedi, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explains how we\u2019ve made Apache Spark\u2122 versionless for end users by introducing a stable client API, environment versioning and automatic remediation. These capabilities have enabled auto-upgrade of hundreds of millions of workloads with minimal disruption. We\u2019ll also introduce a new approach to dependency management using environments. Admins will learn how to speed up package installation with Default Base Environments, and users will see how to manage custom environments for their own workloads. /Staff Product Manager - serverless jedi"}
{"session_id": "bridging-bi-tools-deep-dive-aibi-dashboards-power-bi-practitioners", "title": "Bridging BI Tools: Deep Dive Into AI/BI Dashboards for Power BI Practitioners", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Snr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the rapidly-evolving field of data analytics, (AI/BI) dashboards and Power BI stand out as two formidable approaches, each offering unique strengths and catering to specific use cases. Power BI has earned its reputation for delivering user-friendly, highly customisable visualisations and reports for data analysis. On the other hand, AI/BI dashboards have gained good traction due to their seamless integration with the Databricks platform, making them an attractive option for data practitioners. This session will provide a comparison of these two tools, highlighting their respective features, strengths and potential limitations. Understanding the nuances between these tools is crucial for organizations aiming to make informed decisions about their data analytics strategy. This session will equip participants with the knowledge needed to select the most appropriate tool or combination of tools to meet their data analysis requirements and drive data-informed decision-making processes. /Senior Solutions Architect\nDatabricks /Snr. Solutions Architect"}
{"session_id": "bridging-big-data-and-ai-empowering-pyspark-lance-format-multi-modal-ai", "title": "Bridging Big Data and AI: Empowering PySpark With Lance Format for Multi-Modal AI Data Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Machine Learning", "Python", "SQL"], "speakers": ["Database Engineer, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark has long been a cornerstone of big data processing, excelling in data preparation, analytics and machine learning tasks within traditional data lakes. However, the rise of multimodal AI and vector search introduces challenges beyond its capabilities. Spark\u2019s new Python data source API enables integration with emerging AI data lakes built on the multi-modal Lance format. Lance delivers unparalleled value with its zero-copy schema evolution capability and robust support for large record-size data (e.g., images, tensors, embeddings, etc), simplifying multimodal data storage. Its advanced indexing for semantic and full-text search, combined with rapid random access, enables high-performance AI data analytics to the level of SQL. By unifying PySpark's robust processing capabilities with Lance's AI-optimized storage, data engineers and scientists can efficiently manage and analyze the diverse data types required for cutting-edge AI applications within a familiar big data framework. /Staff Software Engineer\nDatabricks /Database Engineer"}
{"session_id": "bringing-ai-your-data-using-sql-functions-databricks", "title": "Bringing AI to Your Data: Using SQL Functions in Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks AI Functions make it easy for analysts and data engineers to integrate advanced AI capabilities into data workflows \u2014 no ML expertise is required. These built-in SQL functions let you apply tasks like sentiment analysis, text summarization and language translation directly to data in Databricks, whether you're working in SQL queries, notebooks, DLT or Jobs. This session will walk through practical applications of both general-purpose and task-specific AI Functions, showing how to analyze text data at scale using simple SQL. You'll learn how to convert unstructured content into structured insights, and how to embed AI directly into batch pipelines and analytics processes. What you\u2019ll learn: /Databricks"}
{"session_id": "build-ai-powered-applications-natively-databricks", "title": "Build AI-Powered Applications Natively on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI", "Analytics", "Data Quality", "Scala"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how to build and deploy AI-powered applications natively on the Databricks Data Intelligence Platform. This session introduces best practices and a standard reference architecture for developing production-ready apps using popular frameworks like Dash, Shiny, Gradio, Streamlit and Flask. Learn how to leverage agents for orchestration and explore primary use cases supported by Databricks Apps, including data visualization, AI applications, self-service analytics and data quality monitoring. With serverless deployment and built-in governance through Unity Catalog, Databricks Apps enables seamless integration with your data and AI models, allowing you to focus on delivering impactful solutions without the complexities of infrastructure management. Whether you're a data engineer or an app developer, this session will equip you with the knowledge to create secure, scalable and efficient applications within a Databricks environment. /Staff Software Engineer"}
{"session_id": "build-data-pipelines-dlt", "title": "Build Data Pipelines with DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to define and schedule data pipelines that incrementally ingest and process data through multiple tables on the Data Intelligence Platform, using DLT in Spark SQL and Python. We\u2019ll cover topics like how to get started with DLT, how DLT tracks data dependencies in data pipelines, how to configure and run data pipelines using the DLT. UI, how to use Python or Spark SQL to define data pipelines that ingest and process data through multiple tables on the Data Intelligence Platform, using Auto Loader and DLT, how to use APPLY CHANGES INTO syntax to process Change Data Capture feeds, and how to review event logs and data artifacts created by pipelines and troubleshoot syntax. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "build-your-data-and-ai-culture", "title": "Build Your Data and AI Culture", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Learning & Enablement APJ, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Many studies have indicated that having a strong Data & AI culture helps our businesses be more successful. This can lead to better business performance, becoming more profitable and being more competitive compared to your peer companies as well as attaining and retaining top talent. What does it mean to have a Data & AI culture? It\u2019s the ability for an organization to make data-driven decisions. It means using insights to improve your business results and using data ultimately allows you to enable AI. It tends to be the people that get in the way of having and sustaining an effective Data & AI culture. Do you have people already in your teams that can help you build your Data & AI culture? Can you attract and retain that talent to your organization? Can you help integrate that great talent into your organization to promote a Data & AI culture? It\u2019s also ensuring that you fundamentally change the way you/your teams/organizations work. /Sr.Director Customer Enablement\nDatabricks /Director, Learning & Enablement APJ"}
{"session_id": "building-ai-models-health-care-using-semi-synthetic-data", "title": "Building AI Models In Health Care Using Semi-Synthetic Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Co-founder, Fight Health Insurance INC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Regulated or restricted fields like Health Care make collecting training data complicated. We all want to do the right thing, but how? This talk will look at how Fight Health Insurance used de-identified public and proprietary information to create a semi-synthetic training set for use in fine-tuning machine learning models to power Fight Paperwork. We'll explore how to incorporate the latest \"reasoning\" techniques in fine tuning as well as how to make models that you can afford to serve \u2014 think single GPU inference instead of a cluster of A100s. In addition to the talk we have the code used in a public GitHub repo \u2014 although it is a little rough, so you might want to use it more as a source of inspiration rather than directly forking it. /Co-founder"}
{"session_id": "building-and-scaling-production-ai-systems-mosaic-ai", "title": "Building and Scaling Production AI Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP of Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ready to go beyond the basics of Mosaic AI? This session will walk you through how to architect and scale production-grade AI systems on the Databricks Data Intelligence Platform. We\u2019ll cover practical techniques for building end-to-end AI pipelines \u2014 from processing structured and unstructured data to applying Mosaic AI tools and functions for model development, deployment and monitoring. You\u2019ll learn how to integrate experiment tracking with MLflow, apply performance tuning and use built-in frameworks to manage the full AI lifecycle. By the end, you\u2019ll be equipped to design, deploy and maintain AI systems that deliver measurable outcomes at enterprise scale. /CTO, Neural Networks\nDatabricks /VP of Engineering"}
{"session_id": "building-dashboards-production-grade-data-product", "title": "Building Dashboards as a Production-Grade Data Product", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Software Development Engineer, Zillow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Zillow, we have accelerated the volume and quality of our dashboards by leveraging a modern SDLC with version control and CI/CD. In the past three months, we have released 32 production-grade dashboards and shared them securely across the organization while cutting error rates in half over that span. In this session, we will provide an overview of how we utilize Databricks asset bundles and GitLab CI/CD to create performant dashboards that can be confidently used for mission-critical operations. As a concrete example, we'll then explore how Zillow's Data Platform team used this approach to automate our on-call support analysis, leveraging our dashboard development strategy alongside Databricks LLM offerings to create a comprehensive view that provides actionable performance metrics alongside AI-generated insights and action items from the hundreds of requests that make up our support workload. /Senior Software Development Engineer"}
{"session_id": "building-intelligent-ai-agents-claude-models-and-databricks-mosaic-ai", "title": "Building Intelligent AI Agents With Claude Models and Databricks Mosaic AI Framework", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Technical Staff, Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how Anthropic's frontier models power AI agents in Databricks Mosaic AI Agent Framework. Learn to leverage Claude's state-of-the-art capabilities for complex agentic workflows while benefiting from Databricks unified governance, credential management and evaluation tools. We'll demonstrate how Anthropic's models integrate seamlessly to create production-ready applications that combine Claude's reasoning with Databricks data intelligence capabilities. /Technical Staff"}
{"session_id": "building-knowledge-agents-automate-document-workflows", "title": "Building Knowledge Agents to Automate Document Workflows", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-founder and CEO, LlamaIndex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "One of the biggest promises for LLM agents is automating all knowledge work over unstructured data \u2014 we call these \"knowledge agents\". To date, while there are fragmented tools around data connectors, storage and agent orchestration, AI engineers have trouble building and shipping production-grade agents beyond basic chatbots. In this session, we first outline the highest-value knowledge agent use cases we see being built and deployed at various enterprises. These are: We then define the core architectural components around knowledge management and agent orchestration required to build these use cases. By the end you'll not only have an understanding of the core technical concepts, but also an appreciation of the ROI you can generate for end-users by shipping these use cases to production. /Co-founder and CEO"}
{"session_id": "building-real-time-sport-model-insights-spark-structured-streaming", "title": "Building Real-Time Sport Model Insights with Spark Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Lead Data Science Engineer, Draftkings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the dynamic world of sports betting, precision and adaptability are key. Sports traders must navigate risk management, limitations of data feeds, and much more to prevent small model miscalculations from causing significant losses. To ensure accurate real-time pricing of hundreds of interdependent markets, traders provide key inputs such as player skill-level adjustments, whilst maintaining precise correlations. Black-box models aren\u2019t enough\u2014 constant feedback loops drive informed, accurate decisions. Join DraftKings as we showcase how we expose real-time metrics from our simulation engine, to empower traders with deeper insights into how their inputs shape the model. Using Spark Structured Streaming, Kafka, and Databricks dashboards, we transform raw simulation outputs into actionable data. This transparency into our engines enables fine-grained control over pricing\u2015 leading to more accurate odds, a more efficient sportsbook, and an elevated customer experience. /Lead Machine Learning Engineer\nDraftkings /Lead Data Science Engineer"}
{"session_id": "building-real-time-trading-dashboards-dlt-and-databricks-apps", "title": "Building Real-Time Trading Dashboards With DLT and Databricks Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Barclays Post Trade real-time trade monitoring platform was historically built on a complex set of legacy technologies including Java, Solace, and custom micro-services.This ssession will demonstrate how the power of DLT new real-time mode, in conjunction with the foreach_batch_sink, can enable simple, cost-effective streaming pipelines that can load high volumes of data into our OLTP database with very low latency. Once in our OLTP database, this can be used to update real-time trading dashboards, securely hosted in Databricks Apps, with the latest stock trades - enabling better, more responsive decision-making and alerting.The session will walk-through the architecture, and demonstrate how simple it is to create and manage the pipelines and apps within the Databricks environment. /Senior Specialist Solution Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "building-reliable-agentic-ai-databricks", "title": "Building Reliable Agentic AI on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["CEO & Co-Founder, Monte Carlo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/CEO & Co-Founder"}
{"session_id": "building-responsible-ai-agents-databricks", "title": "Building Responsible AI Agents on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS APPS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Delivery Solutions Architect (DSA) FINS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation explores how Databricks' Data Intelligence Platform supports the development and deployment of responsible AI in credit decisioning, ensuring fairness, transparency and regulatory compliance. Key areas include bias and fairness monitoring using Lakehouse Monitoring to track demographic metrics and automated alerts for fairness thresholds. Transparency and explainability are enhanced through the Mosaic AI Agent Framework, SHAP values and LIME for feature importance auditing. Regulatory alignment is achieved via Unity Catalog for data lineage and AIBI dashboards for compliance monitoring. Additionally, LLM reliability and security are ensured through AI guardrails and synthetic datasets to validate model outputs and prevent discriminatory patterns. The platform integrates real-time SME and user feedback via Databricks Apps and AI/BI Genie Space. /Senior Resident Solutions Architect\nDatabricks /Delivery Solutions Architect (DSA) FINS"}
{"session_id": "building-responsible-and-resilient-ai-databricks-ai-governance", "title": "Building Responsible and Resilient AI: The Databricks AI Governance Framework", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Sr. Director, Field Security, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "GenAI & machine learning are reshaping industries, driving innovation and redefining business strategies. As organizations embrace these technologies, they face significant challenges in managing AI initiatives effectively, such as balancing innovation with ethical integrity, operational resilience and regulatory compliance. This presentation introduces the Databricks AI Governance Framework (DAGF), a practical framework designed to empower organizations to navigate the complexities of AI. It provides strategies for building scalable, responsible AI programs that deliver measurable value, foster innovation and achieve long-term success. By examining the framework's five foundational pillars \u2014 AI organization, ethics, legal and regulatory compliance, transparency and interpretability, AI operations and infrastructure and AI security \u2014 this session highlights how AI governance aligns programs with the organization's strategic goals, mitigates risks and builds trust across stakeholders. /Senior Specialist Solution Architect\nDatabricks /Sr. Director, Field Security"}
{"session_id": "building-scalable-real-time-concurrency-prediction-service", "title": "Building a Scalable, Real-Time Concurrency Prediction Service", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science", "Machine Learning", "Real-time"], "speakers": ["AVP Data Science & Machine Learning, Dream 11"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dream11's rapid growth has posed critical challenges in scaling infrastructure to handle millions of concurrent users during high-traffic events. Concurrency Prediction Service provides real-time forecasts of peak user activity in 30-minute intervals to optimize resource allocation by the Scaler Service. This presentation covers the critical aspects of building and optimizing the Concurrency Prediction Service, including: /AVP Data Science & Machine Learning"}
{"session_id": "building-seamless-multi-cloud-platform-secure-portable-workloads", "title": "Building a Seamless Multi-Cloud Platform for Secure Portable Workloads", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many challenges to making a data platform actually a platform, something that hides complexity. Data engineers and scientists are looking for a simple and intuitive abstraction to focus on their work, not where it runs to maintain compliance, what credentials it uses to access data or how it generates operational telemetry. At Databricks we\u2019ve developed a data-centric approach to workload development and deployment that enables data workers to stop doing migrations and instead develop with confidence. Attend this session to learn how to run simple, secure and compliant global multi-cloud workloads at scale on Databricks. /Staff Software Engineer"}
{"session_id": "building-self-service-data-platform-small-data-team", "title": "Building a Self-Service Data Platform With a Small Data Team", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Head of Architecture, Dodo Brands"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Dodo Brands, a global pizza and coffee business with over 1,200 retail locations and 40k employees, revolutionized their analytics infrastructure by creating a self-service data platform. This session explores the approach to empowering analysts, data scientists and ML engineers to independently build analytical pipelines with minimal involvement from data engineers. By leveraging Databricks as the backbone of their platform, the team developed automated tools like a \"job-generator\" that uses Jinja templates to streamline the creation of data jobs. This approach minimized manual coding and enabled non-data engineers to create over 1,420 data jobs \u2014 90% of which were auto-generated by user configurations. Supporting thousands of weekly active users via tools like Apache Superset. This session provides actionable insights for organizations seeking to scale their analytics capabilities efficiently without expanding their data engineering teams. /Senior Data Engineer\nDodo Brands /Head of Architecture"}
{"session_id": "building-tool-calling-agents-databricks-agent-framework-and-mcp", "title": "Building Tool-Calling Agents With Databricks Agent Framework and MCP", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Want to create AI agents that can do more than just generate text? Join us to explore how combining Databricks' Mosaic AI Agent Framework with the Model Context Protocol (MCP) unlocks powerful tool-calling capabilities. We'll show you how MCP provides a standardized way for AI agents to interact with external tools, data and APIs, solving the headache of fragmented integration approaches. Learn to build agents that can retrieve both structured and unstructured data, execute custom code and tackle real enterprise challenges. Key takeaways: Whether you're building customer service bots or data analysis assistants, you'll leave with practical know-how to create powerful, governed AI agents. /Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "building-trustworthy-ai-northwestern-mutual-guardrail-technologies-and", "title": "Building Trustworthy AI at Northwestern Mutual: Guardrail Technologies and Strategies", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Science"], "speakers": ["Data Science, Northwestern Mutual"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Science"}
{"session_id": "building-your-talent-pipeline-databricks-university-alliance", "title": "Building Your Talent Pipeline With the Databricks University Alliance", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Program Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks University Alliance represents more than 1,000 universities across the globe. Join us to see how you can leverage this network of faculty and students to broaden your talent pipeline to students learning with the latest tools in data and AI from Databricks. /Program Manager"}
{"session_id": "busting-data-modeling-myths-truths-and-best-practices-data-modeling", "title": "Busting Data Modeling Myths: Truths and Best Practices for Data Modeling in the Lakehouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Modeling", "Data Quality", "SQL", "Scala"], "speakers": ["Practice Lead, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the truth behind data modeling in Databricks. This session will tackle the top 10 myths surrounding relational and dimensional data modeling. Attendees will gain a clear understanding of what Databricks Lakehouse truly supports today, including how to leverage primary and foreign keys, identity columns for surrogate keys, column-level data quality constraints and much more. This session will talk through the lens of medallion architecture, explaining how to implement data models across bronze, silver, and gold tables. Whether you\u2019re migrating from a legacy warehouse or building new analytics solutions, you\u2019ll leave equipped to fully leverage Databricks\u2019 capabilities, and design scalable, high-performance data models for enterprise analytics. /DBSQL Product Specialist\nDatabricks /Practice Lead"}
{"session_id": "capitalizing-alternatives-data-addepar-platform-private-markets", "title": "Capitalizing Alternatives Data on the Addepar Platform: Private Markets Benchmarking", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Pipeline", "ELT", "Scala"], "speakers": ["Addepar"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Addepar possesses an enormous private investment data set with 40% of the $7T assets on the platform allocated to alternatives. Leveraging the Addepar Data Lakehouse (ADL), built on Databricks, we have built a scalable data pipeline that assesses millions of private fund investment cash flows and translates it to a private fund benchmarks data offering. Investors on the Addepar platform can leverage this data seamlessly integrated against their portfolio investments and obtain actionable investment insights. At a high-level, this data offering consists of an extensive data aggregation, filtering, and construction logic that dynamically updates for clients through the Databricks job workflows. This derived dataset has gone through several iterations with investment strategists and academics that leveraged delta shared tables. Irrespective of the data source, the data pipeline coalesces all relevant cash flow activity against a unique identifier before constructing the benchmarks. /Addepar"}
{"session_id": "caring-our-data-how-chick-fil-enables-data-driven-decision-making", "title": "Caring for Our Data: How Chick-fil-A Enables Data-Driven Decision Making with Databricks", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Data architect, Chick-fil-A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Chick-fil-A\u2019s adoption of Databricks has been rewarding, providing us with a modern, scalable, efficient data platform that meets our evolving needs. With lakehouse, Chick-fil-A can unlock new insights, drive business success and continue \u201cWinning Hearts Every Day.\u201d Our top priority is ensuring our customers have an exceptional experience. We have unrivaled discipline in store operations and guest interaction, but our data systems were lagging. Owner/operators were spending too much time sitting in front of screens getting data to manage their stores. That data was often slow and untrusted. This is why we\u2019re enabling AI that enables them to ask questions to get accurate responses, allowing them to spend more time in store working with team members/delighting customers. Databricks also provided BI performance improvement that has addressed major challenges to store operators, helping them to make better decisions with sales forecasting, labor optimizations and inventory management. /Enterprise Architect\nChick-fil-A, Inc /Data architect"}
{"session_id": "cicd-databricks-advanced-asset-bundles-and-github-actions", "title": "CI/CD for Databricks: Advanced Asset Bundles and GitHub Actions", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Python"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Asset Bundles (DABs) provide a way to use the command line to deploy and run a set of Databricks assets \u2014 like notebooks, Python code, DLT pipelines and workflows. To automate deployments, you create a deployment pipeline that uses the power of DABs along with other validation steps to ensure high quality deployments. In this session you will learn how to automate CI/CD processes for Databricks while following best practices to keep deployments easy to scale and maintain. After a brief explanation of why Databricks Asset Bundles are a good option for CI/CD, we will walk through a working project including advanced variables, target-specific overrides, linting, integration testing and automatic deployment upon code review approval. You will leave the session clear on how to build your first GitHub Action using DABs. /Sr. Specialist Solutions Architect"}
{"session_id": "clickhouse-and-databricks-building-strong-integration-your-lakehouse", "title": "ClickHouse and Databricks: Building a Strong Integration With Your Lakehouse", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "ELT", "Real-time"], "speakers": ["Principal Product Manager, ClickHouse"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "ClickHouse is a C++ based, column-oriented database built for real-time analytics. While it has its own internal storage format, the rise of open lakehouse architectures has created a growing need for seamless interoperability. In response, we have developed integrations with your favorite lakehouse ecosystem to enhance compatibility, performance and governance. From integrating with Unity Catalog to embedding the Delta Kernel into ClickHouse, this session will explore the key design considerations behind these integrations, their benefits to the community, the lessons learned and future opportunities for improved compatibility and seamless integration. /Principal Product Manager"}
{"session_id": "code-completion-autonomous-software-engineering-agents", "title": "From Code Completion to Autonomous Software Engineering Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Software Engineer, Princeton University"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As language models have advanced, they have moved beyond code completion and are beginning to tackle software engineering tasks in a more autonomous, agentic way. However, evaluating agentic capabilities is challenging. To address this, we first introduce SWE-bench, a benchmark built from real GitHub issues that has become the standard for assessing AI\u2019s ability to resolve complex software tasks in large codebases. We will discuss the current state of the field, the limitations of today\u2019s models, and how far we still are from truly autonomous AI developers. Next, we will explore the fundamentals of agents based on hands-on demonstrations with SWE-agent, a simple yet powerful agent framework designed for software engineering but adaptable to a variety of domains. By the end of this session, you will have a clear understanding of the current frontier of agentic AI in software engineering, the challenges ahead and how you can experiment with AI agents in your own workflows. /Research Software Engineer"}
{"session_id": "code-insights-leveraging-advanced-infrastructure-and-ai-capabilities", "title": "From Code to Insights: Leveraging Advanced Infrastructure and AI Capabilities.", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Director, Insulet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this talk, we will explore how AI and advanced infrastructure are transforming Insulet's development and operations. We'll highlight how our innovations have reduced scrap part costs through manufacturing analytics, showcasing efficiency and cost savings. On leveraging Databricks AI solutions and productivity, it not only identifies errors but also fixes code and assists in writing complex queries. This goes beyond suggestions, providing actual solutions. On the infrastructure side, integrating Spark with Databricks simplifies setup and reduces costs. Additionally Databricks Lakeflow Connect enables real-time updates and simplification without much coding as we integrate with Salesforce. We'll also discuss real-time processing of patient data, demonstrating how Databricks drives efficiency and productivity. Join us to learn how these innovations enhance efficiency, cost savings and performance. /Director"}
{"session_id": "collaborative-innovation-how-spur-innovation-while-driving-efficiency", "title": "Collaborative Innovation: How to Spur Innovation While Driving Efficiency", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering"], "speakers": ["Head of Data Engineering, Allianz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Financial Services Industry Director\nDatabricks /Managing Director\nAon /Head of Advanced Engineering\nEverest /Head of Data Engineering"}
{"session_id": "composing-high-accuracy-ai-systems-slms-and-mini-agents", "title": "Composing High-Accuracy AI Systems With SLMs and Mini-Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["CEO & Cofounder, Lamini"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "For most companies, building compound AI systems remains aspirational. LLMs are powerful, but imperfect, and their non-deterministic nature makes steering them to high accuracy a challenge. In this session, we\u2019ll demonstrate how to build compound AI systems using SLMs and highly accurate mini-agents that can be integrated into agentic workflows. You'll learn about breakthrough techniques, including: memory RAG, an embedding algorithm that reduces hallucinations using embed-time compute to generate contextual embeddings, improving indexing and retrieval, and memory tuning, a finetuning algorithm that reduces hallucinations using a Mixture of Memory Experts (MoME) to specialize models with proprietary data. We\u2019ll also share real-world examples (text-to-SQL, factual reasoning, function calling, code analysis and more) across various industries. With these building blocks, we\u2019ll demonstrate how to create high accuracy mini-agents that can be composed into larger AI systems. /CEO & Cofounder"}
{"session_id": "comprehensive-data-management-and-governance-azure-data-lake-storage", "title": "Comprehensive Data Management and Governance With Azure Data Lake Storage", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake"], "speakers": ["Principal PDM Manager, Microsoft Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Given that data is the new oil, it must be treated as such. Organizations that pursue greater insight into their businesses and their customers must manage and govern the use of the data that drives these insights in an efficient, cost-effective, compliant and auditable manner without degrading access to that data. Data catalogs provide an end-user experience to describe the management and governance activities associated with tables of data. However, without those data catalogs being integrated into the storage service that holds that data, there is a disconnection that undermines an organization's data management and governance posture. Azure Data Lake Storage is excited to announce Project Metalake (name: TBD) that will allow customers to attach their own data catalog (including Unity Catalog) to their ADLS account and have those management and governance policies natively and efficiently implemented by ADLS. /Principal Program Manager\nMicrosoft /Principal PDM Manager"}
{"session_id": "comprehensive-data-warehouse-migrations-databricks-sql", "title": "Comprehensive Data Warehouse Migrations to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse", "SQL"], "speakers": ["Lead Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks has a free, comprehensive solution for migrating legacy data warehouses from a wide range of source systems. See how we accelerate migrations from legacy data warehouses to Databricks SQL, achieving 50% faster migration than traditional methods. We'll cover the tool\u2019s automated migration process: This comprehensive approach increases the predictability of migration projects, allowing businesses to plan and execute migrations with greater confidence. /Sr Specialist Solutions Architect\nDatabricks /Lead Specialist Solutions Architect"}
{"session_id": "comprehensive-guide-mlops-databricks", "title": "Comprehensive Guide to MLOps on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Staff Data Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This in-depth session explores advanced MLOps practices for implementing production-grade machine learning workflows on Databricks. We'll examine the complete MLOps journey from foundational principles to sophisticated implementation patterns, covering essential tools including MLflow, Unity Catalog, Feature Stores and version control with Git. Dive into Databricks' latest MLOps capabilities including MLflow 3.0, which enhances the entire ML lifecycle from development to deployment with particular focus on generative AI applications. Key session takeaways include: /Software Engineer\nDatabricks /Staff Data Scientist"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Real-time", "Streaming"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Databricks"}
{"session_id": "cooking-sql-ingredients-insights-minimal-prep", "title": "Cooking With SQL: From Ingredients to Insights With Minimal Prep", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Sr. Specialist Solutions Architect DWH, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session we\u2019ll dive into the SQL kitchen and use a combination of SQL staples and nouvelle cuisine such as recursive queries, temporary tables, and stored procedures. We\u2019ll leave you with well-scripted recipes to execute immediately or store for later consumption in your Unity Catalog. Think of this session as building your go-to cookbook of SQL techniques. Bon app\u00e9tit! /Principal Software Engineer\nDatabricks /Sr. Specialist Solutions Architect DWH"}
{"session_id": "cost-effective-data-architecture-and-ai-practice-databricks-funplus", "title": "Cost-Effective Data Architecture and AI Practice With Databricks at FunPlus", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering"], "speakers": ["Data Director, FunPlus"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "FunPlus's journey to building a cost-effective and efficient data platform with Databricks: exploring how FunPlus leveraged Databricks to tackle key challenges, enhance data engineering and ML efficiency, and showcasing best practices and their impact on game development and operations. /Data Director"}
{"session_id": "cost-management-foundations-first-100-days-checklist", "title": "Cost Management Foundations: The First 100 Days Checklist", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session you'll learn how to onboard to Databricks in a way that ensures you can effectively measure and manage ROI of using Databricks down the line. We will show you how to set-up workspaces and compute, decide on a tagging strategy and how to utilize policies to enforce best practices and make future you a happy camper. /Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "cracking-complex-documents-databricks-mosaic-ai", "title": "Cracking Complex Documents with Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Principal Data Scientist, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share how we are transforming the way organizations process unstructured and non-standard documents using Mosaic AI and agentic patterns within the Databricks ecosystem. We have developed a scalable pipeline that turns complex legal and regulatory content into structured, tabular data.We will walk through the full architecture, which includes Unity Catalog for secure and governed data access, Databricks Vector Search for intelligent indexing and retrieval and Databricks Apps to deliver clear insights to business users. The solution supports multiple languages and formats, making it suitable for teams working across different regions. We will also discuss some of the key technical challenges we addressed, including handling parsing inconsistencies, grounding model responses and ensuring traceability across the entire process. If you are exploring how to apply GenAI and large language models, this session is for you. /Principal Data Scientist"}
{"session_id": "crafting-business-brilliance-leveraging-databricks-sql-next-gen", "title": "Crafting Business Brilliance: Leveraging Databricks SQL for Next-Gen Applications", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Software Engineering, Haleon"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Haleon, we've leveraged Databricks APIs and serverless compute to develop customer-facing applications for our business. This innovative solution enables us to efficiently deliver SAP invoice and order management data through front-end applications developed and served via our API Gateway. The Databricks lakehouse architecture has been instrumental in eliminating the friction associated with directly accessing SAP data from operational systems, while enhancing our performance capabilities. Our system acheived response times of less than 3 seconds from API call, with ongoing efforts to optimise this performance. This architecture not only streamlines our data and application ecosystem but also paves the way for integrating GenAI capabilities with robust governance measures for our future infrastructure. The implementation of this solution has yielded significant benefits, including a 15% reduction in customer service costs and a 28% increase in productivity for our customer support team. /Senior Solutions Architect\nDatabricks /Director of Software Engineering"}
{"session_id": "creating-custom-pyspark-stream-reader-pyspark-40", "title": "Creating a Custom PySpark Stream Reader with PySpark 4.0", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Delta Lake", "ELT", "SQL", "Streaming"], "speakers": ["Head of Data Engineering, Entrada"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PySpark supports many data sources out of the box, such as Apache Kafka, JDBC, ODBC, Delta Lake, etc. However, some older systems, such as systems that use JMS protocol, are not supported by default and require considerable extra work for developers to read from them. One such example is ActiveMQ for streaming. Traditionally, users of ActiveMQ have to use a middle-man in order to read the stream with Spark (such as writing to a MySQL DB using Java code and reading that table with Spark JDBC). With PySpark 4.0\u2019s custom data sources (supported in DBR 15.3+) we are able to cut out the middle-man processing using batch or Spark Streaming and consume the queues directly from PySpark, saving developers considerable time and complexity in getting source data into your Delta Lake and governed by Unity Catalog and orchestrated with Databricks Workflows. /Head of Data Engineering"}
{"session_id": "cross-cloud-data-mesh-delta-sharing-and-uniform-mercedes-benz", "title": "Cross-Cloud Data Mesh with Delta Sharing and UniForm in Mercedes-Benz", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Lead Architect, Mercedes-Benz Group AG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we'll show how we achieved a unified development experience for teams working on Mercedes-Benz Data Platforms in AWS and Azure. We will demonstrate how we implemented Azure to AWS and AWS to Azure data product sharing (using Delta Sharing and Cloud Tokens), integration with AWS Glue Iceberg tables through UniForm and automation to drive everything using Azure DevOps Pipelines and DABs. We will also show how to monitor and track cloud egress costs and how we present a consolidated view of all the data products and relevant cost information. The end goal is to show how customers can offer the same user experience to their engineers and not have to worry about which cloud or region the Data Product lives in. Instead, they can enroll in the data product through self-service and have it available to them in minutes, regardless of where it originates. /Sr. Resident Solutions Architect\nDatabricks /Lead Architect"}
{"session_id": "crypto-scale-building-cost-efficient-high-performance-platform-real", "title": "Crypto at Scale: Building a Cost-Efficient, High-Performance Platform for Real-Time Blockchain Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Scala", "Streaming"], "speakers": ["Lead Data Engineer, Elliptic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s fast-evolving crypto landscape, organizations require fast, reliable intelligence to manage risk, investigate financial crime, and stay ahead of evolving threats. In this session, discover how Elliptic built a scalable, high-performance data intelligence platform that delivers real-time, actionable blockchain insights\u2014empowering businesses to future-proof their crypto risk strategies and law enforcement to streamline investigations. We\u2019ll walk you through how we've built a high-performance data platform, leveraging key components of the Databricks ecosystem such as Structured Streaming and Delta Lake. This transformation has fundamentally changed the way we deliver user-facing analytics\u2014improving not only speed and scalability, but also enabling analytics to directly enhance the accuracy and intelligence of our operational systems. Along the way, we\u2019ll share how we overcame critical challenges such as building efficient incremental pipelines, designing robust partitioning strategies for large-scale crypto datasets, and enabling seamless data sharing with our customers.Whether you\u2019re looking to enhance your streaming capabilities, expand your knowledge of how crypto analytics works or simply discover novel approaches to data processing at scale, this session will provide concrete strategies and valuable lessons learned from the front lines of crypto attribution /Specialist Solutions Architect\nDatabricks /Lead Data Engineer"}
{"session_id": "cutting-costs-not-performance-optimizing-databricks-scale", "title": "Cutting Costs, Not Performance: Optimizing Databricks at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Scala"], "speakers": ["Lead Engineer, NTT DATA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As Databricks transforms data processing, analytics and machine learning, managing platform costs has become crucial for organizations aiming to maximize value while staying within budget. While Databricks offers unmatched scalability and performance, inefficient usage can lead to unexpected cost overruns. This presentation will explore common challenges organizations face in controlling Databricks costs and provide actionable best practices for optimizing resource allocation, preventing over-provisioning and eliminating underutilization. Drawing from NTT DATA\u2019s experience, I'll share how we reduced Databricks costs by up to 50% through strategies like choosing the right compute resource, leveraging manage tables and using Unity Catalog features, such as system tables, to monitor consumption. Join this session to gain practical insights and tools that will empower your team to optimize Databricks without overspending. /Project Manager\nNTTDATA /Lead Engineer"}
{"session_id": "daft-and-unity-catalog-multimodalai-native-lakehouse", "title": "Daft and Unity Catalog: A Multimodal/AI-Native Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake"], "speakers": ["Co-Founder, Eventual"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Modern data organizations have moved beyond big data analytics to also incorporate advanced AI/ML data workloads. These workflows often involve multimodal datasets containing documents, images, long-form text, embeddings, URLs and more. Unity Catalog is an ideal solution for organizing and governing this data at scale. When paired with the Daft open source data engine, you can build a truly multimodal, AI-ready data lakehouse. In this session, we\u2019ll explore how Daft integrates with Unity Catalog\u2019s core features (such as volumes and functions) to enable efficient, AI-driven data lakehouses. You will learn how to ingest and process multimodal data (images, text and videos), run AI/ML transformations and feature extractions at scale, and maintain full control and visibility over your data with Unity Catalog\u2019s fine-grained governance. /Co-Founder"}
{"session_id": "data-after-hours", "title": "Data After Hours", "track": "", "level": "", "type": "EVENING EVENT", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "data-drives-dairy-royal-frieslandcampinas-data-centric-transformation", "title": "Data Drives Dairy: Royal FrieslandCampina's Data-Centric Transformation With Databricks", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics"], "speakers": ["Chief Data & Analytics Officer, Royal FrieslandCampina (RFC)"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Chief Data & Analytics Officer"}
{"session_id": "data-ingestion-lakeflow-connect", "title": "Data Ingestion with Lakeflow Connect", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Warehouse", "Delta Lake", "ELT", "ETL", "Python", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to have efficient data ingestion with Lakeflow Connect and manage that data with Databricks. We\u2019ll cover topics such as ingestion with built-in connectors for popular SaaS applications, databases and file sources, as well as ingestion from cloud object storage, and batch and streaming ingestion. Lakeflow Connect is fully integrated with the Data Intelligence Platform including unified governance, observability, and Delta Lake for the foundation of a data lakehouse architecture. We'll cover the new connector components, setting up the pipeline, validating the source and mapping to the destination for each type of connector. We'll also cover how to ingest data with Batch to Streaming ingestion into Delta tables, using the UI with Auto Loader, automating ETL with DLT or using the API. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc. Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "data-intelligence-cybersecurity-forum-insights-sap-anvilogic-capital", "title": "Data Intelligence for Cybersecurity Forum: Insights From SAP, Anvilogic, Capital One, and Wiz", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "MOSAIC AI", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Delta Lake", "ELT", "Scala", "Streaming"], "speakers": ["VP of Product Marketing, Wiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join cybersecurity leaders from SAP, Anvilogic, Capital One, Wiz, and Databricks to explore how modern data intelligence is transforming security operations. Discover how SAP adopted a modular, AI-powered detection engineering lifecycle using Anvilogic on Databricks. Learn how Capital One built a detection and correlation engine leveraging Delta Lake, Apache Spark Streaming, and Databricks to process millions of cybersecurity events per second. Finally, see how Wiz and Databricks\u2019 partnership enhances cloud security with seamless threat visibility. Through expert insights and live demos, gain strategies to build scalable, efficient cybersecurity powered by data and AI. /Chief Security Architect\nSAP /VP of Product\nAnvilogic /Director, Software Engineering\nCapital one /VP of Product Marketing"}
{"session_id": "data-intelligence-marketing-breakout-agentic-systems-bayesian-mmm-and", "title": "Data Intelligence for Marketing Breakout: Agentic Systems for Bayesian MMM and Consumer Testing", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Partner, PyMC Labs"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk dives into leveraging GenAI to scale sophisticated decision intelligence. Learn how an AI copilot interface simplifies running complex Bayesian probabilistic models, accelerating insight generation, and accurate decision making at the enterprise level. We talk through techniques for deploying AI agents at scale to simulate market dynamics or product feature impacts, providing robust, data-driven foresight for high-stakes innovation and strategy directly within your Databricks environment. For marketing teams, this approach will help you leverage autonomous AI agents to dynamically manage media channel allocation while simulating real-world consumer behavior through synthetic testing environments. /Marketing Solutions GTM\nDatabricks /Partner"}
{"session_id": "data-intelligence-marketing-forum-unlocking-future-marketing-ai", "title": "Data Intelligence for Marketing Forum: Unlocking the Future of Marketing With AI", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "180 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Marketing Solutions GTM, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don\u2019t miss the Data Intelligence for Marketing forum at this year\u2019s Databricks Data + AI Summit \u2014 designed for marketing leaders ready to transform with data and AI. Learn how Databricks unified its marketing data with lakehouse architecture, hear Deloitte\u2019s David Geisinger on the evolving CMO role in the GenAI era, and discover how Reckitt\u2019s Bastien Parizot built a cutting-edge GenAI platform to revolutionize marketing operations. Walk away with actionable strategies, real-world blueprints, and insights from top experts to drive agility, customer insights, and AI-powered marketing success. Prioritize this forum to lead your organization\u2019s data-driven future. /Marketing Solutions GTM"}
{"session_id": "data-intelligence-unity-catalog-managed-tables-powered-predictive", "title": "Data Intelligence on Unity Catalog Managed Tables Powered by Predictive Optimization", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "data-management-and-governance-uc", "title": "Data Management and Governance With UC", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Data Lake", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you'll learn concepts and perform labs that showcase workflows using Unity Catalog - Databricks' unified and open governance solution for data and AI. We'll start off with a brief introduction to Unity Catalog, discuss fundamental data governance concepts, and then dive into a variety of topics including using Unity Catalog for data access control, managing external storage and tables, data segregation, and more. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.). Labs: Yes Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "data-modeling-101-data-lakehouse-demystified", "title": "Data Modeling 101 for Data Lakehouse Demystified", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Modeling", "Data Warehouse", "Scala"], "speakers": ["Lead Data Engineer, Pythian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data-driven world, the Data Lakehouse has emerged as a powerful architectural paradigm that unifies the flexibility of data lakes with the reliability and structure of traditional data warehouses. However, organizations must adopt the right data modeling techniques to unlock its full potential to ensure scalability, maintainability and efficiency. This session is designed for beginners looking to demystify the complexities of data modeling for the lakehouse and make informed design decisions. We\u2019ll break down Medallion Architecture, explore key data modeling techniques and walk through the maturity stages of a successful data platform \u2014 transitioning from raw, unstructured data to well-organized, query-efficient models. /Lead Data Engineer"}
{"session_id": "data-modeling-strategies", "title": "Data Modeling Strategies", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Architecture", "Data Integration", "Delta Lake", "ELT"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course offers a deep dive into designing data models within the Databricks Lakehouse environment, and understanding the data products lifecycle. Participants will learn to align business requirements with data organization and model design leveraging Delta Lake and Unity Catalog for defining data architectures, and techniques for data integration and sharing. Prerequisites: Foundational knowledge equivalent to Databricks Certified Data Engineer Associate and familiarity with many topics covered in Databricks Certified Data Engineer Professional. Experience with: Labs: Yes"}
{"session_id": "data-monetization-through-delta-sharing-and-data-clean-rooms", "title": "Data Monetization Through Delta Sharing and Data Clean Rooms", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL", "DELTA SHARING", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "SQL"], "speakers": ["Deep Sync"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Deep Sync, a leader in deterministic identity solutions, leverages the Databricks Lakehouse to manage an identity spine containing the most accurate and up-to-date attribute data for over 98% of U.S. households. In partnership with T-Mobile, Deep Sync is building a Data as a Service (DaaS) business on Databricks, enabling the use of consented subscriber data to create hyper-personalized experiences, measure marketing impact and more. This collaboration represents the first time T-Mobile\u2019s consented data will be brought to market. The technical presentation, jointly delivered by Deep Sync and T-Mobile, will showcase the Databricks solutions powering this initiative, including Lakehouse (DBSQL, GenieRooms), Unity Catalog, Delta Sharing and multi-party Data Clean Rooms. The session will focus on Databricks solutions used to enable management, governance, security and collaboration between T-Mobile, Deep Sync and third-party customers. /Deep Sync"}
{"session_id": "data-preparation-machine-learning", "title": "Data Preparation for Machine Learning", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Python"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn the fundamentals of preparing data for machine learning using Databricks. We\u2019ll cover topics like exploring, cleaning, and organizing data tailored for traditional machine learning applications. We\u2019ll also cover data visualization, feature engineering, and optimal feature storage strategies. Pre-requisites: Familiarity with Databricks workspace, notebooks, as well as Unity Catalog. An intermediate level knowledge of Python (scikit-learn, Matplotlib), Pandas, and PySpark. As well as with concepts of exploratory data analysis, feature engineering, standardization, and imputation methods). Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "data-triggers-and-advanced-control-flow-lakeflow-jobs", "title": "Data Triggers and Advanced Control Flow With Lakeflow Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Jobs is the production-ready fully managed orchestrator for the entire Lakehouse with 99.95% uptime. Join us for a dive into how you can orchestrate your enterprise data operations, from triggering your jobs only when your data is ready to advanced control flow with conditionals, looping and job modularity \u2014 with demos! Attendees will gain practical insights into optimizing their data operations by orchestrating with Lakeflow Jobs: /Product Manager"}
{"session_id": "data-warehousing-databricks", "title": "Data Warehousing with Databricks", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed for data professionals who want to explore the data warehousing capabilities of Databricks. Assuming no prior knowledge of Databricks, it provides an introduction to leveraging Databricks as a modern cloud-based data warehousing solution. Learners will explore how use the Databricks Data Intelligence Platform to ingest, transform, govern, and analyze data efficiently. Learners will also explore Genie, an innovative Databricks feature that simplifies data exploration through natural language queries. By the end of this course, participants will be equipped with the foundational skills to implement and optimize a data warehouse using Databricks. Pre-requisites: Labs: Yes"}
{"session_id": "databricks-action-azures-blueprint-secure-and-cost-effective-operations", "title": "Databricks in Action: Azure\u2019s Blueprint for Secure and Cost-Effective Operations", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Real-time", "Scala"], "speakers": ["Information Security Specialist Expert, Erste Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Erste Group's transition to Azure Databricks marked a significant upgrade from a legacy system to a secure, scalable and cost-effective cloud platform. The initial architecture, characterized by a complex hub-spoke design and stringent compliance regulations, was replaced with a more efficient solution. The phased migration addressed high network costs and operational inefficiencies, resulting in a 60% reduction in networking costs and a 30% reduction in compute costs for the central team. This transformation, completed over a year, now supports real-time analytics, advanced machine learning and GenAI while ensuring compliance with European regulations. The new platform features a Unity Catalogue, separate data catalogs and dedicated workspaces, demonstrating a successful shift to a cloud-based machine learning environment with significant improvements in cost, performance and security. /Senior Solution Manager\nErste Group /Information Security Specialist Expert"}
{"session_id": "databricks-ai-factory-transforming-seven-west-media", "title": "Databricks AI Factory Transforming Seven West Media", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Director, Data & Growth, Seven West Media"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The implementation of the Databricks AI Factory enabled Seven West Media to transform its business by accelerating the launch of AI-driven use cases, fostering innovation and reducing time to market. By leveraging a unified data and AI platform, the company achieved better ROI through optimized workflows, improved operational efficiency and scalable machine learning models. The AI Factory empowered data teams to experiment faster, unlocking deeper audience insights that enhanced engagement and content personalization. This transformation positioned Seven West Media as a leader in AI-driven media, driving measurable business impact and future-proofing its data strategy. /Director, Data & Growth"}
{"session_id": "databricks-apps-turning-data-and-ai-practical-user-friendly", "title": "Databricks Apps: Turning Data and AI Into Practical, User-Friendly Applications", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DATABRICKS APPS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Product, Apps, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we present an overview of the GA release of Databricks Apps, the new app hosting platform that integrates all the Databricks services necessary to build production-ready data and AI applications. With Apps, data and developer teams can build new interfaces into the data intelligence platform, further democratizing the transformative power of data and AI across the organization. We'll cover common use cases, including RAG chat apps, interactive visualizations and custom workflow builders, as well as look at several best practices and design patterns when building apps. Finally, we'll look ahead with the vision, strategy and roadmap for the year ahead. /Director of Product, Apps"}
{"session_id": "databricks-backbone-mlops-orchestration-inference", "title": "Databricks as the Backbone of MLOps: From Orchestration to Inference", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Senior Expert - MLOps, Globe Telecoms"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As machine learning (ML) models scale in complexity and impact, organizations must establish a robust MLOps foundation to ensure seamless model deployment, monitoring and retraining. In this session, we\u2019ll share how we leverage Databricks as the backbone of our MLOps ecosystem \u2014 handling everything from workflow orchestration to large-scale inference. We\u2019ll walk through our journey of transitioning from fragmented workflows to an integrated, scalable system powered by Databricks Workflows. You\u2019ll learn how we built an automated pipeline that streamlines model development, inference and monitoring while ensuring reliability in production. We\u2019ll also discuss key challenges we faced, lessons learned and best practices for organizations looking to operationalize ML with Databricks. /Asst. Director MLOps\nGlobe Telecoms /Senior Expert - MLOps"}
{"session_id": "databricks-best-practices-mitigate-ai-security-risks", "title": "Databricks Best Practices to Mitigate AI Security Risks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is transforming industries, enhancing customer experiences and automating decisions. As organizations integrate AI into core operations, robust security is essential. The Databricks Security team collaborated with top cybersecurity researchers from OWASP, Gartner, NIST, HITRUST and Fortune 100 companies to evolve the Databricks AI Security Framework (DASF) to version 2.0. In this session, we\u2019ll cover an AI security architecture using Unity Catalog, MLflow, egress controls, and AI gateway. Learn how security teams, AI practitioners and data engineers can secure AI applications on Databricks. Walk away with:\u2022 A reference architecture for securing AI applications\u2022 A worksheet with AI risks and controls mapped to industry standards like MITRE, OWASP, NIST and HITRUST\u2022 A DASF AI assistant tool to test your AI security /Principal Staff Security Field Engineer\nDatabricks /Senior Staff Product Manager"}
{"session_id": "databricks-data-privacy", "title": "Databricks Data Privacy", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Delta Lake", "ELT", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to apply patterns to securely store and delete personal information for data governance and compliance on the Data Intelligence Platform. We\u2019ll cover topics like storing sensitive data appropriately to simplify granting access and processing deletes, processing deletes to ensure compliance with the right to be forgotten, performing data masking, and configuring fine-grained access control to configure appropriate privileges to sensitive data. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.). Beginner experience with DLT and streaming workloads. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "databricks-databricks-powering-marketing-insights-lakehouse", "title": "Databricks on Databricks: Powering Marketing Insights with Lakehouse", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Sr Director Marketing Analytics, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation outlines the evolution of our marketing data strategy, focusing on how we\u2019ve built a strong foundation using the Databricks Lakehouse. We will explore key advancements across data ingestion, strategy, and insights, highlighting the transition from legacy systems to a more scalable and intelligent infrastructure. Through real-world applications, we will showcase how unified Customer 360 insights drive personalization, predictive analytics enhance campaign effectiveness, and GenAI optimizes content creation and marketing execution. Looking ahead, we will demonstrate the next phase of our CDP, the shift toward an end-user-first analytics model powered by AIBI, Genie and Matik, and the growing importance of clean rooms for secure data collaboration. This is just the beginning, and we are poised to unlock even greater capabilities in the future. /Sr. Dir, Marketing Tech + Growth\nDatabricks /Sr Director Marketing Analytics"}
{"session_id": "databricks-databricks-transforming-sales-experience-using-genai-agents", "title": "Databricks on Databricks: Transforming the Sales Experience using GenAI Agents at Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Vice President, IT & Deputy CIO, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks is transforming its sales experience with a GenAI agent \u2014 built and deployed entirely on Databricks \u2014 to automate tasks, streamline data retrieval, summarize content, and enable conversational AI for over 4,000 sellers. This agent leverages the AgentEval framework, AI Bricks, and Model Serving to process both structured and unstructured data within Databricks, unlocking deep sales insights. The agent seamlessly integrates across multiple data sources including Salesforce, Google Drive, and Glean securely via OAuth. This session includes a live demonstration and explores the business impact, architecture as well as agent development and evaluation strategies, providing a blueprint for deploying secure, scalable GenAI agents in large enterprises. /Enterprise Architect\nDatabricks /Senior Vice President, IT & Deputy CIO"}
{"session_id": "databricks-good-bad-and-ugly", "title": "Databricks, the Good, the Bad and the Ugly", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks is the bestest platform ever where everything is perfect and nothing else could ever make it any better, right? \u2026right? You and I know, this is not true. Don\u2019t get me wrong, there are features that I absolutely love, but there are also some that require powering through the papercuts. And then there are those that I pretend don\u2019t exist. I\u2019ll be opening up to give my honest take on three of each category, why I do (or don\u2019t) like them, and then telling you which talks to attend to find out more. /Staff Developer Advocate"}
{"session_id": "databricks-lakeflow-foundation-data-ai-innovation-your-industry", "title": "Databricks Lakeflow: the Foundation of Data + AI Innovation for Your Industry", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Pipeline", "Real-time"], "speakers": ["Sr. Manager Product Marketing, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Every analytics, BI and AI project relies on high-quality data. This is why data engineering, the practice of building reliable data pipelines that ingest and transform data, is consequential to the success of these projects. In this session, we'll show how you can use Lakeflow to accelerate innovation in multiple parts of the organization. We'll review real-world examples of Databricks customers using Lakeflow in different industries such as automotive, healthcare and retail. We'll touch on how the foundational data engineering capabilities Lakeflow provides help power initiatives that improve customer experiences, make real-time decisions and drive business results. /Sr. Manager Product Marketing"}
{"session_id": "databricks-observability-using-system-tables-monitor-and-manage-your", "title": "Databricks Observability: Using System Tables to Monitor and Manage Your Databricks Instance", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager, Billing\nDatabricks /Product Manager"}
{"session_id": "databricks-performance-optimization", "title": "Databricks Performance Optimization", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to optimize workloads and physical layout with Spark and Delta Lake and and analyze the Spark UI to assess performance and debug applications. We\u2019ll cover topics like streaming, liquid clustering, data skipping, caching, photons, and more. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.) Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "databricks-streaming-and-dlt", "title": "Databricks Streaming and DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to Incrementally process data to power analytic insights with Structured Streaming and Auto Loader, and how to apply design patterns for designing workloads to perform ETL on the Data Intelligence Platform with DLT. First, we\u2019ll cover topics including ingesting raw streaming data, enforcing data quality, implementing CDC, and exploring and tuning state information. Then, we\u2019ll cover options to perform a streaming read on a source, requirements for end-to-end fault tolerance, options to perform a streaming write to a sink, and creating an aggregation and watermark on a streaming dataset. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc.), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.). Beginner experience with streaming workloads and familiarity with DLT. Labs: No Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "days-seconds-reducing-query-times-large-geospatial-datasets-99", "title": "From Days to Seconds \u2014 Reducing Query Times on Large Geospatial Datasets by 99%", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL", "Streaming"], "speakers": ["Associate Director of Technology, Global Water Security Center"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Global Water Security Center translates environmental science into actionable insights for the U.S. Department of Defense. Prior to incorporating Databricks, responding to these requests required querying approximately five hundred thousand raster files representing over five hundred billion points. By leveraging lakehouse architecture, Databricks Auto Loader, Spark Streaming, Databricks Spatial SQL, H3 geospatial indexing and Databricks Liquid Clustering, we were able to drastically reduce our \u201ctime to analysis\u201d from multiple business days to a matter of seconds. Now, our data scientists execute queries on pre-computed tables in Databricks, resulting in a \u201ctime to analysis\u201d that is 99% faster, giving our teams more time for deeper analysis of the data. Additionally, we\u2019ve incorporated Databricks Workflows, Databricks Asset Bundles, Git and Git Actions to support CI/CD across workspaces. We completed this work in close partnership with Databricks. /Sr. Solutions Archtect\nDatabricks /Associate Director of Technology"}
{"session_id": "de-risking-investment-decisions-qcgs-smarter-deal-evaluation-process", "title": "De-risking Investment Decisions: QCG's Smarter Deal Evaluation Process Leveraging Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Head of Engineering, Quantum Capital Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Engineering"}
{"session_id": "dealing-sensitive-data-databricks-natura", "title": "Dealing With Sensitive Data on Databricks at Natura", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Data Architect, Natura"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Architect"}
{"session_id": "declarative-pipelines-ask-us-anything", "title": "Declarative Pipelines \u2014 Ask Us Anything", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "SQL"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an insightful Ask Me Anything (AMA) session on Declarative Pipelines \u2014 a powerful approach to simplify and optimize data workflows. Learn how to define data transformations using high-level, SQL-like semantics, reducing boilerplate code while improving performance and maintainability. Whether you're building ETL processes, feature engineering pipelines, or analytical workflows, this session will cover best practices, real-world use cases and how Declarative Pipelines can streamline your data applications. Bring your questions and discover how to make your data processing more intuitive and efficient! /Engineering Director\nDatabricks /PM Director, Developer Relations\nDatabricks /Software Engineer"}
{"session_id": "declarative-pipelines-simplifying-data-engineering-workloads", "title": "Declarative Pipelines: Simplifying Data Engineering Workloads", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT has made it dramatically easier to build production-grade pipelines, using a declarative framework that abstracts away orchestration and complexity. It\u2019s become a go-to solution for teams who want reliable, maintainable pipelines without reinventing the wheel. But we\u2019re just getting started. In this session, we\u2019ll take a step back and share how DLT fits into a broader vision for the future of data engineering pipelines \u2014 one that opens the door to a new level of openness, standardization and community momentum. We\u2019ll cover the core concepts behind declarative pipelines, where the architecture is headed, and what this shift means for data engineers building procedural code. Don\u2019t miss this session \u2014 we\u2019ll be sharing something new that sets the direction for what comes next. /Distinguished Engineer\nDatabricks /Software Engineer"}
{"session_id": "defending-revenue-genai", "title": "Defending Revenue With GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Managing Director of Product, Blueprint"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Managing Director of Product"}
{"session_id": "deliver-data-where-its-needed-scale-aibi-dashboards-enterprise", "title": "Deliver Data Where It\u2019s Needed: Scale AI/BI Dashboards for Enterprise Reporting", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Software Engineer\nDatabricks /Product Manager"}
{"session_id": "delivering-sub-second-latency-operational-workloads-databricks", "title": "Delivering Sub-Second Latency for Operational Workloads on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Real-time", "Streaming"], "speakers": ["Head of Streaming, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As enterprise streaming adoption accelerates, more teams are turning to real-time processing to support operational workloads that require sub-second response times. To address this need, Databricks introduced Project Lightspeed in 2022, which recently delivered Real-Time Mode in Apache Spark\u2122 Structured Streaming. This new mode achieves consistent p99 latencies under 300ms for a wide range of stateless and stateful streaming queries. In this session, we\u2019ll define what constitutes an operational use case, outline typical latency requirements and walk through how to meet those SLAs using Real-Time Mode in Structured Streaming. /Staff Software Engineer\nDatabricks /Head of Streaming"}
{"session_id": "delta-and-databricks-cost-effective-exabyte-scale-real-time-web", "title": "Delta and Databricks as a Cost-Effective, Exabyte-Scale, Real-Time Web Application Backend", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "ETL", "Real-time", "SQL", "Streaming"], "speakers": ["VP, Distinguished Engineer, Capital One Financial"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Delta Lake architecture promises to provide a single, highly functional, and high-scale copy of data that can be leveraged by a variety of tools to satisfy a broad range of use cases. To date, most use cases have focused on interactive data warehousing, ETL, model training, and streaming. Real-time access is generally delegated to costly and sometimes difficult-to-scale NoSQL, indexed storage, and domain-specific specialty solutions, which provide limited functionality compared to Spark on Delta Lake. In this session, we will explore the Delta data-skipping and optimization model and discuss how Capital One leveraged it along with Databricks photon and Spark Connect to implement a real-time web application backend. We\u2019ll share how we built a highly-functional and performant security information and event management user experience (SIEM UX) that is cost effective. /VP, Distinguished Engineer"}
{"session_id": "delta-kernel-rs-unparalleled-interoperability-across-query-engines", "title": "Delta-Kernel-RS: Unparalleled Interoperability Across Query Engines", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we introduce Delta-Kernel-RS, a new Rust implementation of the Delta Lake protocol designed for unparalleled interoperability across query engines. In this session, we will explore how maintaining a native implementation of the Delta specification \u2014 with native C and C++ FFI support \u2014 can deliver consistent benefits across diverse data processing systems, eliminating the need for repetitive, engine-specific reimplementations. We will dive deep into a real-world case study where a query engine harnessed Delta-Kernel-RS to unlock significant data skipping improvements \u2014 enhancements achieved \u201cfor free\u201d by leveraging the kernel. Attendees will gain insights into the architectural decisions, interoperability strategies and the practical impact of this innovation on performance and development efficiency in modern data ecosystems. /software engineer/ski bum\ndatabricks /Staff Developer Advocate"}
{"session_id": "delta-kernel-rust-and-java", "title": "Delta Kernel for Rust and Java", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Code Monkey, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Kernel makes it easy for engines and connectors to read and write Delta tables. It supports many Delta features and robust connectors, including DuckDB, Clickhouse, Spice AI and delta-dotnet. In this session, we'll cover lessons learned about how to build a high-performance library that lets engines integrate the way they want, while not having to worry about the details of the Delta protocol. We'll talk through how we streamlined the API as well as its changes and underlying motivations. We'll discuss some new highlight features like write support, and the ability to do CDF scans. Finally we'll cover the future roadmap for the Kernel project and what you can expect from the project over the coming year. /Code Monkey"}
{"session_id": "delta-lake-data-mesh", "title": "Delta Lake on the Data Mesh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Mesh", "Delta Lake", "ELT"], "speakers": ["Principal Engineer, Nextdata"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake has proven to be an excellent storage format. Coupled with the Databricks platform, the storage format has shined as a component of a distributed system on the lakehouse. The pairing of Delta and Spark provides an excellent platform, but users often struggle to perform comparable work outside of the Spark ecosystem. Tools such as delta-rs, Polars and DuckDb have brought access to users outside of Spark, but they are only building blocks of a larger system. In this 40-minute talk we will demonstrate how users can use data products on the Nextdata OS data mesh to interact with the Databricks platform to drive Delta Lake workflows. Additionally, we will show how users can build autonomous data products that interact with their Delta tables both inside and outside of the lakehouse platform. Attendees will learn how to integrate the Nextdata OS data mesh with the Databricks platform as both an external and integral component. /Principal Engineer"}
{"session_id": "delta-lake-liquid-clustering-lightning-fast-queries-massive-datasets", "title": "Delta Lake Liquid Clustering: Lightning-Fast Queries on Massive Datasets", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we\u2019ll dive into the power of Liquid Clustering\u2014an innovative, out-of-the-box solution that automatically tunes your data layout to scale effortlessly with your datasets. You\u2019ll get a deep look at how Liquid Clustering works, along with real-world examples of customers leveraging it to unlock blazing-fast query performance on petabyte-scale datasets. We\u2019ll also give you an exciting sneak peek into the roadmap ahead, with upcoming features and enhancements to come. /Product Manager\nDatabricks /Software Engineer"}
{"session_id": "delta-rs-turning-five-growing-pains-and-life-lessons", "title": "Delta-rs Turning Five: Growing Pains and Life Lessons", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Python"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Five years ago, the delta-rs project embarked on a journey to bring Delta Lake's robust capabilities to the Rust & Python ecosystem. In this talk, we'll delve into the triumphs, tribulations and lessons learned along the way. We'll explore how delta-rs has matured alongside the thriving Rust data ecosystem, adapting to its evolving landscape and overcoming the challenges of maintaining a complex data project. Join us as we share insights into the project's evolution, the symbiotic relationship between delta-rs and the Rust community, and the current hurdles and future directions that lie ahead. /Staff Developer Advocate"}
{"session_id": "delta-sharing-action-architecture-and-best-practices", "title": "Delta Sharing in Action: Architecture and Best Practices", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Sharing is revolutionizing how enterprises share live data and AI assets securely, openly and at scale. As the industry\u2019s first open data-sharing protocol, it empowers organizations to collaborate seamlessly across platforms and with any partner, whether inside or outside the Databricks ecosystem. In this deep-dive session, you\u2019ll learn best practices and real-world use cases that show how Delta Sharing helps accelerate collaboration and fuel AI-driven innovation. We\u2019ll also unveil the latest advancements, including: Whether you\u2019re a data engineer, architect, or data leader, you\u2019ll leave with practical strategies to future-proof your data-sharing architecture. Don\u2019t miss the live demos, expert guidance and an exclusive look at what\u2019s next in data collaboration. /Engineering Manager\nDatabricks /Staff Product Manager"}
{"session_id": "delta-sharing-demystified-options-use-cases-and-how-it-works", "title": "Delta Sharing Demystified: Options, Use Cases and How it Works", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT"], "speakers": ["Senior Business Analyst Consultant, Amesto Nextbride"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data sharing doesn\u2019t have to be complicated. In this session, we\u2019ll take a practical look at Delta Sharing in Databricks \u2014 what it is, how it works and how it fits into your organization\u2019s data ecosystem. The focus will be on giving an overview of the different ways to share data using Databricks, from direct sharing setups to broader distribution via the Databricks Marketplace and more collaborative approaches like Clean Rooms. This talk is meant for anyone curious about modern, secure data sharing \u2014 whether you're just getting started or looking to expand your use of Databricks. Attendees will walk away with a clearer picture of what\u2019s possible, what\u2019s required to get started and how to choose the right sharing method for the right scenario. /Senior Business Analyst Consultant"}
{"session_id": "democratizing-data-regulated-industry-best-practices-and-outcomes-jp", "title": "Democratizing Data in a Regulated Industry: Best Practices and Outcomes With J.P. Morgan Payments", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Chief Data Officer - JP Morgan Payments, J.P.Morgan"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join our 2024 Databricks Disruptor award winners for a session on how they leveraged the Databricks and AWS platforms to build an internal technology marketplace in the highly regulated banking industry empowering end-users to innovate and own their data sets while maintaining strict compliance. In this talk, leaders from the J.P. Morgan Payments Data team share how they\u2019ve done it \u2014 from keeping customer needs at the center of all decision-making to promoting a culture of experimentation. They\u2019ll also expand upon how J.P. Morgan Payments products team now leverages the data platform they\u2019ve built to create customer products including Cash Flow Intelligence. /Chief Data Officer - JP Morgan Payments"}
{"session_id": "demystifying-upgrading-unity-catalog-challenges-design-and-execution", "title": "Demystifying Upgrading to Unity Catalog \u2014 Challenges, Design and Execution", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["CEO & Co-Founder, Celebal Technologies"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Unity Catalog (UC) is the industry\u2019s only unified and open governance solution for data and AI, built into the Databricks Data Intelligence Platform. UC provides a single source of truth for organization\u2019s data and AI, providing open connectivity to any data source, any format, lineage, monitoring and support for open sharing and collaboration. In this session we will discuss the challenges in upgrading to UC from your existing databricks Non-UC set up. We will discuss a few customer use cases and how we overcame difficulties and created a repeatable pattern and reusable assets to replicate the success of upgrading to UC across some of the largest databricks customers. It is co-presented with our partner Celebal Technologies. /Lead Specialist Solutions Architect\nDatabricks /CEO & Co-Founder"}
{"session_id": "deploy-and-scale-ai-models-mosaic-model-serving", "title": "Deploy and Scale AI Models With Mosaic Model Serving", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ever struggled with getting your AI models into production? Join us to discover how Databricks' Mosaic AI Model Serving takes the headache out of deploying both traditional ML and generative AI models at scale. This session demonstrates how to implement a highly available, auto-scaling service that optimizes performance while controlling costs. Discover deployment strategies for various model types \u2014 from custom models to foundation models like Llama 4 and external models from OpenAI and Anthropic \u2014 with proper governance through Mosaic AI Gateway. Key takeaways: /Databricks"}
{"session_id": "deploy-workloads-lakeflow-jobs-previously-databricks-workflows", "title": "Deploy Workloads with Lakeflow Jobs (previously Databricks Workflows)", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to orchestrate data pipelines with LakeFlow Jobs (previously Databricks Workflows) and schedule dashboard updates to keep analytics up-to-date. We\u2019ll cover topics like getting started with LakeFlow Jobs, how to use Databricks SQL for on-demand queries, and how to configure and schedule dashboards and alerts to reflect updates to production data pipelines. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "deploying-databricks-asset-bundles-dabs-scale", "title": "Deploying Databricks Asset Bundles (DABs) at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Managing data and AI workloads in Databricks can be complex. Databricks Asset Bundles (DABs) simplify this by enabling declarative, Git-driven deployment workflows for notebooks, jobs, DLT pipelines, dashboards, ML models and more. Join the DABs Team for a Deep Dive and learn about: If you're a data engineer, ML practitioner or platform architect, this talk will provide practical insights to improve reliability, efficiency and compliance in your Databricks workflows. /Sr. Staff Software Engineer\nDatabricks /Product Management"}
{"session_id": "deploying-unity-catalog-oss-kubernetes-simplifying-infrastructure", "title": "Deploying Unity Catalog OSS on Kubernetes: Simplifying Infrastructure Management", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Developer, Nebius"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Software Developer"}
{"session_id": "devconnect-keynote", "title": "DevConnect Keynote", "track": "", "level": "", "type": "MEETUP", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The DevConnect Meetup is a multi-track event designed to explore the latest news, innovations, and trends in the data, analytics, and AI ecosystem. This technical meetup series focuses on GenAI and agentic systems by connecting researchers, engineers, and industry leaders to share insights, discuss challenges, and showcase cutting-edge developments from data pipelines to AI architectures. Attendees will have the opportunity to hear from researchers and engineers representing academic institutions, startups, and enterprises at the forefront of AI innovation."}
{"session_id": "developing-dreamers-data-ais-future-how-8451-builds-upskilling", "title": "Developing the Dreamers of Data + AI\u2019s Future: How 84.51\u02da builds upskilling to accelerate adoption", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning"], "speakers": ["Data Science L&D Director, 84.51\u00b0"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "\u201cOnce an idea has taken hold of the brain it's almost impossible to eradicate. An idea that is fully formed \u2014 fully understood \u2014 that sticks, right in there somewhere.\u201d The Data Scientists and Engineers at 84.51\u02da utilize the Databricks Lakehouse for a wide array of tasks, including data exploration, analysis, machine learning operations, orchestration, automated deployments and collaboration. In this talk, 84.51\u02da\u2019s Data Science Learning Lead, Michael Carrico, will share their approach to upskilling a diverse workforce to support the company\u2019s strategic initiatives. This approach includes creating tailored learning experiences for a variety of personas using content curated in partnership with Databricks\u2019 educational offerings. Then he will demonstrate how he puts his 11 years of data science and engineering experience to work by using the Databricks Lakehouse not just as a subject, but also as a tool to create impactful training experiences and a learning culture at 84.51\u02da. /Data Science L&D Director"}
{"session_id": "disneys-foundational-medallion-journey-next-generation-data", "title": "Disney's Foundational Medallion: A Journey Into Next-Generation Data Architecture", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT", "Scala", "Streaming"], "speakers": ["Director, Data Engineering, Disney"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Step into the world of Disney Streaming as we unveil the creation of our Foundational Medallion, a cornerstone in our architecture that redefines how we manage data at scale. In this session, we'll explore how we tackled the multi-faceted challenges of building a consistent, self-service surrogate key architecture \u2014 a foundational dataset for every ingested stream powering Disney Streaming's data-driven decisions. Learn how we streamlined our architecture and unlocked new efficiencies by leveraging cutting-edge Databricks features such as liquid clustering, Photon with dynamic file pruning, Delta's identity column, Unity Catalog and more \u2014 transforming our implementation into a simpler, more scalable solution. Join us on this thrilling journey as we navigate the twists and turns of designing and implementing a new Medallion at scale \u2014 the very heartbeat of our streaming business! /Director, Data Engineering"}
{"session_id": "dlt-integrations-and-interoperability-get-data-and-anywhere", "title": "DLT Integrations and Interoperability: Get Data From \u2014 and to \u2014 Anywhere", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "ELT", "ETL", "Python"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, you will learn how to integrate DLT with external systems in order to ingest and send data virtually anywhere. DLT is most often used in ingestion and ETL into the Lakehouse. New DLT capabilities like the DLT Sinks API and added support for Python Data Source and ForEachBatch have opened up DLT to support almost any integration. This includes popular Apache Spark\u2122 integrations like JDBC, Kafka, External and managed Delta tables, Azure CosmosDB, MongoDB and more. /Sr. Staff Product Manager"}
{"session_id": "doordash-customer-360-data-store-and-its-evolution-become-entity", "title": "Doordash Customer 360 Data Store and its Evolution to Become an Entity Management Framework", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Governance", "Delta Lake", "ELT", "Scala"], "speakers": ["Data Engineer, DoorDash"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The \"Doordash Customer 360 Data Store\" represents a foundational step in centralizing and managing customer profile to enable targeting and personalized customer experiences built on Delta Lake. This presentation will explore the initial goals and architecture of the Customer 360 Data Store, its journey to becoming a robust entity management framework, and the challenges and opportunities encountered along the way. We will discuss how the evolution addressed scalability, data governance and integration needs, enabling the system to support dynamic and diverse use cases, including customer lifecycle analytics, marketing campaign targeting using segmentation. Attendees will gain insight into key design principles, technical innovations and strategic decisions that transformed the system into a flexible platform for entity management, positioning it as a critical enabler of data-driven growth at Doordash. /Data Engineering Manager\nDoordash /Data Engineer"}
{"session_id": "driving-databricks-platform-revenue-intelligence-roi", "title": "Driving Databricks Platform With Revenue Intelligence ROI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Director, Data Management, Veeam"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Demonstrating a real ROI is key to driving executive and stakeholder buy-in for major technology changes. At Veeam, we aligned our Databricks Platform change with projects to increase sales pipeline and improve customer retention. By delivering targeted improvements on those critical business metrics, we created positive ROI in short order while at the same time setting the foundation for long term Databricks Platform success. This session targets data and business leaders looking to understand how they can turn their infrastructure change into a business revenue driver. /Senior Director, Data Management"}
{"session_id": "driving-secure-ai-innovation-obsidian-security-databricks-and", "title": "Driving Secure AI Innovation with Obsidian Security, Databricks, and PointGuard AI", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["PointGuard AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As enterprises adopt AI and Large Language Models (LLMs), securing and governing these models\u2014and the data used to train them\u2014is essential. In this session, learn how the Databricks AI Security Framework helps organizations like PointGuard AI manage AI-specific risks, ensuring security, compliance, and governance across the entire AI lifecycle. Then, discover how Obsidian Security provides a robust approach to AI security, enabling organizations to confidently scale AI applications. /Sr. Specialist Solutions Architect\nDatabricks /CISO\nObsidian Security /PointGuard AI"}
{"session_id": "driving-trusted-insights-aibi-and-unity-catalog-metric-views", "title": "Driving Trusted Insights With AI/BI and Unity Catalog Metric Views", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Deliver trusted, high-performance insights by incorporating Unity Catalog metric views and business semantics into your AI/BI workflows. This session dives into the architecture and best practices for defining reusable metrics, implementing governance and enhancing query performance in AI/BI Dashboards and Genie. Learn how to manage business semantics effectively to ensure data consistency while empowering business users with governed, self-service analytics. Ideal for teams looking to streamline analytics at scale, this session provides practical strategies for driving data accuracy and governance. /Sr. Staff Product Manager\nDatabricks /Databricks"}
{"session_id": "dspy-30-and-dspy-databricks", "title": "DSPy 3.0 \u2014 and DSPy at Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Research Scientist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The DSPy OSS team at Databricks and beyond is excited to present DSPy 3.0, targeted for release close to DAIS 2025. We will present what DSPy is and how it evolved over the past year. We will discuss greatly improved prompt optimization and finetuning/RL capabilities, improved productionization and observability via thorough and native integration with MLflow, and lessons from usage of DSPy in various Databricks R&D and professional services contexts. /Research Scientist"}
{"session_id": "dusting-cobwebs-moving-26-year-old-heritage-platform-databricks", "title": "Dusting off the Cobwebs \u2014 Moving off a 26-year-old Heritage Platform to Databricks [Teradata]", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Data Platforms Executive, NAB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to hear about how National Australia Bank (NAB) successfully completed a significant milestone in its data strategy by decommissioning its 26-year-old Teradata environment and migrating to a new strategic data platform called 'Ada'. This transition marks a pivotal shift from legacy systems to a modern, cloud-based data and AI platform powered by Databricks. The migration process, which spanned two years, involved ingesting 16 data sources, transferring 456 use cases, and collaborating with hundreds of users across 12 business units. This strategic move positions NAB to leverage the full potential of cloud-native data analytics, enabling more agile and data-driven decision-making across the organization. The successful migration to Ada represents a significant step forward in NAB's ongoing efforts to modernize its data infrastructure and capitalize on emerging technologies in the rapidly evolving financial services landscape /Data Platforms Executive"}
{"session_id": "easy-ways-optimize-your-databricks-costs", "title": "Easy Ways to Optimize Your Databricks Costs", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Resident Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will explore effective strategies for optimizing costs on the Databricks platform, a leading solution for handling large-scale data workloads. Databricks, known for its open and unified approach, offers several tools and methodologies to ensure users can maximize their return on investment (ROI) while managing expenses efficiently. Key points: By the end of this session, you will have a comprehensive understanding of how to leverage Databricks' built-in tools for cost optimization, ensuring that their data and AI projects not only deliver value but do so in a cost-effective manner. This session is ideal for data engineers, financial analysts, and decision-makers looking to enhance their organization\u2019s efficiency and financial performance through strategic cost management on Databricks. /Solutions Architect\nDatabricks /Senior Resident Solutions Architect"}
{"session_id": "elevate-sql-productivity-power-notebooks-and-sql-editor", "title": "Elevate SQL Productivity: The Power of Notebooks and SQL Editor", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["SQL"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Writing SQL is a core part of any data analyst\u2019s workflow, but small inefficiencies can add up, slowing down analysis and making it harder to iterate quickly. In this session, we\u2019ll explore our powerful features in the Databricks SQL editor and notebook that help you to be more productive when writing SQL on Databricks. We\u2019ll demo the new features and the customer use cases that inspired them. /Senior Product Manager"}
{"session_id": "elevating-data-quality-standards-databricks-dqx", "title": "Elevating Data Quality Standards With Databricks DQX", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Batch Processing", "Data Quality", "Python", "Real-time", "Streaming"], "speakers": ["RSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an introductory session on Databricks DQX, a Python-based framework designed to validate the quality of PySpark DataFrames. Discover how DQX can empower you to proactively tackle data quality challenges, enhance pipeline reliability and make more informed business decisions with confidence. Traditional data quality tools often fall short by providing limited, actionable insights, relying heavily on post-factum monitoring, and being restricted to batch processing. DQX overcomes these limitations by enabling real-time quality checks at the point of data entry, supporting both batch and streaming data validation and delivering granular insights at the row and column level. If you\u2019re seeking a simple yet powerful data quality framework that integrates seamlessly with Databricks, this session is for you. /Sr. Resident Solutions Architect\nDatabricks /RSA"}
{"session_id": "embracing-unity-catalog-and-empowering-innovation-genie-room", "title": "Embracing Unity Catalog and Empowering Innovation With Genie Room", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "ETL"], "speakers": ["Data Engineer, Bagelcode"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bagelcode, a leader in the social casino industry, has utilized Databricks since 2018 and manages over 10,000 tables via Hive Metastore. In 2024, we embarked on a transformative journey to resolve inefficiencies and unlock new capabilities. Over five months, we redesigned ETL pipelines with Delta Lake, optimized partitioned table logs and executed a seamless migration with minimal disruption. This effort improved governance, simplified management and unlocked Unity Catalog\u2019s advanced features. Post-migration, we integrated the Genie Room with Slack to enable natural language queries, accelerating decision-making and operational efficiency. Additionally, a lineage-powered internal tool allowed us to quickly identify and resolve issues like backfill needs or data contamination. Unity Catalog has revolutionized our data ecosystem, elevating governance and innovation. Join us to learn how Bagelcode unlocked its data\u2019s full potential and discover strategies for your own transformation. /Data Engineer\nBagelcode /Data Engineer"}
{"session_id": "empowering-business-users-databricks-integrating-aibi-genie-microsoft", "title": "Empowering Business Users With Databricks \u2014 Integrating AI/BI Genie With Microsoft Teams", "track": "ANALYTICS AND BI", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Real-time"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we'll explore how Rooms To Go enhances organizational collaboration by integrating AI/BI Genie with Microsoft Teams. Genie enables warehouse employees and members of the sales team to interact with data using natural language, simplifying data exploration and analysis. By connecting Genie to Microsoft Teams, we bring real-time data insights directly to a user\u2019s phone. We'll provide a comprehensive overview on setting up this integration as well as a demo of how the team uses it daily. Attendees will gain practical knowledge to implement this integration, empowering their teams to access and interact with data seamlessly within Microsoft Teams. /Manager Data Engineering & Integration\nRooms To Go /Solutions Architect"}
{"session_id": "empowering-data-collaboration-using-delta-sharing", "title": "Empowering Data Collaboration Using Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["APACHE SPARK", "DELTA LAKE", "DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "ELT"], "speakers": ["Staff Data Engineer, Procore"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Procore, we're transforming the construction industry through innovative data solutions. This session unveils how we've supercharged our analytics offerings using a unified lakehouse architecture and Delta Sharing, delivering game-changing results for our customers and our business and how data professionals can unlock the full potential of their data assets and drive meaningful business outcomes. Key highlights: /Staff Data Engineer"}
{"session_id": "empowering-fundraising-ai-journey-databricks-mosaic-ai", "title": "Empowering Fundraising With AI: A Journey With Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PUBLIC SECTOR", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Director of Data Science, Doctors Without Borders"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Artificial Intelligence (AI) is more than a corporate tool; it\u2019s a force for good. At Doctors Without Borders/M\u00e9decins Sans Fronti\u00e8res (MSF), we use AI to optimize fundraising, ensuring that every dollar raised directly supports life-saving medical aid worldwide. With Databricks, Mosaic AI and Unity Catalog, we analyze donor behavior, predict giving patterns and personalize outreach, increasing contributions while upholding ethical AI principles. This session will showcase how AI maximizes fundraising impact, enabling faster crisis response and resource allocation. We\u2019ll explore predictive modeling for donor engagement, secure AI governance with Unity Catalog and our vision for generative AI in fundraising, leveraging AI-assisted storytelling to deepen donor connections. AI is not just about efficiency; it\u2019s about saving lives. Join us to see how AI-driven fundraising is transforming humanitarian aid on a global scale. /Director of Data Science"}
{"session_id": "empowering-healthcare-insights-unified-lakehouse-approach-databricks", "title": "Empowering Healthcare Insights: A Unified Lakehouse Approach With Databricks", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATA MARKETPLACE", "DELTA LAKE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Scala"], "speakers": ["Databricks Solution Architect Champion, BJSS"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "NHS England is revolutionizing healthcare research by enabling secure, seamless access to de-identified patient data through the Federated Data Platform (FDP). Despite vast data resources spread across regional and national systems, analysts struggle with fragmented, inconsistent datasets. Enter Databricks: powering a unified, virtual data lake with Unity Catalog at its core \u2014 integrating diverse NHS systems while ensuring compliance and security. By bridging AWS and Azure environments with a private exchange and leveraging the Iceberg connector to interface with Palantir, analysts gain scalable, reliable and governed access to vital healthcare data. This talk explores how this innovative architecture is driving actionable insights, accelerating research and ultimately improving patient outcomes. /Specialist Solutions Architect\nDatabricks /Databricks Solution Architect Champion"}
{"session_id": "empowering-progress-building-personalized-training-goal-ecosystem", "title": "Empowering Progress: Building a Personalized Training Goal Ecosystem with Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Sr Manager, Data Science and AI, Tonal"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Tonal Trainer is the world\u2019s most intelligent home gym, combining cutting-edge hardware and sensors with AI-powered software to deliver personalized fitness experiences. Members share needs with us through interviews and through social media platforms. One item that consistently came up was having difficulty measuring progress on the machine. We created and deployed a robust Training Goal (TG) ecosystem for our users. TG is a four-part solution: Databricks enabled us to deploy each of these components by the feature launch deadline. /Senior Director, Data and AI\nTonal /Sr Manager, Data Science and AI"}
{"session_id": "empowering-warfighter-ai", "title": "Empowering the Warfighter With AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Science", "Delta Lake", "ELT"], "speakers": ["Data Science Director, Navy"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The new Budget Execution Validation process has transformed how the Navy reviews unspent funds. Powered by Databricks Workflows, MLflow, Delta Lake and Apache Spark\u2122, this data-driven model predicts which financial transactions are most likely to have errors, streamlining reviews and increasing accuracy. In FY24, it helped review $40 billion, freeing $1.1 billion for other priorities, including $260 million from active projects. By reducing reviews by 80%, cutting job runtime by over 50% and lowering costs by 60%, it saved 218,000 work hours and $6.7 million in labor costs. With automated workflows and robust data management, this system exemplifies how advanced tools can improve financial decision-making, save resources and ensure efficient use of taxpayer dollars. /Data Science Director\nNavy /Data Science Director"}
{"session_id": "enabling-sleep-science-research-databricks-and-delta-sharing", "title": "Enabling Sleep Science Research With Databricks and Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Security", "ELT"], "speakers": ["Sr Director Architecture & Data Platform, Sleep Number"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Leveraging Databricks as a platform, we facilitate the sharing of anonymized datasets across various Databricks workspaces and accounts, spanning multiple cloud environments such as AWS, Azure, and GCP. This capability, powered by Delta Sharing, extends both within and outside Sleep Number, enabling accelerated insights while ensuring compliance with data security and privacy standards. In this session, we will showcase our architecture and implementation strategy for data sharing, highlighting the use of Databricks\u2019 Unity Catalog and Delta Sharing, along with integration with platforms like Jira, Jenkins, and Terraform to streamline project management and system orchestration. /Director, Data Platform\nSleep Number Labs /Sr Director Architecture & Data Platform"}
{"session_id": "end-end-interoperable-data-platform-how-bosch-leverages-databricks", "title": "End-to-End Interoperable Data Platform: How Bosch Leverages Databricks Supply Chain Consolidation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Project Lead Logistics Innovations, Robert Bosch GmbH"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will showcase Bosch\u2019s journey in consolidating supply chain information using the Databricks platform. It will dive into how Databricks not only acts as the central data lakehouse but also integrates seamlessly with transformative components such as dbt and Large Language Models (LLMs). The talk will highlight best practices, architectural considerations, and the value of an interoperable platform in driving actionable insights and operational excellence across complex supply chain processes. Key Topics and Sections /Development Lead\nRobert Bosch GmbH /Project Lead Logistics Innovations"}
{"session_id": "energy-and-utilities-industry-forum", "title": "Energy and Utilities Industry Forum", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "DELTA SHARING", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Energy and Utilities, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for a compelling forum exploring how energy leaders are harnessing data and AI to build a more sustainable future. As the industry navigates the complex balance between rising global energy demands and ambitious decarbonization goals, innovative companies are discovering that intelligence-driven operations are the key to success. From optimizing renewable energy integration to revolutionizing grid management, learn how energy pioneers are using AI to transform traditional operations while accelerating the path to net zero. This session reveals how Databricks is empowering energy companies to turn their sustainability aspirations into reality, proving that the future of energy is both clean and intelligent. /Director of Energy and Utilities"}
{"session_id": "enhancing-efficiency-security-how-morgan-stanley-adopting-fully-managed", "title": "Enhancing Efficiency With Security: How Morgan Stanley is Adopting a Fully-Managed Lakehouse", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Distinguished Engineer, Morgan Stanley"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Morgan Stanley, a highly regulated financial institution, needs to meet stringent security and regulatory requirements around data storage and processing. Traditionally, this has necessitated maintaining control over data and compute within their own accounts with the associated management overhead. In this session, we will cover how Morgan Stanley has partnered with Databricks on a fully-managed compute and storage solution that allows them to meet their regulatory obligations with significantly reduced effort. This innovative approach enables rapid onboarding of new projects onto the platform, improving operational efficiency while maintaining the highest levels of security and compliance. /Senior Staff Product Manager\nDatabricks /Distinguished Engineer"}
{"session_id": "enterprise-cost-management-data-warehousing-databricks-sql", "title": "Enterprise Cost Management for Data Warehousing with Databricks SQL", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Software Engineer\nDatabricks /Product Manager"}
{"session_id": "enterprise-financial-crime-detection-lakehouse-framework-fatf-basel-iii", "title": "Enterprise Financial Crime Detection: A Lakehouse Framework for FATF, Basel III, and BSA Compliance", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Quality"], "speakers": ["Engineering Lead - Data & ML, Barclays"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We will present a framework for FinCrime detection leveraging Databricks lakehouse architecture specifically how institutions can achieve both data flexibility & ACID transaction guarantees essential for FinCrime monitoring. The framework incorporates advanced ML models for anomaly detection, pattern recognition, and predictive analytics, while maintaining clear data lineage & audit trails required by regulatory bodies. We will also discuss some specific improvements in reduction of false positives, improvement in detection speed, and faster regulatory reporting, delve deep into how the architecture addresses specific FATF recommendations, Basel III risk management requirements, and BSA compliance obligations, particularly in transaction monitoring and SAR. The ability to handle structured and unstructured data while maintaining data quality and governance makes it particularly valuable for large financial institutions dealing with complex, multi-jurisdictional compliance requirements. /Field Engineering\nDatabricks /Engineering Lead - Data & ML Engineering"}
{"session_id": "entity-resolution-best-outcomes-your-data", "title": "Entity Resolution for the Best Outcomes on Your Data", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING", "technologies": ["DATABRICKS APPS", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["DSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many ways to implement entity resolution (ER) system \u2014 both using vendor software and open-source libraries that enable DIY Entity Resolution. However, generally we see common challenges with any approach \u2014 scalability, bound to a single model architecture, lack of metrics and explainability, and stagnant implementations that do not \"learn\" with experience. Recent experiments with transformer-based approaches, fast lookups with vector search and Databricks components such as Databricks Apps and Agent Eval provide the foundations for a composable ER system that can get better with time on your data. In this presentation, we include a demo of how to use these components to build a composable ER that has the best outcomes for your data. /Staff Data Scientist\nDatabricks /DSA"}
{"session_id": "evaluating-domain-specific-agent-performance-and-metrics", "title": "DISCOVER DATA INTELLIGENCE", "track": "", "level": "", "type": "", "industry": "", "technologies": [], "duration": "", "experience": "", "areas_of_interest": ["AI", "Apache Spark", "Delta Lake", "ELT"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data + AI Summit speakers include leading experts, researchers and open source contributors \u2014 from Databricks and across the data and AI community Ali\nDimon Chairman and CEO, JPMorgan Chase Kasey\nAmodei Co-founder and CEO, Anthropic Matei\nBrown Product Manager, FedEx Miranda\nXin Co-founder and Chief Architect, Databricks Arvindram\nSutara Field CTO, Databricks 700+ sessions \u2014 from data intelligence to data warehousing, governance to AI. Here are some key highlights. Databricks Walmart Rivian Automotive, LLC Rivian T-Mobile T-Mobile Databricks Mastercard Databricks PepsiCo Join thousands of data leaders, engineers, scientists and architects to explore the convergence of data and AI Explore the latest advances in Delta Lake, Apache Iceberg\u2122, agentic systems, MLflow, Apache Spark\u2122, Unity Catalog, DLT, DSPy, LangChain, PyTorch, dbt, Trino, as well as the newest innovations from our sponsors, partners and the Databricks Data Intelligence Platform. Interact with thousands of your peers from the data and AI community, and grow your professional network through social meetups, the Expo and exclusive event parties. Sharpen your expertise with instructor-led, half-day training. Boost your credentials with our hands-on, lab-based format and get onsite certifications. Reduced prices are available for group purchases and government, military, education and nonprofit attendees GENERAL ADMISSION $1,895 STANDARD PRICE MAY 1 -> JUNE 12 Don\u2019t wait \u2014 secure your spot at Data + AI Summit 2025! Who should attend Data + AI Summit? What is Data + AI Summit? Where in San Francisco will the event take place? Where should I stay for the event? What is included in the full conference pass? Data + AI Summit couldn\u2019t happen without our awesome sponsors. Interested in sponsoring? Reach out to our Sponsorship Management team to learn about available opportunities."}
{"session_id": "evaluation-driven-development-workflows-best-practices-and-real-world", "title": "Evaluation-Driven Development Workflows: Best Practices and Real-World Scenarios", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In enterprise AI, Evaluation-Driven Development (EDD) ensures reliable, efficient systems by embedding continuous assessment and improvement into the AI development lifecycle. High-quality evaluation datasets are created using techniques like document analysis, synthetic data generation via Mosaic AI\u2019s synthetic data generation API, SME validation, and relevance filtering, reducing manual effort and accelerating workflows. EDD focuses on metrics such as context relevance, groundedness, and response accuracy to identify and address issues like retrieval errors or model limitations. Custom LLM judges, tailored to domain-specific needs like PII detection or tone assessment, enhance evaluations. By leveraging tools like Mosaic AI Agent Framework and Agent Evaluation, MLflow, EDD automates data tracking, streamlines workflows, and quantifies improvements, transforming AI development for delivering scalable, high-performing systems that drive measurable organizational value. /Senior Specialist Solutions Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "evolving-agent-complexity-building-multi-agent-systems-mosaic-ai", "title": "Evolving Agent Complexity: Building Multi-Agent Systems With Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Machine Learning Engineer, Greenlight Financial Technology"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session dives into building multi-agent systems on the Mosaic AI Platform, exploring the techniques, architectures and lessons learned from experiences building Greenlight\u2019s real-world agent applications. This presentation is well suited for executives, product managers and engineers alike, breaking down AI Agents into easy-to-understand concepts, while presenting an architecture for building complex systems. We\u2019ll examine the core components of generative AI Agents and different ways to assemble them into agents, including different prompting and reasoning techniques. We\u2019ll cover how the Mosaic AI Platform has enabled our small team to build, deploy and monitor our AI Agents, touching on vector search, feature and model serving endpoints, and the evaluation framework. Finally, we\u2019ll discuss the pros and cons of building a multi-agent system consisting of specialized agents vs. a single large agent for Greenlight\u2019s AI Assistant, and the challenges we encountered. /Staff Machine Learning Engineer\nGreenlight Financial Technology /Machine Learning Engineer"}
{"session_id": "evolving-data-insights-privacy-mastercard", "title": "Evolving Data Insights With Privacy at Mastercard", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP Data Strategy, Mastercard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Mastercard is a global technology company whose role is anchored in trust. It supports 3.4 billion cards and over 143 billion transactions annually. To address customers\u2019 increasing data volume and complex privacy needs, Mastercard has developed a novel service atop Databricks\u2019 Clean Rooms and broader Data Intelligence Platform. This service combines several Databricks components with Mastercard\u2019s IP, providing an evolved method for data-driven insights and value-added services while ensuring a unique standalone turnkey service. The result is a secure environment where multiple parties can collaborate on sensitive data without directly accessing each other\u2019s information. After this session, attendees will understand how Mastercard used its expertise in privacy-enhancing technologies to create collaboration tools powered by Databricks\u2019 Clean Rooms, AI/BI, Apps, Unity Catalog, Workflows and DatabricksIQ \u2014 as well as how to take advantage of this new privacy-enhancing service directly. /Lead Solutions Architect\nDatabricks /VP Data Strategy"}
{"session_id": "extending-lakehouse-power-interoperable-compute-unity-catalog-open-apis", "title": "Extending the Lakehouse: Power Interoperable Compute With Unity Catalog Open APIs", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The lakehouse is built for storage flexibility, but what about compute? In this session, we\u2019ll explore how Unity Catalog enables you to connect and govern multiple compute engines across your data ecosystem. With open APIs and support for the Iceberg REST Catalog, UC lets you extend access to engines like Trino, DuckDB, and Flink while maintaining centralized security, lineage, and interoperability. Learn how to bring flexibility to your compute layer\u2014without compromising control. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "factset-collaboration-leading-customer-harnessing-delta-sharing", "title": "FactSet in Collaboration with a Leading Customer: Harnessing Delta Sharing for Accelerated Insights", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "ELT", "ETL"], "speakers": ["VP, Principal Product Manager, Factset"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Integrating external vendor data often means facing fragmented pipelines, costly ETL, and lengthy onboarding cycles. Join us to learn how a customer partnered with FactSet, using Delta Sharing to break through these barriers and significantly elevate their analytics capabilities. Why Attend? We'll explore how your organization can overcome similar challenges and set the stage for future innovation. Discover practical ways to enhance your data capabilities, advance strategic initiatives, and achieve faster insights with seamless integration of FactSet data on Databricks. /VP, Principal Product Manager"}
{"session_id": "federated-data-pipelines", "title": "Federated Data Pipelines", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline"], "speakers": ["Senior Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Are you struggling to keep up with rapid business changes that demand constant updates to your data pipelines? Is your data engineering team growing rapidly just to manage this complexity? Databricks was not immune to this challenge either. Managing our BI with contributions from hundreds of Product Engineering Teams across the company while maintaining central oversight and quality posed significant hurdles. Join us to learn how we developed a config-driven data pipeline framework using Metric Store and UC Metrics that helped us reduce engineering effort \u2014 achieving the work of 100 classical data engineers with just two platform engineers. /Senior Software Engineer\nDatabricks /Senior Software Engineer"}
{"session_id": "financial-services-industry-forum-shifting-financial-intelligence", "title": "Financial Services Industry Forum: Shifting to Financial Intelligence", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "MOSAIC AI"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": ["SVP, Field Engineering & Co-Founder, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join the 60-minute kickoff session at the Financial Services Forum to explore how data and AI transform finance. Featuring keynotes from top innovators in banking, capital markets, and insurance and exciting announcements from Databricks, this event offers invaluable insights. What to expect: Connect with C-suite executives and industry pioneers shaping financial services. Leave with actionable strategies to drive growth, ensure compliance and transform your organization through intelligence-driven decisions! /Financial Services, Cyber, Public Sector\nDatabricks /Partner, Chief Data Officer, Head of Data Engineering\nGoldman Sachs /SVP, Field Engineering & Co-Founder"}
{"session_id": "fine-grained-access-control-unstructured-data-volume-path-policies", "title": "Fine-Grained Access Control for Unstructured Data With Volume Path Policies", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unstructured data \u2014 images, documents, videos and more \u2014 is growing in importance with AI and ML. Yet managing access control at scale is challenging. Unity Catalog Volumes offer a secure foundation, but access control has remained volume-level \u2014 until now. This session introduces Volume Path Policies, a new feature enabling fine-grained access within volumes. Built on Unity Catalog\u2019s ABAC (Attribute-based Access Control), they let you define rules for users and groups based on path prefixes. We\u2019ll cover the governance model, share examples and demonstrate how to enforce least-privilege access. By the end, you\u2019ll know how to manage file-level access with Unity Catalog\u2019s flexibility and control. /Sr. Staff Product Manager\nDatabricks /Sr. Product Manager"}
{"session_id": "finops-scale-best-practices-cost-efficient-growth-databricks", "title": "FinOps at Scale: Best Practices for Cost-Efficient Growth on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "You\u2019ve seen your usage grow on Databricks, across departments, use cases, product lines and users. What can you do to ensure your end-users (data practitioners) of the platform remain cost-efficient and productive, while staying accountable to your budget? We\u2019ll discuss spend monitoring, chargeback models and developing a culture of cost efficiency by using Databricks tools. /Lead Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "franchise-ip-and-data-governance-krafton-driving-cost-efficiency-and", "title": "Franchise IP and Data Governance at Krafton: Driving Cost Efficiency and Scalability", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Pipeline", "Real-time", "Scala"], "speakers": ["KRAFTON"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we explore how KRAFTON optimized data governance for PUBG IP, enhancing cost efficiency and scalability. With over 300 million downloads in India and 700,000 MAU on PUBG IP, KRAFTON operates a massive data ecosystem, processing tens of terabytes daily. As real-time analytics demands increased, traditional Batch-based processing faced scalability challenges. To address this, we redesigned data pipelines and governance models, improving performance while reducing costs. Learn more: https://www.databricks.com/customers/krafton /KRAFTON"}
{"session_id": "fueling-efficiency-how-pilot-uses-vector-stores-data-quality-and-genai", "title": "Fueling Efficiency: How Pilot Uses Vector Stores, Data Quality, and GenAI to Deliver Business Value", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, TRAVEL AND HOSPITALITY", "technologies": ["AI/BI", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning"], "speakers": ["Sr Manager, Machine Learning Engineering, Pilot Travel Centers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Sr Manager, Machine Learning Engineering"}
{"session_id": "full-stack-innovation-building-data-and-ai-products-databricks-apps", "title": "The Full Stack of Innovation: Building Data and AI Products With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "MOSAIC AI"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Regional Architect, Africa & Middle East, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this deep-dive technical session, Ivan Trusov (Sr. SSA @ Databricks) and Giran Moodley (SA @ Databricks) \u2014 will explore the full-stack development of Databricks Apps, covering everything from frameworks to deployment. We\u2019ll walk through essential topics, including: Expect a highly practical session with several live demos, showcasing the development loop, testing workflows and CI/CD automation. Whether you\u2019re building internal tools or AI-powered products, this talk will equip you with the knowledge to ship robust, scalable Databricks Apps. /Senior Specialist Solutions Architect\nDatabricks /Regional Architect, Africa & Middle East"}
{"session_id": "future-anti-cheat", "title": "The Future of Anti-Cheat", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "APACHE SPARK", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science", "Machine Learning", "Scala"], "speakers": ["Rebel Data Science"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As online gaming evolves, so do cheating methods that exploit client-server vulnerabilities. Traditional anti-cheat, such as kernel-level drivers and runtime detections, has long been the primary defense. However, recent high-profile failures expose the risks of operating in kernel space. More critically, advanced cheats like Direct Memory Access (DMA) exploits and AI-powered Computer Vision (CV) hacks increasingly render client-side detection ineffective. This presentation examines the escalating arms race between cheat creators and developers, highlighting client-side limitations. With CV cheats mimicking human behavior, anti-cheat must shift toward server-side, data-driven detection. By leveraging AI, machine learning, and behavioral analytics to analyze player patterns, input anomalies, and decision inconsistencies, future solutions can move beyond static detection to adaptive security models, ensuring fair play at scale. /Rebel Data Science"}
{"session_id": "future-dsv2-apache-sparktm", "title": "The Future of DSv2 in Apache Spark\u2122", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DSv2, Spark's next-generation Catalog API, is gaining traction among data source developers. It shifts complexity to Apache Spark\u2122, improves connector reliability and unlocks new functionality such as catalog federation, MERGE operations, storage-partitioned joins, aggregate pushdown, stored procedures and more. This session covers the design of DSv2, current strengths and gaps and its evolving roadmap. It's intended for Spark users and developers working with data sources, whether custom-built or off-the-shelf. /Databricks"}
{"session_id": "future-open-table-formats", "title": "The Future of Open Table Formats", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Member of Technical Staff, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/PM Director, Developer Relations\nDatabricks /Member of Technical Staff"}
{"session_id": "gaining-insight-image-data-databricks-using-multi-modal-foundation", "title": "Gaining Insight From Image Data in Databricks Using Multi-Modal Foundation Model API", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the hidden potential in your image data without specialized computer vision expertise! This session explores how to leverage Databricks' multi-modal Foundation Model APIs to analyze, classify and extract insights from visual content. Learn how Databricks provides a unified API to understand images using powerful foundation models within your data workflows. Key takeaways: Whether analyzing product images, processing visual documents or building content moderation systems, you'll discover how to extract valuable insights from your image data within the Databricks ecosystem. /Sr. Software Engineer"}
{"session_id": "games-industry-forum-games-executive-perspective-impact-data-and-ai", "title": "Games Industry Forum: The Games Executive Perspective on the Impact of Data and AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "MOSAIC AI"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Global Games GTM Leader, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Come hear from some of the biggest names in games about how Data and AI is helping them shape their future, build better games and create player-centric experiences. In this session you\u2019ll hear, first, what Databricks is hearing from Games studios globally as their key priorities. We then shift to customers sharing their stories and perspectives. You\u2019ll leave invigorated on the impact Data and AI can have on games, and our global players and have new ideas on ways you can further your impact. /Global Games GTM Leader"}
{"session_id": "games24x7s-revolutionizing-online-skill-gaming-databricks", "title": "Games24x7's Revolutionizing Online Skill Gaming With Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Engineering Manager, Games24x7"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Games24x7 deals with terabytes of player data generated by 5.5B+ games played on RummyCircle and 500M+ teams created on My11Circle every year, to deliver immersive gaming experiences to 120M+ players. In this presentation we will talk about: /Games24x7 /Engineering Manager"}
{"session_id": "gen-ai-application-development", "title": "Gen AI Application Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides participants with information and practical experience in building advanced LLM (Large Language Model) applications using multi-stage reasoning LLM chains and agents. In the initial section, participants will learn how to decompose a problem into its components and select the most suitable model for each step to enhance business use cases. Following this, participants will construct a multi-stage reasoning chain utilizing LangChain and HuggingFace transformers. Finally, participants will be introduced to agents and will design an autonomous agent using generative models on Databricks. Pre-requisites: Solid understanding of natural language processing (NLP) concepts, familiarity with prompt engineering and prompt engineering best practices, experience with the Databricks Data Intelligence Platform, experience with retrieval-augmented generation (RAG) techniques including data preparation, building RAG architectures, and concepts like embeddings, vectors, and vector databases Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-deployment-and-monitoring", "title": "Gen AI Deployment and Monitoring", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course introduces learners to deploying, operationalizing, and monitoring generative artificial intelligence (AI) applications. First, learners will develop knowledge and skills in deploying generative AI applications using tools like Model Serving. Next, the course will discuss operationalizing generative AI applications following modern LLMOps best practices and recommended architectures. Finally, learners will be introduced to the idea of monitoring generative AI applications and their components using Lakehouse Monitoring. Pre-requisites: Familiarity with prompt engineering and retrieval-augmented generation (RAG) techniques, including data preparation, embeddings, vectors, and vector databases. A foundational knowledge of Databricks Data Intelligence Platform tools for evaluation and governance (particularly Unity Catalog). Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-evaluation-and-governance", "title": "Gen AI Evaluation and Governance", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["security systems. Next, the course will connect evaluation and governance systems to Databricks Data Intelligence Platform. Third, learners be introduced a variety of techniques for specific components types applications. Finally, conclude with an analysis evaluating entire AI respect performance cost. Pre-requisites: Familiarity prompt engineering, experience Additionally, knowledge retrieval-augmented generation (RAG) including data preparation, embeddings, vectors, vector databases Labs: Yes Certification Path: Certified Generative Engineer Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course introduces learners to evaluating and governing GenAI (generative artificial intelligence) systems. First, learners will explore the meaning behind and motivation for building evaluation and governance/security systems. Next, the course will connect evaluation and governance systems to the Databricks Data Intelligence Platform. Third, learners will be introduced to a variety of evaluation techniques for specific components and types of applications. Finally, the course will conclude with an analysis of evaluating entire AI systems with respect to performance and cost. Pre-requisites: Familiarity with prompt engineering, and experience with the Databricks Data Intelligence Platform. Additionally, knowledge of retrieval-augmented generation (RAG) techniques including data preparation, embeddings, vectors, and vector databases Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-solution-development", "title": "Gen AI Solution Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed to introduce participants to contextual GenAI (generative artificial intelligence) solutions using the retrieval-augmented generation (RAG) method. Firstly, participants will be introduced to the RAG architecture and the significance of contextual information using Mosaic AI Playground. Next, the course will demonstrate how to prepare data for GenAI solutions and connect this process with building an RAG architecture. Finally, participants will explore concepts related to context embedding, vectors, vector databases, and the utilization of the Mosaic AI Vector Search product. Pre-requisites: Familiarity with embeddings, prompt engineering best practices, and experience with the Databricks Data Intelligence Platform Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "genai-finance-and-audit-getting-production", "title": "GenAI for Finance and Audit\u2014Getting to Production", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Head of Audit Analytics & AI, KPMG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "During this session, we\u2019ll demonstrate how KPMG\u2019s audit practice has used the Databricks Data Intelligence Platform to transform the assessment of a company\u2019s annual report and accounts. You\u2019ll learn how we\u2019ve built a robust GenAI solution that is live in production\u2014including a multi-model optimized RAG pipeline, automated evaluations and end-to-end orchestration and monitoring within Databricks. We\u2019re now able to perform thousands of legal and regulatory compliance checks on high complexity documents in minutes\u2014a task that used to take auditors days and weeks. We\u2019ll wrap up with a view on the broader impact AI is having on the audit, accounting and finance sector\u2014and how the solution we\u2019ve built is providing the foundations to accelerate future use cases. /Head of Audit Analytics & AI"}
{"session_id": "genai-observability-customer-care", "title": "GenAI Observability in Customer Care", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Real-time"], "speakers": ["Senior Machine Learning Engineer, EarnIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Customer support is going through the GenAI revolution, but how can we use AI to foster deeper empathy with our end users? To enable this, Earnin has built its GenAI observability platform on Databricks, leveraging DLTs, Kafka and Databricks AI/BI. This session covers how we use DLT to monitor our customer care chatbot in near real-time and how we leverage Databricks to better anticipate our customers' needs. /Senior Staff Software Engineer\nEarnin /Senior Machine Learning Engineer"}
{"session_id": "genai-powered-shopping-assistant-prada-e-commerce-search-bar", "title": "GenAI-Powered Shopping Assistant for Prada e-Commerce Search Bar", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Data Scientist, Data Reply IT"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Prada has developed a complex solution, leveraging MosaicAI to propose an interactive and natural language product discovery capability that could improve its e-commerce search bar. The backbone is a 70B model and a Vector Store, which collaborates with additional filterings and AI solutions to suggest not only the perfect outfit for each occasion, but also provide alternative solutions and similar items. /Lead Data Scientist\nPrada Group /Data Scientist"}
{"session_id": "generating-laughter-testing-and-evaluating-success-llms-comedy", "title": "Generating Laughter: Testing and Evaluating the Success of LLMs for Comedy", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Sr. Developer Experience Engineer, Galileo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Nondeterministic AI models, like large language models (LLMs), offer immense creative potential but require new approaches to testing and scalability. Drawing from her experience running New York Times-featured Generative AI comedy shows, Erin uncovers how traditional benchmarks may fall short and how embracing unpredictability can lead to innovative, laugh-inducing results. This talk will explore methods like multi-tiered feedback loops, chaos testing and exploratory user testing, where AI outputs are evaluated not by rigid accuracy standards but by their adaptability and resonance across different contexts \u2014 from comedy generation to functional applications. Erin will emphasize the importance of establishing a root source of truth \u2014 a reliable dataset or core principle \u2014 to manage consistency while embracing creativity. Whether you\u2019re looking to generate a few laughs of your own or explore creative uses of Generative AI, this talk will inspire and delight enthusiasts of all levels. /Sr. Developer Experience Engineer"}
{"session_id": "generating-zero-shot-hard-case-hallucinations-synthetic-and-open-data", "title": "Generating Zero-Shot Hard-Case Hallucinations: A Synthetic and Open Data Approach", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LLAMA", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal Research Scientist, Nvidia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present a novel framework for designing and inducing controlled hallucinations in long-form content generation by LLMs across diverse domains. The purpose is to create fully-synthetic benchmarks and mine hard cases for iterative refinement of zero-shot hallucination detectors. We will first demonstrate how Gretel Navigator can be used to design realistic, high-quality long-context datasets across various domains. Second, we will describe our reasoning-based approach to hard-case mining. Specifically, our methodology relies on chain-of-thought-based generation of both faithful and deceptive question-answer pairs based upon long-context samples. Subsequently, a consensus labeling and detector framework is employed to filter synthetic examples to zero-shot hard cases. The result of this process is a fully-automated system, operating under open data licenses such as Apache-2.0, for the generation of hallucinations at the edge-of-capabilities for a target LLM to detect. /Principal Research Scientist"}
{"session_id": "generative-ai-merchant-matching", "title": "Generative AI Merchant Matching", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LLAMA", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, Mastercard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Our project demonstrates building enterprise AI systems cost-effectively, focusing on matching merchant descriptors to known businesses. Using fine-tuned LLMs and advanced search, we created a solution rivaling alternatives at minimal cost. The system works in three steps: A fine-tuned Llama 3 8B model parses merchant descriptors into standardized components. A hybrid search system uses these components to find candidate matches in our database. A Llama 3 70B model then evaluates top candidates, with an AI judge reviewing results for hallucination. We achieved a 400% latency improvement while maintaining accuracy and keeping costs low and each fine-tuning round cost hundreds of dollars. Through careful optimization and simple architecture for a balance between cost, speed and accuracy, we show that small teams with modest budgets can tackle complex problems effectively using this technology. We share key insights on prompt engineering, fine-tuning and cost and latency management. /Senior Data Scientist"}
{"session_id": "genie-engineering-optimizing-hvac-design-and-operational-insights-data", "title": "Genie for Engineering: Optimizing HVAC Design and Operational Insights With Data and AI", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Data Science", "Delta Lake", "ELT", "Real-time"], "speakers": ["Principal AI Engineer, Lennox International"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will explore how Genie, an AI-driven platform transformed HVAC operational insights by leveraging Databricks offerings like Apache Spark, Delta Lake and the Databricks Data Intelligence Platform. Key contributions: By analyzing real-time data from HVAC installations, Genie identified discrepancies between design specs and field performance, allowing engineers to optimize algorithms, reduce inefficiencies and improve customer satisfaction. Discover how Genie revolutionized HVAC management and apply to your projects. /Manager, Data Science & AI\nLennox /Principal AI Engineer"}
{"session_id": "geo-powering-insights-art-spatial-data-integration-and-visualization", "title": "Geo-Powering Insights: The Art of Spatial Data Integration and Visualization", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we will explore how to leverage Databricks' SQL engine to efficiently ingest and transform geospatial data. We'll demonstrate the seamless process of connecting to external systems such as ArcGIS to retrieve datasets, showcasing the platform's versatility in handling diverse data sources. We'll then delve into the power of Databricks Apps, illustrating how you can create custom geospatial dashboards using various frameworks like Streamlit and Flask, or any framework of your choice. This flexibility allows you to tailor your visualizations to your specific needs and preferences. Furthermore, we'll highlight the Databricks Lakehouse's integration capabilities with popular dashboarding tools such as Tableau and Power BI. This integration enables you to combine the robust data processing power of Databricks with the advanced visualization features of these specialized tools. /Specialist Solutions Architect"}
{"session_id": "geospatial-insights-databricks-sql-techniques-and-applications", "title": "Geospatial Insights With Databricks SQL: Techniques and Applications", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Lead Geospatial Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager\nDatabricks /Lead Geospatial Product Specialist"}
{"session_id": "getting-data-ai-ready-testimonial-good-governance-practices", "title": "Getting Data AI Ready: Testimonial of Good Governance Practices Constructing Accurate Genie Spaces", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Business Intelligence"], "speakers": ["Sr Business Intelligence Mgr, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Genie Rooms have played an integral role in democratizing important datasets like Cell Tower and Lease Information. However, in order to ensure that this exciting new release from Databricks was configured as optimally as possible from development to deployment, we needed additional scaffolding around governance. In this talk we will describe the four main components we used in conjunction with the Genie Room to build a successful product and will provide generalizable lessons to help others get the most out of this object. At the core are a declarative, metadata approach to creating UC tables deployed on a robust framework. Second, a platform that efficiently crowdsourced targeted feedback from different user groups. Third, a tool that balances the LLM\u2019s creativity with human wisdom. And finally, a platform that enforces our principle of separating Storage from Compute to manage access to the room at a fine-grained level and enables a whole host of interesting use-cases. /Principal Architect, ML Solutions\nT-Mobile /Sr Business Intelligence Mgr"}
{"session_id": "getting-most-out-dlt-deep-dive-whats-new-and-best-practices", "title": "Getting the Most Out of DLT: A Deep Dive on What\u2019s New and Best Practices", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline", "Scala"], "speakers": ["Distinguished Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This deep dive covers advanced usage patterns, tips and best practices for maximizing the potential of DLT. Attendees will explore new features, enhanced workflows and cost-optimization strategies through a demo-heavy presentation. The session will also address complex use cases, showcasing how DLT simplifies the management of robust data pipelines while maintaining scalability and efficiency across diverse data engineering challenges. /Distinguished Engineer"}
{"session_id": "getting-started-lakeflow-connect", "title": "Getting Started With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Hundreds of customers are already ingesting data with Lakeflow Connect from SQL Server, Salesforce, ServiceNow, Google Analytics, SharePoint, PostgreSQL and more to unlock the full power of their data. Lakeflow Connect introduces built-in, no-code ingestion connectors from SaaS applications, databases and file sources to help unlock data intelligence. In this demo-packed session, you\u2019ll learn how to ingest ready-to-use data for analytics and AI with a few clicks in the UI or a few lines of code. We\u2019ll also demonstrate how Lakeflow Connect is fully integrated with the Databricks Data Intelligence Platform for built-in governance, observability, CI/CD, automated pipeline maintenance and more. Finally, we\u2019ll explain how to use Lakeflow Connect in combination with downstream analytics and AI tools to tackle common business challenges and drive business impact. /Sr. Product Marketing Manager\nDatabricks /Senior Product Manager"}
{"session_id": "got-metrics-build-metric-store-tour-developing-metrics-through-uc", "title": "Got Metrics? Build a Metric Store \u2014 A Tour of Developing Metrics Through UC Metric Views", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Business Intelligence"], "speakers": ["Sr. Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "I have metrics, you have metrics \u2014 we all have metrics. But the real problem isn\u2019t having metrics, it\u2019s that the numbers never line up, leading to endless cycles of reconciliation and confusion. Join us as we share how our Data Team at Databricks tackled this fundamental challenge in Business Intelligence by building an internal Metric Store \u2014 creating a single source of truth for all business metrics using the newly-launched UC Metric Views. Imagine a world where numbers always align, metric definitions are consistently applied across the organization and every metric comes with built-in ML-based forecasting, AI-powered anomaly detection and automatic explainability. That\u2019s the future we\u2019ve built \u2014 and we\u2019ll show you how you can get started today. /Staff Software Engineer\nDatabricks /Sr. Software Engineer"}
{"session_id": "gpu-accelerated-spark-connect", "title": "GPU Accelerated Spark Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "ETL", "SQL"], "speakers": ["Sr. Manager, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Spark Connect, first included for SQL/DataFrame API in Apache Spark 3.4 and recently extended to MLlib in 4.0, introduced a new way to run Spark applications over a gRPC protocol. This has many benefits, including easier adoption for non-JVM clients, version independence from applications and increased stability and security of the associated Spark clusters. The recent Spark Connect extension for ML also included a plugin interface to configure enhanced server-side implementations of the MLlib algorithms when launching the server. In this talk, we shall demonstrate how this new interface, together with Spark SQL\u2019s existing plugin interface, can be used with NVIDIA GPU-accelerated plugins for ML and SQL to enable no-code change, end-to-end GPU acceleration of Spark ETL and ML applications over Spark Connect, with optimal performance up to 9x at 80% cost reduction compared to CPU baselines. /Principal Distributed Systems Engineer\nNVIDIA /Sr. Manager"}
{"session_id": "graph-powered-observability-data-analysis-databricks-credential-vending", "title": "Graph-Powered Observability Data Analysis in Databricks With Credential Vending", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Software Engineer, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Observability data \u2014 logs, metrics, and traces \u2014 captures the complex interactions within modern distributed systems. A graph query engine on top of Databricks enables complex traversal of massive observability data, helping users trace service dependencies, analyze upstream/downstream impacts, and uncover recurring error patterns, making it easier to diagnose issues and optimize system performance. A critical challenge in handling observability data is managing dynamic RBAC for the sensitive system telemetry. This session explains how Coinbase leverages credential vending, a method for issuing short-lived credentials to enable fine-grained, secure access to observability data stored in Databricks without long-lived secrets. Key takeaways: /Staff Software Engineer"}
{"session_id": "harnessing-databricks-advanced-llm-time-series-models-healthcare", "title": "Harnessing Databricks for Advanced LLM Time-Series Models in Healthcare Forecasting", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["AI Scientist Dir, IQVIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/AI Scientist Dir"}
{"session_id": "harnessing-databricks-asset-bundles-transforming-pipeline-management", "title": "Harnessing Databricks Asset Bundles: Transforming Pipeline Management at Scale at Stack Overflow", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Security", "Scala"], "speakers": ["Staff Data Engineer, Stack Overflow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Stack Overflow optimized its data engineering workflows using Databricks Asset Bundles (DABs) for scalable and efficient pipeline deployments. This session explores the structured pipeline architecture, emphasizing code reusability, modular design and bundle variables to ensure clarity and data isolation across projects. Learn how the data team leverages enterprise infrastructure to streamline deployment across multiple environments. Key topics include DRY-principled modular design, essential DAB features for automation and data security strategies using Unity Catalog. Designed for data engineers and teams managing multi-project workflows, this talk offers actionable insights on optimizing pipelines with Databricks evolving toolset. /Staff Data Engineer"}
{"session_id": "harnessing-real-time-data-and-ai-retail-innovation", "title": "Harnessing Real-Time Data and AI for Retail Innovation", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "Machine Learning", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk explores using advanced data processing and generative AI techniques to revolutionize the retail industry. Using Databricks, we will discuss how cutting-edge technologies enable real-time data analysis and machine learning applications, creating a powerful ecosystem for large-scale, data-driven retail solutions. Attendees will gain insights into architecting scalable data pipelines for retail operations and implementing advanced analytics on streaming customer data. Discover how these integrated technologies drive innovation in retail, enhancing customer experiences, streamlining operations and enabling data-driven decision-making. Learn how retailers can leverage these tools to gain a competitive edge in the rapidly evolving digital marketplace, ultimately driving growth and adaptability in the face of changing consumer behaviors and market dynamics. /Lead Solutions Architect\nDatabricks /Senior Specialist Solutions Architect"}
{"session_id": "health-data-delivered-how-dlt-powers-healthverity-marketplace", "title": "Health Data, Delivered: How DLT Powers the HealthVerity Marketplace", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "ETL", "Scala"], "speakers": ["Principal Data Engineer, HealthVerity"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building scalable, reliable ETL pipelines is a challenge for organizations managing large, diverse data sources. Theseus, our custom ETL framework, streamlines data ingestion and transformation by fully leveraging Databricks-native capabilities, including DLT, auto loader and event-driven orchestration. By decoupling supplier logic and implementing structured bronze, silver, and gold layers, Theseus ensures high-performance, fault-tolerant data processing with minimal operational overhead. The result? Faster time-to-value, simplified governance and improved data quality \u2014 all within a declarative framework that reduces engineering effort. In this session, we\u2019ll explore how Theseus automates complex data workflows, optimizes cost efficiency and enhances scalability, showcasing how Databricks-native tools drive real business outcomes. /Principal Data Engineer"}
{"session_id": "healthcare-and-life-sciences-industry-forum", "title": "Healthcare and Life Sciences Industry Forum", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Vice President, Data & Analytics, Cencora"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an engaging 60-minute Healthcare and Life Sciences Industry Forum at the year\u2019s premier Databricks event! You\u2019ll hear directly from Databricks experts and industry leaders about how unifying data, governing AI models and empowering teams with data intelligence can drive meaningful change across the healthcare and life sciences continuum. Discover how data and AI are transforming the industry \u2014 helping streamline healthcare operations, personalize patient care and accelerate breakthroughs in research and development. Don\u2019t miss this opportunity to learn about the future of data-driven healthcare. /AVP, Healthcare & Life Sciences GTM\nDatabricks /Vice President, Data & Analytics"}
{"session_id": "healthcare-interoperability-end-end-streaming-fhir-pipelines-databricks", "title": "Healthcare Interoperability: End-to-End Streaming FHIR Pipelines With Databricks & Redox", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "ETL", "SQL", "Streaming"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Redox & Databricks direct integration can streamline your interoperability workflows from responding in record time to preauthorization requests to letting attending physicians know about a change in risk for sepsis and readmission in near real time from ADTs. Data engineers will learn how to create fully-streaming ETL pipelines for ingesting, parsing and acting on insights from Redox FHIR bundles delivered directly to Unity Catalog volumes. Once available in the Lakehouse, AI/BI Dashboards and Agentic Frameworks help write FHIR messages back to Redox for direct push down to EMR systems. Parsing FHIR bundle resources has never been easier with SQL combined with the new VARIANT data type in Delta and streaming table creation against Serverless DBSQL Warehouses. We'll also use Databricks accelerators dbignite and redoxwrite for writing and posting FHIR bundles back to Redox integrated EMRs and we'll extend AI/BI with Unity Catalog SQL UDFs and the Redox API for use in Genie. /Field CTO\nRedox, Inc. /Solutions Architect"}
{"session_id": "healthcare-providers-leverage-data-ai-streamline-operations-and-impact", "title": "Healthcare Providers Leverage Data + AI to Streamline Operations and Impact Experience", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Industry Marketing Lead, HLS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Healthcare is undergoing rapid transformation as leading providers and innovators harness the power of data and AI to deliver a comprehensive Patient 360, elevate staff and clinician experience, streamline operations and advance precision health. This session convenes thought leaders at the forefront of healthcare innovation, showcasing how AI-driven insights and robust data strategies are enabling holistic patient views, automating administrative workflows and supporting personalized care pathways. Attendees will explore real-world examples of AI optimizing clinical decisions, improving operational efficiency, and enhancing both patient and staff satisfaction, while addressing challenges in data interoperability, privacy and responsible technology adoption. /Industry Marketing Lead, HLS"}
{"session_id": "high-throughput-ml-mastering-efficient-model-serving-enterprise-scale", "title": "High-Throughput ML: Mastering Efficient Model Serving at Enterprise Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ever wondered how industry leaders handle thousands of ML predictions per second? This session reveals the architecture behind high-performance model serving systems on Databricks. We'll explore how to build inference pipelines that efficiently scale to handle massive request volumes while maintaining low latency. You'll learn how to leverage Feature Store for consistent, low-latency feature lookups and implement auto-scaling strategies that optimize both performance and cost. Key takeaways: Whether you're serving recommender systems or real-time fraud detection models, you'll gain practical strategies for building enterprise-grade ML serving systems."}
{"session_id": "highways-and-hexagons-processing-large-geospatial-datasets-h3", "title": "Highways and Hexagons: Processing Large Geospatial Datasets With H3", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Scala"], "speakers": ["Austroads Ltd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The problem of matching GPS locations to roads and local government areas (LGAs) involves handling large datasets and a number of geospatial operations. In this deep dive, we will outline the challenges of developing scalable solutions for these tasks. We will discuss our multi-step approach, first focusing on the use of H3 indexing to isolate matches with single candidates, then explaining use of different geospatial computational techniques to accurately match points with multiple candidates. From technical perspective, the talk will showcase the use of broadcasting and partitioning techniques, their effect on autoscaling, memory usage and effective data parallelization. This session is for anyone interested in geospatial data, spark performance optimization and the real-world challenges of large-scale data engineering. This session will be co-presented by Prad Dias (Austroads Senior Implementation Manager) and Petr Andreev (Mantel Group Senior Data Engineer) /Senior Data Engineer\nMantel Group /Austroads Ltd"}
{"session_id": "hipaa-without-headache-hinge-health-simple-phi-governance-fine-grain", "title": "HIPAA Without the Headache at Hinge Health: Simple PHI Governance With Fine Grain Access Control", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Manager, Engineering, Hinge Health"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Hinge Health faced challenges in hiring global data teams due to the complexities of PHI (Protected Health Information) governance. Unable to hire without compliant data sharing and unable to scale PHI governance without a larger team, we overcame this chicken or the egg challenge by adopting Unity Catalog's Fine-Grain Access Control. In this session, we will share our journey migrating to Unity Catalog, securing PHI with row filters/column masks, lessons learned and how our efforts surpassed our own expectations. This session equips data teams with strategies for HIPAA compliance without compromising flexibility and collaboration. Hinge Health is the leading digital MSK clinic, serving 11M+ members and 500+ employer health plans offering virtual physical therapy to reduce pain, surgeries and opioid use. /Sr. Solutions Architect\nDatabricks /Manager, Engineering"}
{"session_id": "hitchhikers-guide-delta-lake-streaming-agentic-universe", "title": "The Hitchhiker's Guide to Delta Lake Streaming in an Agentic Universe", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT", "Streaming"], "speakers": ["Distinguished Software Engineer, Nike"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data engineering continues to evolve the shift from batch-oriented to streaming-first has become standard across the enterprise. The reality is these changes have been taking shape for the past decade \u2014 we just now also happen to be standing on the precipice of true disruption through automation, the likes of which we could only dream about before. Yes, AI Agents and LLMs are already a large part of our daily lives, but we (as data engineers) are ultimately on the frontlines ensuring that the future of AI is powered by consistent, just-in-time data \u2014 and Delta Lake is critical to help us get there. This session will provide you with best practices learned the hard way by one of the authors of The Delta Lake Definitive Guide including: /Distinguished Software Engineer"}
{"session_id": "how-ad-tech-runs-databricks", "title": "How Ad Tech Runs on Databricks", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Data Scientist, The Trade Desk"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Global Head, Media & Advertising GTM\nDatabricks /VP, Engineering\nIntegral Ad Science /Sr. Data Scientist"}
{"session_id": "how-adobe-extracts-value-data-agentic-ai-powered-data-and-analytics", "title": "How Adobe Extracts Value From Data With Agentic AI-Powered Data and Analytics Engineering Workbench", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Global Head, Media & Advertising GTM, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how an AI-first, agentic solution is redefining the Data Development and Delivery Lifecycle (DDLC). This next-generation engineering workbench equips data engineers, analysts and practitioners with intelligent automation, context-aware guidance and collaborative capabilities to accelerate and streamline every stage of the data lifecycle \u2014 from business use case definition to actionable insights. By embedding agentic AI across the workflow, the platform significantly enhances development speed, quality and delivery efficiency, enabling organizations to realize data and analytics outcomes faster and with greater confidence. /Global Head, Media & Advertising GTM"}
{"session_id": "how-anthropic-transforms-financial-services-teams-genai", "title": "How Anthropic Transforms Financial Services Teams With GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Anthropic"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Anthropic"}
{"session_id": "how-arctic-wolf-modernizes-cloud-security-and-enhances-threat-detection", "title": "How Arctic Wolf Modernizes Cloud Security and Enhances Threat Detection with Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Delivery Solutions Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, you\u2019ll gain actionable insights to modernize your security operations and strengthen cyber resilience. Arctic Wolf will highlight how they eliminated data silos & enhanced their MDR pipeline to investigate suspicious threat actors for customers using Databricks. /Distinguished Data Architect\nArctic Wolf /Sr. Delivery Solutions Engineer"}
{"session_id": "how-blue-origin-accelerates-innovation-databricks-and-aws-govcloud", "title": "How Blue Origin Accelerates Innovation With Databricks and AWS GovCloud", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PUBLIC SECTOR", "technologies": ["DATABRICKS SQL", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Blue Origin is revolutionizing space exploration with a mission-critical data strategy powered by Databricks on AWS GovCloud. Learn how they leverage Databricks to meet ITAR and FedRAMP High compliance, streamline manufacturing and accelerate their vision of a 24/7 factory. Key use cases include predictive maintenance, real-time IoT insights and AI-driven tools that transform CAD designs into factory instructions. Discover how Delta Lake, Structured Streaming and advanced Databricks functionalities like Unity Catalog enable real-time analytics and future-ready infrastructure, helping Blue Origin stay ahead in the race to adopt generative AI and serverless solutions. /Staff Product Manager"}
{"session_id": "how-build-open-lakehouse-best-practices-interoperability", "title": "How to Build an Open Lakehouse: Best Practices for Interoperability", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager"}
{"session_id": "how-corning-harnesses-unity-catalog-enhanced-finops-maturity-and-cost", "title": "How Corning Harnesses Unity Catalog for Enhanced FinOps Maturity and Cost Optimization", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Data Office, Corning"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We will explore how leveraging Databricks' Unity Catalog has accelerated our FinOps maturity, enabling us to optimize platform utilization and achieve significant cost reductions. By implementing Unity Catalog, we've gained comprehensive visibility and governance over our data assets, leading to more informed decision-making and efficient resource allocation. Learn how Corning discovered actionable insights and leveraged best practices on utilizing Unity Catalog to streamline data management, enhance financial operations and drive substantial savings within your organization. /Director, Data Office"}
{"session_id": "how-danone-enhanced-global-data-sharing-delta-sharing", "title": "How Danone Enhanced Global Data Sharing with Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Scala"], "speakers": ["Lead Data Engineer, Danone"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how Danone, a global leader in the food industry, improved its data-sharing processes using Delta Sharing, an open protocol developed by Databricks. This session will explore how Danone migrated from a traditional hub-and-spoke model to a more efficient and scalable data-sharing approach that works seamlessly across regions and platforms. We\u2019ll discuss practical concepts such as in-region and cross-region data sharing, fine-grained access control, data discovery, and the implementation of data contracts. You\u2019ll also hear about the strategies Danone uses to deliver governed data efficiently while maintaining compliance with global regulations. Additionally, we\u2019ll discuss a cost comparison between direct data access and replication. Finally, we\u2019ll share insights into the challenges faced by global organizations in managing data sharing at scale and how Danone addressed these issues. Attendees will gain practical knowledge on building a reliable and secure data-sharing framework for international collaboration. /Resident Solution Architect\nDatabricks /Lead Data Engineer"}
{"session_id": "how-data-sharing-transforming-healthcare-real-world-insights", "title": "How Data Sharing is Transforming Healthcare: Real World Insights", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATA MARKETPLACE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Head of Revenue Strategy, Komodo Health"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Revenue Strategy"}
{"session_id": "how-databricks-powers-real-time-threat-detection-barracuda-xdr", "title": "How Databricks Powers Real-Time Threat Detection at Barracuda XDR", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DLT", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Machine Learning", "Real-time", "Scala", "Streaming"], "speakers": ["Manager of Detection Engineering, Barracuda Networks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As cybersecurity threats grow in volume and complexity, organizations must efficiently process security telemetry for best-in-class detection and mitigation. Barracuda\u2019s XDR platform is redefining security operations by layering advanced detection methodologies over a broad range of supported technologies. Our vision is to deliver unparalleled protection through automation, machine learning and scalable detection frameworks, ensuring threats are identified and mitigated quickly. To achieve this, we have adopted Databricks as the foundation of our security analytics platform, providing greater control and flexibility while decoupling from traditional SIEM tools. By leveraging DLTs, Spark Structured Streaming and detection-as-code CI/CD pipelines, we have built a real-time detection engine that enhances scalability, accuracy and cost efficiency. This session explores how Databricks is shaping the future of XDR through real-time analytics and cloud-native security. /Director, SOC Offensive Security\nBarracuda Networks /Manager of Detection Engineering"}
{"session_id": "how-feastables-partners-engine-leverage-advanced-data-models-and-ai", "title": "How Feastables Partners With Engine to Leverage Advanced Data Models and AI for Smarter BI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science", "Real-time", "Scala"], "speakers": ["Senior Vice President Strategy and Commercialization, Feastables"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Feastables, founded by YouTube sensation MrBeast, partnered with Engine to build a modern, AI-enabled BI ecosystem that transforms complex, disparate data into actionable insights, driving smarter decision-making across the organization. In this session, learn how Engine, a Built-On Databricks Partner, brought expertise combined with strategic partnerships that enabled Feastables to rapidly stand up a secure, modern data estate to unify complex internal and external data sources into a single, permissioned analytics platform. Feastables unlocked the power of cross-functional collaboration by democratizing data access throughout their enterprise and seamlessly integrating financial, retailer, supply chain, syndicated, merchandising and e-commerce data. Discover how a scalable analytics framework combined with advanced AI models and tools empower teams with Smarter BI across sales, marketing, supply chain, finance and executive leadership to enable real-time decision-making at scale. /SVP of Data Science\nEngine /Senior Vice President Strategy and Commercialization"}
{"session_id": "how-fedex-achieved-self-serve-analytics-and-data-democratization", "title": "How FedEx Achieved Self-Serve Analytics and Data Democratization on Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Real-time"], "speakers": ["Product Manager, Fedex"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "FedEx, a global leader in transportation and logistics, faced a common challenge in the era of big data: how to democratize data and foster data-driven decision making with thousands of data practitioners at FedEx wanting to build models, get real-time insights, explore enterprise data, and build enterprise-grade solutions to run the business. This breakout session will highlight how FedEx overcame challenges in data governance and security using Unity Catalog, ensuring that sensitive information remains protected while still allowing appropriate access across the organization. We'll share their approach to building intuitive self-service interfaces, including the use of natural-language processing to enable non-technical users to query data effortlessly. The tangible outcomes of this initiative are numerous, but chiefly: increased data literacy across the company, faster time-to-insight for business decisions, and significant cost-savings through improved operational efficiency. /Product Manager"}
{"session_id": "how-get-most-out-your-bi-tools-databricks", "title": "How to Get the Most Out of Your BI Tools on Databricks", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Warehouse", "SQL", "Scala"], "speakers": ["DBSQL Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock the full potential of your BI tools with Databricks. This session explores how features like Photon, Databricks SQL, Liquid Clustering, AI/BI Genie and Publish to Power BI enhance performance, scalability and user experience. Learn how Databricks accelerates query performance, optimizes data layouts and integrates seamlessly with BI tools. Gain actionable insights and best practices to improve analytics efficiency, reduce latency and drive better decision-making. Whether migrating from a data warehouse or optimizing an existing setup, this talk provides the strategies to elevate your BI capabilities. /DBSQL Product Specialist"}
{"session_id": "how-hms-federation-powered-nationwides-seamless-and-efficient-unity", "title": "How HMS Federation Powered Nationwide\u2019s Seamless and Efficient Unity Catalog Migration", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Scala"], "speakers": ["Lead Data Engineer, Nationwide"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This talk takes you through the Nationwide Security and Infrastructure data team's journey of migrating from HMS to UC. Discover how HMS federation simplified our transition to UC, allowing for an incremental migration that minimized disruption to data consumers while optimizing our data layout. We\u2019ll share the key technical decisions, challenges faced and lessons learned along the way. The migration process wasn\u2019t without its hurdles, so we\u2019ll walk you through our detailed, step-by-step approach covering planning, execution and validation. We will also showcase the benefits realized, such as improved data governance, more efficient data access and enhanced operational performance. Join us to gain practical insights into executing complex data migrations with a focus on security, flexibility and long-term scalability. /Lead Data Engineer"}
{"session_id": "how-hp-optimizing-3d-printing-supply-chain-using-delta-sharing", "title": "How HP Is Optimizing the 3D Printing Supply Chain Using Delta Sharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT", "Scala"], "speakers": ["Principal Data Engineer, HP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "HP\u2019s 3D Print division empowers manufacturers with telemetry data to optimize operations and streamline maintenance. Using Delta Sharing, Unity Catalog and AI/BI dashboards, HP provides a secure, scalable solution for data sharing and analytics. Delta Sharing D2O enables seamless data access, even for customers not on Databricks. Apigee masks private URLs, and Unity Catalog enhances security by managing data assets. Predictive maintenance with Mosaic AI boosts uptime by identifying issues early and alerting support teams. Custom dashboards and sample code let customers run analytics using any supported client, while Apigee simplifies access by abstracting complexity. Insights from A/BI dashboards help HP refines data strategy, aligning solutions with customer needs despite the complexity of diverse technologies, fragmented systems and customer-specific requirements. This fosters trust, drives innovation,and strengthens HP as a trusted partner for scalable, secure data solutions. /Principal Data Engineer"}
{"session_id": "how-migrate-oracle-databricks-sql", "title": "How to Migrate From Oracle to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Migrating your legacy Oracle data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. In this session, learn the top strategies for completing this data migration. We will cover data type conversion, basic to complex code conversions, validation and reconciliation best practices. Discover the pros and cons of using CSV files to PySpark or using pipelines to Databricks tables. See before-and-after architectures of customers who have migrated, and learn about the benefits they realized. /Sr. Specialist Solutions Architect"}
{"session_id": "how-migrate-snowflake-databricks-sql", "title": "How to Migrate From Snowflake to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse", "ELT", "SQL"], "speakers": ["DBSQL Adoption Lead, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Migrating your Snowflake data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. Though a cloud platform-to-cloud platform migration should be relatively easy, the breadth of the Databricks Platform provides flexibility and hence requires careful planning and execution. In this session, we present the migration methodology, technical approaches, automation tools, product/feature mapping, a technical demo and best practices using real-world case studies for migrating data, ELT pipelines and warehouses from Snowflake to Databricks. /DBSQL Adoption Lead"}
{"session_id": "how-migrate-teradata-databricks-sql", "title": "How to Migrate from Teradata to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": ["Sr. Specialist Solutions Architect DWH, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Storage and processing costs of your legacy Teradata data warehouses impact your ability to deliver. Migrating your legacy Teradata data warehouse to the Databricks Data Intelligence Platform can accelerate your data modernization journey. In this session, learn the top strategies for completing this data migration. We will cover data type conversion, basic to complex code conversions, validation and reconciliation best practices. How to use Databricks natively hosted LLMs to assist with migration activities. See before-and-after architectures of customers who have migrated, and learn about the benefits they realized. /SSA\nDatabricks /Sr. Specialist Solutions Architect DWH"}
{"session_id": "how-mparticle-easyjet-unlock-advanced-personalization-delta-sharing", "title": "How mParticle & EasyJet Unlock Advanced Personalization With Delta Sharing & Databricks Marketplace", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture", "ELT"], "speakers": ["Head of customer data, easyJet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation highlights how EasyJet leverages Delta Sharing to enable secure and frictionless data sharing with mParticle. By utilizing Delta Sharing, EasyJet leverages mParticle\u2019s customer data platform (CDP), available on Databricks Marketplace, without data duplication, enhancing audience segmentation and personalization strategies. The integration empowers EasyJet to streamline its data sharing workflows while maintaining robust governance and compliance, fostering advanced customer personalization and operational efficiency across platforms /Head of Data Architecture\neasyjet /Head of customer data"}
{"session_id": "how-navy-federals-enterprise-data-ecosystem-leverages-unity-catalog", "title": "How Navy Federal's Enterprise Data Ecosystem Leverages Unity Catalog for Data + AI Governance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Lake", "Data Pipeline", "Data Warehouse", "Machine Learning"], "speakers": ["AVP, Data Analytics Engineering, NFCU"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Navy Federal Credit Union has 200+ enterprise data sources in the enterprise data lake. These data assets are used for training 100+ machine learning models and hydrating a semantic layer for serving, at an average 4,000 business users daily across the credit union. The only option for extracting data from analytic semantic layer was to allow consuming application to access it via an already-overloaded cloud data warehouse. Visualizing data lineage for 1,000 + data pipelines and associated metadata is impossible and understanding the granular cost for running data pipelines is a challenge. Implementing Unity Catalog opened alternate path for accessing analytic semantic data from lake. It also opened the doors to remove duplicate data assets stored across multiple lakes which will save hundred thousands of dollars in data engineering efforts, compute and storage costs. /Field CTO\nDatabricks /AVP, Data Analytics Engineering"}
{"session_id": "how-nubank-improves-governance-security-and-user-experience-unity", "title": "How Nubank improves Governance, Security and User Experience with Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance"], "speakers": ["Engineering Manager, Nubank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Nubank, we successfully migrated to Unity Catalog, addressing the needs of our large-scale data environment with 3k active users, over 4k notebooks and jobs and 1.1 million tables, including sensitive PII data. Our primary objectives were to enhance data governance, security and user experience.Key points: This migration significantly improved our data governance capabilities, enhanced security measures and provided a more user-friendly experience for our large user base, ultimately leading to better control and utilization of our vast data resources. /Engineering Manager"}
{"session_id": "how-serverless-empowered-nationwide-build-cost-efficient-and-world", "title": "How Serverless Empowered Nationwide to Build Cost-Efficient and World Class BI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Data Engineer, Nationwide"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks\u2019 Serverless compute streamlines infrastructure setup and management, delivering unparalleled performance and cost optimization for Data and BI workflows. In this presentation, we will explore how Nationwide is leveraging Databricks\u2019 serverless technology and unified governance through Unity Catalog to build scalable, world-class BI solutions. Key features like AI/BI Dashboards, Genie, Materialized Views, Lakehouse Federation and Lakehouse Apps, all powered by serverless, have empowered business teams to deliver faster, scalable and smarter insights. We will show how Databricks\u2019 serverless technology is enabling Nationwide to unlock new levels of efficiency and business impact, and how other organizations can adopt serverless technology to realize similar benefits. /Lead Data Engineer"}
{"session_id": "how-should-enterprises-tackle-ai-agents-and-data-cloud-databricks", "title": "How Should Enterprises Tackle AI, Agents and Data Cloud With Databricks?", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Founder & Investor, Aisera"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session we will cover how enterprises can build and buy solutions for AI, Agents and Data Coud. Databricks provides a data cloud platform and foundational layer. Companies have good established choices for public or hybrid cloud for compute, GPU, network, storage, and microservices. We will discuss Gen AI platform and Agents OS platform as next foundational layers on top of Data Cloud to support third-party AI apps or internally-developed AI apps. Users and organizations need Killer AI & Agent apps to realize the promise and vision of AI and ROI. /VP & Head of Cloud, Data, and Analytics\nGilead Sciences /Founder & Investor"}
{"session_id": "how-texas-rangers-use-unified-data-platform-drive-world-class-baseball", "title": "How the Texas Rangers Use a Unified Data Platform to Drive World Class Baseball Analytics", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Data Engineer, Texas Rangers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don't miss this session where we demonstrate how the Texas Rangers baseball team is staying one step ahead of the competition by going back to the basics. After implementing a modern data strategy with Databricks and winnng the 2023 World Series the rest of the league quickly followed suit. Now more than ever, data and AI are a central pillar of every baseball team's strategy driving profound insights into player performance and game dynamics. With a 'fundamentals win games' back to the basics focus, join us as we explain our commmitment to world-class data quality, engineering, and MLOPS by taking full advantage of the Databricks Data Intelligence Platform. From system tables to federated querying, find out how the Rangers use every tool at their disposal to stay one step ahead in the hyper competitive world of baseball. /Assistant Director, Baseball R&D\nTexas Rangers /Data Engineer"}
{"session_id": "how-we-transformed-two-businesses-databricks-cornerstone", "title": "How We Transformed Two Businesses With Databricks as the Cornerstone", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Index Platform Technology, Nasdaq OMX Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this talk, we will discuss the lessons learned and future vision of transforming two business units to a modern financial data platform at Nasdaq. We'll highlight the transition from disjointed systems to a unified platform using Databricks. Our target audience includes financial engineers, data architects and technical leaders. The agenda covers challenges of legacy systems, reasons for choosing Databricks and key architectural decisions. /Vice President Software Engineering\nNasdaq, Inc. /VP, Index Platform Technology"}
{"session_id": "how-we-turned-200-business-users-analysts-aibi-genie", "title": "How We Turned 200+ Business Users Into Analysts With AI/BI Genie", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Sr. Marketing Analytics Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI/BI Genie has transformed self-service analytics for the Databricks Marketing team. This user-friendly conversational AI tool empowers marketers to perform advanced data analysis using natural language \u2014 no SQL required. By reducing reliance on data teams, Genie increases productivity and enables faster, data-driven decisions across the organization. But realizing Genie\u2019s full potential takes more than just turning it on. In this session, we\u2019ll share the end-to-end journey of implementing Genie for over 200 marketing users, including lessons learned, best practices and the real business impact of this Databricks-on-Databricks solution. Learn how Genie democratizes data access, enhances insight generation and streamlines decision-making at scale. /Sr. Marketing Analytics Manager"}
{"session_id": "hps-data-platform-migration-journey-redshift-lakehouse", "title": "HP's Data Platform Migration Journey: Redshift to Lakehouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ETL", "SQL", "Scala"], "speakers": ["Data Engineer, HP Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "HP Print's data platform team took on a migration from a monolithic, shared resource of AWS Redshift, to a modular and scalable data ecosystem on Databricks lakehouse. The result was 30\u201340% cost savings, scalable and isolated resources for different data consumers and ETL workloads, and performance optimization for a variety of query types. Through this migration, there were technical challenges and learnings relating to the ETL migrations with DBT, new Databricks features like Liquid Clustering, predictive optimization, Photon, SQL serverless warehouses, managing multiple teams on Unity Catalog, and others. This presentation dives into both the business and technical sides of this migration. Come along as we share our key takeaways from this journey. /Data Engineer\nHP Inc. /Data Engineer"}
{"session_id": "iceberg-geo-type-transforming-geospatial-data-management-scale", "title": "Iceberg Geo Type: Transforming Geospatial Data Management at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Warehouse"], "speakers": ["Co-founder and chief architect, Wherobots Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Apache Iceberg\u2122 community is introducing native geospatial type support, addressing key challenges in managing geospatial data at scale, including fragmented formats and inefficiencies in storing large spatial datasets. This talk will delve into the origins of the Iceberg geo type, its specification design and future goals. We will examine the impact on both the geospatial and Iceberg communities, in introducing a standard data warehouse storage layer to the geospatial community, and enabling optimized geospatial analytics for Iceberg users. We will also present a live demonstration of the Iceberg geo data type with Apache Sedona\u2122 and Apache Spark\u2122, showcasing how it simplifies and accelerates geospatial analytics workflows and queries. Finally, we will also provide an in-depth look at its current capabilities and outline the roadmap for future developments, and offer a perspective on its role in advancing geospatial data management in the industry. /Software Engineer\nDatabricks /Co-founder and chief architect"}
{"session_id": "iceberg-modern-table-standard-apple", "title": "Iceberg as the Modern Table Standard at Apple", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Streaming"], "speakers": ["Software Engineer, Apple"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore Apache Iceberg as a modern table format that has revolutionized data storage and processing. We\u2019ll dive into its core benefits, such as schema evolution, hidden partitioning and time-travel capabilities, and share how Apple leverages these features to optimize internal workflows. The session will highlight Iceberg\u2019s interoperability across compute engines commonly used at Apple: Spark, Trino and Flink, as well as its integration with streaming platforms like Kafka where it supports large scale batch and streaming workloads. Finally, we\u2019ll discuss the emerging support for AWS-managed Iceberg metadata, and how this can greatly improve large-scale data workflows, paving the way for future advancements. /Software Engineer"}
{"session_id": "iceberg-table-format-adoption-and-unified-metadata-catalog", "title": "Iceberg Table Format Adoption and Unified Metadata Catalog Implementation in Lakehouse Platform", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Doordash"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Lead Data Engineer\nDoorDash /Software Engineer"}
{"session_id": "imperative-declarative-paradigm-rebuilding-cicd-infrastructure-using", "title": "From Imperative to Declarative Paradigm: Rebuilding a CI/CD Infrastructure Using Hatch and DABs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Site Reliability Engineer, FreeWheel, a Comcast Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building and deploying Pyspark pipelines to Databricks should be effortless. However, our team at FreeWheel has, for the longest time, struggled with a convoluted and hard-to-maintain CI/CD infrastructure. It followed an imperative paradigm, demanding that every project implement custom scripts to build artifacts and deploy resources, and resulting in redundant boilerplate code and awkward interactions with the Databricks REST API. We set our mind on rebuilding it from scratch, following a declarative paradigm instead. We will share how we were able to eliminate thousands of lines of code from our repository, create a fully configuration-driven infrastructure where projects can be easily onboarded, and improve the quality of our codebase using Hatch and Databricks Asset Bundles as our tools of choice. In particular, DAB has made deploying across our 3 environments a breeze, and has allowed us to quickly adopt new features as soon as they are released by Databricks. /Sr. Software Engineer\nFreeWheel, a Comcast Company /Sr. Site Reliability Engineer"}
{"session_id": "implementing-greenops-databricks-practical-guide-regulated-environments", "title": "Implementing GreenOps in Databricks: A Practical Guide for Regulated Environments", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ABN Amro"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us on a technical journey into GreenOps at ABN AMRO Bank using Databricks system tables. We'll explore security, implementation challenges and best-practice verification, with practical examples and actionable reports. Discover how to optimize resource usage, ensure compliance and maintain agility. We'll discuss best practices, potential pitfalls and the nuanced 'it depends' scenarios, offering a comprehensive guide for intermediate to advanced practitioners. /Abn AMRO /ABN Amro"}
{"session_id": "improve-ai-training-first-synthetic-personas-dataset-aligned-real-world", "title": "Improve AI Training With the First Synthetic Personas Dataset Aligned to Real-World Distributions", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Staff Applied Scientist, Gretel.ai"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A big challenge in LLM development and synthetic data generation is ensuring data quality and diversity. While data incorporating varied perspectives and reasoning traces consistently improves model performance, procuring such data remains impossible for most enterprises. Human-annotated data struggles to scale, while purely LLM-based generation often suffers from distribution clipping and low entropy. In a novel compound AI approach, we combine LLMs with probabilistic graphical models and other tools to generate synthetic personas grounded in real demographic statistics. The approach allows us to address major limitations in bias, licensing and persona skew of existing methods. We release the first open source dataset aligned with real-world distributions and show how enterprises can leverage it with its Gretel Navigator extensions to bring diversity and quality to model training on the Databricks Platform, all while addressing model collapse and data provenance concerns head-on. /Principal Research Scientist\nNVIDIA /Staff Applied Scientist"}
{"session_id": "improving-user-experience-and-efficiency-using-dbsql", "title": "Improving User Experience and Efficiency Using DBSQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL", "Scala"], "speakers": ["Data Platform Manager, PicPay"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "To scale Databricks SQL to 2,000 users efficiently and cost-effectively, we adopted serverless, ensuring dynamic scalability and resource optimization. During peak times, resources scale up automatically; during low demand, they scale down, preventing waste. Additionally, we implemented a strong content governance model. We created continuous monitoring to assess query and dashboard performance, notifying users about adjustments and ensuring only relevant content remains active. If a query exceeds time or impact limits, access is reviewed and, if necessary, deactivated. This approach brought greater efficiency, cost reduction and an improved user experience, keeping the platform well-organized and high-performing. /Data Platform Manager"}
{"session_id": "incremental-iceberg-table-replication-scale", "title": "Incremental Iceberg Table Replication at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Software Engineer, Apple"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Iceberg is a popular table format for managing large analytical datasets. But replicating iceberg tables at scale can be a daunting task \u2014 especially when dealing with its hierarchical metadata. In this talk, we present an end-to-end workflow for replicating Apache Iceberg tables, leveraging Apache Spark to ensure that backup tables remain identical to their source counterparts. More excitingly, we have contributed these libraries back to the open-source community. Attendees will gain a comprehensive understanding of how to set up replication workflows for Iceberg tables, as well as practical guidance on how to manage and maintain replicated datasets at scale. This talk is ideal for data engineers, platform architects and practitioners looking to apply replication and disaster recovery for Apache Iceberg in complex data ecosystems. /Software Engineer\nDatabricks /Software Engineer"}
{"session_id": "industry-forum-networking-reception", "title": "Industry Forum Networking Reception", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "innovating-retail-data-unilevers-transformation-databricks-dlt", "title": "Innovating Retail Data: Unilever\u2019s Transformation with Databricks DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Data Science Manager, Unilever"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retail data is expanding at an unprecedented rate, demanding a scalable, cost-efficient, and near real-time architecture. At Unilever, we transformed our data management approach by leveraging Databricks DLT, achieving approximately $500K in cost savings while accelerating computation speeds by 200\u2013500%. By adopting a streaming-driven architecture, we built a system where data flows continuously across processing layers, enabling real-time updates with minimal latency. DLT\u2019s serverless simplicity replaced complex-dependency management, reducing maintenance overhead, and improving pipeline reliability. DLT Direct Publishing further enhanced data segmentation, concurrency, and governance, ensuring efficient and scalable data operations while simplifying workflows. This transformation empowers Unilever to manage data with greater efficiency, scalability, and reduced costs, creating a future-ready infrastructure that evolves with the needs of our retail partners and customers. /Senior Data Science Manager"}
{"session_id": "inside-spark-41-preview-latest-innovations", "title": "Inside Spark 4.1: A Preview of the Latest Innovations", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Quality", "ETL", "Python", "Real-time"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance. In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility \u2014 including the simplified Python Data Source API \u2014 and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution. /Databricks"}
{"session_id": "insights-all-bayer-consumer-healths-journey-self-service-analytics", "title": "Insights for All \u2014 Bayer Consumer Health\u2019s Journey on Self-Service Analytics at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Scala"], "speakers": ["Principal Cloud Platform Architect, Bayer AG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayer\u2019s mission, \"health for all, hunger for none,\" focuses on delivering innovative healthcare and agricultural products to address major global challenges in health and nutrition. This presentation will showcase how Bayer\u2019s Consumer Health division, known for products like Aspirin and Bepanthen, utilizes the Databricks Data Intelligence Platform to develop reusable core data assets and scalable data products. This globally distributed platform supports cost-efficient dashboarding, ad hoc analytics, machine learning and AI solutions, empowering thousands of stakeholders worldwide. The management of core data assets, which serve as integrated and reusable data models, will be discussed, highlighting their role in accelerating the creation of targeted data products. Additionally, a self-service analytics strategy will be presented to enhance access to insights and foster a data-driven culture at Bayer, addressing the needs of stakeholders in various markets and global headquarters. /Principal Cloud Platform Architect"}
{"session_id": "integrating-ai-data-unified-strategy-business", "title": "Integrating AI With Data: A Unified Strategy for Business", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Vice President of Data, Analytics & AI, EXL"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the modern business landscape, AI and data strategies can no longer operate in isolation. To drive meaningful outcomes, organizations must align these critical components within a unified framework tied to overarching business objectives. This presentation explores the necessity of integrating AI and data strategies, emphasizing the importance of high-quality data, scalable architectures and robust governance. Attendees will learn three essential steps that need to be taken: Additionally, the talk will highlight the cultural shift required to bridge IT and business silos, fostering roles that combine technical and business expertise. We\u2019ll dive into specific practical steps that can be taken to ensure an organization has a cohesive and blended AI and data strategy, using specific case examples. /Vice President of Data, Analytics & AI"}
{"session_id": "intelligent-applications-lakehouse", "title": "Intelligent Applications for the Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Senior Product Manager"}
{"session_id": "intro-mosaic-ai-platform-building-data-intelligence-your-ai-solutions", "title": "Intro to the Mosaic AI Platform: Building Data Intelligence Into Your AI Solutions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Technical Marketing Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Take a front-row seat for a comprehensive, high-level introduction to Mosaic AI through the lens of Data Intelligence. In this session, we\u2019ll spotlight the Databricks Platform\u2019s newest features and announcements, showcase how Mosaic AI transforms raw enterprise data into actionable insights and share real-world examples of success. Whether you\u2019re beginning your AI journey or scaling your existing efforts, this talk will provide you with the foundational knowledge and inspiration to fully leverage Mosaic AI for Data Intelligence and next-generation GenAI solutions. /AI/ML Product Mgmt\nDatabricks /Staff Technical Marketing Manager"}
{"session_id": "introducing-lakeflow-future-data-engineering-databricks", "title": "Introducing Lakeflow: The Future of Data Engineering on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Scala"], "speakers": ["Sr. Director of Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to explore Lakeflow, Databricks' end-to-end solution for simplifying and unifying the most complex data engineering workflows. This session builds on keynote announcements, offering an accessible introduction for newcomers while emphasizing the transformative value Lakeflow delivers. We\u2019ll cover: Discover how Lakeflow equips data teams with a seamless experience for ingestion, transformation and orchestration, reducing complexity and driving productivity. By unifying these capabilities, Lakeflow lays the groundwork for scalable, reliable and efficient data pipelines in a governed and high-performing environment. /Distinguished Engineer\nDatabricks /Sr. Director of Product Management"}
{"session_id": "introducing-simplified-state-tracking-apache-sparktm-structured", "title": "Introducing Simplified State Tracking in Apache Spark\u2122 Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Streaming"], "speakers": ["Sr. SSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation will review the new change feed and snapshot capabilities in Apache Spark\u2122 Structured Streaming\u2019s State Reader API. The State Reader API enables users to access and analyze Structured Streaming's internal state data. Readers will learn how to leverage the new features to debug, troubleshoot and analyze state changes efficiently, making streaming workloads easier to manage at scale. /Sr. SSA"}
{"session_id": "introduction-databricks-sql", "title": "Introduction to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Product Management, Databricks, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Technical Product Marketing Engineer\nDatabricks /Director, Product Management, Databricks"}
{"session_id": "introduction-modern-open-table-formats-and-catalogs", "title": "Introduction to Modern Open Table Formats and Catalogs", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "DEEP DIVE", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "DELTA LAKE"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, learn about why modern open table formats like Delta and Iceberg are a big deal and how they work with catalogs. Learn about what motivated their creation, how they work, what benefits they can bring to your data and AI platform. Hear about how these formats are becoming increasingly interoperable and what our vision is for their future. /Sr. Staff Product Manager\nDatabricks /Principal Software Engineer"}
{"session_id": "introduction-unity-catalog-metrics-define-your-business-metrics-once", "title": "Introduction to Unity Catalog Metrics: Define Your Business Metrics Once, Trust Everywhere", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Today\u2019s organizations need faster, more reliable insights \u2014 but metric sprawl and inconsistent KPIs make that difficult. In this session, you\u2019ll learn how Unity Catalog Metrics helps unify business semantics across your organization. Define your KPIs once, apply enterprise-grade governance with fine-grained access controls, auditing and lineage, and use them across any Databricks tool \u2014 from AI/BI Dashboards and Genie to notebooks and Lakeflow. You\u2019ll learn how to eliminate metric chaos by centrally defining and governing metrics with Unity Catalog. You\u2019ll walk away with strategies to boost trust through built-in governance and empower every team \u2014 regardless of technical skill \u2014 to work from the same certified metrics. /Staff Software Engineer\nDatabricks /Sr. Staff Product Manager"}
{"session_id": "intuits-privacy-safe-lending-marketplace-leveraging-databricks-clean", "title": "Intuit's Privacy-Safe Lending Marketplace: Leveraging Databricks Clean Rooms", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Staff Software Engineer, Intuit Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Intuit leverages Databricks Clean Rooms to create a secure, privacy-safe lending marketplace, enabling small business lending partners to perform analytics and deploy ML/AI workflows on sensitive data assets. This session explores the technical foundations of building isolated clean rooms across multiple partners and cloud providers, differentiating Databricks Clean Rooms from market alternatives. We'll demonstrate our automated approach to clean room lifecycle management using APIs, covering creation, collaborator onboarding, data asset sharing, workflow orchestration and activity auditing. The integration with Unity Catalog for managing clean room inputs and outputs will also be discussed. Attendees will gain insights into harnessing collaborative ML/AI potential, support various languages and workloads, and enable complex computations without compromising sensitive information in Clean Rooms. /Staff Software Engineer"}
{"session_id": "iqvias-analytics-patient-support-services-transforming-scalability", "title": "IQVIA's Analytics for Patient Support Services: Transforming Scalability, Performance and Governance", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Governance", "Scala"], "speakers": ["Head of Data Practice, Customertimes"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation will explore the transformation of IQVIA's decade-old patient support platform through the implementation of Databricks Data Intelligence Platform. Facing scalability challenges, performance bottlenecks and rising costs, the existing platform required significant redesign to handle growing data volumes and complex analytics. Key issues included static metrics limiting workflow optimization, fragmented data governance and heightened compliance and security demands. By partnering with Customertimes (a Databricks Partner) and adopting Databricks' centralized, scalable analytics solution with enhanced self-service capabilities, IQVIA achieved improved query performance, cost efficiency and robust governance, ensuring operational effectiveness and regulatory compliance in an increasingly complex environment. /Director, Data and Analytics\nIQVIA /Head of Data Practice"}
{"session_id": "iqvias-serverless-journey-enabling-data-and-ai-regulated-world", "title": "IQVIA\u2019s Serverless Journey: Enabling Data and AI in a Regulated World", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Your data and AI use-cases are multiplying. At the same time, there is increased focus and scrutiny to meet sophisticated security and regulatory requirements. IQVIA utilizes serverless use-cases across data engineering, data analytics, and ML and AI, to empower their customers to make informed decisions, support their R&D processes and improve patient outcomes. By leveraging native controls on the platform, serverless enables them to streamline their use cases while maintaining a strong security posture, top performance and optimized costs. This session will go over IQVIA\u2019s journey to serverless, how they met their security and regulatory requirements, and the latest and upcoming enhancements to the Databricks Platform. /Data Enablement / Architect\nIQVIA /Staff Product Manager"}
{"session_id": "italgas-ai-factory-and-future-gas-distribution", "title": "Italgas\u2019 AI Factory and the Future of Gas Distribution", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Lead Data Architect, Italgas"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Italgas, Europe\u2019s leading gas distributor both by network size and number of customers, we are spearheading digital transformation through a state-of-the-art, fully-fledged Databricks Intelligent platform. We first achieved 50% cost reduction and a 20% performance boost migrating from Azure Synapse to Databricks SQL and ensured that 100% of workloads are governed by Unity Catalog. Now we have 41ML/GenAI models in production: e.g., AI Customer Complaint Resolution. In AI ICT ticket resolution case, Lakeflow connector with ServiceNow allowed to halve the development time (eliminating DataFactory) and an automatic resolution of 40% of cases. Genie Dashboards and self-BI is used by 80% of our employees, while Genie allows the grid control-room operators to analyze network status data in natural language. Finally, our AI faculty will spread and boost AI literacy across the board and empower employees. /Lead\nCluster Reply /Lead Data Architect"}
{"session_id": "japanese-mega-banks-journey-modern-genai-powered-governed-data-platform", "title": "A Japanese Mega-Bank\u2019s Journey to a Modern, GenAI-Powered, Governed Data Platform", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal, Deloitte Consulting LLP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "SMBC-AD, a major Japanese multinational financial services institution, has embarked on an initiative to build a GenAI-powered, modern and well-governed cloud data platform on Azure/Databricks. This initiative aims to build an enterprise data foundation encompassing loans, deposits, securities, derivatives, and other data domains. Its primary goals are: Deloitte and SMBC leveraged the Brickbuilder asset \u201cData as a Service for Banking\u201d to accelerate this highly strategic transformation. /Principal"}
{"session_id": "jll-training-and-upskill-program-our-warehouse-migration-databricks", "title": "The JLL Training and Upskill Program for Our Warehouse Migration to Databricks", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Global Technology Director, JLL"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Odyssey is JLL\u2019s bespoke training program designed to upskill and prepare data professionals for a new world of data lakehouse. Based on the concepts of learn, practice and certify, participants earn points, moving through five levels by completing activities with business application of Databricks key features. Databricks Odyssey facilitates cloud data warehousing migration by providing best practice frameworks, ensuring efficient use of pay-per-compute platforms. JLL/T Insights and Data fosters a data culture through learning programs that develop in-house talent and create career pathways. Databricks Odyssey offers: Benefits include: /Global Technology Director"}
{"session_id": "kafka-forwarder-simplifying-kafka-consumption-openai", "title": "Kafka Forwarder: Simplifying Kafka Consumption at OpenAI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala", "Streaming"], "speakers": ["Member of Technical Staff, Open AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At OpenAI, Kafka fuels real-time data streaming at massive scale, but traditional consumers struggle under the burden of partition management, offset tracking, error handling, retries, Dead Letter Queues (DLQ), and dynamic scaling \u2014 all while racing to maintain ultra-high throughput. As deployments scale, complexity multiplies. Enter Kafka Forwarder \u2014 a game-changing Kafka Consumer Proxy that flips the script on traditional Kafka consumption. By offloading client-side complexity and pushing messages to consumers, it ensures at-least-once delivery, automated retries, and seamless DLQ management via Databricks. The result? Scalable, reliable and effortless Kafka consumption that lets teams focus on what truly matters. Curious how OpenAI simplified self-service, high-scale Kafka consumption? Join us as we walk through the motivation, architecture and challenges behind Kafka Forwarder, and share how we structured the pipeline to seamlessly route DLQ data into Databricks for analysis. /Member of Technical Staff"}
{"session_id": "kernel-catalog-action-reimagining-our-delta-spark-connector-dsv2", "title": "Kernel, Catalog, Action! Reimagining our Delta-Spark Connector With DSv2", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Delta Lake", "ELT"], "speakers": ["Sr Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake is redesigning its Spark connector through the combination of three key technologies: First, we're updating our Spark APIs to DSv2 to achieve deeper catalog integration and improved integration with the Spark optimizer. Second, we're fully integrating on top of Delta Kernel to take advantage of its simplified abstraction of Delta protocol complexities, accelerating feature adoption and improving maintainability. Third, we are transforming Delta to become a catalog-aware lakehouse format with Catalog Commits, enabling more efficient metadata management, governance and query performance. Join us to explore how we're advancing Delta Lake's architecture, pushing the boundaries of metadata management and creating a more intelligent, performant data lakehouse platform. /Sr Software Engineer"}
{"session_id": "kill-bill-ing-revenge-dish-best-served-optimized-genai", "title": "Kill Bill-ing? Revenge is a Dish Best Served Optimized with GenAI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, PROFESSIONAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sportsbet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era where cloud costs can spiral out of control, Sportsbet achieved a remarkable 49% reduction in Total Cost of Ownership (TCO) through an innovative AI-powered solution called 'Kill Bill.' This presentation reveals how we transformed Databricks' consumption-based pricing model from a challenge into a strategic advantage through an intelligent automation and optimization. Attendees will leave with a clear understanding of how to implement AI within Databricks solutions to address similar cost challenges in their environments. /Sportsbet"}
{"session_id": "lakeflow-connect-easy-efficient-ingestion-databases", "title": "Lakeflow Connect: Easy, Efficient Ingestion From Databases", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Connect streamlines the ingestion of incremental data from popular databases like SQL Server and PostgreSQL. In this session, we\u2019ll review best practices for networking, security, minimizing database load, monitoring and more \u2014 tailored to common industry scenarios. Join us to gain practical insights into Lakeflow Connect's functionality so that you\u2019re ready to build your own pipelines. Whether you're looking to optimize data ingestion or enhance your database integrations, this session will provide you with a deep understanding of how Lakeflow Connect works with databases. /Senior Product Manager\nDatabricks /Databricks"}
{"session_id": "lakeflow-connect-game-changer-complex-event-driven-architectures", "title": "Lakeflow Connect: The Game-Changer for Complex Event-Driven Architectures", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["DLT", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Real-time"], "speakers": ["Cloud Solutions Architect, European Food Safety Authority"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In 2020, Delaware implemented a state-of-the-art, event-driven architecture for EFSA, enabling a highly decoupled system landscape, presented at the Data&AI Summit 2021. By centrally brokering events in near real-time, consumer applications react instantly to events from producer applications as they occur. Event producers are decoupled from consumers via a publisher/subscriber mechanism. Over the past years, we noticed some drawbacks. The processing of these custom events, primarily aimed for process integration weren\u2019t covering all edge cases, the data quality was not always optimal due to missing events and we needed to create a complex logic for SCD2 tables. Lakeflow Connect allows us to extract the data directly from the source without the complex architecture in between, avoiding data loss and thus, data quality issues, and with some simple adjustments, an SCD2 table is created automatically. Lakeflow Connect allows us to create more efficient and intelligent data provisioning. /Senior Consultant\ndelaware /Business Analyst\ndelaware /Cloud Solutions Architect"}
{"session_id": "lakeflow-connect-seamless-data-ingestion-enterprise-apps", "title": "Lakeflow Connect: Seamless Data Ingestion From Enterprise Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, TRAVEL AND HOSPITALITY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics"], "speakers": ["Director, Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow Connect enables you to easily and efficiently ingest data from enterprise applications like Salesforce, ServiceNow, Google Analytics, SharePoint, NetSuite, Dynamics 365 and more. In this session, we\u2019ll dive deep on using connectors for the most popular SaaS applications, reviewing common use cases such as analyzing consumer behavior, predicting churn and centralizing HR analytics. You'll also hear from an early customer about how Lakeflow Connect helped unify their customer data to drive an improved automotive experience. We\u2019ll wrap up with a Q&A so you have the opportunity to learn from our experts. /Team Lead Data-Driven Business Solutions\nPorsche Informatik GmbH /Director, Product Management"}
{"session_id": "lakeflow-connect-simplify-ingestion-iot-clickstreams-and-telemetry", "title": "Lakeflow Connect: Simplify Ingestion for IoT, Clickstreams and Telemetry", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Sr. Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we\u2019ll explore new patterns that simplify real-time data ingestion into your lakehouse \u2014 especially for sources like IoT, clickstreams, telemetry and more. We\u2019ll begin with a quick look at the evolution of the ingestion landscape and the growing need to embed data flows closer to the source. Then, we\u2019ll introduce a modern approach that helps you \u201cshift left\u201d on ingestion \u2014 making it easier to integrate analytics and AI directly into operational systems, rather than treating them as afterthoughts. This shift can lead to simpler architectures that reduce unnecessary hops and scale with your operations. To close, we\u2019ll share a practical decision framework to help you determine which ingestion options best fit your use case, followed by a live Q&A to help you get started quickly. /Member of Technical Staff\nDatabricks /Sr. Staff Software Engineer"}
{"session_id": "lakeflow-connect-smarter-simpler-file-ingestion-next-generation-auto", "title": "Lakeflow Connect: Smarter, Simpler File Ingestion With the Next Generation of Auto Loader", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "ELT"], "speakers": ["Sr Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is the definitive tool for ingesting data from cloud storage into your lakehouse. In this session, we\u2019ll unveil new features and best practices that simplify every aspect of cloud storage ingestion. We\u2019ll demo out-of-the-box observability for pipeline health and data quality, walk through improvements for schema management, introduce a series of new data formats and unveil recent strides in Auto Loader performance. Along the way, we\u2019ll provide examples and best practices for optimizing cost and performance. Finally, we\u2019ll introduce a preview of what\u2019s coming next \u2014 including a REST API for pushing files directly to Delta, a UI for creating cloud storage pipelines and more. Join us to help shape the future of file ingestion on Databricks. /Staff Software Engineer\nDatabricks /Sr Staff Software Engineer"}
{"session_id": "lakeflow-effect", "title": "The LakeFlow Effect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Director of Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lakeflow brings much excitement, simplicity and unification to Databricks\u2019 engineering experience. Databricks\u2019 Bilal Aslam (Sr. Director of Product Management) and Josue A. Bogran (Databricks MVP & content creator) provide an overview of the history of Lakeflow, current value to your organization and the direction its capabilities are going toward. The session covers: The session will also provide you with an opportunity to ask questions to the team behind Lakeflow. /VP of Data + AI Architecture\nJosueBogran.com & zeb.co /Sr. Director of Product Management"}
{"session_id": "lakeflow-observability-ui-monitoring-deep-analytics", "title": "Lakeflow Observability: From UI Monitoring to Deep Analytics", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Monitoring data pipelines is key to reliability at scale. In this session, we\u2019ll dive into the observability experience in Lakeflow, Databricks\u2019 unified DE solution \u2014 from intuitive UI monitoring to advanced event analysis, cost observability and custom dashboards. We\u2019ll walk through the revamped UX for Lakeflow observability, showing how to: This session will help you unlock full visibility into your data workflows. /Product Management\nDatabricks /Product Manager"}
{"session_id": "lakeflow-production-cicd-testing-and-monitoring-scale", "title": "Lakeflow in Production: CI/CD, Testing and Monitoring at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Data Pipeline", "Data Quality", "Scala"], "speakers": ["Principal Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building robust, production-grade data pipelines goes beyond writing transformation logic \u2014 it requires rigorous testing, version control, automated CI/CD workflows and a clear separation between development and production. In this talk, we\u2019ll demonstrate how Lakeflow, paired with Databricks Asset Bundles (DABs), enables Git-based workflows, automated deployments and comprehensive testing for data engineering projects. We\u2019ll share best practices for unit testing, CI/CD automation, data quality monitoring and environment-specific configurations. Additionally, we\u2019ll explore observability techniques and performance tuning to ensure your pipelines are scalable, maintainable and production-ready. /Sr. Staff Product Manager\nDatabricks /Principal Engineer"}
{"session_id": "lakehouse-powerhouse-reckitts-enterprise-ai-transformation-story", "title": "Lakehouse to Powerhouse: Reckitt's Enterprise AI Transformation Story", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Boston Consulting Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we showcase Reckitt\u2019s journey to develop and implement a state-of-the-art Gen AI platform, designed to transform enterprise operations starting with the marketing function. We will explore the unique technical challenges encountered and the innovative architectural solutions employed to overcome them. Attendees will gain insights into how cutting-edge Gen AI technologies were integrated to meet Reckitt\u2019s specific needs. This session will not only highlight the transformative impacts on Reckitt\u2019s marketing operations but also serve as a blueprint for AI-driven innovation in the Consumer Goods sector, demonstrating a successful model of partnership in technology and business transformation. /VP - Global Data & Analytics @ Reckitt\nReckitt /Boston Consulting Group"}
{"session_id": "lancedb-complete-search-and-analytical-store-serving-production-scale", "title": "LanceDB: A Complete Search and Analytical Store for Serving Production-scale AI Applications", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Co-founder, LanceDB"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "If you're building AI applications, chances are you're solving a retrieval problem somewhere along the way. This is why vector databases are popular today. But if we zoom out from just vector search, serving AI applications also requires handling KV workloads like a traditional feature store, as well as analytical workloads to explore and visualize data. This means that building an AI application often requires multiple data stores, which means multiple data copies, manual syncing, and extra infrastructure expenses. LanceDB is the first and only system that supports all of these workloads in one system. Powered by Lance columnar format, LanceDB completely breaks open the impossible triangle of performance, scalability, and cost for AI serving. Serving AI applications is different from previous waves of technology, and a new paradigm demands new tools. /Software Engineer\nDatabricks /CEO / Co-founder"}
{"session_id": "largest-best-how-we-transformed-databricks-biggest-workspace-unity", "title": "From Largest to Best: How We Transformed Databricks\u2019 Biggest Workspace With Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we unveil how we transformed the largest Databricks workspace into the best-in-class lakehouse through Unity Catalog. Discover how we harnessed lineage and unified access management to build ultimate governance automation. /Platform Lead\nDatabricks /Senior Engineering Manager"}
{"session_id": "last-mile-data-delivery-fast-federated-and-fully-compliant", "title": "Last-Mile Data Delivery: Fast, Federated, and Fully Compliant", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "ELT", "Scala"], "speakers": ["software engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations scale, evolving privacy requirements and decentralized team ownership often lead to fragmented data lakehouses\u2014where data is produced and accessed across separate workspaces. Mission-critical workflows like AI model training or customer support triage frequently span datasets scattered across teams and systems, each with varying sensitivity and access controls. Engineers spend more time discovering and fetching data than delivering outcomes. They're slowed by increasing compliance hurdles\u2014and yet, direct access still exposes organizations to significant risk. To solve this, we built a secure, scalable serving layer on Databricks that brings the right data to the right users at the right time, for just the right amount of time. Powered by Unity Catalog, Delta Sharing, and Databricks Apps, our solution ensures governed, efficient access across the lakehouse\u2014without compromising speed, security, or user experience. /Engineering Manager\nDatabricks /software engineer"}
{"session_id": "latest-innovations-aibi-dashboards-and-genie", "title": "Latest Innovations in AI/BI Dashboards and Genie", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Sr Eng Director, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how the latest innovations in Databricks AI/BI Dashboards and Genie are transforming self-service analytics. This session offers a high-level tour of new capabilities that empower business users to ask questions in natural language, generate insights faster and make smarter decisions. Whether you're a long-time Databricks user or just exploring what's possible with AI/BI, you'll walk away with a clear understanding of how these tools are evolving \u2014 and how to leverage them for greater business impact. /Product Management\nDatabricks /Sr Eng Director"}
{"session_id": "learn-how-virtue-foundation-saves-lives-optimizing-health-care-delivery", "title": "Learn How the Virtue Foundation Saves Lives by Optimizing Health Care Delivery Across the Globe", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-Founder and President, Virtue Foundation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Virtue Foundation uses cutting-edge techniques in AI to optimize global health care delivery to save lives. With Unity Catalog as a foundation, they are using advanced Gen AI with model serving, vector search and MLflow to radically change how they map volunteer health resources with the right locations and facilities. /Co-Founder and President"}
{"session_id": "learn-program-not-write-prompts-dspy", "title": "Learn to Program Not Write Prompts with DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Writing prompts for our GenAI applications is long, tedious, and unmaintainable. A proper software development lifecycle requires proper testing and maintenance, something incredibly difficult to do on a block of text. Our current prompt engineering best practices have largely been manual trial and error, testing which of our prompts work well in certain situations. This process worsens as our prompts become more complex, adding multiple tasks and functionality within one long singular prompt. Enter DSPy, your PROGRAMATIC way of building GenAI Applications. Learn how DSPy allows you to modularize your prompt into modules and enforce typing through signatures. Then, utilize state of the art algorithms to optimize the prompts and weights against your evaluation datasets, just like machine learning! We will compare DSPy to a restaurant to help illustrate and demo DSPy\u2019s capabilities. It's time to start programming, rather than prompting, again! /Delivery Solutions Architect"}
{"session_id": "learning-goldman-sachs-legend-lakehouse-data-governance", "title": "Learning from Goldman Sachs' Legend Lakehouse for Data Governance", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time"], "speakers": ["Managing Director, Goldman Sachs"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data is the backbone of modern decision-making, but centralizing it is only the tip of the iceberg. Entitlements, secure sharing and just-in-time availability are critical challenges to any large-scale platform. Join Goldman Sachs as we reveal how our Legend Lakehouse, coupled with Databricks, overcomes these hurdles to deliver high-quality, governed data at scale. By leveraging an open table format (Apache Iceberg) and open catalog format (Unity Catalog), we ensure platform interoperability and vendor neutrality. Databricks Unity Catalog then provides a robust entitlement system that aligns with our data contracts, ensuring consistent access control across producer and consumer workspaces. Finally, Legend functions, integrating with Databricks User Defined Functions (UDF), offer real-time data enrichment and secure transformations without exposing raw datasets. Discover how these components unite to streamline analytics, bolster governance and power innovation. /Managing Director"}
{"session_id": "lessons-learned-building-scalable-game-analytics-platform-netflix", "title": "Lessons Learned: Building a Scalable Game Analytics Platform at Netflix", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Snr. Software Engineer, Netflix"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Over the past three years, Netflix has built a catalog of 100+ mobile and cloud games across TV, mobile and web platforms. With both internal and external studios contributing to this diverse ecosystem, building a robust game analytics platform became crucial for gaining insights into player behavior, optimizing game performance and driving member engagement.In this talk, we\u2019ll share our journey of building Netflix\u2019s Game Analytics platform from the ground up. We\u2019ll highlight key decisions around data strategy, such as whether to develop an in-house solution or adopt an external service. We\u2019ll discuss the challenges of balancing developer autonomy with data integrity and the complexities of managing data contracts for custom game telemetry, with an emphasis on self-service analytics. Attendees will learn how the Games Data team navigated these challenges, the lessons learned and the trade-offs involved in building a multi-tenant data ecosystem that supports diverse stakeholders. /Senior Data Engineer\nNETFLIX INC /Snr. Software Engineer"}
{"session_id": "let-llm-write-prompts-intro-dspy-compound-ai-pipelines", "title": "Let the LLM Write the Prompts: An Intro to DSPy in Compound AI Pipelines", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["AI/BI", "DSPY", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Data Science Leader & Strategist, Overture Maps Foundation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Large Language Models (LLMs) excel at understanding messy, real-world data, but integrating them into production systems remains challenging. Prompts can be unruly to write, vary by model and can be difficult to manage in the large context of a pipeline. In this session, we'll demonstrate incorporating LLMs into a geospatial conflation pipeline, using DSPy. We'll discuss how DSPy works under the covers and highlight the benefits it provides pipeline creators and managers. /Data Science Leader & Strategist"}
{"session_id": "lets-elevate-open-source-model-data-sharing-and-collaboration-retail", "title": "Let\u2019s Elevate: An Open Source Model for Data Sharing and Collaboration in Retail and Consumer Goods", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATA MARKETPLACE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Integration"], "speakers": ["Director of Value Acceleration, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retailers and suppliers face persistent financial and technical challenges to data sharing \u2014 including expensive proprietary platforms, complex data integration hurdles, fragmented governance and more \u2014 which currently restrict seamless data exchange primarily to their largest trading partners. In this session, we\u2019ll provide an in-depth explanation of Elevate, an industry alliance focused on building open source standards for data sharing and collaboration to drive greater efficiency across the entire ecosystem. This session will highlight proposed standards for data sharing, data models, business cases on the ROI and potential areas of innovation to democratize data sharing, drastically reduce costs, simplify integration processes and foster transparent, trusted collaboration. Learn about the Elevate industry data-sharing initiative and how your company can participate and help guide standards to improve data sharing with your key partners. /Global VP | Consumer Industries GTM\nDatabricks /Director of Value Acceleration"}
{"session_id": "lets-save-tons-money-cloud-native-data-ingestion", "title": "Let's Save Tons of Money With Cloud-Native Data Ingestion!", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Valued Employee, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake is a fantastic technology for quickly querying massive data sets, but first you need those massive data sets! In this session we will dive into the cloud-native architecture Scribd has adopted to ingest data from AWS Aurora, SQS, Kinesis Data Firehose and more. By using off-the-shelf open source tools like kafka-delta-ingest, oxbow and Airbyte, Scribd has redefined its ingestion architecture to be more event-driven, reliable, and most importantly: cheaper. No jobs needed! Attendees will learn how to use third-party tools in concert with a Databricks and Unity Catalog environment to provide a highly efficient and available data platform. This architecture will be presented in the context of AWS but can be adapted for Azure, Google Cloud Platform or even on-premise environments. /Valued Employee"}
{"session_id": "leveling-gaming-analytics-how-supercell-evolved-player-experiences", "title": "Leveling Up Gaming Analytics: How Supercell Evolved Player Experiences With Snowplow and Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala"], "speakers": ["CEO and Co-Founder, Snowplow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the competitive gaming industry, understanding player behavior is key to delivering engaging experiences. Supercell, creators of Clash of Clans and Brawl Stars, faced challenges with fragmented data and limited visibility into user journeys. To address this, they partnered with Snowplow and Databricks to build a scalable, privacy-compliant data platform for real-time insights. By leveraging Snowplow\u2019s behavioral data collection and Databricks\u2019 Lakehouse architecture, Supercell achieved: This session explores Supercell\u2019s data journey and AI-driven player engagement strategies. /CEO and Co-Founder"}
{"session_id": "leveraging-databricks-unity-catalog-enhanced-data-governance-unipol", "title": "Leveraging Databricks Unity Catalog for Enhanced Data Governance in Unipol", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Engineer, Data Reply"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the contemporary landscape of data management, organizations are increasingly faced with the challenges of data segregation, governance and permission management, particularly when operating within complex structures such as holding companies with multiple subsidiaries. Unipol comprises seven subsidiary companies, each with a diverse array of workgroups, leading to a cumulative total of multiple operational groups. This intricate organizational structure necessitates a meticulous approach to data management, particularly regarding the segregation of data and the assignment of precise read-and-write permissions tailored to each workgroup. The challenge lies in ensuring that sensitive data remains protected while enabling seamless access for authorized users. This speech wants to demonstrate how Unity Catalog emerges as a pivotal tool in the daily use of the data platform, offering a unified governance solution that supports data management across diverse AWS environments. /Data Platform Manager\nUnipol S.p.A. /Senior Data Engineer"}
{"session_id": "leveraging-genai-synthetic-data-generation-improve-spark-testing-and", "title": "Leveraging GenAI for Synthetic Data Generation to Improve Spark Testing and Performance in Big Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "LLAMA", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Principal Data Engineer, Independent Community"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Testing Spark jobs in local environments is often difficult due to the lack of suitable datasets, especially under tight timelines. This creates challenges when jobs work in development clusters but fail in production, or when they run locally but encounter issues in staging clusters due to inadequate documentation or checks. In this session, we\u2019ll discuss how these challenges can be overcome by leveraging Generative AI to create custom synthetic datasets for local testing. By incorporating variations and sampling, a testing framework can be introduced to solve some of these challenges, allowing for the generation of realistic data to aid in performance and load testing. We\u2019ll show how this approach helps identify performance bottlenecks early, optimize job performance and recognize scalability issues while keeping costs low. This methodology fosters better deployment practices and enhances the reliability of Spark jobs across environments. /Principal Data Engineer"}
{"session_id": "life-sciences-how-ai-transforming-drug-discovery-commercialization", "title": "Life Sciences: How AI is Transforming Drug Discovery to Commercialization", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Industry Marketing Lead, HLS, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Artificial intelligence is revolutionizing the life sciences industry, accelerating progress from drug discovery to commercialization. By harnessing vast datasets and advanced analytics, AI enables researchers to identify promising compounds, predict outcomes and optimize clinical trials with unprecedented speed and accuracy. In manufacturing and supply chain, AI streamlines operations, enhances quality control and improves forecasting. Commercial teams leverage AI-driven insights for targeted market strategies and personalized engagement. This session explores real-world applications and success stories, highlighting how AI is driving innovation, efficiency and better patient outcomes across the entire life sciences value chain. /Industry Marketing Lead, HLS"}
{"session_id": "llmops-intermountain-health-case-study-ai-inventory-agents", "title": "LLMOps at Intermountain Health: A Case Study on AI Inventory Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead MLOps Engineer, Intermountain Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will delve into the creation of an infrastructure, CI/CD processes and monitoring systems that facilitate the responsible and efficient deployment of Large Language Models (LLMs) at Intermountain Healthcare. Using the \"AI Inventory Agents\" project as a case study, we will showcase how an LLM Agent can assist in effort and impact estimates, as well as provide insights into various AI products, both custom-built and third-party hosted. This includes their responsible AI certification status, development status and monitoring status (lights on, performance, drift, etc.). Attendees will learn how to build and customize their own LLMOps infrastructure to ensure seamless deployment and monitoring of LLMs, adhering to responsible AI practices. /Lead MLOps Engineer"}
{"session_id": "low-emission-oil-gas-engineering-balance-between-clean-and-reliable", "title": "Low-Emission Oil & Gas: Engineering the Balance Between Clean and Reliable", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science"], "speakers": ["Technical lead, BP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Manager of Data Science and Applied AI\nNOV /bp /Director of Data Science and Analytics\nNOV /Technical lead"}
{"session_id": "machine-learning-aimbot-detection-call-duty", "title": "Machine Learning Aimbot Detection in Call of Duty", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Machine Learning", "Scala"], "speakers": ["Machine Learning Research Engineer, Activision"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As online gaming grows, maintaining fair play is increasingly difficult. Call of Duty, a highly competitive first-person shooter, faces a surge in aimbot usage\u2014cheats that enable near-perfect accuracy, undermining ranked play. Traditional detection methods are ineffective against advanced cheats that mimic human behavior. Machine learning presents a scalable and adaptive solution. We developed a data pipeline that collects features such as angle velocity, acceleration, etc. to train a deep neural network and deployed it. We are processing 30 million rows of data per hour for this detection on Databricks Platform. As cheat developers evolve, so must detection techniques. This session will explore our methodologies, challenges and future directions, demonstrating how machine learning is transforming anti-cheat strategies and preserving competitive integrity in online gaming and how Databricks enabling us to do so. /Machine Learning Research Engineer"}
{"session_id": "machine-learning-model-deployment", "title": "Machine Learning Model Deployment", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "Machine Learning", "Python", "Real-time"], "speakers": ["ML like Scikit-Learn, awareness of model deployment strategies) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed to introduce three primary machine learning deployment strategies and illustrate the implementation of each strategy on Databricks. Following an exploration of the fundamentals of model deployment, the course delves into batch inference, offering hands-on demonstrations and labs for utilizing a model in batch inference scenarios, along with considerations for performance optimization. The second part of the course comprehensively covers pipeline deployment, while the final segment focuses on real-time deployment. Participants will engage in hands-on demonstrations and labs, deploying models with Model Serving and utilizing the serving endpoint for real-time inference. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. common Python libraries for DS/ML like Scikit-Learn, awareness of model deployment strategies) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-model-development", "title": "Machine Learning Model Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Machine Learning", "Python"], "speakers": ["ML like Scikit-Learn, fundamental algorithms regression and classification, model evaluation with common metrics) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to develop traditional machine learning models on Databricks. We\u2019ll cover topics like using popular ML libraries, executing common tasks efficiently with AutoML and MLflow, harnessing Databricks' capabilities to track model training, leveraging feature stores for model development, and implementing hyperparameter tuning. Additionally, the course covers AutoML for rapid and low-code model training, ensuring that participants gain practical, real-world skills for streamlined and effective machine learning model development in the Databricks environment. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. common Python libraries for DS/ML like Scikit-Learn, fundamental ML algorithms like regression and classification, model evaluation with common metrics) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-operations", "title": "Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Machine Learning", "Python"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course will guide participants through a comprehensive exploration of machine learning model operations, focusing on MLOps and model lifecycle management. The initial segment covers essential MLOps components and best practices, providing participants with a strong foundation for effectively operationalizing machine learning models. In the latter part of the course, we will delve into the basics of the model lifecycle, demonstrating how to navigate it seamlessly using the Model Registry in conjunction with the Unity Catalog for efficient model management. By the course's conclusion, participants will have gained practical insights and a well-rounded understanding of MLOps principles, equipped with the skills needed to navigate the intricate landscape of machine learning model operations. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. understanding of basic MLOps concepts and practices as well as infrastructure and importance of monitoring MLOps solutions) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-scale", "title": "Machine Learning at Scale", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Python", "Scala"], "speakers": ["ML concepts, common model metrics and python libraries as well a basic understanding of scaling workloads with Spark) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course intends to equip professional-level machine learning practitioners with knowledge and hands-on experience in utilizing Apache Spark\u2122 for machine learning purposes, including model fine-tuning. Additionally, the course covers using the Pandas library for scalable machine learning tasks. The initial section of the course focuses on comprehending the fundamentals of Apache Spark\u2122 along with its machine learning capabilities. Subsequently, the second section delves into fine-tuning models using the hyperopt library. The final segment involves learning the implementation of the Pandas API within Apache Spark\u2122, encompassing guidance on Pandas UDFs (User-Defined Functions) and the Functions API for model inference. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. basic understanding of DS/ML concepts, common model metrics and python libraries as well as a basic understanding of scaling workloads with Spark) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "managed-and-foreign-tables-unity-catalog", "title": "Managed and Foreign Tables in Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog brings open, interoperable table formats to the heart of the Databricks Lakehouse. In this session, we\u2019ll introduce new capabilities that apply fine-grained governance across all data and unify access across catalogs. Learn how Databricks is eliminating data silos, simplifying performance with predictive optimization and advancing an open lakehouse architecture. /Staff Product Manager"}
{"session_id": "managing-data-and-ai-security-risks-dasf-20-and-customer-story", "title": "Managing Data and AI Security Risks With DASF 2.0 \u2014 and a Customer Story", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["LLAMA", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Artificial Intelligence, US AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Security team led a broad working group that significantly evolved the Databricks AI Security Framework (DASF) to its 2.0 version since its first release by closely collaborating with the top cyber security researchers at industry organizations such as OWASP, Gartner, NIST, HITRUST, FAIR Institute and several Fortune 100 companies to address the evolving risks and associated controls of AI systems in enterprises. Join us to to learn how The CLEVER GenAI pipeline, an AI-driven innovation in healthcare, processes over 1.5 million clinical notes daily to classify social determinants impacting veteran care while adhering to robust security measures like NIST 800-53 controls and by leveraging Databricks AI Security Framework. We will discuss robust AI security guidelines to help data and AI teams understand how to deploy their AI applications securely. This session will give a security framework for security teams, AI practitioners, data engineers and governance teams. /Principal Staff Security Field Engineer\nDatabricks /Director of Artificial Intelligence"}
{"session_id": "managing-databricks-scale", "title": "Managing Databricks at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Delta Lake", "ELT", "Scala"], "speakers": ["Senior Manager, Network Data & AI, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "T-Mobile\u2019s leadership in 5G innovation and its rapid growth in the fixed wireless business have led to an exponential increase in data, reaching 100s of terabytes daily. This session explores how T-Mobile uses Databricks to manage this data efficiently, focusing on scalable architecture with Delta Lake, auto-scaling clusters, performance optimization through data partitioning and caching and comprehensive data governance with Unity Catalog. Additionally, it covers cost management, collaborative tools and AI-driven productivity tools, highlighting how these strategies empower T-Mobile to innovate, streamline operations and maximize data impact across network optimization, supporting the community, energy management and more. /Senior Manager, Network Data & AI"}
{"session_id": "managing-governed-cloud", "title": "Managing the Governed Cloud", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["DATA MARKETPLACE", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Scala"], "speakers": ["Data Person, GM"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations increasingly adopt Databricks as a unified platform for analytics and AI, ensuring robust data governance becomes critical for compliance, security, and operational efficiency. This presentation will explore the end-to-end framework for governing the Databricks cloud, covering key use cases, foundational governance principles, and scalable automation strategies. We will discuss best practices for metadata, data access, catalog, classification, quality, and lineage, while leveraging automation to streamline enforcement. Attendees will gain insights into best practices and real-world approaches to building a governed data cloud that balances innovation with control. /Data Person"}
{"session_id": "manufacturing-and-transportation-industry-forum", "title": "Manufacturing and Transportation Industry Forum", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP Data & AI, Virgin Atlantic Airways"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an inspiring forum showcasing how manufacturers and transportation leaders are turning today's challenges into tomorrow's opportunities. From automotive giants revolutionizing product development with generative AI to logistics providers optimizing routes for both cost and sustainability, discover how industry pioneers are reshaping the future of industrial operations. Highlighting this session is an exciting collaboration between Heathrow Airport and Virgin Atlantic, demonstrating how partnership and innovation are transforming the air travel experience. Learn how these leaders and other companies are using Databricks to tackle their most pressing challenges \u2014 from smart factory transformations to autonomous systems development \u2014 proving that the path to profitability and sustainability runs through intelligent operations. /Global Industry Leader - Manufacturing\nDatabricks /Head of Technology, Cloud and Data\nHeathrow /Manufacturing & Energy Marketing Lead\nDatabricks /VP Data & AI"}
{"session_id": "manufacturing-cleaner-how-data-intelligence-cuts-carbon-not-profits", "title": "Manufacturing Cleaner: How Data Intelligence Cuts Carbon, Not Profits", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture"], "speakers": ["Lead Data & ML Engineer, Dow Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join industry leaders from Dow and Michelin as they reveal how data intelligence is revolutionizing sustainable manufacturing without compromising profitability. Dow demonstrates how their implementation of Databricks' Data Intelligence Platform has transformed their ability to track and reduce carbon footprints while driving operational efficiencies, resulting in significant cost savings through optimized maintenance and reduced downtime. Michelin follows with their ambitious strategy to achieve 3% energy consumption reduction by 2026, leveraging Databricks to turn this environmental challenge into operational excellence. Together, these manufacturing giants showcase how modern data architecture and AI are creating a new paradigm where sustainability and profitability go hand-in-hand. /Senior Solution Manager\nDow Inc. /Product Owner\nCGI /Lead Data & ML Engineer"}
{"session_id": "marketing-runs-your-data-why-it-holds-keys-customer-growth", "title": "Marketing Runs on Your Data: Why IT Holds the Keys to Customer Growth", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Vice President, Digital Solutions, Epsilon"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Senior Vice President, Digital Solutions"}
{"session_id": "master-schema-translations-era-open-data-lake", "title": "Master Schema Translations in the Era of Open Data Lake", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "ETL"], "speakers": ["Head of Data Platform, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog puts variety of schemas into a centralized repository, now the developer community wants more productivity and automation for schema inference, translation, evolution and optimization especially for the scenarios of ingestion and reverse-ETL with more code generations.Coinbase Data Platform attempts to pave a path with \"Schemaster\" to interact with data catalog with the (proposed) metadata model to make schema translation and evolution more manageable across some of the popular systems, such as Delta, Iceberg, Snowflake, Kafka, MongoDB, DynamoDB, Postgres...This Lighting Talk covers 4 areas: Takeaway: standardize schema lineage & translation /Head of Data Platform"}
{"session_id": "mastering-change-data-capture-dlt", "title": "Mastering Change Data Capture With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Streaming"], "speakers": ["Software Engineer, Square"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Transactional systems are a common source of data for analytics, and Change Data Capture (CDC) offers an efficient way to extract only what\u2019s changed. However, ingesting CDC data into an analytics system comes with challenges, such as handling out-of-order events or maintaining global order across multiple streams. These issues often require complex, stateful stream processing logic. This session will explore how DLT simplifies CDC ingestion using the Apply Changes function. With Apply Changes, global ordering across multiple change feeds is handled automatically \u2014 there is no need to manually manage state or understand advanced streaming concepts like watermarks. It supports both snapshot-based inputs from cloud storage and continuous change feeds from systems like message buses, reducing complexity for common streaming use cases. /Product Management\nDatabricks /Software Engineer"}
{"session_id": "mastering-data-security-and-compliance-coorsteks-journey-databricks", "title": "Mastering Data Security and Compliance: CoorsTek's Journey With Databricks Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PUBLIC SECTOR", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Security"], "speakers": ["Chief Business Officer, Tredence"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ensuring data security & meeting compliance requirements are critical priorities for businesses operating in regulated industries, where the stakes are high and the standards are stringent. We will showcase how CoorsTek, a global leader in technical ceramics MFG, partnered with Databricks to leverage the power of UC for addressing regulatory challenges while achieving significant operational efficiency gains. We'll dive into the migration journey, highlighting the adoption of key features such as RBAC, comprehensive data lineage tracking and robust auditing capabilities. Attendees will gain practical insights into the strategies and tools used to manage sensitive data, ensure compliance with industry standards and optimize cloud data architectures. Additionally, we\u2019ll share real-world lessons learned, best practices for integrating compliance into a modern data ecosystem and actionable takeaways for leveraging Databricks as a catalyst for secure and compliant data innovation. /Director of Data and Analytics\nCoorsTek /Chief Business Officer"}
{"session_id": "maximize-retail-data-insights-genie-deltasharing-crisps-collaborative", "title": "Maximize Retail Data Insights in Genie with DeltaSharing via Crisp\u2019s Collaborative Commerce Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Crisp"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Crisp"}
{"session_id": "maximizing-business-value-and-ensuring-data-privacy-databricks", "title": "Maximizing Business Value and Ensuring Data Privacy with Databricks in Connected Vehicles", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Privacy", "Delta Lake", "ELT"], "speakers": ["Manager, Field Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As global data privacy regulations tighten, balancing user data protection with maximizing its business value is crucial.This presentation explores how integrating Databricks into our connected-vehicle data platform enhances both governance and business outcomes. We\u2019ll highlight a case where migrating from EMR to Databricks improved deletion performance and cut costs by 99% with Delta Lake. This shift not only ensures compliance with data-privacy regulations but also maximizes the potential of connected-vehicle data. We are developing a platform that balances compliance with business value and sets a global standard for data usage, inviting partners to join us in building a secure, efficient mobility ecosystem. /General Manager\nTOYOTA MOTOR CORPORATION /Manager, Field Engineering"}
{"session_id": "measuring-user-adoption-and-kpis-data-products-using-databricks", "title": "Measuring User Adoption and KPIs for Data Products Using Databricks", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATA MARKETPLACE", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director of Product, Kythera Labs"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, attendees will learn how to leverage Databricks' system tables to measure user adoption and track key performance indicators (KPIs) for data products. The session will focus on how organizations can use system tables to analyze user behavior, assess engagement with data products and identify usage trends that can inform product development. By measuring KPIs such as user retention, frequency of use and data queries, organizations can optimize their data products for better performance and ROI. /Director of Product"}
{"session_id": "media-and-advertising-industry-forum", "title": "Media and Advertising Industry Forum", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "MOSAIC AI"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Vice President, Acxiom"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us at the Media & Advertising Forum to explore how data and AI are transforming media and advertising from content to creative and identity to outcomes. Featuring innovators from leading agencies, platforms, streamers and ad tech \u2014 plus exciting announcements from Databricks \u2014 this session delivers must-have insights for industry leaders and change agents. What to expect: /Global Head, Media & Advertising GTM\nDatabricks /SVP of Emerging Technology\nFox Corporation /Senior Vice President"}
{"session_id": "meeting-global-electricity-demands-strategic-role-nuclear-and-hydro", "title": "Meeting Global Electricity Demands: The Strategic Role of Nuclear and Hydro", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ML Engineer, Hydro Quebec"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for a groundbreaking session featuring two pioneers at the forefront of AI and data innovation in the energy sector: Westinghouse Electric Company and Hydro-Quebec. In the first segment, we'll explore AI's crucial role in enhancing safety, efficiency and compliance in nuclear operations through technologies like HiVE and Bertha, focusing on the unique reliability and credibility requirements of the regulated nuclear industry. Following this, Hydro-Quebec will demonstrate how they're tackling growing energy demands through innovative hierarchical forecasting systems that process terabytes of IoT sensor data across multiple levels. Their bottom-up approach leverages distributed computing paradigms and MLOps principles to train models on millions of time series datasets, ensuring grid efficiency and resilience. Together, these presentations showcase how AI and advanced data systems are transforming critical infrastructure management across the energy sector. /CTO, EVP R&D and Innovation\nWestinghouse Electric Company /ML Engineer"}
{"session_id": "metadata-agents-building-future-content-understanding-coactive-ai", "title": "From Metadata to Agents: Building the future of content understanding with Coactive AI + Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "ELT"], "speakers": ["SVP Data Science, Media Group, NBC Universal"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Media enterprises generate vast amounts of visual content, but unlocking its full potential requires multimodal AI at scale. Coactive AI and NBCUniversal\u2019s Corporate Decision Sciences team are transforming how enterprises discover and understand visual content. We explore how Coactive AI and Databricks \u2014 from Delta Share to Genie \u2014 can revolutionize media content search, tagging and enrichment, enabling new levels of collaboration. Attendees will see how this AI-powered approach fuels AI workflows, enhances BI insights and drives new applications \u2014 from automating cut sheet generation to improving content compliance and recommendations. By structuring and sharing enriched media metadata, Coactive AI and NBCU are unlocking deeper intelligence and laying the groundwork for agentic AI systems that retrieve, interpret and act on visual content. This session will showcase real-world examples of these AI agents and how they can reshape future content discovery and media workflows. /Field CTO & Co-Founder\nCoactive Systems Inc /SVP Data Science, Media Group"}
{"session_id": "metadata-driven-streaming-ingestion-using-dlt-azure-event-hubs-and", "title": "Metadata-Driven Streaming Ingestion Using DLT, Azure Event Hubs and a Schema Registry", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Streaming"], "speakers": ["Principal Data Engineer, Plexure"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Plexure, we ingest hundreds of millions of customer activities and transactions into our data platform every day, fuelling our personalisation engine and providing insights into the effectiveness of marketing campaigns. We're on a journey to transition from infrequent batch ingestion to near real-time streaming using Azure Event Hubs and DLT. This transformation will allow us to react to customer behaviour as it happens, rather than hours or even days later. It also enables us to move faster in other ways. By leveraging a Schema Registry, we've created a metadata-driven framework that allows data producers to: Join us to learn more about our journey and see how we're implementing this with DLT meta-programming - including a live demo of the end-to-end process! /Principal Data Engineer"}
{"session_id": "metadata-marathon-how-three-storage-projects-are-racing-forward", "title": "The Metadata Marathon: How Three Storage Projects are Racing Forward", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With the enormous amount of discussion about open storage formats between nerds and even not-nerds, it can be hard to keep track of who\u2019s doing what and how this actually makes any impact on day to day data projects. I want us to take a closer look at the three big projects in this space; Delta, Hudi and Iceberg. They\u2019re all trying to solve for similar data problems and have tackled the various challenges in different ways. This talk with start with the very basics of how we got here, what the history is before diving deep into the underlying tech, their roadmaps and their impacts on the data landscape as a whole. /Staff Developer Advocate"}
{"session_id": "migrating-legacy-sas-code-databricks-lakehouse-what-we-learned-along", "title": "Migrating Legacy SAS Code to Databricks Lakehouse: What We Learned Along the Way", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Warehouse"], "speakers": ["Senior Data Platforms Developer, PacificSource Health Plans"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In PacificSource Health Plans, a health insurance company in the US, we are on a successful multi-year journey to migrate all of our data and analytics ecosystem to Databricks Enterprise Data Warehouse (lakehouse). A particular obstacle on this journey was a reporting data mart which relied on copious amounts of legacy SAS code that applied sophisticated business logic transformations for membership, claims, premiums and reserves. This core data mart was driving many of our critical reports and analytics. In this session we will share the unique and somewhat unexpected challenges and complexities we encountered in migrating this legacy SAS code. How our partner (T1A) leveraged automation technology (Alchemist) and some unique approaches to reverse engineer (analyze), instrument, translate, migrate, validate and reconcile these jobs; and what lessons we learned and carried from this migration effort. /Principal Architect\nTier One Analytics Inc. /Senior Data Platforms Developer"}
{"session_id": "migration-unity-catalog-american-airlines-using-automation", "title": "Migration to Unity Catalog at American Airlines Using Automation", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["Sr. Manager , IT Data, American Airlines"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "American Airlines migrated from Hive Metastore to Unity Catalog using automated processes with Databricks APIs and GitHub Actions. This automation streamlined the migration for many applications within AA, ensuring consistency, efficiency and minimal disruption while enhancing data governance and disaster recovery capabilities. /Sr. Principal Data Architect\nAmerican Airlines /Sr. Manager , IT Data"}
{"session_id": "missing-link-between-lakehouse-and-data-intelligence", "title": "The Missing Link Between the Lakehouse and Data Intelligence", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "What connects your lakehouse to real data intelligence? The answer: the catalog. But not just any catalog. In this session, we break down why Unity Catalog is purpose-built for the lakehouse, and how it goes beyond operational or business catalogs to deliver cross-platform interoperability and a shared understanding of your data. You\u2019ll walk away with a clear view of how the right data foundation unlocks smarter decisions and trusted AI. /Director, Product\nDatabricks /Staff Product Manager"}
{"session_id": "mlops-databricks", "title": "MLOps With Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["MLOps Tech Lead, Marvelous MLOps"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/MLOps Tech Lead"}
{"session_id": "mlops-ships-accelerating-ai-deployment-vizient-databricks", "title": "MLOps That Ships: Accelerating AI Deployment at Vizient with Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Lead Machine Learning Engineer, Vizient"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Deploying AI models efficiently and consistently is a challenge many organizations face. This session will explore how Vizient built a standardized MLOps stack using Databricks, Azure DevOps and GitHub Actions to streamline model development, deployment and monitoring. Attendees will gain insights into how Databricks Asset Bundles were leveraged to create reproducible, scalable pipelines and how Infrastructure-as-Code principles accelerated onboarding for new AI projects.The talk will cover: By the end of this session, participants will have a roadmap for implementing a scalable, reusable MLOps framework that enhances operational efficiency across AI initiatives. /Director- Technology Delivery, Data & AI\nVizient Inc. /Lead Machine Learning Engineer"}
{"session_id": "monitor-data-and-ai-quality-scale-data-intelligence-powered-unity", "title": "Monitor Data and AI Quality at Scale With Data Intelligence Powered by Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delivering trusted, high-impact data and AI requires more than basic checks \u2014 it demands intelligent, automated quality monitoring. In this session, discover how quality monitoring scales with Unity Catalog and brings data intelligence to all your assets. Learn how Lakehouse Monitoring, anomaly detection, data classification, and Lakeflow come together to provide end-to-end visibility into the health of your data and AI pipelines. Discover how you can shift from reactive firefighting to proactive, scalable quality monitoring across your entire data estate. /Product Manager"}
{"session_id": "moodys-ai-screening-agent-automating-compliance-decisions", "title": "Moody's AI Screening Agent: Automating Compliance Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Assc Dir - Machine Learning, Moody's"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The AI Screening Agent automates Level 1 (L1) screening process, essential for Know Your Customer (KYC) and compliance due diligence during customer onboarding. This system aims to minimize false positives, significantly reducing human review time and costs. Beyond typical Retrieval-Augmented Generation (RAG) applications like summarization and chat-with-your-data (CWYD), the AI Screening Agent employs a ReAct architecture with intelligent tools, enabling it to perform complex compliance decision-making with human-like accuracy and greater consistency. In this talk, I will explore the screening agent architecture, demonstrating its ability to meet evolving client policies. I will discuss evaluation and configuration management using MLflow LLM-as-judge and Unity Catalog, and discuss challenges, such as, data fidelity and customization. This session underscores the transformative potential of AI agents in compliance workflows, emphasizing their adaptability, accuracy, and consistency. /Assc Dir - Machine Learning"}
{"session_id": "multi-format-multi-table-multi-statement-transactions-unity-catalog", "title": "Multi-Format, Multi-Table, Multi-Statement Transactions on Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "Delta Lake", "ELT"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Get a first look at multi-statement transactions in Databricks. In this session, we will dive into their capabilities, exploring how multi-statement transactions enable atomic updates across multiple tables in your data pipelines, ensuring data consistency and integrity for complex operations. We will also share how we are enabling unified transactions across Delta Lake and Iceberg with Unity Catalog \u2014 powering our vision for an open and interoperable lakehouse. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "multi-statement-transactions-how-improve-data-consistency-and", "title": "Multi-Statement Transactions: How to Improve Data Consistency and Performance", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Principal Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Multi-statement transactions bring the atomicity and reliability of traditional databases to modern data warehousing on the lakehouse. In this session, we\u2019ll explore real-world patterns enabled by multi-statement transactions \u2014 including multi-table updates, deduplication pipelines and audit logging \u2014 and show how Databricks ensures atomicity and consistency across complex workflows. We\u2019ll also dive into demos and share tips to getting started and migrations with this feature in Databricks SQL. /Principal Solutions Architect"}
{"session_id": "new-competitive-edge-building-resilient-supply-chains-data-ai", "title": "The New Competitive Edge: Building Resilient Supply Chains With Data + AI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["VP Data, Analytics and AI (CDO), Danone"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Consumer-facing industries are evolving faster than ever \u2014 and in today\u2019s competitive landscape, it\u2019s supply chains, not companies, that are truly competing. While data and AI offer huge potential for optimization, many organizations struggle to turn use cases into real business impact. In this session, leaders from retail, consumer goods, travel and hospitality will share how they\u2019re building strong data foundations to unlock AI-driven supply chain optimization. Learn how they're using generative AI to boost productivity, streamline operations and improve insights through better data collaboration. /Global VP | Consumer Industries GTM\nDatabricks /SAP /VP Data, Analytics and AI (CDO)"}
{"session_id": "news-insights-unlock-actionable-outcomes-lsegs-machine-readable-news", "title": "From News to Insights: Unlock Actionable Outcomes With LSEG\u2019s Machine Readable News and DeltaSharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA SHARING", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Real-time"], "speakers": ["Director, Data Distribution, LSEG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s fast-paced business environment, access to comprehensive, credible, real-time and historical data in one place is essential for effective decision-making and risk management. LSEG\u2019s Machine Readable News (MRN) solution transforms unstructured data into actionable insights, with ultra-low latency delivery and extensive historical news archives powered by AI and natural language processing (NLP). Leveraging Databricks Delta Sharing, businesses can seamlessly integrate this valuable data with their own internal datasets, additional LSEG data and third-party sources. This combination enables clients to optimize workflows, enhance risk models and make smarter, faster decisions across diverse use cases \u2014 from event-based trading to risk management. In this session, we\u2019ll explore how Delta Sharing facilitates the integration of diverse, trustworthy data, unlocking new opportunities and driving smarter investment strategies. /Global Head of Cloud Partners\nLSEG /Director, Data Distribution"}
{"session_id": "next-gen-data-science-how-posit-and-databricks-are-transforming", "title": "Next-Gen Data Science: How Posit and Databricks Are Transforming Analytics at Scale", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Product Mgr. Cloud Integrations, Posit, PBC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Senior Product Mgr. Cloud Integrations"}
{"session_id": "next-level-pyspark-udf-debugging", "title": "Next-Level PySpark UDF Debugging", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Debugging PySpark User Defined Functions (UDFs) has long been challenging due to the distributed execution model and limited runtime visibility. Traditional methods often require manually searching through scattered logs, making debugging slow and inefficient. In this talk, we introduce a set of powerful UDF debugging improvements, including a new logging framework that provides structured, queryable insights into UDF execution. We also cover timeouts to stop long-running tasks, better error messages for easier debugging, and best practices for common UDF issues. /Staff Software Engineer\nDatabricks /Sr. Software Engineer"}
{"session_id": "next-wave-ai-applications-driven-agentic-workflow-adidas-using", "title": "The Next Wave of AI Applications Driven by Agentic Workflow at Adidas Using Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["MLFLOW", "MOSAIC AI", "PARTNER CONNECT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["ML Engineer, Adidas AG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious to know how Adidas is transforming customer experience and business impact with agentic workflows, powered by Databricks? By leveraging cutting-edge tools like MosaicML\u2019s deployment capabilities, Mosaic AI Gateway, and MLflow, Adidas built a scalable GenAI agentic infrastructure that delivers actionable insights from growing 2 million product reviews annually. With remarkable results: Join us to explore how Adidas turned agentic workflows infra into a strategic advantage using Databricks and learn how you can do the same! /Resident Solutions Architect\nDatabricks /ML Engineer"}
{"session_id": "no-code-change-your-python-udf-arrow-optimization", "title": "No-Code Change in Your Python UDF for Arrow Optimization", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Python"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark\u2122 has introduced Arrow-optimized APIs such as Pandas UDFs and the Pandas Functions API, providing high performance for Python workloads. Yet, many users continue to rely on regular Python UDFs due to their simple interface, especially when advanced Python expertise is not readily available. This talk introduces a powerful new feature in Apache Spark that brings Arrow optimization to regular Python UDFs. With this enhancement, users can leverage performance gains without modifying their existing UDFs \u2014 simply by enabling a configuration setting or toggling a UDF-level parameter. Additionally, we will dive into practical tips and features for using Arrow-optimized Python UDFs effectively, exploring their strengths and limitations. Whether you\u2019re a Spark beginner or an experienced user, this session will allow you to achieve the best of both simplicity and performance in your workflows with regular Python UDFs. /Staff Software Engineer"}
{"session_id": "no-code-ml-forecasting-platform-retail-and-cpg-companies", "title": "A No-Code ML Forecasting Platform for Retail and CPG companies", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Director, Antuit - A Zebra Technologies company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Director"}
{"session_id": "no-time-dad-bod-automating-life-ai-and-databricks", "title": "No Time for the Dad Bod: Automating Life with AI and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "EDUCATION, HEALTH AND LIFE SCIENCES, MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DELTA LAKE", "DSPY"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Real-time", "Scala"], "speakers": ["AI Entrepreneur in Residence, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Life as a father, tech leader, and fitness enthusiast demands efficiency. To reclaim my time, I\u2019ve built AI-driven solutions that automate everyday tasks\u2014from research agents that prep for podcasts to multi-agent systems that plan meals\u2014all powered by real-time data and automation. This session dives into the technical foundations of these solutions, focusing on event-driven agent design and scalable patterns for robust AI systems. You\u2019ll discover how Databricks technologies like Delta Lake, for reliable and scalable data management, and DSPy, for streamlining the development of generative AI workflows, empower seamless decision-making and deliver actionable insights. Through detailed architecture diagrams and a live demo, I\u2019ll showcase how to design systems that process data in motion to tackle complex, real-world problems. Whether you\u2019re an engineer, architect, or data scientist, you\u2019ll leave with practical strategies to integrate AI-driven automation into your workflows. /AI Entrepreneur in Residence"}
{"session_id": "no-trust-all-value-monetizing-analytics-databricks-clean-rooms", "title": "No-Trust, All Value: Monetizing Analytics With Databricks Clean Rooms", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT", "Scala"], "speakers": ["CTO, Koantek"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In a world where data collaboration is essential but trust is scarce, Databricks Clean Rooms delivers a game-changing model: no data shared, all value gained. Discover how data providers can unlock new revenue streams by launching subscription-based analytics and \u201cBuilt-on-Databricks\u201d services that run on customer data \u2014 without exposing raw data or violating compliance. Clean Rooms integrates Unity Catalog\u2019s governance, Delta Sharing\u2019s secure exchange and serverless compute to enable true multi-party collaboration \u2014 without moving data. See how privacy-preserving models like fraud detection, clinical analytics and ad measurement become scalable, productizable and monetizable across industries. Walk away with a proven pattern to productize analytics, preserve compliance and turn trustless collaboration into recurring revenue. /CTO"}
{"session_id": "one-stop-machine-translation-solution-game-domain-real-time-ugc-content", "title": "One-Stop Machine Translation Solution in Game Domain From Real-Time UGC Content to In-Game Text", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "LLAMA"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Data Scientist, Tencent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We present Level Infinite AI Translation, a translation engine developed by Tencent, tailored specifically for the gaming industry. The primary challenge in game machine translation (MT) lies in accurately interpreting the intricate context of game texts, effectively handling terminology and adapting to the highly diverse translation formats and stylistic requirements across different games. Traditional MT approaches cannot effectively address the aforementioned challenges due to their weak context representation ability and lack of common knowledge. Leveraging large language model and related technology, our engine is crafted to capture the subtleties of localized language expression while ensuring optimization for domain-specific terminology, jargon and required formats and styles. To date, the engine has been successfully implemented in 15 international projects, translating over one billion words across 23 languages, and has demonstrated cost savings exceeding 25% for partners. /Lead Researcher\nProxima Beta (Tencent) /Senior Data Scientist"}
{"session_id": "open-source-unity-catalog-getting-started-best-practices-and-governance", "title": "Open Source Unity Catalog: Getting Started, Best Practices and Governance at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How to use UC OSS, what features are available, and intro to the ecosystem. We'll dive into the latest release and get hands-on with demos for working with your UC data and AI assets \u2014 including tables, volumes, models and AI functions. /Staff Software Engineer\nDatabricks /Sr Software Engineer"}
{"session_id": "optimize-cost-and-user-value-through-model-routing-ai-agent", "title": "Optimize Cost and User Value Through Model Routing AI Agent", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Machine Learning Lead, Meta"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Each LLM has unique strengths and weaknesses, and there is no one-size-fits-all solution. Companies strive to balance cost reduction with maximizing the value of their use cases by considering various factors such as latency, multi-modality, API costs, user need, and prompt complexity. Model routing helps in optimizing performance and cost along with enhanced scalability and user satisfaction. Overview of cost-effective models training using AI gateway logs, user feedback, prompt, and model features to design an intelligent model-routing AI agent. Covers different strategies for model routing, deployment in Mosaic AI, re-training, and evaluation through A/B testing and end-to-end Databricks workflows. Additionally, it will delve into the details of training data collection, feature engineering, prompt formatting, custom loss functions, architectural modifications, addressing cold-start problems, query embedding generation and clustering through VectorDB, and RL policy-based exploration. /Machine Learning Lead"}
{"session_id": "optimizing-analytics-infrastructure-lessons-migrating-snowflake", "title": "Optimizing Analytics Infrastructure: Lessons from Migrating Snowflake to Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Data Quality", "Scala"], "speakers": ["Architect, DeeplearningAPI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores the strategic migration from Snowflake to Databricks, focusing on the journey of transforming a data lake to leverage Databricks\u2019 advanced capabilities. It outlines the assessment of key architectural differences, performance benchmarks, and cost implications driving the decision. Attendees will gain insights into planning and execution, including data ingestion pipelines, schema conversion and metadata migration. Challenges such as maintaining data quality, optimizing compute resources and minimizing downtime are discussed, alongside solutions implemented to ensure a seamless transition. The session highlights the benefits of unified analytics and enhanced scalability achieved through Databricks, delivering actionable takeaways for similar migrations. /Architect"}
{"session_id": "optimizing-ev-charging-experience-machine-learning-accurate-charge-time", "title": "Optimizing EV Charging Experience: Machine Learning for Accurate Charge Time Estimation", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "ELT", "Machine Learning", "Real-time", "Scala"], "speakers": ["AI Engineer, Rivian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Accurate charge time estimation is key to vehicle performance and user experience. We developed a scalable ML model that enhances real-time charge predictions in vehicle controls. Traditional rule-based methods struggle with dynamic factors like environment, vehicle state, and charging conditions. Our adaptive ML solution improves accuracy by 10%. We use Unity Catalog for data governance, Delta Tables for storage, and Liquid Clustering for data layout. Job schedulers manage data processing, while AutoML accelerates model selection. MLflow streamlines tracking, versioning, and deployment. A dedicated serving endpoint enables A/B testing and real-time insights. As our data ecosystem grew, scalability became critical. Our flexible ML framework was integrated into vehicle control systems within months. With live accuracy tracking and software-driven blending, we support 50,000+ weekly charge sessions, improving energy management and user experience. /Senior Manager, Machine Learning & AI\nRivian Automotive, LLC /Sr. Machine Learning/AI Engineer"}
{"session_id": "optimizing-smart-meter-iiot-data-databricks-scale-interactive", "title": "Optimizing Smart Meter IIoT Data in Databricks for At-Scale Interactive Electrical Load Analytics", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL"], "speakers": ["Senior Director - Strategic Partnerships, Plotly"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Octave is a Plotly Dash application used daily by about 1,000 Hydro-Qu\u00e9bec technicians and engineers to analyze smart meter load and voltage data from 4.5M meters across the province. As adoption grew, Octave\u2019s back end was migrated to Databricks to address increasingly massive scale (>1T data points), governance and security requirements. This talk will summarize how Databricks was optimized to support performant at-scale interactive Dash application experiences while in parallel managing complex back-end ETL processes. The talk will outline optimizations targeted to further optimize query latency and user concurrency, along with plans to increase data update frequency. Non-technology related success factors to be reviewed will include the value of: subject matter expertise, operational autonomy, code quality for long-term maintainability and proactive vendor technical support. /Senior Director - Strategic Partnerships"}
{"session_id": "orchestration-lakeflow-jobs", "title": "Orchestration With Lakeflow Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "ETL", "Machine Learning"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious about orchestrating data pipelines on Databricks? Join us for an introduction to Lakeflow Jobs (formerly Databricks Workflows) \u2014 an easy-to-use orchestration service built into the Databricks Data Intelligence Platform. Lakeflow Jobs simplifies automating your data and AI workflows, from ETL pipelines to machine learning model training. In this beginner-friendly session, you'll learn how to: We\u2019ll walk through common use cases, share demos and offer tips to help you get started quickly. If you're new to orchestration or just getting started with Databricks, this session is for you. /Product Management\nDatabricks /Product Manager"}
{"session_id": "our-journey-operations-excellence-finops-observability", "title": "Our Journey to Operations Excellence (FinOps, Observability)", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["WBD"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data platforms scale, managing costs and ensuring system reliability become increasingly complex. Achieving operational excellence requires a strategic approach to FinOps (Cloud Cost Optimization) and Observability (End-to-End Monitoring). In this session, we\u2019ll share our journey in: /Sr.Manager, Software Engineering\nWarnerBros Discovery /WBD"}
{"session_id": "over-architected-live", "title": "Over Architected: LIVE", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Developer Advocate\nDatabricks /Staff Developer Advocate"}
{"session_id": "overwhelmed-empowered-how-sap-democratizing-data-ai-databricks-solve", "title": "From Overwhelmed to Empowered: How SAP is Democratizing Data & AI with Databricks to Solve Problems", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Technology & Innovation Manager, SAP SE"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Technology & Innovation Manager"}
{"session_id": "patients-are-waiting-accelerating-healthcare-innovation-data-ai-and", "title": "Patients Are Waiting... Accelerating Healthcare Innovation With Data, AI and Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture", "Scala"], "speakers": ["Head of Automation & Digital Innovation, Novo Nordisk"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era of exponential data growth, organizations across industries face common challenges in transforming raw data into actionable insights. This presentation showcases how Novo Nordisk is pioneering insights generation approaches to clinical data management and AI.Using our clinical trials platform FounData, built on Databricks, we demonstrate how proper data architecture enables advanced AI applications. We'll introduce a multi-agent AI framework that revolutionizes data interaction, combining specialized AI agents to guide users through complex datasets. While our focus is on clinical data, these principles apply across sectors \u2013 from manufacturing to financial services.Learn how democratizing access to data and AI capabilities can transform organizational efficiency while maintaining governance. Through this real-world implementation, participants will gain insights on building scalable data architectures and leveraging multi-agent AI frameworks for responsible innovation. /Principal Platform Architect\nNovo Nordisk /VP, Data & AI Platform Engineering\nNovo Nordisk A/S /Head of Automation & Digital Innovation"}
{"session_id": "payer-digital-transformation-impact-data-ai", "title": "Payer Digital Transformation: The Impact of Data + AI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance"], "speakers": ["ML Engineering"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Payer organizations are rapidly embracing digital transformation, leveraging data and AI to drive operational efficiency, improve member experiences and enhance decision-making. This session explores how advanced analytics, robust data governance and AI-powered insights are enabling payers to streamline claims processing, personalize member engagement, manage pharmacy operations, and optimize care management. Thought leaders from Evernorth will share real-world examples of data-driven innovation, discuss strategies for overcoming interoperability and privacy challenges, and highlight the future potential of AI in reshaping the payer landscape. /Sr. Directory Industry GTM\nDatabricks /Head of Data and Analytics Architecture\nCigna/Evernorth /IT Senior Principal\nCigna /Vice President Healthcare\nFractal /VP, AI/ML Engineering"}
{"session_id": "pdf-document-ingestion-accelerator-genai-applications", "title": "PDF Document Ingestion Accelerator for GenAI Applications", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Streaming"], "speakers": ["Specialist Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Financial Service customers in the GenAI space have a common use case of ingestion and processing of unstructured documents \u2014 PDF/images \u2014 then performing downstream GenAI tasks such as entity extraction and RAG based knowledge Q&A. The pain points for the customers for these types of use cases are: In this talk we will present an optimized structured streaming workflow for complex PDF ingestion. The key techniques include Apache Spark\u2122 optimization, multi-threading, PDF object extraction, skew handling and auto retry logics /Lead SSA\nDatabricks /Specialist Solution Architect"}
{"session_id": "pella-next-generation-implement-optimization-techniques-and-automated", "title": "Pella Next Generation: Implement Optimization Techniques and Automated Deployments for Cost Savings", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PROFESSIONAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Data Quality"], "speakers": ["Pella Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will explore the \u201cPella Next Generation\u201d project, towards performance optimization and cost savings using various tools within Databricks data analytics in an Azure cloud. Liquid clustering, managed schema/table within Unity Catalog, BI analytical dashboard to monitor job performance and cost \u2014 compute and storage \u2014 are used towards performance optimization. Auto Loader and DLT with data quality checks/constraints and a common framework to support data ingestion into the medallion layers of the data lake. Databricks Asset Bundles for workflows/jobs deployment resulted in significant cost savings of $30K/year \u2014 conservative number \u2014 with a potential savings of up to $40K/year for the organization. This presentation will provide valuable takeaways for professionals looking to implement similar solutions in their own organizations. /Pella Corporation"}
{"session_id": "performance-best-practices-fast-queries-high-concurrency-and-scaling", "title": "Performance Best Practices for Fast Queries, High Concurrency, and Scaling on Databricks SQL", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data warehousing in enterprise and mission-critical environments needs special consideration for price/performance and security. This session will explain how Databricks SQL addresses the most challenging requirements for high-concurrency, low-latency performance and managing user identity, access and governance at scale. We will also cover the latest advancements in resource-based scheduling, autoscaling and caching enhancements that allow for seamless performance and workload management. /Product Manager\nDatabricks /Principal Software Engineer"}
{"session_id": "perks-using-unity-catalog-managed-tables", "title": "The Perks of Using Unity Catalog Managed Tables", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, HEALTH AND LIFE SCIENCES, MANUFACTURING", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Architecture"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session provides actionable insights into how organizations can transition to Unity Catalog managed tables to unlock the full potential of predictive optimization and future-proof their data architecture. Whether you\u2019re managing thousands of tables or looking to streamline operations, this talk will equip you with the tools and strategies to succeed in the era of intelligent data management. Key highlights include: /Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "petabyte-scale-chain-insights-real-time-intelligence-next-gen-financial", "title": "Petabyte-Scale On-Chain Insights: Real-Time Intelligence for the Next-Gen Financial Backbone", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Apache Spark", "Data Lake", "Delta Lake", "ELT", "Machine Learning", "Real-time", "SQL"], "speakers": ["Founder, CipherOwl Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019ll explore how CipherOwl Inc. constructed a near real-time, multi-chain data lakehouse to power anti-money laundering (AML) monitoring at a petabyte scale. We will walk through the end-to-end architecture, which integrates cutting-edge open-source technologies and AI-driven analytics to handle massive on-chain data volumes seamlessly. Off-chain intelligence complements this to meet rigorous AML requirements. At the core of our solution is ChainStorage, an OSS started by Coinbase that provides robust blockchain data ingestion and block-level serving. We enhanced it with Apache Spark\u2122 and Arrow\u2122, coupled for high-throughput processing and efficient data serialization, backed by Delta Lake and Kafka. For the serving layer, we employ StarRocks to deliver lightning-fast SQL analytics over vast datasets. Finally, our system incorporates machine learning and AI agents for continuous data curation and near real-time insights, which are crucial for tackling on-chain AML challenges. /Founder"}
{"session_id": "petrobras-mlops-transformation-mlflow-and-databricks", "title": "Petrobras MLOps Transformation With MLflow and Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR", "technologies": ["DATABRICKS WORKFLOWS", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Scala"], "speakers": ["Consultant, Petrobras"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As a global energy leader, Petrobras relies on machine learning to optimize operations, but manual model deployment and validation processes once created bottlenecks that delayed critical insights. In this session, we\u2019ll reveal how we revolutionized our MLOps framework using MLflow, Databricks Asset Bundles (DABs) and Unity Catalog to: Discover how we enabled data scientists to focus on innovation\u2014not infrastructure\u2014through standardized pipelines while ensuring compliance and scalability in one of the world\u2019s most complex energy ecosystems. /Sr. Solutions Architect\nDatabricks /Consultant"}
{"session_id": "platform-strategies-ai-models-cross-region-migration-and-dr", "title": "Platform Strategies for AI Models\u2014Cross-Region Migration and DR", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL"], "speakers": ["Sr Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI for enterprises, particularly in the era of GenAI, requires rapid experimentation and the ability to productionize models and agents quickly. This means that the availability of the entire Lakehouse- from raw data to ETL pipelines to notebooks and ultimately the serving layer- is more important than ever. A strong Disaster Recovery strategy has become a key part of a strong AI practice; as cloud providers struggle with rising demand for GPUs in environments, VM shortages have become commonplace, and add to the pressure of general cloud outages. Enterprises that can quickly leverage GPU capacity in other cloud regions will be better equipped to capitalize on the promise of AI. Leveraging Unity Catalog Models is a strong recommendation going forward for Disaster Recovery readiness and good AI governance practices. In this presentation we will show an end-to-end example of how UC enables DR for fully-governed AI models. Co-presenters: Tony Farias and Greg Wood /Lead SSA\nDatabricks /Sr Solutions Architect"}
{"session_id": "power-bi-and-databricks-practical-best-practices", "title": "Power BI and Databricks: Practical Best Practices", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr. Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Power BI has long been the dominant BI tool in the market. In this session, we'll discuss how to get the most out of PowerBI and Databricks, beginning with high-level architecture and moving down into detailed how-to guides for troubleshooting common failure points. At the end, you'll receive a cheat-sheet which summarizes those best practices into an easy-to-reference format. /Sr. Solution Architect"}
{"session_id": "powering-secure-and-scalable-data-governance-pepsico-unity-catalog-open", "title": "Powering Secure and Scalable Data Governance at PepsiCo With Unity Catalog Open APIs", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Enterprise Data Operations Director, PepsiCo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "PepsiCo, given its scale, has numerous teams leveraging different tools and engines to access data and perform analytics and AI. To streamline governance across this diverse ecosystem, PepsiCo unifies its data and AI assets under an open and enterprise-grade governance framework with Unity Catalog. In this session, we'll explore real-world examples of how PepsiCo extends Unity Catalog\u2019s governance to all its data and AI assets, enabling secure collaboration even for teams outside Databricks. Learn how PepsiCo architects permissions using service principals and service accounts to authenticate with Unity Catalog, building a multi-engine architecture with seamless and open governance. Attendees will gain practical insights into designing a scalable, flexible data platform that unifies governance across all teams while embracing openness and interoperability. /Lead Specialist Solutions Architect\nDatabricks /Enterprise Data Operations Director"}
{"session_id": "practical-ai-solutions-customer-care-supply-chain-excellence", "title": "Practical AI Solutions: From Customer Care to Supply Chain Excellence", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Senior Programmer, Hypertherm Associates"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director of AI\nLippert /VP, Data + AI\nLippert /Director - AI, Analytics & Automation\nHypertherm Associates /Senior Programmer"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /"}
{"session_id": "practitioners-guide-databricks-serverless", "title": "A Practitioner\u2019s Guide to Databricks Serverless", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Engineering", "Data Pipeline"], "speakers": ["Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows. /Product Specialist"}
{"session_id": "prediction-prevention-transforming-risk-management-insurance", "title": "From Prediction to Prevention: Transforming Risk Management in Insurance", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Machine Learning"], "speakers": ["Manager AI & Data Engineering, Intact Financial Corp"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Protecting insurers against emerging threats is critical. This session reveals how leading companies use Databricks\u2019 Data Intelligence Platform to transform risk management, enhance fraud detection, and ensure compliance. Learn how advanced analytics, AI, and machine learning process vast data in real time to identify risks and mitigate threats. Industry leaders will share strategies for building resilient operations that protect against financial losses and reputational harm. Key takeaways: Discover how data intelligence is revolutionizing insurance risk management and safeguarding the industry\u2019s future. /Global Head of Insurance ProServ\nDatabricks /AVP, P&C Actuarial Data and Technology\nNationwide /Head of Data & Analytics\nTravelers /Manager AI & Data Engineering"}
{"session_id": "prescription-success-leveraging-dabs-faster-deployment-and-better", "title": "A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Principal Data Engineer, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Health Catalyst (HCAT) transformed its CI/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements. HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making. /Sr. Solutions Architect\nDatabricks /Principal Data Engineer"}
{"session_id": "public-sector-industry-forum", "title": "Public Sector Industry Forum", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join the 60-minute kickoff session at the Public Sector Forum for an opportunity to to accelerate innovation into your enterprise through governance, compliance and GenAI. Featuring keynotes from data-driven agency leaders and providing a future-looking journey from Databricks, this event offers invaluable insights. Understand the outcomes of Data and AI powering transformation across common areas of government and beyond: You will not want to miss this exclusive opportunity to own your data and eliminate government silos. Discover the Data + AI Company with deep compliance experience and widespread adoption. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "pushing-limits-what-your-warehouse-can-do-using-python-and-databricks", "title": "Pushing the Limits of What Your Warehouse Can Do Using Python and Databricks", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager"}
{"session_id": "race-real-time-low-latency-streaming-etl-next-gen-oltp-db", "title": "Race to Real-Time: Low-Latency Streaming ETL With Next-Gen OLTP-DB", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Pipeline", "ETL", "Real-time", "Scala", "Streaming"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s digital economy, real-time insights and rapid responsiveness are paramount to delivering exceptional user experiences and lowering TCO. In this session, discover a pioneering approach that leverages a low-latency streaming ETL pipeline built with Apache Spark\u2122 Structured Streaming and a next-gen OLTP-DB solution. Validated in a live customer scenario, this architecture achieves sub-2 second end-to-end latency by seamlessly ingesting streaming data from Kinesis and merging it into OLTP-DB. This breakthrough not only enhances performance and scalability but also provides a replicable blueprint for transforming data pipelines across various verticals. Join us as we delve into the advanced optimization techniques and best practices that underpin this innovation, demonstrating how Databricks next-generation solutions can revolutionize real-time data processing and unlock a myriad of new use cases in data landscape. /Specialist Solutions Architect"}
{"session_id": "real-time-analytics-pipeline-iot-device-monitoring-and-reporting", "title": "Real-Time Analytics Pipeline for IoT Device Monitoring and Reporting", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Quality", "ELT", "Real-time", "Streaming"], "speakers": ["Lead Data Engineer, CKDelta"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will show how we implemented a solution to support high-frequency data ingestion from smart meters. We implemented a robust API endpoint that interfaces directly with IoT devices. This API processes messages in real time from millions of distributed IoT devices and meters across the network.The architecture leverages cloud storage as a landing zone for the raw data, followed by a streaming pipeline built on DLT. This pipeline implements a multi-layer medallion architecture to progressively clean, transform and enrich the data. The pipeline operates continuously to maintain near real-time data freshness in our gold layer tables. These datasets connect directly to Databricks Dashboards, providing stakeholders with immediate insights into their operational metrics. This solution demonstrates how modern data architecture can handle high-volume IoT data streams while maintaining data quality and providing accessible real-time analytics for business users. /Data Scientist\nCK Delta /Lead Data Engineer"}
{"session_id": "real-time-botnet-defense-cvs-ai-driven-detection-and-mitigation", "title": "Real-Time Botnet Defense at CVS: AI-Driven Detection and Mitigation on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Principal Data Scientist, CVS"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Botnet attacks mobilize digital armies of compromised devices that continuously evolve, challenging traditional security frameworks with their high-speed, high-volume nature. In this session, we will reveal our advanced system \u2014 developed on the Databricks platform \u2014 that leverages cutting-edge AI/ML capabilities to detect and mitigate bot attacks in near-real time. We will dive into the system\u2019s robust architecture, including scalable data ingestion, feature engineering, MLOps strategies & production deployment of the system. We will address the unique challenges of processing bulk HTTP traffic data, time-series anomaly detection and attack signature identification. We will demonstrate key business values through downtime minimization and threat response automation. With sectors like healthcare facing heightened risks, ensuring data integrity and service continuity is vital. Join us to uncover lessons learned while building an enterprise-grade solution that stays ahead of adversaries. /Sr. Data Scientist\nCVS /Principal Data Scientist"}
{"session_id": "real-time-market-insights-powering-optivers-live-trading-dashboard", "title": "Real-Time Market Insights \u2014 Powering Optiver\u2019s Live Trading Dashboard with Databricks Apps and Dash", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS APPS", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala", "Streaming"], "speakers": ["Data Engineer, Optiver"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the fast-paced world of trading, real-time insights are critical for making informed decisions. This presentation explores how Optiver, a leading high-frequency trading firm, harnesses Databricks apps to power its live trading dashboards. The technology enables traders to analyze market data, detect patterns and respond instantly. In this talk, we will showcase how our system leverages Databricks\u2019 scalable infrastructures such as Structured Streaming to efficiently handle vast streams of financial data while ensuring low-latency performance. In addition, we will show how the integration of Databricks apps with Dash has empowered traders to rapidly develop and deploy custom dashboards, minimizing dependency on developers. Attendees will gain insights into our architecture, data processing techniques and lessons learned in integrating Databricks apps with Dash in order to drive rapid, data-driven trading decisions. /Data Engineer"}
{"session_id": "real-time-mode-technical-deep-dive-how-we-built-sub-300-millisecond", "title": "Real-Time Mode Technical Deep Dive: How We Built Sub-300 Millisecond Streaming Into Apache Spark\u2122", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark", "Machine Learning", "Real-time", "SQL", "Streaming"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Real-time mode is a new low-latency execution mode for Apache Spark\u2122 Structured Streaming. It can consistently provide p99 latencies less than 300 milliseconds for a broad set of stateless and stateful streaming queries. Our talk focuses on the technical aspects of making this possible in Spark. We\u2019ll dive into the core architecture that enables these dramatic latency improvements, including a concurrent stage scheduler and a non-blocking shuffle. We\u2019ll explore how we maintained Spark\u2019s fault-tolerance guarantees, and we\u2019ll also share specific optimizations we made to our streaming SQL operators. These architectural improvements have already enabled Databricks customers to build workloads with latencies up to 10x lower than before. Early adopters in our Private Preview have successfully implemented real-time enrichment pipelines and feature engineering for machine learning \u2014 use cases that were previously impossible at these latencies. /Staff Software Engineer\nDatabricks /Software Engineer"}
{"session_id": "real-world-use-cases-gaming-data-and-ai", "title": "Real World Use Cases for Gaming with Data and AI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this breakout session you will hear from some of the biggest names in Gaming. FanDuel will share their experiences with Databricks and Monte Carlo simulations and GeoComply will share how they\u2019ve integrated Databricks into their solution to help Gaming customers across the globe address regulatory requirements and keep bettors safe. /Sr. Solutions Architect"}
{"session_id": "real-world-use-cases-gaming-data-and-ai-fanduel", "title": "Real World Use Cases for Gaming With Data and AI at FanDuel", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Data Quality"], "speakers": ["Sr. Director, Data Governance, FanDuel"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session you'll receive an initial primer on different ways that Databricks is serving gaming worldwide. You will then hear from FanDuel regarding how they've found quality is always your best bet. They will share how reinforcing your data through Governance and Data Quality using Monte Carlo and Databricks has helped them further their success. /Global Games GTM Leader\nDatabricks /Ludia /Data Governance Engineer\nYelp /Sr. Director, Data Governance"}
{"session_id": "recsys-topic-modeling-and-agents-bridging-genai-traditional-ml-divide", "title": "RecSys, Topic Modeling and Agents: Bridging the GenAI-Traditional ML Divide", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr GenAI Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The rise of GenAI has led to a complete reinvention of how we conceptualize Data + AI. In this breakout, we will recontextualize the rise of GenAI in traditional ML paradigms, and hopefully unite the pre- and post-LLM eras. We will demonstrate when and where GenAI may prove more effective than traditional ML algorithms, and highlight problems for which the wheel is unnecessarily being reinvented with GenAI. This session will also highlight how MLflow provides a unified means of benchmarking traditional ML against GenAI, and lay out a vision for bridging the divide between Traditional ML and GenAI practitioners. /Sr GenAI Product Specialist"}
{"session_id": "redesigning-kaizens-cloud-data-lake-future", "title": "Redesigning Kaizen's Cloud Data Lake for the Future", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Scala"], "speakers": ["DevOps Engineer, Kaizen Gaming"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Kaizen Gaming, data drives our decision-making, but rapid growth exposed inefficiencies in our legacy cloud setup \u2014 escalating costs, delayed insights and scalability limits. Operating in 18 countries with 350M daily transactions (1PB+), shared quotas and limited cost transparency hindered efficiency. To address this, we redesigned our cloud architecture with Data Landing Zones, a modular framework that decouples resources, enabling independent scaling and cost accountability. Automation streamlined infrastructure, reduced overhead and enhanced FinOps visibility, while Unity Catalog ensured governance and security. Migration challenges included maintaining stability, managing costs and minimizing latency. A phased approach, Delta Sharing, and DBx Asset Bundles simplified transitions. The result: faster insights, improved cost control and reduced onboarding time, fostering innovation and efficiency. We share our transformation, offering insights for modern cloud optimization. /Data Platform Team Lead\nKaizen Gaming /Senior SRE/DevOps Engineer"}
{"session_id": "reduce-risk-while-you-improve-services-and-operations", "title": "Reduce Risk While You Improve Services and Operations", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Supporting the scale of Public Sector data and the need to protect sensitive information is essential to public sector organizations. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "reducing-transaction-conflicts-databricks-fundamentals-and-applications", "title": "Reducing Transaction Conflicts in Databricks\u2014Fundamentals and Applications at Asana", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Asana"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "When using ACID-guaranteed transactions on Databricks concurrently, we can run into transaction conflicts. The first part of this talk discusses the basics of concurrent transaction functionality in Databricks\u2014what happens when various combinations of INSERT, UPDATE and MERGE INTO happen concurrently. We discuss how table isolation level, partitioning and deletion vectors affect this. The second part of this talk focuses on a particular pipeline evolution at Asana to reduce transaction conflicts. As the number of writers to a table grew, we first implemented writer-specific partitioning to reduce transaction conflicts. Later on, we implemented an intermediate blind append stage to be able to avoid transaction conflicts while leveraging liquid clustering rather than partitioning for improved read and write performance. /Software Engineer"}
{"session_id": "reimagining-data-governance-and-access-atlassian", "title": "Reimagining Data Governance and Access at Atlassian", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Scala"], "speakers": ["Senior Software Engineer, Atlassian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Atlassian is rebuilding its central lakehouse from the ground up to deliver a more secure, flexible and scalable data environment. In this session, we\u2019ll share how we leverage Unity Catalog for fine-grained governance and supplement it with Immuta for dynamic policy management, enabling row and column level security at scale. By shifting away from broad, monolithic access controls toward a modern, agile solution, we\u2019re empowering teams to securely collaborate on sensitive data without sacrificing performance or usability. Join us for an inside look at our end-to-end policy architecture, from how data owners declare metadata and author policies to the seamless application of access rules across the platform. We\u2019ll also discuss lessons learned on streamlining data governance, ensuring compliance, and improving user adoption. Whether you\u2019re a data architect, engineer or leader, walk away with actionable strategies to simplify and strengthen your own governance and access practices. /Senior Software Engineer"}
{"session_id": "reinvent-government-data-intelligence-era", "title": "Reinvent Government in an Data Intelligence Era", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Streaming"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "To dramatically transform the way citizen services are delivered, organizations must bring all data together \u2014 streaming, structured and unstructured \u2014 in a secure and governed platform. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "responsible-ai-scale-balancing-democratization-and-regulation-financial", "title": "Responsible AI at Scale: Balancing Democratization and Regulation in the Financial Sector", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["EVP and Chief Architect for State Street, State Street"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/EVP and Chief Architect for State Street"}
{"session_id": "retail-consumer-goods-industry-forum-how-ai-transforming-how-brands", "title": "Retail & Consumer Goods Industry Forum: How AI is Transforming How Brands Connect With Consumers", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS APPS", "DELTA SHARING"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Real-time"], "speakers": ["SVP, Head of Enterprise Data, PepsiCo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Consumer industries are being transformed by AI as physical and digital experiences converge. In this flagship session for retail, travel, restaurants and consumer goods attendees at Data + AI Summit, Databricks and a panel of industry leaders will explore how real-time data and machine learning are enabling brands to gain deeper consumer insights, personalize interactions and move closer to true 1:1 marketing. From AI agents shopping on behalf of consumers to consumer-centric supply chains, discover how the most innovative companies will use AI to reshape customer relationships and drive growth in an increasingly connected world. /Global Head of Brand Intelligence, Data & Analytics\nValentino /Global VP | Consumer Industries GTM\nDatabricks /Head of Consumer Industries Marketing\nDatabricks /SVP, Head of Enterprise Data"}
{"session_id": "retail-genie-no-code-ai-apps-empowering-bi-users-be-self-sufficient", "title": "Retail Genie: No-Code AI Apps for Empowering BI Users to be Self-Sufficient", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore how Databricks AI/BI Genie revolutionizes retail analytics, empowering business users to become self-reliant data explorers. This session highlights no-code AI apps that create a conversational interface for retail data analysis. Genie spaces harness NLP and generative AI to convert business questions into actionable insights, bypassing complex SQL queries. We'll showcase retail teams effortlessly analyzing sales trends, inventory and customer behavior through Genie's intuitive interface. Witness real-world examples of AI/BI Genie's adaptive learning, enhancing accuracy and relevance over time. Learn how this technology democratizes data access while maintaining governance via Unity Catalog integration. Discover Retail Genie's impact on decision-making, accelerating insights and cultivating a data-driven retail culture. Join us to see the future of accessible, intelligent retail analytics in action. /Sr. Solutions Architect\nDatabricks /Solutions Architect"}
{"session_id": "revolutionizing-banking-data-analytics-and-ai-building-enterprise-data", "title": "Revolutionizing Banking Data, Analytics and AI: Building an Enterprise Data Hub With Databricks", "track": "DATA WAREHOUSING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Chief Information Officer, First Horizon Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore the transformative journey of a regional bank as it modernizes its enterprise data infrastructure amidst the challenges of legacy systems and past mergers and acquisitions. The bank is creating an Enterprise Data Hub using Deloitte's industry experience and the Databricks Data Intelligence Platform to drive growth, efficiency and Large Financial Institution readiness needs. This session will showcase how the new data hub will be a one-stop-shop for LOB and enterprise needs, while unlocking the advanced analytics and GenAI possibilities. Discover how this initiative is going to empower the ambitions of a regional bank to realize their \u201cbig bank muscle, small bank hustle.\u201d /Principal\nDeloitte /Chief Information Officer"}
{"session_id": "revolutionizing-counterparty-credit-risk-saccr-how-morgan-stanley", "title": "Revolutionizing Counterparty Credit Risk (SACCR) \u2013 How Morgan Stanley Scaled With Databricks", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how Morgan Stanley scaled one of their most significant regulatory calculators (SACCR) by leveraging Databricks for horizontal and vertical scaling. Discover how we harnessed Databricks to improve performance, improve calculation accuracy, regulatory compliance and more. /Executive Director\nMorgan Stanley /Sr. Solutions Architect"}
{"session_id": "revolutionizing-cybersecurity-scbs-journey-self-managed-siem", "title": "Revolutionizing Cybersecurity: SCB's Journey to a Self-Managed SIEM", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Global Head, Cybersecurity Operations, Standard Chartered Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to explore how Standard Chartered Bank's (SCB) groundbreaking strategy is reshaping the future of the cybersecurity landscape by replacing traditional SIEM with a cutting-edge Databricks solution, achieving remarkable business outcomes: This talk unveils SCB's journey to a distributed, multi-cloud lakehouse architecture that unlocks unprecedented performance and commercial optimization. Explore why a unified data and AI platform is becoming the cornerstone of next-generation, self-managed SIEM solutions for forward-thinking organizations in this era of AI-powered banking transformation. /Global Head, Cybersecurity Operations"}
{"session_id": "revolutionizing-data-insights-and-buyer-experience-gm-financial-cloud", "title": "Revolutionizing Data Insights and the Buyer Experience at GM Financial with Cloud Data Modernization", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance"], "speakers": ["Head Senior VP, Data Analytics, GM Financial"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Deloitte and GM (General Motors) Financial have collaborated to design and implement a cutting-edge cloud analytics platform, leveraging Databricks. In this session, we will explore how we overcame challenges including dispersed and limited data capabilities, high-cost hardware and outdated software, with a strategic and comprehensive approach. With the help of Deloitte and Databricks, we were able to develop a unified Customer360 view, integrate advanced AI-driven analytics, and establish robust data governance and cyber security measures. Attendees will gain valuable insights into the benefits realized, such as cost savings, enhanced customer experiences, and broad employee upskilling opportunities. Unlock the impact of cloud data modernization and advanced analytics in the automotive finance industry and beyond with Deloitte and Databricks. /Principal\nDeloitte Consulting, LLP /Head Senior VP, Data Analytics"}
{"session_id": "revolutionizing-insurance-how-drive-growth-and-innovation", "title": "Revolutionizing Insurance: How to Drive Growth and Innovation", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Modeling", "Real-time"], "speakers": ["Standard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The insurance industry is rapidly evolving as advances in data and artificial intelligence (AI) drive innovation, enabling more personalized customer experiences, streamlined operations, and improved efficiencies. With powerful data analytics and AI-driven solutions, insurers can automate claims processing, enhance risk management, and make real-time decisions. Leveraging insights from large and complex datasets, organizations are delivering more customer-centric products and services than ever before. Key takeaways: Real-world applications of data and AI in claims automation, underwriting, and customer engagementHow predictive analytics and advanced data modeling help anticipate risks and meet customer needs. Personalization of policies, optimized pricing, and more efficient workflows for greater ROI. Discover how data and AI are fueling growth, improving protection, and shaping the future of the insurance industry! /Principal Solutions Architect\nDatabricks /Standard"}
{"session_id": "revolutionizing-nuclear-ai-hive-and-bertha-databricks-architecture", "title": "Revolutionizing Nuclear AI With HiVE and Bertha on Databricks Architecture", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["CTO, EVP R&D and Innovation, Westinghouse Electric Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session we will explore the revolutionary advancements in nuclear AI capabilities with HiVE and Bertha on Databricks architecture. HiVE, developed by Westinghouse, leverages over a century of proprietary data to deliver unparalleled AI capabilities. At its core is Bertha, a generative AI model designed to tackle the unique challenges of the nuclear industry. This session will delve into the technical architecture of HiVE and Bertha, showcasing how Databricks' scalable environment enhances their performance. We will discuss the secure data infrastructure supporting HiVE, ensuring data integrity and compliance. Real-world applications and use cases will demonstrate the impact of HiVE and Bertha on improving efficiency, innovation and safety in nuclear operations. Discover how the fusion of HiVE and Bertha with Databricks architecture is transforming the nuclear AI landscape and driving the future of nuclear technology. /CTO, EVP R&D and Innovation"}
{"session_id": "revolutionizing-pepsico-bi-capabilities-traditional-bi-next-gen", "title": "Revolutionizing PepsiCo BI Capabilities: From Traditional BI to Next-Gen Analytics Powerhouse", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Director, Data Services, PepsiCo"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will provide an in-depth overview of how PepsiCo, a global leader in food and beverage, transformed its outdated data platform into a modern, unified and centralized data and AI-enabled platform using the Databricks SQL serverless environment. Through three distinct implementations that transpired at PepsiCo in 2024, we will demonstrate how the PepsiCo Data Analytics & AI Group unlocked pivotal capabilities that facilitated the delivery of diverse data-driven insights to the business, reduced operational expenses and enhanced overall performance through the newly implemented platform. /Lead Global D&AI Solution Architect\nPepsiCo Inc. /Director, Data Services"}
{"session_id": "route-success-scalable-routing-agents-databricks-and-dspy", "title": "Route to Success: Scalable Routing Agents With Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Data Scientist - ML Practice, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As companies increasingly adopt Generative AI, they're faced with a new challenge: managing multiple AI assistants. What if you could have a single, intuitive interface that automatically directs questions to the best assistant for the task? Join us to discover how to implement a flexible Routing Agent that streamlines working with multiple AI Assistants. We'll show you how to leverage Databricks and DSPy 3.0 to simplify adding this powerful pattern to your system. We'll dive into the essential aspects including: We'll share real-world examples that you can apply today. You'll leave with the knowledge to make your AI system run smoothly and efficiently. /Staff Data Scientist - ML Practice"}
{"session_id": "rust-and-lakehouse-format-ask-us-anything", "title": "Rust and Lakehouse Format \u2014 Ask Us Anything", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT"], "speakers": ["Valued Employee, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an in-depth Ask Me Anything (AMA) on how Rust is revolutionizing Lakehouse formats like Delta Lake and Apache Iceberg through projects like delta-rs and iceberg-rs! Discover how Rust\u2019s memory safety, zero-cost abstractions and fearless concurrency unlock faster development and higher-performance data operations. Whether you\u2019re a data engineer, Rustacean or Lakehouse enthusiast, bring your questions on how Rust is shaping the future of open table formats! /Staff Developer Advocate\nDatabricks /PM Director, Developer Relations\nDatabricks /Valued Employee"}
{"session_id": "saas-data-ingestion-lakeflow-connect", "title": "SaaS Data Ingestion With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality"], "speakers": ["Sr. Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Are you looking to make informed business decisions by analyzing data from multiple SaaS platforms, but struggling with custom APIs, governance and compliance? Join us as we showcase how Databricks built our ingestion platform using our own Lakeflow Connect product that seamlessly ingests data from all our SaaS systems while addressing challenges like fragmentation, data quality, discoverability, observability and governance. Our streamlined approach empowers your teams to focus on strategic insights instead of the technical burdens of data management, ensuring robust compliance and a faster path to actionable intelligence. /Software Engineer\nDatabricks /Sr. Engineering Manager"}
{"session_id": "saving-millions-millions-navigating-towards-cost-efficiency-pinterests", "title": "Saving Millions From Millions: Navigating Towards Cost-Efficiency in Pinterest's Spark Jobs", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Staff Software Engineer, Pinterest"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "While Spark offers powerful processing capabilities for massive data volumes, cost-efficiency challenges are always bothering users operating at large scales. At Pinterest, where we run millions of Spark jobs monthly, maintaining infra cost efficiency is crucial to support our rapid business growth. To tackle this challenge, we have developed several strategies that have saved us tens of millions of dollars across numerous job instances. We will share our analytical methodology for identifying performance bottlenecks, and the technical solutions to overcome various challenges. Our approach includes extracting insights from billions of collected metrics, leveraging remote shuffle services to address shuffle slowness and improve memory utilization and reduce costs while hosting hundreds of millions of pods. The presentation aims to trigger more discussions about cost efficiency topics of Apache Spark in the community and help the community to tackle the common challenge. /Staff Software Engineer"}
{"session_id": "scalable-data-governance-across-hundreds-thousands-tables", "title": "Scalable Data Governance Across Hundreds of Thousands of Tables", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Data Quality"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Your lakehouse is full of data \u2014 but do you know which datasets are truly trustworthy and high quality? This session introduces the Data Governance Score, a framework developed for Databricks\u2019 internal lakehouse to evaluate, surface and enforce best practices in data governance at the per-UC-table level. Designed to scale across hundreds of thousands of tables, it provides a systematic approach to ensuring data quality. We\u2019ll share our journey toward enhancing visibility and governance in Unity Catalog, with the ultimate goal of maximizing data reliability and trustworthiness \u2014 critical pillars of any Data Intelligent Platform. /Staff Software Engineer\nDatabricks /Staff Software Engineer"}
{"session_id": "scaling-aibi-genie-best-practices-curating-and-managing-production", "title": "Scaling AI/BI Genie: Best Practices for Curating and Managing Production Spaces", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unlock Genie's full potential with best practices for curating, deploying and monitoring Genie spaces at scale. This session offers a deep dive into the latest enhancements and provides practical guidance on designing high-quality spaces, streamlining deployment workflows and implementing robust monitoring to ensure accuracy and performance in production. Ideal for teams aiming to scale conversational analytics, you\u2019ll leave with actionable strategies to keep your Genie spaces efficient, reliable and aligned with business outcomes. /Software Engineering Manager\nDatabricks /Product Manager"}
{"session_id": "scaling-blockchain-ml-databricks-graph-analytics-graph-machine-learning", "title": "Scaling Blockchain ML With Databricks: From Graph Analytics to Graph Machine Learning", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "MLFLOW", "PYTORCH"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Scala"], "speakers": ["Staff ML Engineer, Coinbase"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Coinbase leverages Databricks to scale ML on blockchain data, turning vast transaction networks into actionable insights. This session explores how Databricks\u2019 scalable infrastructure, powered by Delta Lake, enables real-time processing for ML applications like NFT floor price predictions. We\u2019ll show how GraphFrames helps us analyze billion-node transaction graphs (e.g., Bitcoin) for clustering and fraud detection, uncovering structural patterns in blockchain data. But traditional graph analytics has limits. We\u2019ll go further with Graph Neural Networks (GNNs) using Kumo AI, which learn from the transaction network itself rather than relying on hand-engineered features. By encoding relationships directly into the model, GNNs adapt to new fraud tactics, capturing subtle relationships that evolve over time. Join us to see how Coinbase is advancing blockchain ML with Databricks and deep learning on graphs. /Staff ML Engineer"}
{"session_id": "scaling-data-engineering-pipelines-preparing-credit-card-transactions", "title": "Scaling Data Engineering Pipelines: Preparing Credit Card Transactions Data for Machine Learning", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Delta Lake", "ELT"], "speakers": ["Lead Data Engineer, Mastercard"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We discuss two real-world use cases in big data engineering, focusing on constructing stable pipelines and managing storage at a petabyte scale. The first use case highlights the implementation of Delta Lake to optimize data pipelines, resulting in an 80% reduction in query time and a 70% reduction in storage space. The second use case demonstrates the effectiveness of the Workflows \u2018ForEach\u2019 operator in executing compute-intensive pipelines across multiple clusters, significantly reducing processing time from months to days. This approach involves a reusable design pattern that isolates notebooks into units of work, enabling data scientists to independently test and develop. /Director, Data Scientist\nMastercard /Lead Data Engineer"}
{"session_id": "scaling-data-governance-how-unity-catalog-empowering-picpays-data", "title": "Scaling Data Governance: How Unity Catalog is Empowering Picpay's Data Governance Strategy", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["LLAMA", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": ["Picpay"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With massive data volume and complexity, scaling data governance became a significant challenge. Centralizing metadata management, ensuring regulatory compliance and controlling data access across multiple platforms turned to be critical to maintaining efficiency and trust. /Data Manager\nPicPay /Picpay"}
{"session_id": "scaling-data-intelligence-nab-balancing-innovation-enterprise-grade", "title": "Scaling Data Intelligence at NAB: Balancing Innovation with Enterprise-Grade Governance", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Distinguished Engineer, National Australia Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, discover how National Australia Bank (NAB) is reshaping its data and AI strategy by positioning data as a strategic enabler. Driven by a vision to unlock data like electricity\u2014continuous and reliable\u2014NAB has established a scalable foundation for data intelligence that balances agility with enterprise-grade control. We'll delve into the key architectural, security, and governance capabilities underpinning this transformation, including Unity Catalog, Serverless, Lakeflow and GenAI. The session will highlight NAB's adoption of Databricks Serverless, platform security controls like private link, and persona-based data access patterns. Attendees will walk away with practical insights into building secure, scalable, and cost-efficient data platforms that fuel innovation while meeting the demands of compliance in highly regulated environments. /Senior Solutions Architect\nDatabricks /Distinguished Engineer"}
{"session_id": "scaling-data-quality-zillow-migrating-and-enhancing-data-quality", "title": "Scaling Data Quality at Zillow: Migrating and Enhancing Data Quality Systems on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "Scala"], "speakers": ["Sr. Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Zillow has well-established, comprehensive systems for defining and enforcing data quality contracts and detecting anomalies. In this session, we will share how we evaluated Databricks\u2019 native data quality features and why we chose DLT expectations for DLT pipelines, along with a combination of enforced constraints and self-defined queries for other job types. Our evaluation considered factors such as performance overhead, cost and scalability. We\u2019ll highlight key improvements over our previous system and demonstrate how these choices have enabled Zillow to enforce scalable, production-grade data quality. Additionally, we are actively testing Databricks\u2019 latest data quality innovations, including enhancements to lakehouse monitoring and the newly released DQX project from Databricks Labs. In summary, we will cover Zillow\u2019s approach to data quality in the lakehouse, key lessons from our migration and actionable takeaways. /Software Dev Engineer, Big Data\nZillow /Sr. Solutions Architect"}
{"session_id": "scaling-demand-forecasting-nikon-automating-camera-accessories-sales", "title": "Scaling Demand Forecasting at Nikon: Automating Camera Accessories Sales Planning with Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Senior Associate Researcher, Nikon Corporation"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Nikon, camera accessories are essential in meeting the diverse needs of professional photographers worldwide, making their timely availability a priority. Forecasting accessories, however, presents unique challenges including dependencies on parent products, sparse demand patterns, and managing predictions for thousands of items across global subsidiaries. To address this, we leveraged Databricks' unified data and AI platform to develop and deploy an automated, scalable solution for accessory sales planning. Our solution employs a hybrid approach that auto-selects best algorithm from a suite of ML and time-series models, incorporating anomaly detection and methods to handle sparse and low-demand scenarios. MLflow is utilized to automate model logging and versioning, enabling efficient management, and scalable deployment. The framework includes data preparation, model selection and training, performance tracking, prediction generation, and output processing for downstream systems. /Senior Associate Researcher"}
{"session_id": "scaling-genai-inference-prototype-production-real-world-lessons-speed", "title": "Scaling GenAI Inference From Prototype to Production: Real-World Lessons in Speed & Cost", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "EDUCATION, MEDIA AND ENTERTAINMENT", "technologies": ["DATA MARKETPLACE", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Engineer, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This lightning talk dives into real-world GenAI projects that scaled from prototype to production using Databricks\u2019 fully managed tools. Facing cost and time constraints, we leveraged four key Databricks features\u2014Workflows, Model Serving, Serverless Compute, and Notebooks\u2014to build an AI inference pipeline processing millions of documents (text and audiobooks). This approach enables rapid experimentation, easy tuning of GenAI prompts and compute settings, seamless data iteration and efficient quality testing\u2014allowing Data Scientists and Engineers to collaborate effectively. Learn how to design modular, parameterized notebooks that run concurrently, manage dependencies and accelerate AI-driven insights. Whether you're optimizing AI inference, automating complex data workflows or architecting next-gen serverless AI systems, this session delivers actionable strategies to maximize performance while keeping costs low. /Lead Engineer"}
{"session_id": "scaling-generative-ai-batch-inference-strategies-foundation-models", "title": "Scaling Generative AI: Batch Inference Strategies for Foundation Models", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Curious how to apply resource-intensive generative AI models across massive datasets without breaking the bank? This session reveals efficient batch inference strategies for foundation models on Databricks. Learn how to architect scalable pipelines that process large volumes of data through LLMs, text-to-image models and other generative AI systems while optimizing for throughput, cost and quality. Key takeaways: You'll discover how to process any scale of data through your generative AI models efficiently. /Engineering Lead, AI Serving\nDatabricks /Software Engineer"}
{"session_id": "scaling-identity-graph-ingestion-1m-eventssec-spark-streaming-delta", "title": "Scaling Identity Graph Ingestion to 1M Events/Sec with Spark Streaming & Delta Lake", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Sr Software Engineer, Adobe"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Adobe\u2019s Real-Time Customer Data Platform relies on the identity graph to connect over 70 billion identities and deliver personalized experiences. This session will showcase how the platform leverages Databricks, Spark Streaming and Delta Lake, along with 25+ Databricks deployments across multiple regions and clouds \u2014 Azure & AWS \u2014 to process terabytes of data daily and handle over a million records per second. The talk will highlight the platform\u2019s ability to scale, demonstrating a 10x increase in ingestion pipeline capacity to accommodate peak traffic during events like the Super Bowl. Attendees will learn about the technical strategies employed, including migrating from Flink to Spark Streaming, optimizing data deduplication, and implementing robust monitoring and anomaly detection. Discover how these optimizations enable Adobe to deliver real-time identity resolution at scale while ensuring compliance and privacy. /Sr. Data Engineer\nAdobe /Sr Software Engineer"}
{"session_id": "scaling-modern-mdm-databricks-delta-sharing-and-dun-bradstreet", "title": "Scaling Modern MDM With Databricks, Delta Sharing and Dun & Bradstreet", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA SHARING", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Real-time", "Scala"], "speakers": ["GM, Partnerships & Alliances, Dun & Bradstreet"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Master Data Management (MDM) is the foundation of a successful enterprise data strategy \u2014 delivering consistency, accuracy and trust across all systems that depend on reliable data. But how can organizations integrate trusted third-party data to enhance their MDM frameworks? How can they ensure that this master data is securely and efficiently shared across internal platforms and external ecosystems? This session explores how Dun & Bradstreet\u2019s pre-mastered data serves as a single source of truth for customers, suppliers and vendors \u2014 reducing duplication and driving alignment across enterprise systems. With Delta Sharing, organizations can natively ingest Dun & Bradstreet data into their Databricks environment and establish a scalable, interoperable MDM framework. Delta Sharing also enables secure, real-time distribution of master data across the enterprise ensuring that every system operates from a consistent and trusted foundation. /GM, Partnerships & Alliances"}
{"session_id": "scaling-real-time-fraud-detection-databricks-lessons-draftkings", "title": "Scaling Real-Time Fraud Detection With Databricks: Lessons From DraftKings", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Machine Learning", "Real-time", "Streaming"], "speakers": ["Principal Data Science Engineer, DraftKings"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At DraftKings, ensuring secure, fair gaming requires detecting fraud in real time with both speed and precision. In this talk, we\u2019ll share how Databricks powers our fraud detection pipeline, integrating real-time streaming, machine learning and rule-based detection within a PySpark framework. Our system enables rapid model training, real-time inference and seamless feature transformation across historical and live data. We use shadow mode to test models and rules in live environments before deployment. Collaborating with Databricks, we push online feature store performance and enhance real-time PySpark capabilities. We'll cover PySpark-based feature transformations, real-time inference, scaling challenges and our migration from a homegrown system to Databricks. This session is for data engineers and ML practitioners optimizing real-time AI workloads, featuring a deep dive, code snippets and lessons from building and scaling fraud detection. /Principal Software Engineers\nDraftkings /Principal Data Science Engineer"}
{"session_id": "scaling-sales-excellence-how-databricks-uses-its-own-tech-train-gtm", "title": "Scaling Sales Excellence: How Databricks Uses Its Own Tech to Train GTM Teams", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director, Sales Performance, APJ, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, discover how Databricks leverages the power of Gen AI, MosaicML, Model Serving and Databricks Apps to revolutionize sales enablement. We\u2019ll showcase how we built an advanced chatbot that equips our go-to-market team with the tools and knowledge needed to excel in customer-facing interactions. This AI-driven solution not only trains our salespeople but also enhances their confidence and effectiveness in demonstrating the transformative potential of Databricks to future customers. Attendees will gain insights into the architecture, development process and practical applications of this innovative approach. The session will conclude with an interactive demo, offering a firsthand look at the chatbot in action. Join us to explore how Databricks is using its own platform to drive sales excellence through cutting-edge AI solutions. /Senior Solutions Architect\nDatabricks /Director, Sales Performance, APJ"}
{"session_id": "scaling-smarter-technical-dive-how-databricks-optimizes-model-serving", "title": "Scaling Smarter: Technical Dive Into How Databricks Optimizes Model Serving", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn from the experts on how Databricks\u2019 Mosaic AI Model Serving delivers unparalleled speed and scalability for deploying AI models. This session delves into the architecture and innovations that showcase the impressive improvements in throughput for the AI-serving infrastructure that powers Mosaic AI. /Software Engineer\nDatabricks /Databricks"}
{"session_id": "scaling-success-how-banks-are-unlocking-growth-data-and-ai", "title": "Scaling Success: How Banks are Unlocking Growth With Data and AI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning"], "speakers": ["Chief Data and AI Officer, FIS GLOBAL"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Growth in banking isn\u2019t just about keeping pace\u2014it\u2019s about setting the pace. This session explores how leading banks leverage Databricks\u2019 Data Intelligence Platform to uncover new revenue opportunities, deepen customer relationships, and expand market reach. Hear from industry leaders who have transformed their growth strategies by harnessing the power of advanced analytics and machine learning. Learn how personalized customer experiences, predictive insights and unified data platforms are driving innovation and helping banks scale faster than ever. Key takeaways: Join us in discovering how data intelligence is redefining growth in banking and thriving throughout uncertainty. /Field CTO\nDatabricks /E&Y /Chief Data and AI Officer"}
{"session_id": "scaling-trust-bi-how-bolt-manages-thousands-metrics-across-databricks", "title": "Scaling Trust in BI: How Bolt Manages Thousands of Metrics Across Databricks, dbt, and Looker", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Staff Analytics Engineer, Bolt"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Managing metrics across teams can feel like everyone\u2019s speaking a different language, which often leads to loss of trust in numbers. Based on a real-world use case, we\u2019ll show you how to establish a governed source of truth for metrics that works at scale and builds a solid foundation for AI integration. You\u2019ll explore how Bolt.eu\u2019s data team governs consistent metrics for different data users and leverages Euno\u2019s automations to navigate the overlap between Looker and dbt. We\u2019ll cover best practices for deciding where your metrics belong and how to optimize engineering and maintenance workflows across Databricks, dbt and Looker. For curious analytics engineers, we\u2019ll dive into thinking in dimensions & measures vs. tables & columns and determining when pre-aggregations make sense. The goal is to help you contribute to a self-serve experience with consistent metric definitions, so business teams and AI agents can access the right data at the right time without endless back-and-forth. /Co-Founder & CEO\nEuno /Staff Analytics Engineer"}
{"session_id": "scaling-xgboost-spark-connect-ml-grace-blackwell", "title": "Scaling XGBoost With Spark Connect ML on Grace Blackwell", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Engineer, NVIDIA Semiconductor Co., Ltd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "XGBoost is one of the off-the-shelf gradient boosting algorithms for analyzing tabular datasets. Unlike deep learning, gradient-boosting decision trees require the entire dataset to be in memory for efficient model training. To overcome the limitation, XGBoost features a distributed out-of-core implementation that fetches data in batch, which benefits significantly from the latest NVIDIA GPUs and the NVLink-C2C\u2019s ultra bandwidth. In this talk, we will share our work on optimizing XGBoost using the Grace Blackwell super chip. The fast chip-to-chip link between the CPU and the GPU enables XGBoost to scale up without compromising performance. Our work has effectively increased XGBoost\u2019s training capacity to over 1.2TB on a single node. The approach is scalable to GPU clusters using Spark, enabling XGBoost to handle terabytes of data efficiently. We will demonstrate combining XGBoost out-of-core algorithms with the latest connect ML from Spark 4.0 for large model training workflows. /Engineer\nNvidia Corp /Engineer"}
{"session_id": "schiphol-groups-transformation-unity-catalog", "title": "Schiphol Group\u2019s Transformation to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR, TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Europe\u2019s third-busiest airport, Schiphol Group, is elevating its data operations by transitioning from a standard Databricks setup to the advanced capabilities of Unity Catalog. In this session, we will share the motivations, obstacles and strategic decisions behind executing a seamless migration in a large-scale environment \u2014 one that spans hundreds of workspaces and demands continuous availability. Gain insights into planning and governance, learn how to safeguard data integrity and maintain operational flow, and understand the process of integrating Unity Catalog\u2019s enhanced security and governance features. Attendees will leave with practical lessons from our hands-on experience, proven methods for similar migrations, and a clear perspective on the benefits this transition offers for complex, rapidly evolving organizations. /Manager, Field Engineering\nDatabricks /Solutions Architect"}
{"session_id": "searching-meaning-age-ai", "title": "Searching for Meaning in the Age of AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["CoFounder and CTO, You.com"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bryan McCann, You.com\u2019s co-founder and CTO, shares his journey from studying philosophy and meaning to the Stanford Computer Science Department working on groundbreaking AI research alongside Richard Socher. Right now, AI is reshaping everything we hold dear \u2014 our jobs, creativity, and identities. It\u2019s also our greatest source of inspiration. The Age of AI is simultaneously a Renaissance, Enlightenment, Industrial Revolution and likely source of humanity\u2019s greatest existential crisis. To surmount this, Bryan will discuss how he uses AI responses as new starting points rather than answers, building teams like neural networks optimized for learning and how the answer to our meaning crisis may be for humans to be more like AI. Exploring AI\u2019s impact on politics, economics, healthcare, education and culture, Bryan asserts that we must all take part in authoring humanity\u2019s new story \u2014 AI can inspire us to become something new, rather than merely replace what we are now. /CoFounder and CTO"}
{"session_id": "securely-deploying-aibi-all-users-your-enterprise", "title": "Securely Deploying AI/BI to All Users in Your Enterprise", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bringing AI/BI to every business user starts with getting security, access and governance right. In this session, we\u2019ll walk through the latest best practices for configuring Databricks accounts, setting up workspaces, and managing authentication protocols to enable secure and scalable onboarding. Whether you're supporting a small team or an entire enterprise, you'll gain practical insights to protect your data while ensuring seamless and governed access to AI/BI tools. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "securing-capital-markets-ai-powered-risk-management-resilience", "title": "Securing Capital Markets: AI-Powered Risk Management for Resilience", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "DATA MARKETPLACE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Machine Learning"], "speakers": ["Chief Data Officer, Moody's Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Mitigating risk is vital for protecting reputation, assets and clients in capital markets. This session highlights how firms use Databricks\u2019 Data Intelligence Platform to enhance risk management, ensure compliance and safeguard operations from emerging threats. Discover how advanced analytics and machine learning models enable anomaly detection, fraud prevention and precise regulatory management. Industry leaders share proactive risk strategies that balance security with operational efficiency. Key takeaways: Learn how data intelligence transforms risk management in capital markets, securing the future while driving success! /Senior Vice President\nState Street /Global Head of FS Industry Marketing\nDatabricks /Chief Data Officer"}
{"session_id": "securing-data-collaboration-deep-dive-security-frameworks-and-use-cases", "title": "Securing Data Collaboration: A Deep Dive Into Security, Frameworks, and Use Cases", "track": "DATA SHARING AND COLLABORATION", "level": "ADVANCED", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Scala"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will focus on the security aspects of Databricks Delta Sharing, Databricks Cleanrooms and Databricks Marketplace, providing an exploration of how these solutions enable secure and scalable data collaboration while prioritizing privacy. Highlights: /Work hard, have fun, make money\nDatabricks /Principal Product Specialist\nDatabricks /Specialist Solutions Architect"}
{"session_id": "securing-databricks-using-databricks-siem", "title": "Securing Databricks Using Databricks as SIEM", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Sr. Manager, Security Engineering\nDatabricks /Staff Software Engineer"}
{"session_id": "securing-future-how-banks-are-reducing-risk-data-and-ai", "title": "Securing the Future: How Banks are Reducing Risk With Data and AI", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["CIO, Nationwide Building SOCIETY"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Global Banking & EMEA FINSERV\nDatabricks /CIO"}
{"session_id": "selectively-overwrite-data-delta-lakes-dynamic-insert-overwrite", "title": "Selectively Overwrite Data With Delta Lake\u2019s Dynamic Insert Overwrite", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dynamic Insert Overwrite is an important Delta Lake feature that allows fine-grained updates by selectively overwriting specific rows, eliminating the need for full-table rewrites. For examples, this capability is essential for: In this lightning talk, we will: /Software Engineer\nDatabricks /Principal Software Engineer"}
{"session_id": "self-improving-agents-and-agent-evaluation-arize-databricks-ml-flow", "title": "Self-Improving Agents and Agent Evaluation With Arize & Databricks ML Flow", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Co-Founder and Chief Product Officer, Arize"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As autonomous agents become increasingly sophisticated and widely deployed, the ability for these agents to evaluate their own performance and continuously self-improve is essential. However, the growing complexity of these agents amplifies potential risks, including exposure to malicious inputs and generation of undesirable outputs. In this talk, we'll explore how to build resilient, self-improving agents. To drive self-improvement effectively, both the agent and the evaluation techniques must simultaneously improve with a continuously iterating feedback loop. Drawing from extensive real-world experiences across numerous productionized use cases, we will demonstrate practical strategies for combining tools from Arize, Databricks MLflow and Mosaic AI to evaluate and improve high-performing agents. /Co-Founder and Chief Product Officer"}
{"session_id": "self-service-assortment-and-space-analytics-walmart-scale", "title": "Self-Service Assortment and Space Analytics at Walmart Scale", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Integration"], "speakers": ["Sr. Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Assortment and space analytics optimizes product selection and shelf allocation to boost sales, improve inventory management and enhance customer experience. However, challenges like evolving demand, data accuracy and operational alignment hinder success. Older approaches struggled due to siloed tools, slow performance and poor governance. Databricks unified platform resolved these issues, enabling seamless data integration, high-performance analytics and governed sharing. The innovative AI/BI Genie interface empowered self-service analytics, driving non-technical user adoption. This solution helped Walmart cut time to value by 90% and saved $5.6M annually in FTE hours leading to increased productivity. Looking ahead, AI agents will let store managers and merchants execute decisions via conversational interfaces, streamlining operations and enhancing accessibility. This transformation positions retailers to thrive in a competitive, customer-centric market. /Senior Manager, Assortment & Space\nWalmart /Sr. Delivery Solutions Architect"}
{"session_id": "semiconductor-ai-success-marvells-data-ai-governance", "title": "Semiconductor AI Success: Marvell\u2019s Data + AI Governance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Security"], "speakers": ["Marvell Semiconductors Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Marvell\u2019s AI-driven solutions, powered by Databricks\u2019 Data Intelligence Platform, provide a robust framework for secure, compliant and transparent Data and AI workflows leveraging Data & AI Governance through Unity Catalog. Marvell ensures centralized management of data and AI assets with quality, security, lineage and governance guardrails. With Databricks Unity Catalog, Marvell achieves comprehensive oversight of structured and unstructured data, AI models and notebooks. Automated governance policies, fine-grained access controls and lineage tracking help enforce regulatory compliance while streamlining AI development. This governance framework enhances trust and reliability in AI-powered decision-making, enabling Marvell to scale AI innovation efficiently while minimizing risks. By integrating data security, auditability and compliance standards, Marvell is driving the future of responsible AI adoption with Databricks. /Solutions Architect\nDatabricks /Head of Data\nMarvell Technology, Inc. /Marvell Semiconductors Inc"}
{"session_id": "serverless-compute-notebooks-jobs-and-dlt", "title": "Serverless Compute for Notebooks, Jobs and DLT", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Databricks serverless compute revolutionizes data workflows by eliminating infrastructure management, enabling rapid scaling and optimizing costs for Notebooks, Jobs and DLT. This session will delve into the serverless architecture, highlighting its ability to dynamically allocate resources, reduce idle costs and simplify development cycles. Learn about recent advancements, including cost savings and practical strategies for migration and optimization. Tailored for Data Engineers and Architects, this talk will also explore use cases, features, limitations and future roadmap, empowering you to make informed infrastructure decisions while unlocking the full potential of Databricks\u2019 serverless capabilities. /Product Manager"}
{"session_id": "serverless-new-easy-button-how-hp-inc-used-serverless-turbocharge-their", "title": "Serverless as the New \"Easy Button\": How HP Inc. Used Serverless to Turbocharge Their Data Pipeline", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Scala"], "speakers": ["Senior Databricks Engineer, Zahlen Solutions"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How do you wrangle over 8TB of granular \u201chit-level\u201d website analytics data with hundreds of columns, all while eliminating the overhead of cluster management, decreasing runtime and saving money? In this session, we\u2019ll dive into how we helped HP Inc. use Databricks serverless compute and DLT to streamline Adobe Analytics data ingestion while making it faster, cheaper and easier to operate. We\u2019ll walk you through our full migration story \u2014 from managing unwieldy custom-defined AWS-based Apache Spark\u2122 clusters to spinning up Databricks serverless pipelines and workflows with on-demand scalability and near-zero overhead. If you want to simplify infrastructure, optimize performance and get more out of your Databricks workloads, this talk is for you. /CEO\nZahlen Solutions LLC /Senior Databricks Engineer"}
{"session_id": "servicenow-walks-talk-databricks-revolutionizing-go-market-ai", "title": "ServiceNow \u2018Walks the Talk\u2019 With Databricks: Revolutionizing Go-To-Market With AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Senior AI Product Management, ServiceNow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At ServiceNow, we\u2019re not just talking about AI innovation \u2014 we\u2019re delivering it. By harnessing the power of Databricks, we\u2019re reimagining Go-To-Market (GTM) strategies, seamlessly integrating AI at every stage of the deal journey \u2014 from identifying high-value leads to generating hyper-personalized outreach and pitch materials. In this session, learn how we\u2019ve slashed data processing times by over 90%, reducing workflows from an entire day to just 30 minutes with Databricks. This unprecedented speed enables us to deploy AI-driven GTM initiatives faster, empowering our sellers with real-time insights that accelerate deal velocity and drive business growth. As Agentic AI becomes a game-changer in enterprise GTM, ServiceNow and Databricks are leading the charge \u2014 paving the way for a smarter, more efficient future in AI-powered sales. /Senior NLP Data Scientist\nServiceNow /Senior AI Product Management"}
{"session_id": "shifting-left-setting-your-genai-ecosystem-work-business-analysts", "title": "Shifting Left \u2014 Setting up Your GenAI Ecosystem to Work for Business Analysts", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science"], "speakers": ["ML Innovation, Experian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Data and AI in 2022, Databricks pioneered the term to shift left in how AI workloads would enable less data science driven people to create their own apps. In 2025, we take a look at how Experian is doing on that journey. This session highlights Databricks services that assist with the shift left paradigm for Generative AI, including how AI/BI Genie helps with Generative analytics, and how Agent Studio helps with synthetic generation of test cases to validate model performance. /Head of AI/ML Innovation"}
{"session_id": "simon-denny-unfiltered-unscripted", "title": "Simon + Denny - Unfiltered & Unscripted", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture"], "speakers": ["PM Director, Developer Relations, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Two industry veterans have been debating data architecture, tearing apart trends and tinkering with tech for decades and they\u2019re bringing the conversation live \u2014 and you\u2019re in control. Got a burning question about lake structures or internal performance? Worried about AI taking over the world? Want straight-talking opinions on the latest hype? Need real-world advice from the people who the experts get advice from? Want to get the juicy behind-the-scenes gossip about any announcements and shockwaves from the Keynotes? This is your chance to have your questions answered! Submit your questions ahead of time or bring them on the day \u2014 no topic is off-limits (though there's always a risk of side quests into coffee, sci-fi, or the quirks of English weather). Come for the insights, stay for the chaos. /CTO\nAdvancing Analytics /PM Director, Developer Relations"}
{"session_id": "simplified-delta-sharing-network-security", "title": "Simplified Delta Sharing With Network Security", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Scala"], "speakers": ["Principal Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Sharing enables cross-domain sharing of data assets for collaboration. A practical concern providers and recipients face in doing so is the need to manually configure network and storage firewalls. This is particularly challenging for large-scale providers and recipients with strict compliance requirements. In this talk, we will describe our solution to fully eliminate these complexities. This enhances user experience, scalability and security, facilitating seamless data collaboration across diverse environments and cloud platforms. /Senior Staff Product Manager\nDatabricks /Principal Engineer"}
{"session_id": "simplify-data-ingest-and-egress-new-python-data-source-api", "title": "Simplify Data Ingest and Egress with the New Python Data Source API", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Data Integration", "Python"], "speakers": ["Sr. SSA, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data engineering teams are frequently tasked with building bespoke ingest and/or egress solutions for myriad custom, proprietary, or industry-specific data sources or sinks. Many teams find this work cumbersome and time-consuming. Recognizing these challenges, Databricks interviewed numerous companies across different industries to better understand their diverse data integration needs. This comprehensive feedback led us to develop the Python Data Source API for Apache Spark\u2122. /Sr. SSA"}
{"session_id": "simplifying-data-pipelines-dlt-beginners-guide", "title": "Simplifying Data Pipelines With DLT: A Beginner\u2019s Guide", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Pipeline", "ETL", "Real-time", "SQL", "Streaming"], "speakers": ["Data Engineer, 84.51"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As part of the new Lakeflow data engineering experience, DLT makes it easy to build and manage reliable data pipelines. It unifies batch and streaming, reduces operational complexity and ensures dependable data delivery at scale \u2014 from batch ETL to real-time processing. DLT excels at declarative change data capture, batch and streaming workloads, and efficient SQL-based pipelines. In this session, you\u2019ll learn how we\u2019ve reimagined data pipelining with DLT, including: Join us to see how DLT powers better analytics and AI with reliable, unified pipelines. /Senior Product Marketing Manager\nDatabricks /Data Engineer"}
{"session_id": "simplifying-migration-experience-unity-catalog", "title": "Simplifying the Migration Experience to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "smart-data-smarter-vehicles-building-foundation-future-transportation", "title": "Smart Data, Smarter Vehicles: Building the Foundation for the Future of Transportation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Architect, Boeing"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Data Management\nCARIAD SE /Architect"}
{"session_id": "smart-inbox-cutting-edge-ai-automated-customer-email-classification", "title": "Smart Inbox: A Cutting-Edge AI for Automated Customer Email Classification by ENGIE", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "PYTORCH", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["ENGIE"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era of digital communication overload, businesses struggle to efficiently process and categorize vast volumes of customer emails. Smart Inbox transforms this challenge into an opportunity by combining classic AI techniques with state-of-the-art generative AI, delivering a highly accurate and business-impactful email classification system. Built on Databricks\u2019 powerful data and AI ecosystem, Smart Inbox integrates semantic analysis, large language models and distributed computing to enhance classification precision and reduce manual processing efforts. By leveraging both structured and unstructured data insights, this hybrid AI approach ensures not only operational efficiency but also improved customer engagement and faster response times. This presentation will unveil the key innovations behind Smart Inbox, demonstrating how cutting-edge AI can transform customer interaction workflows, enhance operational agility and redefine the future of intelligent email processing. /ENGIE"}
{"session_id": "smart-vehicles-secure-data-recreating-vehicle-environments-privacy", "title": "Smart Vehicles, Secure Data: Recreating Vehicle Environments for Privacy-Preserving Machine Learning", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "APACHE SPARK", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Senior Data Scientist, Mercedes-Benz R&D"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As connected vehicles generate vast amounts of personal and sensitive data, ensuring privacy and security in machine learning (ML) processes is essential. This session explores how Trusted Execution Environments (TEEs) and Azure Confidential Computing can enable privacy-preserving ML in cloud environments. We\u2019ll present a method to recreate a vehicle environment in the cloud, where sensitive data remains private throughout model training, inference and deployment. Attendees will learn how Mercedes-Benz R&D North America builds secure, privacy-respecting personalized systems for the next generation of connected vehicles. /Senior Data Scientist"}
{"session_id": "smashing-silos-shaping-future-data-all-next-gen-ecosystem", "title": "Smashing Silos, Shaping the Future: Data for All in the Next-Gen Ecosystem", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Director, Core Data, Rivian Volkswagen Group Technology"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A successful data strategy requires the right platform and the ability to empower the broader user community by creating simple, scalable and secure patterns that lower the barrier to entry while ensuring robust data practices. Guided by the belief that everyone is a data person, we focus on breaking down silos, democratizing access and enabling distributed teams to contribute through a federated \"data-as-a-product\" model. We\u2019ll share the impact and lessons learned in creating a single source of truth on Unity Catalog, consolidated from diverse sources and cloud platforms. We\u2019ll discuss how we streamlined governance with Databricks Apps, Workflows and native capabilities, ensuring compliance without hindering innovation. We\u2019ll also cover how we maximize the value of that catalog by leveraging semantics to enable trustworthy, AI-driven self-service in AI/BI dashboards and downstream apps. Come learn how we built a next-gen data ecosystem that empowers everyone to be a data person. /Director, Core Data"}
{"session_id": "solving-exclusive-data-access-role-based-access-control", "title": "Solving Exclusive Data Access With Role-Based Access Control", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Do you have users that wear multiple hats over a day? Like working with data from various customers and hoping they don\u2019t inadvertently aggregate data? Or are they working on sensitive datasets such as clinical trials that should not be combined, or are data sets that are subject to regulations? We have a solution! In this session, we will present a new capability that allows users wearing multiple hats to switch roles in the Databricks workspace to work exclusively on a dedicated project, data of a particular client or clinical trial. When switching to a particular role, the workspace adapts in such a way that only workspace objects and UC data of that particular role are accessible. We will also showcase the administrative experience of setting up exclusive access using groups and UC permissions. /Director, Product Management\nDatabricks /Product Management"}
{"session_id": "somebody-set-us-bomb-identifying-list-bombing-end-users-email-anti-spam", "title": "Somebody Set Up Us the Bomb: Identifying List Bombing of End Users in an Email Anti-Spam Context", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "ETL", "Streaming"], "speakers": ["Security Research Technical Leader, Cisco Talos"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Traditionally, spam emails are messages a user does not want, containing some kind of threat like phishing. Because of this, detection systems can focus on malicious content or sender behavior. List bombing upends this paradigm. By abusing public forms such as marketing signups, attackers can fill a user's inbox with high volumes of legitimate mail. These emails don't contain threats, and each sender is following best practices to confirm the recipient wants to be subscribed, but the net effect for an end user is their inbox being flooded with dozens of emails per minute. This talk covers the the exploration and implementation for identifying this attack in our company's anti-spam telemetry: from reading and writing to Kafka, Delta table streaming for ETL workflows, multi-table liquid clustering design for efficient table joins, curating gold tables to speed up critical queries and using Delta tables as an auditable integration point for interacting with external services. /Security Research Technical Leader"}
{"session_id": "spaghetti-bowl-pipeline-dlt-efficiency", "title": "From Spaghetti Bowl Pipeline to DLT Efficiency", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Apache Spark", "Data Engineering", "Delta Lake", "ELT", "Scala"], "speakers": ["Analytics Engineer, Intermountain Healthcare"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today's data-driven world, the ability to efficiently manage and transform data is crucial for any organization. This presentation will explore the process of converting a complex and messy workflow into a clean and simple DLT pipeline at a large integrated health system, Intermountain Health. Alteryx is a powerful tool for data preparation and blending, but as workflows grow in complexity, they can become difficult to manage and maintain. DLT, on the other hand, offers a more democratized, streamlined and scalable approach to data engineering, leveraging the power of Apache Spark and Delta Lake. We will begin by examining a typical legacy workflow, identifying common pain points such as tangled logic, performance bottlenecks and maintenance challenges. Next, we will demonstrate how to translate this workflow into a DLT pipeline, highlighting key steps such as data transformation, validation and delivery. /Analytics Engineer"}
{"session_id": "spark-40-and-delta-40-streaming-data", "title": "Spark 4.0 and Delta 4.0 For Streaming Data", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Real-time"], "speakers": ["Chief Digital Technology Advisor, Shell"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Real-time data is one of the most important datasets for any Data and AI Platform across any industry. Spark 4.0 and Delta 4.0 include new features that make ingestion and querying of real-time data better than ever before. Features such as: In this presentation you will learn how data teams can leverage these latest features to build industry-leading, real-time data products using Spark and Delta and includes real world examples and metrics of the improvements they make in performance and processing of data in the real time space. /Chief Digital Technology Advisor"}
{"session_id": "spark-connect-flexible-local-access-apache-spark-scale", "title": "Spark Connect: Flexible, Local Access to Apache Spark at Scale", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "What if you could run Spark jobs without worrying about clusters, versions and upgrades? Did you know Spark has this functionality built-in today? Join us to take a look at this functionality \u2014 Spark Connect. Join us to dig into how Spark Connect works \u2014 abstracting away Spark clusters away in favor of the DataFrame API and unresolved logical plans. You will learn some of the cool things Spark Connect unlocks, including: /Databricks"}
{"session_id": "spark-databricks-tips-and-tricks", "title": "Spark on Databricks: Tips and Tricks", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark"], "speakers": ["Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores a collection of advanced and lesser-known use cases in Apache Spark\u2122, drawn from real-world scenarios and internal experimentation. Topics include: We'll also cover additional patterns and tooling tips that can help solve operational challenges and optimize performance in production Spark environments. /Specialist Solutions Architect\nDatabricks /Specialist Solutions Architect"}
{"session_id": "spark-right-sizing-secret-saving-millions-dollars-linkedin", "title": "Spark Right-Sizing: The Secret to Saving Millions of Dollars at LinkedIn", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": ["Senior Software Engineer, LinkedIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At LinkedIn, we manage over 400,000 daily Spark applications consuming 200+ PBHrs of compute daily. To address the challenges posed by manual configuration of Spark's memory tuning options, which led to low memory utilization and frequent OOM errors, we developed an automated Spark executor memory right-sizing system. Our approach, utilizing a policy-based system with nearline and real-time feedback loops, automates memory tuning, leading to more efficient resource allocation, improved user productivity and increased job reliability. By leveraging historical data and real-time error classification, we dynamically adjust memory, significantly narrowing the gap between allocated and utilized resources while reducing failures. This initiative has achieved a 13% increase in memory utilization and a 90% drop in OOM-related job failures, saving us 1000s of PBHrs of compute every year. /Senior Software Engineer"}
{"session_id": "sponsored-actian-beyond-lakehouse-unlocking-enterprise-wide-ai-ready", "title": "Sponsored by: Actian | Beyond the Lakehouse: Unlocking Enterprise-Wide AI-Ready Data with Unified Metadata Intelligence", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As organizations scale AI initiatives on platforms like Databricks, one challenge remains: bridging the gap between the data in the lakehouse and the vast, distributed data that lives elsewhere. Turning massive volumes of technical metadata into trusted, business-ready insight requires more than cataloging what's inside the lakehouse\u2014it demands true enterprise-wide intelligence. Actian CTO Emma McGrattan will explore how combining Databricks Unity Catalog with the Actian Data Platform extends visibility, governance, and trust beyond the lakehouse. Learn how leading enterprises are:"}
{"session_id": "sponsored-alation-better-together-enterprise-catalog-databricks-alation", "title": "Sponsored by: Alation | Better Together: Enterprise Catalog with Databricks & Alation at American Airlines", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the era of data-driven enterprises, true democratization requires more than just access\u2013it demands context, trust, and governance at scale. In this session, discover how to seamlessly integrate Databricks Unity Catalog with Alation\u2019s Enterprise Data Catalog to deliver:"}
{"session_id": "sponsored-astronomer-scaling-data-teams-future", "title": "Sponsored by: Astronomer | Scaling Data Teams for the Future", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering"], "speakers": ["CD, and Infrastructure-as-code) from the DevOps movement are gradually making their way into data engineering. We believe these changes have led to rise of DataOps a new wave best practices that will transform discipline But how do you reactive team proactive force for innovation? We\u2019ll explore key principles building resilient, high-impact team\u2014from structuring collaboration, testing, automation, leveraging modern orchestration tools. Whether you\u2019re leading or looking future-proof your career, you\u2019ll walk away with actionable insights on stay ahead in rapidly changing landscape."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The role of data teams and data engineers is evolving. No longer just pipeline builders or dashboard creators, today\u2019s data teams must evolve to drive business strategy, enable automation, and scale with growing demands. Best practices seen in the software engineering world (Agile development, CI/CD, and Infrastructure-as-code) from the DevOps movement are gradually making their way into data engineering. We believe these changes have led to the rise of DataOps and a new wave of best practices that will transform the discipline of data engineering. But how do you transform a reactive team into a proactive force for innovation? We\u2019ll explore the key principles for building a resilient, high-impact data team\u2014from structuring for collaboration, testing, automation, to leveraging modern orchestration tools. Whether you\u2019re leading a team or looking to future-proof your career, you\u2019ll walk away with actionable insights on how to stay ahead in the rapidly changing data landscape."}
{"session_id": "sponsored-astronomer-unlocking-future-data-orchestration-introducing", "title": "Sponsored by: Astronomer | Unlocking the Future of Data Orchestration: Introducing Apache Airflow\u00ae 3", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Airflow 3 is here, bringing a new era of flexibility, scalability, and security to data orchestration. This release makes building, running, and managing data pipelines easier than ever. In this session, we will cover the key benefits of Airflow 3, including: (1) Ease of Use: Airflow 3 rethinks the user experience\u2014from an intuitive, upgraded UI to DAG Versioning and scheduler-integrated backfills that let teams manage pipelines more effectively than ever before (2) Stronger Security: By decoupling task execution from direct database connections, Airflow 3 enforces task isolation and minimal-privilege access. This meets stringent compliance standards while reducing the risk of unauthorized data exposure. (3) Ultimate Flexibility: Run tasks anywhere, anytime with remote execution and event-driven scheduling. Airflow 3 is designed for global, heterogeneous modern data environments with an architecture that facilitates edge and hybrid-cloud to GPU-based deployments."}
{"session_id": "sponsored-aveva-aveva-and-databricks-it-ot-convergence-industrial", "title": "Sponsored by: AVEVA | AVEVA and Databricks IT-OT Convergence for Industrial Intelligence at Scale", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, HEALTH AND LIFE SCIENCES, MANUFACTURING", "technologies": ["AI/BI", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Business Intelligence", "Data Science", "ELT", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Industrial organizations are unlocking new possibilities through the partnership between AVEVA and Databricks. The seamless, no-code, zero-copy solution\u2014powered by Delta Sharing and CONNECT\u2014enables companies to combine IT and OT data effortlessly. By bridging the gap between operational and enterprise data, businesses can harness the power of AI, data science, and business intelligence at an unprecedented scale to drive innovation. In this session, explore real-world applications of this integration, including how industry leaders are using CONNECT and Databricks to boost efficiency, reduce costs, and advance sustainability\u2014all without fragmented point solutions. You\u2019ll also see a live demo of the integration, showcasing how secure, scalable access to trusted industrial data is enabling new levels of industrial intelligence across sectors like mining, manufacturing, power, and oil and gas."}
{"session_id": "sponsored-aws-buy-aws", "title": "Sponsored by: AWS | Buy with AWS", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATA MARKETPLACE", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AWS Marketplace is transforming how enterprises worldwide find, buy, deploy, and govern software, data, and professional services. Easily find and deploy the software, data, and services that you need from over 5,000+ Sellers with fast procurement and flexible pricing and terms. AWS Marketplace enables seamless integration with AWS services as well as consolidated AWS billing, centralized governance, and simplified vendor management. AWS delivers the solutions you need, when and where you need them including Buy with AWS direct on AWS Partner websites. Learn more about how AWS Marketplace is driving value for customers and AWS Partners."}
{"session_id": "sponsored-boomi-lp-pipelines-agents-manage-data-and-ai-one-platform", "title": "Sponsored by: Boomi, LP | From Pipelines to Agents: Manage Data and AI on One Platform for Maximum ROI", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the age of agentic AI, competitive advantage lies not only in AI models, but in the quality of the data agents reason on and the agility of the tools that feed them. To fully realize the ROI of agentic AI, organizations need a platform that enables high-quality data pipelines and provides scalable, enterprise-grade tools. In this session, discover how a unified platform for integration, data management, MCP server management, API management, and agent orchestration can help you to bring cohesion and control to how data and agents are used across your organization."}
{"session_id": "sponsored-capgemini-unlocking-business-value-sap-business-data-cloud", "title": "Sponsored by: Capgemini | Unlocking Business Value With SAP Business Data Cloud and Databricks: Real-World Use Cases", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how SAP Business Data Cloud and Databricks can transform your business by unifying SAP and non-SAP data for advanced analytics and AI. In this session, we\u2019ll highlight Optimizing Cash Flow with AI with integrated diverse data sources and AI algorithms that enable accurate cash flow forecasting to help businesses identify trends, prevent bottlenecks, and improve liquidity. You\u2019ll also learn about the importance of high-quality, well-governed data as the foundation for reliable AI models and actionable reporting. Key Takeaways: \u2022 How to integrate and leverage SAP and external data in Databricks \u2022 Using AI for predictive analytics and better decision-making \u2022 Building a trusted data foundation to drive business performance Leave this session with actionable strategies to optimize your data, enhance efficiency, and unlock new growth opportunities."}
{"session_id": "sponsored-capital-one-software-how-capital-one-balances-lower-cost-and", "title": "Sponsored by: Capital One Software | How Capital One Balances Lower Cost and Peak Performance in Databricks", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Companies need a lot of data to build and deploy AI models\u2014and they want it quickly. To meet this demand, platform teams are quickly scaling their Databricks usage, resulting in excess cost driven by inefficiencies and performance anomalies. Capital One has over 4,000 users leveraging Databricks to power advanced analytics and machine learning capabilities at scale. In this talk, we\u2019ll share lessons learned from optimizing our own Databricks usage while balancing lower cost with peak performance. Attendees will learn how to identify top sources of waste, best practices for cluster management, tips for user governance and methods to keep costs in check."}
{"session_id": "sponsored-capital-one-software-how-manage-high-quality-secure-data-and", "title": "Sponsored by: Capital One Software | How to Manage High-Quality, Secure Data and Cost Visibility for AI", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Companies need robust data management capabilities to build and deploy AI. Data needs to be easy to find, understandable, and trustworthy. And it\u2019s even more important to secure data properly from the beginning of its lifecycle, otherwise it can be at risk of exposure during training or inference. Tokenization is a highly efficient method for securing data without compromising performance. In this session, we\u2019ll share tips for managing high-quality, well-protected data at scale that are key for accelerating AI. In addition, we\u2019ll discuss how to integrate visibility and optimization into your compute environment to manage the hidden cost of AI \u2014 your data."}
{"session_id": "sponsored-coalesce-bringing-order-chaos-how-succeed-data-analytics", "title": "Sponsored by: Coalesce | Bringing Order to Chaos: How to Succeed in a Data & Analytics World", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Priorities shift, requirements change, resources fluctuate, and the demands on data teams are only continuing to grow. Join this session, led by Coalesce Sales Engineering Director, Michael Tantrum, to hear about the most efficient way to deliver high quality data to your organization at the speed they need to consume it. Learn how to sidestep the common pitfalls of data development for maximum data team productivity."}
{"session_id": "sponsored-coalesce-raw-data-real-time-retention-powering-customer", "title": "Sponsored by: Coalesce | From Raw Data to Real-Time Retention: Powering Customer Health Scores on Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Understanding customer engagement and retention isn\u2019t optional\u2014it\u2019s mission-critical. Join us for a live demo to see how you can build a scalable, governed customer health scoring model by transforming raw signals into actionable insights. Discover how Coalesce\u2019s low-code development platform works seamlessly with Databricks\u2019 lakehouse architecture to unify and operationalize customer data at scale. With built-in governance, automation, and metadata intelligence, you\u2019ll deliver trusted scores that support proactive decision-making across the business. Why Attend?"}
{"session_id": "sponsored-dbt-labs-cooking-data-success-how-chick-fil-optimizes-trust", "title": "Sponsored by: dbt Labs | Cooking Up Data Success: How Chick-fil-A Optimizes Trust, Efficiency and Speed with dbt & Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DBT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Accelerating development, gaining better control over data models, and increasing confidence in data accuracy benefit both internal stakeholders and customers. Join this session to discover how Chick-fil-A\u2014a leading fast-food chain renowned for its efficiency, customer service, and iconic chicken sandwiches\u2014is transforming its data operations with dbt and Databricks. Learn how their \u201cdata recipe,\u201d has enhanced trust, velocity, efficiency, and governance. Through streamlined development, improved collaboration, and higher data quality, Chick-fil-A is achieving faster time-to-market and reducing errors, driving a significant impact across its organization."}
{"session_id": "sponsored-dbt-labs-leveling-data-engineering-riot-how-we-rolled-out-dbt", "title": "Sponsored by: dbt Labs | Leveling Up Data Engineering at Riot: How We Rolled Out dbt and Transformed the Developer Experience", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DBT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Riot Games reduced its Databricks compute spend and accelerated development cycles by transforming its data engineering workflows\u2014migrating from bespoke Databricks notebooks and Spark pipelines to a scalable, testable, and developer-friendly dbt-based architecture. In this talk, members of the Developer Experience & Automation (DEA) team will walk through how they designed and operationalized dbt to support Riot\u2019s evolving data needs."}
{"session_id": "sponsored-deloitte-accelerating-biopharmaceutical-breakthroughs", "title": "Sponsored by: Deloitte | Accelerating Biopharmaceutical Breakthroughs with an Innovative Enterprise Data Strategy", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Data Integration", "Data Quality", "Real-time", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the rapidly evolving life sciences and healthcare industry, leveraging data-as-a-product is crucial for driving innovation and achieving business objectives. Join us to explore how Deloitte is revolutionizing data strategy solutions by overcoming challenges such as data silos, poor data quality, and lack of real-time insights with the Databricks Data Intelligence Platform. Learn how effective data governance, seamless data integration, and scalable architectures support personalized medicine, regulatory compliance, and operational efficiency. This session will highlight how these strategies enable biopharma companies to transform data into actionable insights, accelerate breakthroughs and enhance life sciences outcomes."}
{"session_id": "sponsored-deloitte-ai-innovation-and-governance-att-and-deloitte", "title": "Sponsored by: Deloitte | AI Innovation and Governance: AT&T and Deloitte Leverage Databricks for Secure AI Expansion", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AT&T and Deloitte are driving AI innovation across their respective enterprises. Both companies are leveraging the Databricks Data Intelligence Platform to enable AI expansion and adoption as well as to address the complexities and risks associated with AI implementation in large, dynamic organizations. This session will explore how AT&T and Deloitte are transforming their businesses with advanced AI and Gen AI solutions. Attendees will also gain insights into how AT&T and Deloitte are managing enterprise risks, security threats, and data governance challenges without stifling innovation. Both companies will discuss how Databricks Data Intelligence Platform is enabling secure and robust AI development in support of AI security and governance requirements."}
{"session_id": "sponsored-deloitte-transforming-nestle-usas-nusa-data-platform-unlock", "title": "Sponsored by: Deloitte | Transforming Nestl\u00e9 USA\u2019s (NUSA) data platform to unlock new analytics and GenAI capabilities", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Nestl\u00e9 USA, a division of the world\u2019s largest food and beverage company, Nestl\u00e9 S.A., has embarked on a transformative journey to unlock GenAI capabilities on their data platform. Deloitte, Databricks, and Nestl\u00e9 have collaborated on a data platform modernization program to address gaps associated with Nestl\u00e9\u2019s existing data platform. This joint effort introduces new possibilities and capabilities, ranging from development of advanced machine learning models, implementing Unity Catalog, and adopting Lakehouse Federation, all while adhering to confidentiality protocols. With help from Deloitte and Databricks, Nestl\u00e9 USA is now able to meet its advanced enterprise analytics and AI needs with the Databricks Data Intelligence Platform."}
{"session_id": "sponsored-ey-navigating-future-knowledge-powered-insights-ai", "title": "Sponsored by: EY | Navigating the Future: Knowledge-Powered Insights on AI, Information Governance, Real-Time Analytics", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["AI/BI", "DATABRICKS APPS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In an era where data drives strategic decision-making, organizations must adapt to the evolving landscape of business analytics. This session will focus on three pivotal themes shaping the future of data management and analytics in 2025. Join our panel of experts, including a Business Analytics Leader, Head of Information Governance, and Data Science Leader, as they explore: - Knowledge-Powered AI: Discover trends in Knowledge-Powered AI and how these initiatives can revolutionize business analytics, with real-world examples of successful implementations. - Information Governance: Explore the role of information governance in ensuring data integrity and compliance. Our experts will discuss strategies for establishing robust frameworks that protect organizational assets. - Real-Time Analytics: Understand the importance of real-time analytics in today\u2019s fast-paced environment. The panel will highlight how organizations can leverage real-time data for agile decision-making."}
{"session_id": "sponsored-ey-xoople-fueling-enterprise-ai-earth-data-intelligence", "title": "Sponsored by: EY | Xoople: Fueling enterprise AI with Earth data intelligence products", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Xoople aims to provide its users with trusted AI-Ready Earth data and accelerators that unlock new insights for enterprise AI. With access to scientific-grade Earth data that provides spatial intelligence on real-world changes, data scientists and BI analysts can increase forecast accuracy for their enterprise processes and models. These improvements drive smarter, data-driven business decisions across various business functions, including supply chain, finance, and risk across industries. Xoople, which has recently introduced their product, Enterprise AI-Ready Earth Data\u2122, on the Databricks Marketplace, will have their CEO, Fabrizio Pirondini, discuss the importance of the Databricks Data Intelligence Platform in making Xoople\u2019s product a reality for use in the enterprise."}
{"session_id": "sponsored-firebolt-10ms-queries-iceberg-turbocharging-your-lakehouse", "title": "Sponsored by: Firebolt | 10ms Queries on Iceberg: Turbocharging Your Lakehouse for Interactive Experiences with Firebolt", "track": "ANALYTICS AND BI", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Delta Lake", "ELT"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Open table formats such as Apache Iceberg or Delta Lake have transformed the data landscape. For the first time, we\u2019re seeing a real open storage ecosystem emerging across database vendors. So far, open table formats have found little adoption powering low-latency, high-concurrency analytics use-cases. Data stored in open formats often gets transformed and ingested into closed systems for serving. The reason for this is simple: most modern query engines don\u2019t properly support these workloads. In this talk we take a look under the hood of Firebolt and dive into the work we\u2019re doing to support low-latency and high concurrency on Iceberg: caching of data and metadata, adaptive object storage reads, subresult reuse, and multi-dimensional scaling. After this session, you will know how you can build low-latency data applications on top of Iceberg. You\u2019ll also have a deep understanding of what it takes for modern high-performance query engines to do well on these workloads."}
{"session_id": "sponsored-firebolt-power-low-latency-data-ai-apps", "title": "Sponsored by: Firebolt | The Power of Low-latency Data for AI Apps", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retrieval-augmented generation (RAG) has transformed AI applications by grounding responses with external data. It can be better. By pairing RAG with low latency SQL analytics, you can enrich responses with instant insights, leading to a more interactive and insightful user experience with fresh, data-driven intelligence. In this talk, we\u2019ll demo how low latency SQL combined with an AI application can deliver speed, accuracy, and trust."}
{"session_id": "sponsored-fivetran-scalable-data-ingestion-building-custom-pipelines", "title": "Sponsored by: Fivetran | Scalable Data Ingestion: Building custom pipelines with the Fivetran Connector SDK and Databricks", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Integration", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Organizations have hundreds of data sources, some of which are very niche or difficult to access. Incorporating this data into your lakehouse requires significant time and resources, hindering your ability to work on more value-add projects. Enter the Fivetran Connector SDK- a powerful new tool that enables your team to create custom pipelines for niche systems, custom APIs, and sources with specific data filtering requirements, seamlessly integrating with Databricks. During this session, Fivetran will demonstrate how to (1) Leverage the Connector SDK to build scalable connectors, enabling the ingestion of diverse data into Databricks (2) Gain flexibility and control over historical and incremental syncs, delete capture, state management, multithreading data extraction, and custom schemas (3) Utilize practical examples, code snippets, and architectural considerations to overcome data integration challenges and unlock the full potential of your Databricks environment."}
{"session_id": "sponsored-fivetran-under-hood-general-motors-data-engine-supply-chain", "title": "Sponsored by: Fivetran | Under the Hood: General Motor\u2019s Data Engine for Supply Chain Innovation", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING", "technologies": ["DATABRICKS APPS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "General Motors, a global leader in the automotive and manufacturing industry, is on a mission to modernize its data infrastructure to drive supply chain efficiency and unlock new software-based revenue opportunities. By leveraging Fivetran\u2019s data movement platform, GM can reliably capture real-time data from a complex landscape of SaaS and on-premises sources and securely ingest it into the Databricks Data Intelligence Platform. Join this session to learn:"}
{"session_id": "sponsored-impetus-supercharge-ai-automated-migration-databricks-impetus", "title": "Sponsored by: Impetus | Supercharge AI with automated migration to Databricks with Impetus", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Migrating legacy workloads to a modern, scalable platform like Databricks can be complex and resource-intensive. Impetus, an Elite Databricks Partner and the Databricks Migration Partner of the Year 2024, simplifies this journey with LeapLogic, an automated solution for data platform modernization and migration services. LeapLogic intelligently discovers, transforms, and optimizes workloads for Databricks, ensuring minimal risk and faster time-to-value. In this session, we\u2019ll showcase real-world success stories of enterprises that have leveraged Impetus\u2019 LeapLogic to modernize their data ecosystems efficiently. Join us to explore how you can accelerate your migration journey, unlock actionable insights, and future-proof your analytics with a seamless transition to Databricks."}
{"session_id": "sponsored-infosys-ai-driven-growth-expedite-potential-agentic-ai-and", "title": "Sponsored by: Infosys | AI-Driven Growth: Expedite Potential of Agentic AI and Drive Beyond Customer Experience and Operational Efficiency", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "MOSAIC AI", "UNITY CATALOG"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Agentic AI has the power to revolutionize mission-critical domains. Yet this journey is not without its challenges \u2013 the inevitable barriers, frustrations, and setbacks that mark all progress. This session dives into how Infosys Topaz helps enterprises to strategically implement AI at scale in as little as two months to personalize customer journeys, optimize operations, and unlock new revenue streams. Learn how different enterprises have architected foundation capabilities such as Agentic AI factory to build & accommodate hundreds or even thousands of intelligent agents, setting up data fingerprinting and data harvesting to make enterprise Data ready for AI and ensuring interoperability among diverse AI systems"}
{"session_id": "sponsored-infosys-beyond-hype-scale-democratize-agentic-ai-across", "title": "Sponsored by: Infosys | Beyond Hype: Scale & Democratize Agentic AI across enterprise to realize business outcomes.", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Agentic AI and multimodal data are the next frontiers for realizing intelligent and autonomous business systems. Learn how Infosys innovates with Databricks for accelerating data to AI agent journey at scale across an enterprise. Hear our pragmatic capability driven approach instead of use case-based approach to bring the data universe, AI foundations, agent management, data and AI governance and collaboration under unified management."}
{"session_id": "sponsored-infosys-data-ai-governance-action-policy-practice", "title": "Sponsored by: Infosys | Data & AI Governance in Action: From Policy to Practice", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session focuses on the practical aspects of implementing AI governance within organizations. Learn how to move beyond policy documents to establish effective governance structures, integrate ethical AI practices into daily operations, and utilize tools and frameworks to ensure responsible AI development. Gain insights on transforming AI governance from theory to practice"}
{"session_id": "sponsored-microsoft-leverage-power-microsoft-ecosystem-azure-databricks", "title": "Sponsored by: Microsoft | Leverage the power of the Microsoft Ecosystem with Azure Databricks", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS", "DATABRICKS WORKFLOWS"], "duration": "", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for this insightful session to learn how you can leverage the power of the Microsoft ecosystem along with Azure Databricks to take your business to the next level. Azure Databricks is a fully integrated, native, first-party solution on Microsoft Azure. Databricks and Microsoft continue to actively collaborate on product development, ensuring tight integration, optimized performance, and a streamlined support experience. Azure Databricks offers seamless integrations with Power BI, Azure Open AI, Microsoft Purview, Azure Data Lake Storage (ADLS) and Foundry. In this session, you\u2019ll learn how you can leverage deep integration between Azure Databricks and the Microsoft solutions to empower your organization to do more with your data estate. You\u2019ll also get an exclusive sneak peek into the product roadmap."}
{"session_id": "sponsored-monte-carlo-investing-data-quality-nasdaqs-journey-data-ai", "title": "Sponsored by: Monte Carlo | Investing in Data Quality: Nasdaq\u2019s Journey to Data + AI Reliability", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As one of the world\u2019s largest stock exchanges, Nasdaq knows that high-quality data + AI isn\u2019t just a nice-to-have \u2013 it\u2019s mission critical. That means choosing technology partners that can deliver on performance, scale, and reliability is just as important. That\u2019s why their team has invested heavily in improving their organization\u2019s data quality with data + AI observability. Join Michael Weiss to learn about the evolution of data at Nasdaq, how they partner with Monte Carlo to meet the needs of their modern data + AI challenges in their highly-regulated and scrutinized industry, and best practices and tips for how to implement a data quality program at any scale."}
{"session_id": "sponsored-moveworks-unlocking-full-stack-ai-transformation-moveworks", "title": "Sponsored by: Moveworks | Unlocking Full-stack AI Transformation with the Moveworks Platform", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "PARTNER CONNECT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how visionaries from the world\u2019s leading organizations use Moveworks to give employees a single place to find information, automate tasks, and be more productive. See the Moveworks AI Assistant in action and experience how its reasoning-based architecture allows it to be a one-stop-shop for all employee requests (across IT, HR, finance, sales, and more), how Moveworks empowers developers to easily build new AI agents atop this architecture, and how we give stakeholders tools to implement effective AI governance. Finally, experience how customers and partners alike leverage information in Databricks to supplement their employees' AI journeys."}
{"session_id": "sponsored-onehouse-open-default-fast-design-one-lakehouse-scales-bi-ai", "title": "Sponsored by: Onehouse | Open By Default, Fast By Design: One Lakehouse That Scales From BI to AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "You already see the value of the lakehouse. But are you truly maximizing its potential across all workloads, from BI to AI? In this session, Onehouse unveils how our open lakehouse architecture unifies your entire stack, enabling true interoperability across formats, catalogs, and engines. From lightning-fast ingestion at scale to cost-efficient processing and multi-catalog sync, Onehouse helps you go beyond trade-offs. Discover how Apache XTable (Incubating) enables cross-table-format compatibility, how OpenEngines puts your data in front of the best engine for the job, and how OneSync keeps data consistent across Snowflake, Athena, Redshift, BigQuery, and more. Meanwhile, our purpose-built lakehouse runtime slashes ingest and ETL costs. Whether you\u2019re delivering BI, scaling AI, or building the next big thing, you need a lakehouse that\u2019s open and powerful. Onehouse opens everything\u2014so your data can power anything."}
{"session_id": "sponsored-prophecy-how-marks-spencer-building-platform-future-deliver", "title": "Sponsored by: Prophecy | How Marks & Spencer is Building the Platform of the Future to Deliver Premium Retail Experiences", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As one of Britain\u2019s most iconic brands, Marks & Spencer (M&S) delivers premium food, clothing, and homeware to millions of customers. Behind customer experiences is data that must adapt to demand changes, supply chain volatility, and shifting digital consumers. To meet these challenges, the M&S Data + AI team is building the Platform for the Future. With Databricks and Prophecy, the platform will simplify the stack, democratize access, and deliver insights faster. M&S is making a bold shift to AI supercharge pipeline creation, moving from a code and engineer-heavy process to self-service with Prophecy\u2019s visual and AI development. With governance to reliably use data and manage costs. The result? People from across the business are participating, bringing the cost and time of food supply chain analysis down by a third and speeding pipeline creation by 30-60%. Join this session to learn how M&S is delivering AI-powered self-service and charting a roadmap for real transformation."}
{"session_id": "sponsored-prophecy-reinventing-data-prep-age-ai-build-agent-driven", "title": "Sponsored by: Prophecy | Reinventing Data Prep in the Age of AI: Build an Agent-driven Pipeline in 7 Minutes", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Pipeline"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Still coding data transformations by hand? Struggling with rigid, proprietary data prep tools? AI agents are flipping the script, reshaping data teams and delivering production-ready data preparation. Join this session to see how analysts, data scientists, and data engineers can build powerful, production-ready data pipelines simply by describing their intent in natural language. All in under 7 minutes. No complex UI or coding is required. Select datasets, join tables, apply filters, perform calculations - all just by chatting - and watch the pipeline materialize in real time. Ready to leave slow, traditional data prep behind and be part of the next wave of innovation? You won\u2019t want to miss this session."}
{"session_id": "sponsored-sigma-flogistix-flowco-and-role-data-responsible-energy", "title": "Sponsored by: Sigma | Flogistix by Flowco, and the Role of Data in Responsible Energy Production", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As global energy demands continue to rise, organizations must boost efficiency while staying environmentally responsible. Flogistix uses Sigma and Databricks to build a unified data architecture for real-time, data-driven decisions in vapor recovery systems. With Sigma on the Databricks Data Intelligence Platform, Flogistix gains precise operational insights and identifies optimization opportunities that reduce emissions, streamline workflows, and meet industry regulations. This empowers everyone, from executives to field mechanics, to drive sustainable resource production. Discover how advanced analytics are transforming energy practices for a more responsible future."}
{"session_id": "sponsored-sigma-moving-premises-unified-business-intelligence", "title": "Sponsored by: Sigma | Moving from On-premises to Unified Business Intelligence with Databricks & Sigma", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Real-time", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Faced with the limitations of a legacy, on-prem data stack and scalability bottlenecks in MicroStrategy, Saddle Creek Logistics Services needed a modern solution to handle massive data volumes and accelerate insight delivery. By migrating to a cloud-native architecture powered by Sigma and Databricks, the team achieved significant performance gains and operational efficiency. In this session, Saddle Creek will walk through how they leveraged Databricks\u2019 cloud-native processing engine alongside a unified governance layer through Unity Catalog to streamline and secure downstream analytics in Sigma. Learn how embedded dashboards and near real-time reporting\u2014cutting latency from 9 minutes to just 3 seconds\u2014have empowered data-driven collaboration with external partners and driven a major effort to consolidate over 30,000 reports and objects to under 1,000."}
{"session_id": "sponsored-sigma-trading-spreadsheets-speed-tradestations-self-service", "title": "Sponsored by: Sigma | Trading Spreadsheets for Speed: TradeStation\u2019s Self-Service Revolution", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Architecture", "Scala"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "To meet the growing internal demand for accessible, reliable data, TradeStation migrated from fragmented, spreadsheet-driven workflows to a scalable, self-service analytics framework powered by Sigma on Databricks. This transition enabled business and technical users alike to interact with governed data models directly on the lakehouse, eliminating data silos and manual reporting overhead. In brokerage trading operations, the integration supports robust risk management, automates key operational workflows, and centralizes collaboration across teams. By leveraging Sigma\u2019s intuitive interface on top of Databricks\u2019 scalable compute and unified data architecture, TradeStation has accelerated time-to-insight, improved reporting consistency, and empowered teams to operationalize data-driven decisions at scale."}
{"session_id": "sponsored-snowplow-snowplow-signals-powering-tomorrows-customer", "title": "Sponsored by: Snowplow | Snowplow Signals: Powering Tomorrow\u2019s Customer Experiences on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["AI/BI", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The web is on the verge of a major shift. Agentic applications will redefine how customers engage with digital experiences\u2014delivering highly personalized, relevant interactions. In this talk, Snowplow CTO Yali Sassoon explores how Snowplow Signals enables agents to perceive users through short- and long-term memory, natively on the Databricks Data Intelligence Platform."}
{"session_id": "sponsored-tealium-personalizing-experiences-and-improving-engagement", "title": "Sponsored by: Tealium | Personalizing Experiences and Improving Engagement with a Modernized Data Infrastructure", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION", "technologies": ["DATABRICKS WORKFLOWS", "PARTNER CONNECT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Real-time"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this session to hear how Western Governors University leverages Tealium & Databricks to power their data activation strategy with real-time customer data collection, activation and advanced analytics."}
{"session_id": "sponsored-thoughtspot-beauty-numbers-sephoras-journey-data-delight", "title": "Sponsored by: ThoughtSpot | Beauty by the Numbers: Sephora's Journey from Data to Delight with ThoughtSpot", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence", "Data Governance", "Machine Learning"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores how Sephora uses data to create impactful \"micro-moments that matter,\"-- personalized, timely reminders that enhance the customer journey. Discover how Sephora is transforming data into actionable insights, moving beyond traditional business intelligence with ThoughtSpot, within a modern data and AI stack that spans cloud platforms, advanced analytics, and machine learning democratization. The session will also cover Sephora's collaborative approach to data governance and the nuances of building high-performing, nimble teams that can adapt to change and drive innovation. Join us to see intelligence in action and understand how Sephora is using data not just to analyze the past, but to shape a more delightful future for its customers."}
{"session_id": "sponsored-thoughtspot-how-chevron-fuels-cloud-data-modernization", "title": "Sponsored by: ThoughtSpot | How Chevron Fuels Cloud Data Modernization", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, MANUFACTURING", "technologies": ["AI/BI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how Chevron transitioned their central finance and procurement analytics into the cloud using Databricks and ThoughtSpot\u2019s Agentic Analytics Platform. Explore how Chevron leverages ThoughtSpot to unlock actionable insights, enhance their semantic layer with user-driven understanding, and ultimately drive more impactful strategies for customer engagement and business growth. In this session, Chevron explains the dos, don\u2019ts, and best practices of migrating from outdated legacy business intelligence to real time, AI-powered insights."}
{"session_id": "sql-based-etl-options-sql-only-databricks-development", "title": "SQL-Based ETL: Options for SQL-Only Databricks Development", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline", "ELT", "ETL", "SQL"], "speakers": ["Sr. Specialist Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Using SQL for data transformation is a powerful way for an analytics team to create their own data pipelines. However, relying on SQL often comes with tradeoffs such as limited functionality, hard-to-maintain stored procedures or skipping best practices like version control and data tests. Databricks supports building high-performing SQL ETL workloads. Attend this session to hear how Databricks supports SQL for data transformation jobs as a core part of your Data Intelligence Platform. In this session we will cover 4 options to use Databricks with SQL syntax to create Delta tables: /Sr. Specialist Solutions Architect"}
{"session_id": "sql-first-etl-building-easy-efficient-data-pipelines-dlt", "title": "SQL-First ETL: Building Easy, Efficient Data Pipelines With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DLT", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "SQL"], "speakers": ["Sr Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores how SQL-based ETL can accelerate development, simplify maintenance and make data transformation more accessible to both engineers and analysts. We'll walk through how Databricks DLT and Databricks SQL warehouse support building production-grade pipelines using familiar SQL constructs. Topics include: By the end of the session, you\u2019ll understand how SQL-first approaches can streamline ETL development and support both operational and analytical use cases. /Sr Staff Product Manager"}
{"session_id": "startup-forum", "title": "Startup Forum", "track": "", "level": "BEGINNER", "type": "MEETUP", "industry": "ENTERPRISE TECHNOLOGY", "technologies": [], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "state-enterprise-ai-ai-agents-and-beyond", "title": "State of Enterprise AI: AI Agents and Beyond", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture"], "speakers": ["CTO, Neural Networks, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is evolving fast \u2014 from LLMs to intelligent agents and autonomous decision-making \u2014 and organizations are rethinking how they build and deploy data and AI systems. This session, led by Databricks executives, breaks down what\u2019s happening now and what\u2019s coming next. You\u2019ll learn how these technologies are reshaping data architectures, what it takes to support AI systems at scale, and how Databricks is enabling businesses to move from experimentation to real-world impact. What you\u2019ll learn: /AI/ML Product Mgmt\nDatabricks /CTO, Neural Networks"}
{"session_id": "state-street-uses-databricks-cybersecurity-lakehouse-threat", "title": "State Street Uses Databricks as a Cybersecurity Lakehouse for Threat Intelligence & Real-Time Alerts", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time"], "speakers": ["Senior Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Organizations face the challenge of managing vast amounts of data to combat emerging threats. The Databricks Data Intelligence platform represents a paradigm shift in cybersecurity at State Street, providing a comprehensive solution for managing and analyzing diverse security data. Through its partnership with Databricks, State Street has created a capability to: Efficiently manage structured and unstructured data. Scale up to analyze 50 petabytes of data in real-time. Ingest and parse data for critical security data streams. Build advanced cybersecurity data products and use automation & orchestration to streamline cybersecurity operations. By leveraging these capabilities, State Street has positioned itself as a leader in the financial services industry when it comes to cybersecurity. /Managing Director, Sec Arch & Eng\nState Street /Senior Solutions Architect"}
{"session_id": "stop-guessing-spend-where-it-counts-data-driven-decisions-high-impact", "title": "Stop Guessing Spend Where It Counts: Data-Driven Decisions for High-Impact Investments on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling with runaway cloud costs as your organization grows? Join us for an inside look at how Databricks\u2019 own Data Platform team tackled escalating spend in some of the world\u2019s largest workspaces \u2014 saving millions of dollars without sacrificing performance or user experience. We\u2019ll share how we harnessed powerful features like System Tables, Workflows, Unity Catalog, and Photon to monitor and optimize resource usage, all while using data-driven decisions to improve efficiency and ensure we invest in the areas that truly drive business impact. You\u2019ll hear about the real-world challenges we faced balancing governance with velocity and discover the custom tooling and best practices we developed to keep costs in check. By the end of this session, you\u2019ll walk away with a proven roadmap for leveraging Databricks to control cloud spend at scale. /Software Engineer\nDatabricks /Databricks"}
{"session_id": "story-unity-catalog-uc-migration-using-ucx-7-eleven-reorient-complex-uc", "title": "Story of a Unity Catalog (UC) Migration: Using UCX at 7-Eleven to Reorient a Complex UC Migration", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD, TRAVEL AND HOSPITALITY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Director, Architecture and Governance, 7-Eleven"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog (UC) enables governance and security for all data and AI assets within an enterprise\u2019s data lake and is necessary to unlock the full potential of Databricks as a true Data Intelligence Platform. Unfortunately, UC migrations are non-trivial; especially for enterprises that have been using Databricks for more than five years, i.e., 7-Eleven. System Integrators (SIs) offer accelerators, guides, and services to support UC migrations; however, cloud infrastructure changes, anti-patterns within code, and data sprawl can significantly complicate UC migrations. There is no \u201cshortcut\u201d to success when planning and executing a complex UC migration. In this session, we will share how UCX by Databricks Labs, a UC Migration Assistant, allowed 7-Eleven to reorient their UC migration by leveraging assessments and workflows, etc., to assess, characterize, and ultimately plan a tenable approach for their UC migration. /Delivery Solutions Architect\nDatabricks /Director, Architecture and Governance"}
{"session_id": "streaming-meets-governance-building-ai-ready-tables-confluent-tableflow", "title": "Streaming Meets Governance: Building AI-Ready Tables With Confluent Tableflow and Unity Catalog", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "ELT", "Real-time", "Streaming"], "speakers": ["Senior Product Manager, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how Databricks and Confluent are simplifying the path from real-time data to governed, analytics- and AI-ready tables. This session will cover how Confluent Tableflow automatically materializes Kafka topics into Delta tables and registers them with Unity Catalog \u2014 eliminating the need for custom streaming pipelines. We\u2019ll walk through how this integration helps data engineers reduce ingestion complexity, enforce data governance and make real-time data immediately usable for analytics and AI. /Member of Technical Staff\nDatabricks /Senior Product Manager"}
{"session_id": "streamline-your-bi-infrastructure-databricks-aibi-and-save-millions", "title": "Streamline Your BI Infrastructure With Databricks AI/BI and Save Millions on Traditional BI Tools", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Business Intelligence"], "speakers": ["Business Intelligence Senior Analyst, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Earlier this year, we finished migration of all dashboards from a traditional BI system to Databricks AI/BI ecosystem, resulting in annual savings of approximately $900,000. We also unlocked the below advantages: We will speak about our journey and how you can migrate your dashboards from traditional BI to AI/BI. Having listed the advantages above, we will also speak of some challenges faced. Migration steps: Migration shenanigans: We look forward to sharing these lessons learned and insights with you to help you streamline your BI infrastructure and unlock the full potential of Databricks AI/BI. /Business Intelligence Senior Analyst"}
{"session_id": "streamlining-data-platform-architecture-databricks-apps", "title": "Streamlining Data Platform Architecture With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Platform Operations Lead, Takeda"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Apps bring the power of open-source visualization frameworks like Plotly Dash directly into Databricks. Combined with the power of elastic serverless compute, Databricks Apps create a seamless development experience for advanced visualizations with the lowest possible latency to your lakehouse. In this talk, we walk you through a new approach to presenting AI-driven analysis to customers around your organization \u2014 all while eliminating unneeded license costs, halving administrative burden and accelerating delivery time. /Platform Operations Lead"}
{"session_id": "streamlining-dspy-development-track-debug-and-deploy-mlflow", "title": "Streamlining DSPy Development: Track, Debug, and Deploy With MLflow", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DSPY", "MLFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Senior Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DSPy is a framework for authoring GenAI applications with automatic prompt optimization, while MLflow provides powerful MLOps tooling to track, monitor, and productize machine learning workflows. In this lightning talk, we demonstrate how to integrate MLflow with DSPy to bring full observability to your DSPy development. We\u2019ll walk through how to track DSPy module calls, evaluations, and optimizers using MLflow\u2019s tracing and autologging capabilities. By the end, you'll see how combining these two tools makes it easier to debug, iterate, and understand your DSPy workflows, then deploy your DSPy program \u2014 end to end. /Senior Software Engineer"}
{"session_id": "supercharge-your-enterprise-bi-practitioners-guide-migrating-aibi", "title": "Supercharge Your Enterprise BI: A Practitioner\u2019s Guide for Migrating to AI/BI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Are you striving to build a data-driven culture while managing costs and reducing reporting latency? Are your BI operations bogged down by complex data movements rather than delivering insights? Databricks IT faced these challenges in 2024 and embarked on an ambitious journey to make Databricks AI/BI our enterprise-wide reporting platform. In just two quarters, we migrated 2,000 dashboards from a traditional BI tool \u2014 without disrupting business operations. We\u2019ll share how we executed this large-scale transition cost-effectively, ensuring seamless change management and empowering non-technical users to leverage AI/BI. You\u2019ll gain insights into: Join us to learn how your organization can achieve the same transformation with AI-powered enterprise reporting. /Sr. Director, Data\nDatabricks /Senior Engineering Manager"}
{"session_id": "supercharging-sales-intelligence-processing-billions-events-structured", "title": "Supercharging Sales Intelligence: Processing Billions of Events via Structured Streaming", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "ELT", "Real-time", "Scala", "Streaming"], "speakers": ["Senior Data Engineer, DigiCert"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DigiCert is a digital security company that provides digital certificates, encryption and authentication services and serves 88% of the Fortune 500, securing over 28 billion web connections daily. Our project aggregates and analyzes certificate transparency logs via public APIs to provide comprehensive market and competitive intelligence. Instead of relying on third-party providers with limited data, our project gives full control, deeper insights and automation. Databricks has helped us reliably poll public APIs in a scalable manner that fetches millions of events daily, deduplicate and store them in our Delta tables. We specifically use Spark for parallel processing, structured streaming for real-time ingestion and deduplication, Delta tables for data reliability, pools and jobs to ensure our costs are optimized. These technologies help us keep our data fresh, accurate and cost effective. This data has helped our sales team with real-time intelligence, ensuring DigiCert's success. /Director-Data Engineering\nDigiCert /Senior Data Engineer"}
{"session_id": "swimming-our-own-lakehouse-how-databricks-uses-databricks", "title": "Swimming at Our Own Lakehouse: How Databricks Uses Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Peek behind the curtain to learn how Databricks processes hundreds of petabytes of data across every region and cloud where we operate. Learn how Databricks leverages Data and AI to scale and optimize every aspect of the company. From facilities and legal to sales and marketing and of course product research and development. This session is a high-level tour inside Databricks to see how Data and AI enable us to be a better company. We will go into the architecture of things for how Databricks is used for internal use cases like business analytics and SIEM as well as customer-facing features like system tables and assistant. We will cover how data production of our data flow and how we maintain security and privacy while operating a large multi-cloud, multi-region environment. /Head of Data Platform\nDatabricks /Staff Software Engineer"}
{"session_id": "take-it-limit-art-possible-aibi", "title": "Take it to the Limit: Art of the Possible in AI/BI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Think you know everything AI/BI can do? Think again. This session explores the art of the possible with Databricks AI/BI Dashboards and Genie, going beyond traditional analytics to unleash the full power of the lakehouse. From incorporating AI into dashboards to handling large-scale data with ease to delivering insights seamlessly to end users \u2014 we\u2019ll showcase creative approaches that unlock insights and real business outcomes. Perfect for adventurous data professionals looking to push limits and think outside the box. /Delivery Solution Architect\nDatabricks /Staff Software Engineer"}
{"session_id": "talking-all-your-data-building-multi-agent-systems-structured-and", "title": "Talking to All Your Data: Building Multi-Agent Systems for Structured and Unstructured Information", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Learn how to build sophisticated systems that enable natural language interactions with both your structured databases and unstructured document collections. This session explores advanced techniques for creating unified and governed AI systems that can seamlessly interpret questions, retrieve relevant information and generate accurate answers across your entire data ecosystem. Key takeaways include: /Staff Product Manager"}
{"session_id": "taming-llm-wild-west-unified-approach-genai-governance", "title": "Taming the LLM Wild West: A Unified Approach to GenAI Governance", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Whether you're using OpenAI, Anthropic or open-source models like Meta Llama, the Mosaic AI Gateway is the central control plane across any AI model or agent. Learn how you can streamline access controls, enforce guardrails for compliance, ensure an audit trail and monitor costs across providers \u2014 without slowing down innovation. Lastly, we\u2019ll dive even deeper into how AI Gateway works with Unity Catalog to deliver a full governance story for your end-to-end AI agents across models, tools and data. Key takeaways: /Product Manager\nDatabricks /Senior Staff Software Engineer"}
{"session_id": "tech-industry-forum-tip-spear-data-and-ai", "title": "Tech Industry Forum: Tip of the Spear With Data and AI", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Chief Product & Technology Officer, ThredUp Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for the Tech Industry Forum, formerly known as the Tech Innovators Summit, now part of Databricks Industry Experience. This session will feature keynotes, panels and expert talks led by top customer speakers and Databricks experts. Tech companies are pushing the boundaries of data and AI to accelerate innovation, optimize operations and build collaborative ecosystems. In this session, we\u2019ll explore how unified data platforms empower organizations to scale their impact, democratize analytics across teams and foster openness for building tomorrow\u2019s products. Key topics include: After the session, connect with your peers during the exclusive Industry Forum Happy Hour. Reserve your seat today! /Industry Marketing, Digital Natives\nDatabricks /Director, Product\nDatabricks /VP, Data Engineering\nZillow /Data Magician\nDatabricks /VP of Data & AI\nRobinhood /Dropbox /Chief Product & Technology Officer"}
{"session_id": "tech-industry-session-building-collaborative-ecosystems-openness-and", "title": "Tech Industry Session: Building Collaborative Ecosystems With Openness and Portability", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["SVP Platform Engineering, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to discover how leading tech companies accelerate growth using open ecosystems and built-on solutions to foster collaboration, accelerate innovation and create scalable data products. This session will explore how organizations use Databricks to securely share data, integrate with partners and enable teams to build impactful applications powered by AI and analytics. Topics include: Hear real-world examples of how open ecosystems empower organizations to widen the aperture on collaboration, driving better business outcomes. Walk away with insights into how open data sharing and built-on solutions can help your teams innovate faster at scale. /CTO\nAddepar /Sr. Director Data Collaboration\nDatabricks /SVP Engineering\nTealium /SVP Platform Engineering"}
{"session_id": "tech-industry-session-optimizing-costs-and-controls-democratize-data", "title": "Tech Industry Session: Optimizing Costs and Controls to Democratize Data and AI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS APPS", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["YipitData"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for this session focused on how leading tech companies are enabling data intelligence across their organizations while maintaining cost efficiency and governance. Hear the successes and the challenges when Databricks empowers thousands of users\u2014from engineers to business teams\u2014by providing scalable tools for AI, BI and analytics. Topics include: Hear from customers and Databricks experts, followed by a customer panel featuring industry leaders. Gain insights into how Databricks helps tech innovators scale their platforms while maintaining operational excellence. /Product Management\nDatabricks /SVP Product and Engineering\nOT Technology, LLC /YipitData"}
{"session_id": "techcombanks-multi-million-dollar-transformation-leveraging-cloud-and", "title": "Techcombank's Multi-Million Dollar Transformation Leveraging Cloud and Databricks", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Chief Data and Analytics Officer, Techcombank (TCB)"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The migration to the Databricks Data Intelligence Platform has enabled Techcombank to more efficiently unify data from over 50 systems, improve governance, streamline daily operational analytics pipelines and use advanced analytics tools and AI to create more meaningful and personalized experiences for customers. With Databricks, Techcombank has also introduced key solutions that are reshaping its digital banking services: AI-driven lead management system: Techcombank's internally developed AI program called 'Lead Allocation Curated Engine' (LACE) optimizes lead management and provides relationship managers with enriched insights for smarter lead allocation to drive business growth. AI-powered program for digital banking inclusion of small businesses: An AI-powered GeoSense assists frontline workers with analytics-driven insights about which small businesses and merchants to engage in the bank's digital ecosystem. And more examples, which will be presented. /Chief Data and Analytics Officer"}
{"session_id": "telco-reimagined-real-world-journeys-data-and-ai-customer-experience", "title": "Telco Reimagined: Real-World Journeys in Data and AI for Customer Experience Transformation", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Head of Global Telecom Industry, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How are today\u2019s leading telecom operators transforming customer experience at scale with data and AI? Join us for an inspiring fireside chat with senior leaders from Optus and T-Mobile as they share their transformation stories \u2014 from the first steps to major milestones and the tangible business impact achieved with Databricks\u2019 Data Intelligence Platform. You\u2019ll hear firsthand how these forward-thinking CSP\u2019s are driving measurable outcomes through unified data, machine learning and AI. Discover the high-impact use cases they\u2019re prioritizing \u2014 like proactive care and hyper-personalization \u2014 and gain insight into their bold vision for the future of customer experience in telecom. Whether you're just beginning your AI journey or scaling to new heights, this session offers an authentic look at what\u2019s working, what\u2019s next and how data and AI are helping telecoms lead in a competitive landscape. /Head of Global Telecom Industry"}
{"session_id": "telecom-innovation-exchange-demos-and-dialogues", "title": "Telecom Innovation Exchange: Demos and Dialogues", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Head of Global Telecom Industry, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an interactive breakout session designed to explore scalable, real-world solutions powered by Partners with Databricks. In this high-energy session, you'll hear from three of our leading partners \u2014 Accenture, Capgemini and Wipro \u2014 as they each deliver rapid-fire, 5-minute demos of their most impactful, production-grade solutions built for the telecom industry. From network intelligence to customer experience to AI-driven automation, these solutions are already driving tangible outcomes at scale. After the demos, you\u2019ll have the unique opportunity to engage directly with each partner in a \u201cspeed dating\u201d style format. Dive deep into the solutions, ask your questions and explore how these approaches can be tailored to your organization\u2019s needs. Whether you're solving for churn, fraud, network ops or enterprise AI use cases, this session is your chance to connect, collaborate and walk away with practical ideas you can take back to your teams. /Head of Global Telecom Industry"}
{"session_id": "telecommunications-industry-forum-intelligent-telecom-efficiency", "title": "Telecommunications Industry Forum: The Intelligent Telecom: Efficiency, Revenue Growth, Impact", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Head of Global Telecom Industry, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The telecommunications industry is at a pivotal moment. As networks expand to support 5G, IoT and next-generation customer experiences, success will belong to those who harness the full power of data and AI. These leaders will drive greater efficiency, unlock new revenue streams and deliver world-class service. Join us at the DAIS Telecom Forum for an exclusive Telecom Keynote where industry trailblazers reveal how they are transforming their organizations with the Databricks Data Intelligence Platform. Gain firsthand insights into how leading telecoms are leveraging open, interoperable data strategies to break down silos, improve operational agility and deliver measurable business outcomes. Don\u2019t miss this opportunity to connect with global telecom leaders, learn from real-world success stories and be inspired to lead your organization\u2019s next wave of transformation. The future of telecom is being built now \u2014 be part of the conversation. /Head of Global Telecom Industry"}
{"session_id": "thredups-journey-databricks-modernizing-our-data-infrastructure", "title": "ThredUp\u2019s Journey with Databricks: Modernizing Our Data Infrastructure", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Machine Learning", "Real-time"], "speakers": ["VP, Data platform & Enterprise Apps Engg, ThredUp Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building an AI-ready data platform requires strong governance, performance optimization, and seamless adoption of new technologies. At ThredUp, our Databricks journey began with a need for better data management and evolved into a full-scale transformation powering analytics, machine learning, and real-time decision-making. In this session, we\u2019ll cover: Whether you\u2019re new to Databricks or scaling an existing platform, you\u2019ll gain practical insights on navigating the transition, avoiding pitfalls, and maximizing AI and data intelligence. /Data Engineering Manager\nThredup /VP, Data platform & Enterprise Apps Engg"}
{"session_id": "three-big-unlocks-ai-interoperability-databricks", "title": "Three Big Unlocks to AI Interoperability With Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "TRAVEL AND HOSPITALITY", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Vice President, Expedia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The ability for different AI systems to collaborate is more critical than ever. From traditional ML development to fine-tuning GenAI models, Databricks delivers the stability, cost-optimization and productivity Expedia Group (EG) needs. Learn how to unlock the full potential of AI interoperability with Databricks. Join Shiyi Pickrell to understand the future of AI interoperability, how it\u2019s generating business value and driving the next generation of travel AI-powered experiences. /Senior Vice President"}
{"session_id": "thursday-keynote", "title": "Thursday Keynote", "track": "", "level": "", "type": "KEYNOTE", "industry": "", "technologies": [], "duration": "180 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover the latest advances on the Data Intelligence Platform and hear from the companies who are already enjoying success."}
{"session_id": "top-performance-and-cost-optimizations-dlt", "title": "Top Performance and Cost Optimizations for DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Principal Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT simplifies pipeline development and management \u2014 but how do you optimize for performance and cost? In this session, we\u2019ll explore practical strategies for tuning DLT pipelines, including when and how to use autoscaling, Photon and different node types. We'll also cover how to monitor resource usage and decide when serverless is the right choice. You'll learn best practices drawn from real-world customer implementations, along with an overview of the latest performance enhancements available in serverless DLT. /Principal Solutions Architect"}
{"session_id": "tracing-path-row-through-gpu-enabled-query-engine-grace-blackwell", "title": "Tracing the Path of a Row Through a GPU-Enabled Query Engine on the Grace-Blackwell Architecture", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "SQL"], "speakers": ["Senior Developer Technology Engineer, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Grace-Blackwell is NVIDIA\u2019s most recent GPU system architecture. It addresses a key concern of query engines: fast data access. In this session, we will take a close look at how GPUs can accelerate data analytics by tracing how a row flows through a GPU-enabled query engine.Query engines read large data from CPU memory or from disk. On Blackwell GPUs, a query engine can rely on hardware-accelerated decompression of compact formats. The Grace-Blackwell system takes data access performance even further, by reading data at up to 450 GB/s across its CPU to GPU interconnect. We demonstrate full end-to-end SQL query acceleration using GPUs in a prototype query engine using industry standard benchmark queries. We compare the results to existing CPU solutions.Using Apache Spark\u2122 and the RAPIDS Accelerator for Apache Spark, we demonstrate the impact GPU acceleration has on the performance of SQL queries at the 100TB scale using NDS, a suite that simulates real-world business scenarios. /Principal Systems Software Engineer\nNvidia /Senior Developer Technology Engineer"}
{"session_id": "tracking-data-and-ai-lineage-ensuring-transparency-and-compliance", "title": "Tracking Data and AI Lineage: Ensuring Transparency and Compliance", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As AI becomes more deeply integrated into data platforms, understanding where data comes from \u2014 and where it goes \u2014 is essential for ensuring transparency, compliance and trust. In this session, we\u2019ll explore the newest advancements in data and AI lineage across the Databricks Platform, including during model training, evaluation and inference. You\u2019ll also learn how lineage system tables can be used for impact analysis and to gain usage insights across your data estate. We\u2019ll cover newly released capabilities \u2014 such as Bring Your Own Lineage \u2014 that enable an end-to-end view of your data and AI assets in Unity Catalog. Plus, get a sneak peek at what\u2019s coming next on the lineage roadmap! /SWE\nDatabricks /Product Manager"}
{"session_id": "traditional-mdm-dead-how-next-generation-data-products-are-winning", "title": "Traditional MDM is Dead. How Next-Generation Data Products are Winning the Enterprise", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DLT", "LAKEFLOW"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Global Head of Data Management, Quantexa"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Global Head of Data Management"}
{"session_id": "traditional-ml-scale-implementing-classical-techniques-databricks", "title": "Traditional ML at Scale: Implementing Classical Techniques With Databricks Mosaic AI", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala"], "speakers": ["Staff Technical Marketing, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling to implement traditional machine learning models that deliver real business value? Join us for a hands-on exploration of classical ML techniques powered by Databricks' Mosaic AI platform. This session focuses on time-tested approaches like regression, classification and clustering \u2014 showing how these foundational methods can solve real business problems when combined with Databricks' scalable infrastructure and MLOps capabilities. Key takeaways: /AI/ML Product Mgmt\nDatabricks /Staff Technical Marketing"}
{"session_id": "transforming-bio-pharma-manufacturing-eli-lillys-data-driven-journey", "title": "Transforming Bio-Pharma Manufacturing: Eli Lilly's Data-Driven Journey With Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Fabric", "Real-time", "Streaming"], "speakers": ["Executive Director, Eli Lilly and Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Eli Lilly and Company, a leading bio-pharma company, is revolutionizing manufacturing with next-gen fully digital sites. Lilly and Tredence have partnered to establish a Databricks-powered Global Manufacturing Data Fabric (GMDF), laying the groundwork for transformative data products used by various personas at sites and globally. By integrating data from various manufacturing systems into a unified data model, GMDF has delivered actionable insights across several use cases such as batch release by exception, predictive maintenance, anomaly detection, process optimization and more. Our serverless architecture leverages Databricks Auto Loader for real-time data streaming, PySpark for automation and Unity Catalog for governance, ensuring seamless data processing and optimization. This platform is the foundation for data driven processes, self-service analytics, AI and more. This session will provide details on the data architecture and strategy and share a few use cases delivered. /Lead Innovation Architect - Data & AI\nEli Lilly /Head of Delivery - HLS\nTredence /Executive Director"}
{"session_id": "transforming-credit-analytics-compliant-lakehouse-rabobank", "title": "Transforming Credit Analytics With a Compliant Lakehouse at Rabobank", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture"], "speakers": ["Product Manager, Rabobank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This presentation outlines Rabobank Credit analytics transition to a secure, audit-ready data architecture using Unity Catalog (UC), addressing critical regulatory challenges in credit analytics for IRB and IFRS9 regulatory modelling. Key technical challenges: Details: Outcomes: Next: /Sr. Resident Solutions Architect\nDatabricks /Product Manager"}
{"session_id": "transforming-customer-processes-and-gaining-productivity-dlt", "title": "Transforming Customer Processes and Gaining Productivity With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Integration"], "speakers": ["Sr Manager, Banco Bradesco S.A."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bradesco Bank is one of the largest private banks in Latin America, with over 75 million customers and over 80 years of presence in FSI. In the digital business, velocity to react to customer interactions is crucial to succeed. In the legacy landscape, acquiring data points on interactions over digital and marketing channels was complex, costly and lacking integrity due to typical fragmentation of tools. With the new in-house Customer Data Platform powered by Databricks Intelligent Platform, it was possible to completely transform the data strategy around customer data. Using some key components such Uniform and DLT, it was possible to increase data integrity, reduce latency and processing time and, most importantly, boost personal productivity and business agility. Months of reprocessing, weeks of human labor and cumbersome and complex data integrations were dramatically simplified achieving significant operational efficiency. /senior manager\nBradesco Bank /Sr Manager"}
{"session_id": "transforming-data-governance-multimodal-data-amgen-databricks", "title": "Transforming Data Governance for Multimodal Data at Amgen With Databricks", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Fabric", "Data Governance"], "speakers": ["Senior Manager Data & Analytics, Amgen"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Amgen is advancing its Enterprise Data Fabric to securely manage sensitive multimodal data, such as imaging and research data, across formats.Databricks is already the de facto standard for governance on structured data, and Amgen seeks to extend it for unstructured multi modal data too. This approach will also allow Amgen to standardize its GenAI projects on Databricks. Key priorities include: Learn strategies for implementing a comprehensive multimodal data governance framework using Databricks, as we share our experience on standardizing data governance for GenAI use cases. /Sr Data Engineer\nAMGEN /Senior Manager Data & Analytics"}
{"session_id": "transforming-data-pipeline-management-targeted-proof-concept", "title": "Transforming Data Pipeline Management With a Targeted Proof of Concept", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Data Pipeline", "Machine Learning", "Scala"], "speakers": ["VP, Capital One Financial"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Capital One, data-driven decision making is paramount to our success. This session explores how a focused proof of concept (POC) accelerated a shift in our data pipeline management strategy, resulting in operational improvements and expanded analytical capabilities. We'll cover the business challenges that motivated POC initiation, including data latency, cost savings and scalability limitations, and real-world results. We'll also dive into an examination of the before-and-after architecture with highlights for key technological levers. This session offers insights for data engineering and machine learning practitioners seeking to optimize their data pipelines for improved performance, scalability and business value. /VP Data Engineering CARD\nCapital One Financial /VP"}
{"session_id": "transforming-data-rheem-silos-scalable-data-lakehouse-databricks-and", "title": "Transforming Data at Rheem: From Silos to Scalable Data Lakehouse With Databricks and Unity Catalog", "track": "DATA STRATEGY", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Vice President, Enterprise Applications, Rheem"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Rheem's journey from a fragmented data landscape to a robust, scalable data platform powered by Databricks showcases the power of data modernization. In just 1.5 years, Rheem evolved from siloed reporting to 30+ certified data products, integrated with 20+ source systems, including MDM. This transformation has unlocked significant business value across sales, procurement, service and operations, enhancing decision-making and operational efficiency. This session will delve into Rheem's implementation of Databricks, highlighting how it has become the cornerstone of rapid data product development and efficient data sharing across the organization. We will also explore the upcoming enhancements with Unity Catalog, including the full migration from HMS to UC. Attendees will gain insights into best practices for building a centralized data platform, enhancing developer experience, improving governance capabilities as well as tips and tricks for a successful UC migration and enablement. /Senior Manager, AI & Data\nEY /Vice President, Enterprise Applications"}
{"session_id": "transforming-government-data-and-ai-singapore-govtechs-journey", "title": "Transforming Government With Data and AI: Singapore GovTech's Journey With Databricks", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "PUBLIC SECTOR", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Quality"], "speakers": ["Deputy Chief Data Officer, GovTech"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "GovTech is an agency in the Singapore Government focused on tech for good. The GovTech Chief Data Office (CDO) has built the GovTech Data Platform with Databricks at the core. As the government tech agency, we safeguard national-level government and citizen data. A comprehensive data strategy is essential to uplifting data maturity. GovTech has adopted the service model approach where data services are offered to stakeholders based on their data maturity. Their maturity is uplifted through partnership, readying them for more advanced data analytics. CDO offers a plethora of data assets in a \u201cdata restaurant\u201d ranging from raw data to data products, all delivered via Databricks and enabled through fine-grained access control, underpinned by data management best practices such as data quality, security and governance. Within our first year on Databricks, CDO was able to save 8,000 man-hours, democratize data across 50% of the agency and achieve six-figure savings through BI consolidation. /Senior Manager, Data Management\nGovTech Singapore /Deputy Chief Data Officer"}
{"session_id": "transforming-hps-print-elt-reporting-genit-real-time-insights-tool", "title": "Transforming HP\u2019s Print ELT Reporting with GenIT: Real-Time Insights Tool Powered by Databricks AI", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Real-time", "SQL"], "speakers": ["Director, Big Data & Strategic Insights, HP"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Timely and actionable insights are critical for staying competitive in today\u2019s fast-paced environment. At HP Print, manual reporting for executive leadership (ELT) has been labor-intensive, hindering agility and productivity. To address this, we developed the Generative Insights Tool (GenIT) using Databricks Genie and Mosaic AI to create a real-time insights engine automating SQL generation, data visualization, and narrative creation. GenIT delivers instant insights, enabling faster decisions, greater productivity, and improved consistency while empowering leaders to respond to printer trends. With automated querying, AI-powered narratives, and a chatbot, GenIT reduces inefficiencies and ensures quality data and insights. Our roadmap integrates multi-modal data, enhances chatbot functionality, and scales globally. This initiative shows how HP Print leverages GenAI to improve decision-making, efficiency, and agility, and we will showcase this transformation at the Databricks AI Summit. /Director, Big Data & Strategic Insights"}
{"session_id": "transforming-title-insurance-databricks-batch-inference", "title": "Transforming Title Insurance With Databricks Batch Inference", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["MLFLOW", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Sr Director AI, First American"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we explore how First American Data & Analytics, a leading property-centric information provider, revolutionized its data extraction processes using batch inference on the Databricks Platform. Discover how it overcame the challenges of extracting data from millions of historical title policy images and reduced project timelines by 75%. Learn how First American optimized its data processing capabilities, reduced costs by 70% and enhanced the efficiency of its title insurance processes, ultimately improving the home-buying experience for buyers, sellers and lenders. This session will delve into the strategic integration of AI technologies, highlighting the power of collaboration and innovation in transforming complex data challenges into scalable solutions. /VP, Data and AI\nFirst American Financial Corporation /Sr Director AI"}
{"session_id": "trillions-data-records-zero-bottlenecks-investor-decision-making", "title": "Trillions of Data Records, Zero Bottlenecks for Investor Decision-Making", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala"], "speakers": ["Head of Data, J Goldman"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In finance, every second counts. That\u2019s why the Data team at J. Goldman & Co. needed to transform trillions of real-time market data records into a single, actionable insight \u2014 instantly, and without waiting on development resources. By modernizing their internal data platform with a scalable architecture, they built a streamlined, web-native alternative data interface that puts live market data directly in the hands of investment teams. With Databricks\u2019 computational power and Unity Catalog\u2019s secure governance, they eliminated bottlenecks and achieved the fastest time-to-market for critical investor decisions possible. Learn how J. Goldman & Co. Innovates with Databricks and Sigma to: /CTO\nJ. Goldman & Co., L.P. /Head of Data"}
{"session_id": "turn-genie-agent-using-conversation-apis", "title": "Turn Genie Into an Agent Using Conversation APIs", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "SQL"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Transform your AI/BI Genie into a text-to-SQL powerhouse using the Genie Conversation APIs. This session explores how Genie functions as an intelligent agent, translating natural language queries into SQL to accelerate insights and enhance self-service analytics. You'll learn practical techniques for configuring agents, optimizing queries and handling errors \u2014 ensuring Genie delivers accurate, relevant responses in real time. A must-attend for teams looking to level up their AI/BI capabilities and deliver smarter analytics experiences. /Product Manager\nDatabricks /Software Engineer"}
{"session_id": "unified-advanced-analytics-integrating-power-bi-and-databricks-genie", "title": "Unified Advanced Analytics: Integrating Power BI and Databricks Genie for Real-time Insights", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS APPS", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Science", "Machine Learning"], "speakers": ["Senior Mgr of Analytics & Data Science, TurnPoint Services"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data-driven landscape, business users expect seamless, interactive analytics without having to switch between different environments. This presentation explores our web application that unifies a Power BI dashboard with Databricks Genie, allowing users to query and visualize insights from the same dataset within a single, cohesive interface. We will compare two integration strategies: one that leverages a traditional webpage enhanced by an Azure bot to incorporate Genie\u2019s capabilities, and another that utilizes Databricks Apps to deliver a smoother, native experience. We use the Genie API to build this solution. Attendees will learn the architecture behind these solutions, key design considerations and challenges encountered during implementation. Join us to see live demos of both approaches, and discover best practices for delivering an all-in-one, interactive analytics experience. /Associate VP of AI & Machine Learning\nIT Convergence /Senior Mgr of Analytics & Data Science"}
{"session_id": "unified-governance-and-enterprise-sharing-data-ai", "title": "Unified Governance and Enterprise Sharing for Data + AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Lakehouse for Public Sector is the only enterprise data platform that allows you to leverage all your data, from any source, on any workload to always offer better citizen services/warfighter support/student success with the best outcomes, at the lowest cost, with the greatest investment protection. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "unified-solution-data-management-and-model-training-apache-iceberg-and", "title": "A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, PUBLIC SECTOR", "technologies": ["APACHE ICEBERG", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala", "Streaming"], "speakers": ["machine learning system engineer, ByteDance"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming). Magnus uses Iceberg\u2019s branch/tag to manage massive datasets/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability. When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility. In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training. /Infrastructure Engineer\nByteDance /machine learning system engineer"}
{"session_id": "unify-enterprise-data-databricks-zero-copy-data-sharing-sap-salesforce", "title": "Unify Enterprise Data in Databricks With Zero-Copy Data Sharing From SAP & Salesforce Data Cloud", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT"], "speakers": ["Director, Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How can enterprises securely share their SAP and Salesforce data with Databricks \u2014 without the cost and complexity of data duplication or movement? This session explores how Delta Sharing enables zero-copy, secure data access from SAP Business data cloud and Salesforce Data Cloud, making it easier to activate enterprise data for analytics and AI. We\u2019ll walk through two powerful integrations: You\u2019ll also learn implementation best practices to ensure security, governance and efficient workflows \u2014 so you can accelerate decision-making while staying compliant. Ideal for data leaders, architects and engineers looking to drive value from enterprise systems in Databricks. /Director, Product Management"}
{"session_id": "unify-your-data-and-governance-lakehouse-federation", "title": "Unify Your Data and Governance With Lakehouse Federation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Warehouse", "SQL"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today's data landscape, organizations often grapple with fragmented data spread across various databases, data warehouses and catalogs. Lakehouse Federation addresses this challenge by enabling seamless discovery, querying, and governance of distributed data without the need for duplication or migration. This session will explore how Lakehouse Federation integrates external data sources like Hive Metastore, Snowflake, SQL Server and more into a unified interface, providing consistent access controls, lineage tracking and auditing across your entire data estate. Learn how to streamline analytics and AI workloads, enhance compliance and reduce operational complexity by leveraging a single, cohesive platform for all your data needs. /Sr. Staff Product Manager\nDatabricks /Staff Product Manager"}
{"session_id": "unifying-customer-data-drive-new-automotive-experience-lakeflow-connect", "title": "Unifying Customer Data to Drive a New Automotive Experience With Lakeflow Connect", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Integration", "Data Science"], "speakers": ["Team Lead Data-Driven Business Solutions, Porsche Informatik GmbH"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Data Intelligence Platform and Lakeflow Connect have transformed how Porsche manages and uses its customer data. By opting to use Lakeflow Connect instead of building a custom solution, the company has reaped the benefits of both operational efficiency and cost management. Internally, teams at Porsche now spend less time managing data integration processes. \u201cLakeflow Connect has enabled our dedicated CRM and Data Science teams to be more productive as they can now focus on their core work to help innovate, instead of spending valuable time on the data ingestion integration with Salesforce,\u201d says Gruber. This shift in focus is aligned with broader industry trends, where automotive companies are redirecting significant portions of their IT budgets toward customer experience innovations and digital transformation initiatives. This story was also shared as part of a Databricks Success Story \u2014 Elise Georis, Giselle Goicochea. /Team Lead Data-Driven Business Solutions"}
{"session_id": "unifying-data-delivery-using-databricks-your-enterprise-serving-layer", "title": "Unifying Data Delivery: Using Databricks as Your Enterprise Serving Layer", "track": "DATA WAREHOUSING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "ELT", "SQL", "Scala"], "speakers": ["Data Platform Architect, The World Bank"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will take you on our journey of integrating Databricks as the core serving layer in a large enterprise, demonstrating how you can build a unified data platform that meets diverse business needs. We will walk through the steps for constructing a central serving layer by leveraging Databricks\u2019 SQL Warehouse to efficiently deliver data to analytics tools and downstream applications. To tackle low latency requirements, we\u2019ll show you how to incorporate an interim scalable relational database layer that delivers sub-second performance for hot data scenarios. Additionally, we\u2019ll explore how Delta Sharing enables secure and cost-effective data distribution beyond your organization, eliminating silos and unnecessary duplication for a truly end-to-end centralized solution. This session is perfect for data architects, engineers and decision-makers looking to unlock the full potential of Databricks as a centralized serving hub. /Data Platform Architect\nThe World Bank /Data Platform Architect"}
{"session_id": "unifying-gtm-analytics-strategic-shift-native-analytics-and-aibi", "title": "Unifying GTM Analytics: The Strategic Shift to Native Analytics and AI/BI Dashboards at Databricks", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, FINANCIAL SERVICES", "technologies": ["AI/BI", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Scala"], "speakers": ["Director, GTM Analytics and Governance, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The GTM team at Databricks recently launched the GTM Analytics Hub\u2014a native AI/BI platform designed to centralize reporting, streamline insights, and deliver personalized dashboards based on user roles and business needs. Databricks Apps also played a crucial role in this integration by embedding AI/BI Dashboards directly into internal tools and applications, streamlining access to insights without disrupting workflows. This seamless embedding capability allows users to interact with dashboards within their existing platforms, enhancing productivity and collaboration. Furthermore, AI/BI Dashboards leverage Databricks' unified data and governance framework. Join us to learn how we\u2019re using Databricks to build for Databricks\u2014transforming GTM analytics with AI/BI Dashboards, and what it takes to drive scalable, user-centric analytics adoption across the business. /Sr. Manager, GTM Analytics Engineering\nDatabricks /Director, GTM Analytics and Governance"}
{"session_id": "unifying-human-curated-data-ingestion-and-real-time-updates-databricks", "title": "Unifying Human-Curated Data Ingestion and Real-Time Updates with Databricks DLT, Protobuf and BSR", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Data Pipeline", "Real-time", "Streaming"], "speakers": ["Data Platform Architect, Clinician Nexus"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Red Stapler is a streaming-native system on Databricks that merges file-based ingestion and real-time user edits into one DLT pipeline for near real-time feedback. Protobuf definitions, managed in the Buf Schema Registry (BSR), govern schema and data-quality rules, ensuring backward compatibility. All records \u2014 valid or not \u2014 are stored in an SCD Type 2 table, capturing every version for full history and immediate quarantine views of invalid data. This unified approach boosts data governance, simplifies auditing and streamlines error fixes. Running on DLT Serverless and the Kafka-compatible Bufstream keeps costs low by scaling down to zero when idle. Red Stapler\u2019s configuration-driven Protobuf logic adapts easily to evolving survey definitions without risking production. The result is consistent validation, quick updates and a complete audit trail \u2014 all critical for trustworthy, flexible data pipelines. /Data Platform Architect"}
{"session_id": "unity-catalog-deep-dive-practitioners-guide-best-practices-and-patterns", "title": "Unity Catalog Deep Dive: Practitioner's Guide to Best Practices and Patterns", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Data Pipeline", "Machine Learning"], "speakers": ["Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this deep dive session for practitioners on Unity Catalog, Databricks\u2019 unified data governance solution, to explore its capabilities for managing data and AI assets across workflows. Unity Catalog provides fine-grained access control, automated lineage tracking, quality monitoring and policy enforcement and observability at scale. Whether your focus is data pipelines, analytics or machine learning and generative AI workflows, this session offers actionable insights on leveraging Unity Catalog\u2019s open interoperability across tools and platforms to boost productivity and drive innovation. Learn governance best practices, including catalog configurations, access strategies for collaboration and controls for securing sensitive data. Additionally, discover how to design effective multi-cloud and multi-region deployments to ensure global compliance. /Sr. Solutions Architect\nDatabricks /Solutions Architect"}
{"session_id": "unity-catalog-implementation-evolution-edward-jones", "title": "Unity Catalog Implementation & Evolution at Edward Jones", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics"], "speakers": ["Technical Architect, Edward Jones"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Analytics Hub is a product which serves as the central repository for analytics and reporting data across Edward Jones. Analytics Hub followed the UC design aligning to the Medallion architecture: We have good amount of data that are migrated to Analytics Hub and our user base is growing exponentially. This comes with next level of challenge of getting ready for DR. In a nutshell, we will talk about: /Technical Architect"}
{"session_id": "unity-catalog-lakeguard-secure-and-efficient-compute-your-enterprise", "title": "Unity Catalog Lakeguard: Secure and Efficient Compute for Your Enterprise", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "SQL"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Modern data workloads span multiple sources \u2014 data lakes, databases, apps like Salesforce and services like cloud functions. But as teams scale, secure data access and governance across shared compute becomes critical. In this session, learn how to confidently integrate external data and services into your workloads using Spark and Unity Catalog on Databricks. We'll explore compute options like serverless, clusters, workflows and SQL warehouses, and show how Unity Catalog\u2019s Lakeguard enforces fine-grained governance \u2014 even when concurrently sharing compute by multiple users. Walk away ready to choose the right compute model for your team\u2019s needs \u2014 without sacrificing security or efficiency. /Staff Product Manager\nDatabricks /Sr. Staff Product Manager"}
{"session_id": "unity-catalog-managed-tables-powerful-easy-and-interoperable", "title": "Unity Catalog Managed Tables: Powerful, Easy, and Interoperable", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog's Managed tables are the best of all worlds. Learn how they harness the Data Intelligence Platform to deliver lightning-fast performance\u2014without requiring a space shuttle cockpit worth of setting. Learn how they're fully interoperable with 3P clients\u2014be they Delta or Iceberg\u2014while still respecting a single source of governance. And learn about our exciting roadmap for how they will get even more powerful in the coming year. /Sr. Staff Product Manager\nDatabricks /Databricks"}
{"session_id": "unity-catalog-upgrades-made-easy-step-step-guide-databricks-labs-ucx", "title": "Unity Catalog Upgrades Made Easy. Step-by-Step Guide for Databricks Labs UCX", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks labs project UCX aims to optimize the Unity Catalog (UC) upgrade process, ensuring a seamless transition for businesses. This session will delve into various aspects of the UCX project including the installation and configuration of UCX, the use of the UCX Assessment Dashboard to reduce upgrade risks and prepare effectively for a UC upgrade, and the automation of key components such as group, table and code migration. Attendees will gain comprehensive insights into leveraging UCX and Lakehouse Federation for a streamlined and efficient upgrade process. This session is aimed at customers new to UCX as well as veterans. /Lead Solutions Architect\nDatabricks /Lead Product Specialist"}
{"session_id": "unleash-power-automated-data-governance-classify-tag-and-protect-your", "title": "Unleash the Power of Automated Data Governance: Classify, Tag and Protect Your Data \u2014 Effortlessly", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance"], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling to keep up with data governance at scale? Join us to explore how automated data classification, tag policies and ABAC streamline access control while enhancing security and compliance. Get an exclusive look at the new Governance Hub, built to give your teams deeper visibility into data usage, access patterns and metadata \u2014 all in one place. Whether you're managing thousands or millions of assets, discover how to classify, tag and protect your data estate effortlessly with the latest advancements in Unity Catalog. /Product Manager\nDatabricks /Senior Product Manager"}
{"session_id": "unleash-your-content-ai-powered-metadata-targeting-personalization-and", "title": "Unleash your Content: AI-Powered Metadata for Targeting, Personalization, and Brand Safety", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Field CTO & Co-Founder, Coactive Systems Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Field CTO & Co-Founder"}
{"session_id": "unleashing-data-governance-ifoodharnessing-system-tables-and-lineage", "title": "Unleashing Data Governance at iFood:Harnessing System Tables and Lineage for Dynamic Tag Propagation", "track": "DATA AND AI GOVERNANCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Scala"], "speakers": ["Staff Data Engineer, iFood"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With regulations like LGPD (Brazil's General Data Protection Law) and GDPR, managing sensitive data access is critical. This session demonstrates how to leverage Databricks Unity Catalog system tables and data lineage to dynamically propagate classification tags, empowering organizations to monitor governance and ensure compliance. The presentation covers practical steps, including system table usage, data normalization, ingestion with DLT pipelines and classification tag propagation to downstream tables. It also explores permission monitoring with alerts to proactively address governance risks. Designed for advanced audiences, this session offers actionable strategies to strengthen data governance, prevent breaches and avoid regulatory fines while building scalable frameworks for sensitive data management. /Head of Data Platform\niFood /Staff Data Engineer"}
{"session_id": "unlock-agentic-ai-insurance-deloitte-databricks", "title": "Unlock Agentic AI for Insurance With Deloitte & Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["AI & Data Managing Director, Deloitte"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/AI & Data Managing Director"}
{"session_id": "unlock-your-use-cases-deep-dive-structured-streamings-new", "title": "Unlock Your Use Cases: A Deep Dive on Structured Streaming\u2019s New TransformWithState API", "track": "DATA ENGINEERING AND STREAMING", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Streaming"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don\u2019t you just hate telling your customers \u201cNo\u201d? \u201cNo, I can\u2019t get you the data that quickly\u201d, or \u201cNo that logic isn\u2019t possible to implement\u201d really aren\u2019t fun to say. But what if you had a tool that would allow you to implement those use cases? What if it was in a technology you were already familiar with \u2014 say, Spark Structured Streaming? There is a brand new arbitrary stateful operations API called TransformWithState, and after attending this deep dive you won\u2019t have to say \u201cNo\u201d anymore. During this presentation we\u2019ll go through some real-world use cases and build them step-by-step. Everything from state variables, process vs. event time, watermarks, timers, state TTL, and even how you can initialize state with the checkpoint of another stream. Unlock your use cases with the power of Structured Streaming\u2019s TransformWithState! /Lead Solutions Architect\nDatabricks /Software Engineer"}
{"session_id": "unlocking-access-simplifying-identity-management-scale-databricks", "title": "Unlocking Access: Simplifying Identity Management at Scale With Databricks", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Effective Identity and Access Management (IAM) is essential for securing enterprise environments while enabling innovation and collaboration. As companies scale, ensuring users have the right access without adding administrative overhead is critical. In this session, we\u2019ll explore how Databricks is simplifying identity management by integrating with customers\u2019 Identity Providers (IDPs). Learn about Automatic Identity Management in Azure Databricks, which eliminates SCIM for Entra ID users and ensures scalable identity provisioning for other IDPs. We'll also cover externally managed groups, PIM integration and upcoming enhancements like a bring-your-own-IDP model for GCP. Through a customer success story and live demo, see how Databricks is making IAM more scalable, secure and user-friendly. /Specialist Solutions Architect\nDatabricks /Staff Product Manager"}
{"session_id": "unlocking-cross-organizational-collaboration-protect-environment", "title": "Unlocking Cross-Organizational Collaboration to Protect the Environment With Databricks at DEFRA", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PUBLIC SECTOR", "technologies": ["AI/BI", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Governance", "Scala"], "speakers": ["Head of Data Analytics and Science Hub, Defra"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to learn how the UK's Department for Environment, Food & Rural Affairs (DEFRA) transformed data use with Databricks\u2019 Unity Catalog, enabling nationwide projects through secure, scalable analytics. DEFRA safeguards the UK's natural environment. Historical fragmentation of data, talent and tools across siloed platforms and organizations, made it difficult to fully exploit the department\u2019s rich data. DEFRA launched its Data Analytics & Science Hub (DASH), powered by the Databricks Data Intelligence Platform, to unify its data ecosystem. DASH enables hundreds of users to access and share datasets securely. A flagship example demonstrates its power, using Databricks to process aerial photography and satellite data to identify peatlands in need of restoration \u2014 a complex task made possible through unified data governance, scalable compute and AI. Attendees will hear about DEFRA\u2019s journey, learn valuable lessons about building a platform crossing organizational boundaries. /Head of Data Exploitation\nDefra /Head of Data Analytics and Science Hub"}
{"session_id": "unlocking-data-intelligence-beginners-guide-unity-catalog", "title": "Unlocking Data Intelligence: A Beginner\u2019s Guide to Unity Catalog", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Lead Product Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Getting started with data and AI governance in the modern data stack? Unity Catalog is your gateway to secure, discoverable and well-governed data and AI assets. In this session, we\u2019ll break down what Unity Catalog is, why it matters and how it simplifies access control, lineage, discovery, auditing, business semantics and secure, open collaboration \u2014 all from a single place. We\u2019ll explore how it enables open interoperability across formats, tools and platforms, helping you avoid lock-in and build on open standards. Most importantly, you\u2019ll learn how Unity Catalog lays the foundation for data intelligence \u2014 by unifying governance across data and AI, enabling AI tuned to your business. It helps build a deep understanding of your data and delivers contextual, domain-specific insights that boost productivity for both technical and business users across any workload. /Principal Product Marketing Manager\nDatabricks /Lead Product Architect"}
{"session_id": "unlocking-enterprise-potential-key-insights-pgs-deployment-unity", "title": "Unlocking Enterprise Potential: Key Insights from P&G's Deployment of Unity Catalog at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Data Lake"], "speakers": ["Engineering Manager, P&G"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore Databricks Unity Catalog (UC) implementation by P&G to enhance data governance, reduce data redundancy and improve the developer experience through the enablement of a Lakehouse architecture. The presentation will cover: The distinction between data treated as a product and standard application data, highlighting how UC's structure maximizes the value of data in P&G's data lake. Real-life examples from two years of using Unity Catalog, demonstrating benefits such as improved governance, reduced waste and enhanced data discovery. Challenges related to disaster recovery and external data access, along with our collaboration with Databricks to address these issues. Sharing our experience can provide valuable insights for organizations planning to adopt Unity Catalog on an enterprise scale. /Engineering Manager"}
{"session_id": "unlocking-future-dairy-farming-leveraging-data-marketplaces-lely", "title": "Unlocking the Future of Dairy Farming: Leveraging Data Marketplaces at Lely", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "ELT"], "speakers": ["Senior Data Engineer, Lely"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Lely, a Dutch company specializing in dairy farming robotics, helps farmers with advanced solutions for milking, feeding and cleaning. This session explores Lely\u2019s implementation of an Internal Data Marketplace, built around Databricks' Private Exchange Marketplace. The marketplace serves as a central hub for data teams and business users, offering seamless access to data, analytics and dashboards. Powered by Delta Sharing, it enables secure, private listing of data products across business domains, including notebooks, views, models and functions. This session covers the pros and cons of this approach, best practices for setting up a data marketplace and its impact on Lely\u2019s operations. Real-world examples and insights will showcase the potential of integrating data-driven solutions into dairy farming. Join us to discover how data innovation drives the future of dairy farming through Lely\u2019s experience. /Data Platform Engineer\nLely /Senior Data Engineer"}
{"session_id": "unlocking-industrial-intelligence-aveva-and-agnico-eagle", "title": "Unlocking Industrial Intelligence with AVEVA and Agnico Eagle", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality", "ELT"], "speakers": ["VP, Digital Transformation, Agnico Eagle Mines Limited"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Industrial data is the foundation for operational excellence, but sharing and leveraging this data across systems presents significant challenges. Fragmented approaches create delays in decision-making, increase maintenance costs, and erode trust in data quality. This session explores how the partnership between AVEVA and Databricks addresses these issues through CONNECT, which integrates directly with Databricks via Delta Sharing. By accelerating time to value, eliminating data wrangling, ensuring high data quality, and reducing maintenance costs, this solution drives faster, more confident decision-making and greater user adoption. We will showcase how Agnico Eagle Mines\u2014the world\u2019s third-largest gold producer with 10 mines across Canada, Australia, Mexico, and Finland\u2014is leveraging this capability to overcome data intelligence barriers at scale. With this solution, Agnico Eagle is making insights more accessible and actionable across its entire organization. /SVP, Partners and Commercial Strategy\nAVEVA /VP, Digital Transformation"}
{"session_id": "unlocking-power-iceberg-our-journey-unified-lakehouse-databricks", "title": "Unlocking the Power of Iceberg: Our Journey to a Unified Lakehouse on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Scala"], "speakers": ["Group Manager, LSports"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session showcases our journey of adopting Apache Iceberg\u2122 to build a modern lakehouse architecture and leveraging Databricks advanced Iceberg support to take it to the next level. We\u2019ll dive into the key design principles behind our lakehouse, the operational challenges we tackled and how Databricks enabled us to unlock enhanced performance, scalability and streamlined data workflows. Whether you\u2019re exploring Apache Iceberg\u2122 or building a lakehouse on Databricks, this session offers actionable insights, lessons learned and best practices for modern data engineering. /Group Manager"}
{"session_id": "unlocking-power-retail-media-networks-how-data-changing-retail", "title": "Unlocking the Power of Retail Media Networks: How Data is Changing the Retail Promotions Landscape", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture", "Real-time"], "speakers": ["Head of Solutions Engineering, Hightouch"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Retail Media Networks (RMNs) are transforming how brands engage and connect with consumers throughout the omnichannel. In this session, Databricks and Hightouch will explore how data-driven advertising is reshaping retail promotions and enabling real-time activation of customer insights. Learn how unified data architectures and composable customer stacks are driving hyper-personalized, high-ROI campaigns. Whether you're a retailer monetizing first-party data or a brand optimizing ad spend, this session offers practical strategies and real-world examples to thrive in the evolving RMN landscape. /Partner Solutions, Consumer Industries\nDatabricks /Head of Solutions Engineering"}
{"session_id": "unlocking-streaming-power-how-sega-wins-dlt", "title": "Unlocking Streaming Power: How SEGA Wins With DLT", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Quality", "Streaming"], "speakers": ["Head of Data Services, SEGA Europe Limited"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Streaming data is hard and costly \u2014 that's the default opinion, but it doesn\u2019t have to be. In this session, discover how SEGA simplified complex streaming pipelines and turned them into a competitive edge. SEGA sees over 40,000 events per second. That's no easy task, but enabling personalised gaming experiences for over 50 million gamers drives a huge competitive advantage. If you\u2019re wrestling with streaming challenges, this talk is your next checkpoint. We\u2019ll unpack how DLT helped SEGA, from automated schema evolution and simple data quality management to seamless streaming reliability. Learn how DLT drives value by transforming chaos emeralds into clarity, delivering results for a global gaming powerhouse. We'll step through the architecture, approach and challenges we overcame. Join Craig Porteous, Microsoft MVP from Advancing Analytics, and Felix Baker, Head of Data Services at SEGA Europe, for a fast-paced, hands-on journey into DLT\u2019s unique powers. /Associate Head of Data Engineering\nAdvancing Analytics /Head of Data Services"}
{"session_id": "unlocking-tv-viewership-insights-privacy-compliant-data-sharing-clean", "title": "Unlocking TV Viewership Insights: Privacy-Compliant Data Sharing with Clean Rooms", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Solution Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delivering TV viewership data while balancing customer needs and privacy compliance is a complex challenge. After evaluating multiple solutions, FreeWheel, a Comcast company, selected Databricks Clean Rooms for their ability to programmatically enforce k-anonymization constraints and differential privacy, enabling secure and flexible data exploration. Unlike other solutions that require rigid templates and manual approvals, Clean Rooms gives FreeWheel's customers the freedom to run custom SQL queries while automatically excluding non-compliant data. With Clean Rooms, FreeWheel has streamlined workflows, simplified data management and unlocked new opportunities to monetize its data. The solution supports diverse and complex workloads with high performance and security, making it ideal for current needs and adaptable to future use cases Join this session to learn how FreeWheel is leveraging Clean Rooms to revolutionize TV viewership data sharing \u2014 enabling secure collaboration, improving efficiency and driving innovation in data monetization. /Software Engineering Manager\nFreeWheel, A Comcast Company /Solution Architect"}
{"session_id": "upcoming-apache-spark-41-next-chapter-unified-analytics", "title": "The Upcoming Apache Spark 4.1: The Next Chapter in Unified Analytics", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Quality", "ETL", "Python", "Real-time"], "speakers": ["Senior Engineering Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance. In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility (including the simplified Python Data Source API) and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution. Don\u2019t miss this chance to see Spark\u2019s next chapter! /Engineering Director\nDatabricks /Senior Engineering Manager"}
{"session_id": "ursa-augment-your-lakehouse-kafka-compatible-data-streaming", "title": "Ursa: Augment Your Lakehouse With Kafka-Compatible Data Streaming Capabilities", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Batch Processing", "Data Architecture", "Data Lake", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Director of Engineering, Automotive Industry"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data architectures evolve to meet the demands of real-time GenAI applications, organizations increasingly need systems that unify streaming and batch processing while maintaining compatibility with existing tools. The Ursa Engine offers a Kafka-API-compatible data streaming engine built on Lakehouse (Iceberg and Delta Lake). Designed to seamlessly integrate with data lakehouse architectures, Ursa extends your lakehouse capabilities by enabling streaming ingestion, transformation and processing \u2014 using a Kafka-compatible interface. In this session, we will explore how Ursa Engine augments your existing lakehouses with Kafka-compatible capabilities. Attendees will gain insights into Ursa Engine architecture and real-world use cases of Ursa Engine. Whether you're modernizing legacy systems or building cutting-edge AI-driven applications, discover how Ursa can help you unlock the full potential of your data. /Founder and CEO\nStreamNative /Director of Engineering"}
{"session_id": "use-external-models-databricks-connecting-azure-aws-gcp-anthropic-and", "title": "Use External Models in Databricks: Connecting to Azure, AWS, GCP, Anthropic and More", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["LLAMA", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session you will learn how to leverage a wide set of GenAI models in Databricks, including external connections to cloud vendors and other model providers. We will cover establishing connection to externally served models, via Mosaic AI Gateway. This will showcase connection to Azure, AWS & GCP models, as well as model vendors like Anthropic, Cohere, AI21 Labs and more. You will also discover best practices on model comparison, governance and cost control on those model deployments. /Product Manager"}
{"session_id": "using-ai-runtimes-model-training-and-development-databricks", "title": "Using AI Runtimes for Model Training and Development on Databricks", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Tired of managing complex GPU infrastructure for AI projects? Join us to discover how Databricks' AI Runtime simplifies GPU-accelerated model development with a serverless experience. Learn how the Machine Learning Runtime provides access to pre-configured GPU environments with popular ML frameworks and optimizations for unmatched performance. Key takeaways: Whether training custom models or fine-tuning foundation models, learn to focus on building AI solutions rather than managing infrastructure. /Databricks /Databricks"}
{"session_id": "using-catalogs-well-governed-and-efficient-data-ecosystem", "title": "Using Catalogs for a Well-Governed and Efficient Data Ecosystem", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Distinguished Engineer, Capital One"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Distinguished Engineer"}
{"session_id": "using-clean-rooms-privacy-centric-data-collaboration", "title": "Using Clean Rooms for Privacy-Centric Data Collaboration", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Privacy", "ELT"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Clean Rooms make privacy-safe collaboration possible for data, analytics, and AI \u2014 across clouds and platforms. Built on Delta Sharing, Clean Rooms enable organizations to securely share and analyze data together in a governed, isolated environment \u2014 without ever exposing raw data. In this session, you\u2019ll learn how to get started with Databricks Clean Rooms and unlock advanced use cases including: Whether you're a data scientist, engineer or data leader, this session will equip you to drive high-value collaboration while maintaining full control over data privacy and governance. /Databricks /Staff Software Engineer"}
{"session_id": "using-databricks-power-news-sentiment-capital-iq-pro-application", "title": "Using Databricks to Power News Sentiment, a Capital IQ Pro Application", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Data Scientist, S&P Global"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Data Scientist"}
{"session_id": "using-delta-rs-and-delta-kernel-rs-serve-cdc-feeds", "title": "Using Delta-rs and Delta-Kernel-rs to Serve CDC Feeds", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE", "LAKEFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Python", "Scala"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Change data feeds are a common tool for synchronizing changes between tables and performing data processing in a scalable fashion. Serverless architectures offer a compelling solution for organizations looking to avoid the complexity of managing infrastructure. But how can you bring CDFs into a serverless environment? In this session, we'll explore how to integrate Change Data Feeds into serverless architectures using Delta-rs and Delta-kernel-rs\u2014open-source projects that allow you to read Delta tables and their change data feeds in Rust or Python. We\u2019ll demonstrate how to use these tools with Lakestore\u2019s serverless platform to easily stream and process changes. You\u2019ll learn how to: /Senior Resident Solutions Architect\nDatabricks /Software Engineer"}
{"session_id": "using-identity-security-unity-catalog-faster-safer-data-access", "title": "Using Identity Security With Unity Catalog for Faster, Safer Data Access", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Senior Product Marketing Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Managing authentication effectively is key to securing your data platform. In this session, we\u2019ll explore best practices from Databricks for overcoming authentication challenges, including token visibility, MFA/SSO, CI/CD token federation and risk containment. Discover how to map your authentication maturity journey while maximizing security ROI. We'll showcase new capabilities like access token reports for improved visibility, streamlined MFA implementation and secure SSO with token federation. Learn strategies to minimize token risk through TTL limits, scoped tokens and network policies. You'll walk away with actionable insights to enhance your authentication practices and strengthen platform security on Databricks. /Product Management\nDatabricks /Senior Product Marketing Manager"}
{"session_id": "valentino-customer-journey-databricks", "title": "VALENTINO \u2014 Customer Journey with Databricks", "track": "DATA STRATEGY", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline"], "speakers": ["Global Head of Brand Intelligence, Data & Analytics, Valentino"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "VALENTINO decided to adopt Databricks within their own Data Strategy and Digital Transformation in order to address audience targeting across ads and marketing automation. Among several data sources and different legacy systems, Databricks offers the capabilities for an holistic view and the most recent data availability. In the past, at VALENTINO we spent significant time building and exporting CSVs and data pipelines for marketing teams \u2014campaigns often go out late. /Global Head of Brand Intelligence, Data & Analytics"}
{"session_id": "viewshift-dynamic-policy-enforcement-spark-and-sql-views", "title": "ViewShift: Dynamic Policy Enforcement With Spark and SQL Views", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "SQL"], "speakers": ["Senior Staff Software Engineer, LinkedIn"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dynamic policy enforcement is increasingly critical in today's landscape, where data compliance is a top priorities for companies, individuals, and regulators alike. In this talk, Walaa and Khai explore how LinkedIn has implemented a robust dynamic policy enforcement engine, ViewShift, and integrated it within its data lake. They will demystify LinkedIn's query engine stack by demonstrating how catalogs can automatically route table resolutions to compliance-enforcing SQL views. These SQL views possess several noteworthy properties: Auto-Generated: Created automatically from declarative data annotations. User-Centric: They honor user-level consent and preferences. Context-Aware: They apply different transformations tailored to specific use cases. Portable: Despite the SQL logic being implemented in a single dialect, it remains accessible across all engines. Join this session to learn how ViewShift helps ensure that compliance is seamlessly integrated into data processing workflows. /Senior Staff Software Engineer"}
{"session_id": "wednesday-keynote", "title": "Wednesday Keynote", "track": "", "level": "", "type": "KEYNOTE", "industry": "", "technologies": [], "duration": "180 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Be first to witness the latest breakthroughs from Databricks and share the success of innovative data and AI companies."}
{"session_id": "welcome-lakehouse-dwh-transformation-ma-data-sharing", "title": "Welcome Lakehouse, from a DWH transformation to a M&A data sharing", "track": "DATA SHARING AND COLLABORATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, ENTERPRISE TECHNOLOGY", "technologies": ["DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Solution Architect, dxc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Solution Architect"}
{"session_id": "welcome-reception", "title": "Welcome Reception", "track": "", "level": "", "type": "EVENING EVENT", "industry": "", "technologies": [], "duration": "120 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "what-i-wish-i-had-known-my-last-soc-confessions-cybersecurity-executive", "title": "\u201cWhat I Wish I Had Known in My Last SOC.\u201d Confessions of a Cybersecurity Executive.", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Janitor, Ziggiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Janitor"}
{"session_id": "whats-new-and-whats-next-building-impactful-aibi-dashboards", "title": "What's New and What's Next: Building Impactful AI/BI Dashboards", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Ready to take your AI/BI dashboards to the next level? This session dives into the latest capabilities in Databricks AI/BI Dashboards and how to maximize impact across your organization. Learn how data authors can tailor visualizations for different audiences, optimize performance and seamlessly integrate with Genie for a unified analytics experience. We\u2019ll also share practical tips on how business users and data teams can better collaborate \u2014 ensuring insights are accessible, actionable and aligned to business goals. /Sr. Manager, Engineering\nDatabricks /Product Manager"}
{"session_id": "whats-new-apache-sparktm-40", "title": "What\u2019s New in Apache Spark\u2122 4.0?", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Sr. Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join this session for a concise tour of Apache Spark\u2122 4.0\u2019s most notable enhancements: Whether you\u2019re a seasoned Spark user or new to the ecosystem, this talk will prepare you to leverage Spark 4.0\u2019s latest innovations for modern data and AI pipelines. /Senior Staff Software Engineer\nDatabricks /Sr. Staff Software Engineer"}
{"session_id": "whats-new-data-sharing-and-collaboration-live-demos", "title": "What's New with Data Sharing and Collaboration with Live Demos", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DATA MARKETPLACE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director of Product Marketing, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks continues to redefine how organizations securely and openly collaborate on data. With new innovations like Clean Rooms for multi-party collaboration, Sharing for Lakehouse Federation, cross-platform view sharing and Databricks Apps in the Marketplace, teams can now share and access data more easily, cost-effectively and across platforms \u2014 whether or not they\u2019re using Databricks. In this session, we\u2019ll deliver live demos of key capabilities that power this transformation: Join us to see how these tools enable trusted data sharing, accelerate insights and drive innovation across your ecosystem. Bring your questions and walk away with practical ways to put these capabilities into action today. /Director, Engineering\nDatabricks /Director of Product Marketing"}
{"session_id": "whats-new-databricks-assistant-exploration-production", "title": "What\u2019s New with Databricks Assistant: From Exploration to Production", "track": "ANALYTICS AND BI", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Sr. Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Assistant helps you get from initial exploration all the way to production faster and easier than ever. In this session, we'll show you how Assistant simplifies and accelerates common workflows, boosting your productivity across notebooks and the SQL editor. You'll get practical tips, see end-to-end examples in action, and hear about the latest capabilities we're excited about. We'll also discuss how we're continually improving Assistant to make your development experience faster, more contextual and more customizable. Join us to discover how to get the most out of Databricks Assistant and empower your team to build better and faster. /Product Manager\nDatabricks /Sr. Staff Product Manager"}
{"session_id": "whats-new-databricks-marketplace", "title": "What's New With Databricks Marketplace", "track": "DATA SHARING AND COLLABORATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATA MARKETPLACE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Director of Product, Apps, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Marketplace is the fastest-growing open marketplace for discovering, sharing, and monetizing data, AI models, notebooks, and applications \u2014 empowering organizations to accelerate analytics and AI innovation without vendor lock-in. In this session, we\u2019ll showcase the latest enhancements that make Databricks Marketplace more powerful and accessible than ever. Join us for a live walkthrough of new features and capabilities, including: private exchange, partner connect, provider usage analytics, AI Assistant integration etc. Whether you are a data provider or a data consumer, learn how the latest Databricks Marketplace innovations can help you unlock new value from your data and AI investments. /Product Manager\nDatabricks /Director of Product, Apps"}
{"session_id": "whats-new-databricks-sql-latest-features-and-live-demos", "title": "What\u2019s New in Databricks SQL: Latest Features and Live Demos", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Sr Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Product Manager\nDatabricks /Sr Staff Product Manager"}
{"session_id": "whats-new-security-and-compliance-databricks-data-intelligence-platform", "title": "What\u2019s New in Security and Compliance on the Databricks Data Intelligence Platform", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Senior Director, Product Management, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we\u2019ll walk through the latest advancements in platform security and compliance on Databricks \u2014 from networking updates to encryption, serverless security and new compliance certifications across AWS, Azure and GCP. We\u2019ll also share our roadmap and best practices for how to securely configure workloads on Databricks SQL Serverless, Unity Catalog, Mosaic AI and more \u2014 at scale. If you're building on Databricks and want to stay ahead of evolving risk and regulatory demands, this session is your guide. /Staff Product Manager\nDatabricks /Senior Director, Product Management"}
{"session_id": "whats-new-unity-catalog-live-demos", "title": "What\u2019s New in Unity Catalog With Live Demos", "track": "DATA AND AI GOVERNANCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["Data Governance"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join the Unity Catalog product team for an exclusive deep dive into the latest innovations and upcoming features of Unity Catalog! Explore cutting-edge advancements in access control, discovery, lineage and monitoring \u2014 plus get a sneak peek at what\u2019s coming next. Packed with live demos, expert insights and best practices from thousands of customers running Unity Catalog in production, this session is also your chance to engage directly with product experts and get answers to your most pressing questions. Don\u2019t miss this opportunity to stay ahead of the curve and elevate your data governance strategy! /Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "when-ai-helps-saving-lives", "title": "When AI Helps Saving Lives", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "MLFLOW", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Laerdal Medical"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "AI is set to revolutionize healthcare by enhancing diagnostic accuracy, optimising treatment protocols and improving response times in time-critical medical emergencies. As a world leader in this field, Laerdal Medical is continuously working to develop innovative solutions for training and equipment enabling healthcare workers and community responders to provide the best possible care. All to support our overall mission of \"Helping save one million more lives every year from 2030\". From the RevivR app, using AI to enable mobile-based and self-paced CPR training, to LiveBorn using ambient intelligence to reduce neonatal mortality and a new set of AI agents that will make sure we can reach our mission goal. In this presentation we will show how Laerdal is using AI and data to develop this new generation of innovative healthcare products, as well as how we work with partners and solutions like Databricks and MosaicML to make the development and operations of these AI tools efficient. /Laerdal Medical"}
{"session_id": "why-you-should-move-dlt-serverless", "title": "Why You Should Move to DLT Serverless", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "ETL"], "speakers": ["Sr Backline Technical Solutions Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT Serverless offers a range of benefits that make it an attractive option for organizations looking to optimize their ETL (Extract, Transform, Load) processes.Key benefits of DLT Serverless: By moving to DLT Serverless, organizations can achieve faster, more reliable, and cost-effective data pipeline management, ultimately driving better business insights and outcomes. /Sr Backline Technical Solutions Engineer"}
{"session_id": "women-data-ai", "title": "Women in Data + AI", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "150 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "you-mean-i-can-talk-my-data-reimagining-how-kpmg-engages-data-using", "title": "You Mean I Can Talk to My Data? Reimagining How KPMG Engages Data Using AI|BI Genie", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DLT", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Managing Director, KPMG LLP (HQ)"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "\u201cI don\u2019t want to spend time filtering through another dashboard \u2014 I just need an answer now.\u201d We\u2019ve all experienced the frustration of wading through dashboards, yearning for immediate answers. Traditional reports and visualizations, though essential, often complicate the process for decision-makers. The digital enterprise demands a shift towards conversational, natural language interactions with data. At KPMG, AI|BI Genie is reimagining our approach by allowing users to inquire about data just as they would consult a knowledgeable colleague, obtaining precise and actionable insights instantly. Discover how KPMG\u2019s Contract to Cash team leverages AI|BI Genie to enhance data engagement, drive insights and foster business growth. Join us to see AI|BI Genie in action and learn how you can transform your data interaction paradigm. /Managing Director"}
{"session_id": "your-wish-ai-command-get-grips-databricks-genie", "title": "Your Wish is AI Command \u2014 Get to Grips With Databricks Genie", "track": "ANALYTICS AND BI", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "SQL"], "speakers": ["CTO, Advancing Analytics"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Picture the scene \u2014 you're exploring a deep, dark cave looking for insights to unearth when, in a burst of smoke, Genie appears and offers you not three but unlimited data wishes. This isn't a folk tale, it's the growing wave of Generative BI that is going to be a part of analytics platforms. Databricks Genie is a tool powered by a SQL-writing LLM that redefines how we interact with data. We'll look at the basics of creating a new Genie room, scoping its data tables and asking questions. We'll help it out with some complex pre-defined questions and ensure it has the best chance of success. We'll give the tool a personality, set some behavioural guidelines and prepare some hidden easter eggs for our users to discover. Generative BI is going to be a fundamental part of the analytics toolset used across businesses. If you're using Databricks, you should be aware of Genie, if you're not, you should be planning your Generative BI Roadmap, and this session will answer your wishes. /CTO"}
