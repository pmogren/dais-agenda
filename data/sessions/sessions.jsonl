{"session_id": "10-reasons-use-databricks-delta-live-tables-your-next-data-processing", "title": "10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "DLT"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "SQL", "Data Quality", "Streaming", "AI", "ETL"], "speakers": ["Freelance Data Engineer, japila.pl"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling. This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project. I found over 10 reasons why investing in DLT for your next project is worth your time. I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT. The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of. /Freelance Data Engineer"}
{"session_id": "accelerate-end-end-multi-agents-databricks-and-dspy", "title": "Accelerate End-to-End Multi-Agents on Databricks and DSPy", "track": "ARTIFICIAL INTELLIGENCE", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, TRAVEL AND HOSPITALITY", "technologies": ["MLFLOW", "DSPY", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Delivery Solutions Architect, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "A production-ready GenAI application is more than the framework itself. Like ML, you need a unified platform to create an end-to-end workflow for production quality applications. Below is an example of how this works on Databricks: In this session, learn how to build agents to access all your data and models through function calling. Then, learn how DSPy enables agent interaction with each other to ensure the question is answered correctly. We will demonstrate a chatbot, powered by multiple agents, to be able to answer questions and reason answers the base LLM does not know and very specialized topics. /Delivery Solutions Architect"}
{"session_id": "accelerating-analytics-integrating-bi-tools-databricks-sql", "title": "Accelerating Analytics: Integrating BI Tools to Databricks SQL", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["SQL"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Did you know that you can integrate with your favorite BI tools directly from Databricks SQL? You don\u2019t even need to stand up an additional warehouse. This session shows the integrations with Microsoft Power Platform, Power BI, Tableau, Sigma and Looker so you can have a seamless integration experience. Directly connect your Databricks workspace with Fabric and Power BI workspaces or Tableau to publish and sync data models, with defined primary and foreign keys, between the two platforms. /Sr. Staff Product Manager\nDatabricks /Product Manager"}
{"session_id": "accelerating-data-ingestion-new-innovations-auto-loaders-performance", "title": "Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "LAKEFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala", "Streaming", "AI"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including: You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect. /Staff Product Manager\nDatabricks /Staff Software Engineer"}
{"session_id": "accelerating-data-transformation-best-practices-governance-agility-and", "title": "Accelerating Data Transformation: Best Practices for Governance, Agility and Innovation", "track": "DATA WAREHOUSING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT", "Analytics", "SQL", "Data Governance", "Data Quality", "Scala", "AI"], "speakers": ["Practice Director, Data & AI, NCS Australia"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, we will share NCS\u2019s approach to implementing a Databricks Lakehouse architecture, focusing on key lessons learned and best practices from our recent implementations. By integrating Databricks SQL Warehouse, the DBT Transform framework and our innovative test automation framework, we\u2019ve optimized performance and scalability, while ensuring data quality. We\u2019ll dive into how Unity Catalog enabled robust data governance, empowering business units with self-serve analytical workspaces to create insights while maintaining control. Through the use of solution accelerators, rapid environment deployment and pattern-driven ELT frameworks, we\u2019ve fast-tracked time-to-value and fostered a culture of innovation. Attendees will gain valuable insights into accelerating data transformation, governance and scaling analytics with Databricks. /Practice Director, Data & AI"}
{"session_id": "accelerating-growth-capital-markets-data-driven-strategies-success", "title": "Accelerating Growth in Capital Markets: Data-Driven Strategies for Success", "track": "DATA STRATEGY", "level": "BEGINNER", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATA MARKETPLACE", "AI/BI", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Analytics", "AI"], "speakers": ["Software Engineering Manager, B3 - Bolsa, Brasil e Balc\u00e3o"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Growth in capital markets thrives on innovation, agility and real-time insights. This session highlights how leading firms use Databricks\u2019 Data Intelligence Platform to uncover opportunities, optimize trading strategies and deliver personalized client experiences. Learn how advanced analytics and AI help organizations expand their reach, improve decision-making and unlock new revenue streams. Industry leaders share how unified data platforms break down silos, deepen insights and drive success in a fast-changing market. Key takeaways: Discover how data intelligence empowers capital markets firms to thrive in today\u2019s competitive landscape! /Financial Services Industry Director\nDatabricks /CDO - Wealth Management\nNorthern Trust /Software Engineering Manager"}
{"session_id": "accelerating-model-development-and-fine-tuning-databricks-twelvelabs", "title": "Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["DELTA LAKE", "MOSAIC AI", "PYTORCH"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala", "Delta Lake", "ELT", "AI"], "speakers": ["Solutions Architect - NVIDIA, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Scaling large language models (LLMs) and multimodal architectures requires efficient data management and computational power. NVIDIA NeMo Framework Megatron-LM on Databricks is an open source solution that integrates GPU acceleration and advanced parallelism with Databricks Delta Lakehouse, streamlining workflows for pre-training and fine-tuning models at scale. This session highlights context parallelism, a unique NeMo capability for parallelizing over sequence lengths, making it ideal for video datasets with large embeddings. Through the case study of TwelveLabs\u2019 Pegasus-1 model, learn how NeMo empowers scalable multimodal AI development, from text to video processing, setting a new standard for LLM workflows. /Chief Technology Officer & Co-Founder\nTwelve Labs, Inc /Solutions Architect - NVIDIA"}
{"session_id": "achieve-your-mission-ai-driven-decisions", "title": "Achieve Your Mission With AI-Driven Decisions", "track": "ARTIFICIAL INTELLIGENCE", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Government leaders overwhelmingly recognize the potential benefits of AI as critical to long-term strategic goals of efficiency, but implementation challenges and security concerns could be obstacles to success. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "achieving-precision-ai-retrieving-right-data-using-ai-agents", "title": "Achieving Precision in AI: Retrieving the Right Data Using AI Agents", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "PROFESSIONAL SERVICES", "technologies": ["MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Director, Confluent"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Director"}
{"session_id": "adobes-security-lakehouse-ocsf-data-efficiency-and-threat-detection", "title": "Adobe\u2019s Security Lakehouse: OCSF, Data Efficiency and Threat Detection at Scale", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["MLFLOW", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Security", "Real-time", "Scala", "AI"], "speakers": ["AntiMatter"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore how Adobe uses a sophisticated data security architecture built on the Databricks Data Intelligence Platform, along with the Open Cybersecurity Schema Framework (OCSF), to enable scalable, real-time threat detection across more than 10 PB of security data. We\u2019ll compare different approaches to OCSF implementation and demonstrate how Adobe processes massive security datasets efficiently \u2014 reducing query times by 18%, maintaining 99.4% SLA compliance, and supporting 286 security users across 17 teams with over 4,500 daily queries. By using Databricks' Platform for serverless compute, scalable architecture, and LLM-powered recommendations, Adobe has significantly improved processing speed and efficiency, resulting in substantial cost savings. We\u2019ll also highlight how OCSF enables advanced cross-tool analytics and automation, streamlining investigations. Finally, we\u2019ll introduce Databricks\u2019 new open-source OCSF toolkit for scalable security data normalization and invite the community to contribute. /Sr. Manager, Security Software Engineering\nAdobe /AntiMatter"}
{"session_id": "comprehensive-guide-streaming-data-intelligence-platform", "title": "A Comprehensive Guide to Streaming on the Data Intelligence Platform", "track": "DATA ENGINEERING AND STREAMING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, MANUFACTURING, RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Real-time", "Streaming", "AI", "Apache Spark"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT. /Director of Engineering\nDatabricks /Databricks"}
{"session_id": "japanese-mega-banks-journey-modern-genai-powered-governed-data-platform", "title": "A Japanese Mega-Bank\u2019s Journey to a Modern, GenAI-Powered, Governed Data Platform", "track": "DATA WAREHOUSING", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Chief Information Officer, SMBC"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "SMBC-AD, a major Japanese multinational financial services institution, has embarked on an initiative to build a GenAI-powered, modern and well-governed cloud data platform on Azure/Databricks. This initiative aims to build an enterprise data foundation encompassing loans, deposits, securities, derivatives, and other data domains. Its primary goals are: Deloitte and SMBC leveraged the Brickbuilder asset \u201cData as a Service for Banking\u201d to accelerate this highly strategic transformation. /Principal\nDeloitte Consulting LLP /Chief Information Officer"}
{"session_id": "no-code-ml-forecasting-platform-retail-and-cpg-companies", "title": "A No-Code ML Forecasting Platform for Retail and CPG companies", "track": "ARTIFICIAL INTELLIGENCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Director, Antuit - A Zebra Technologies company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Director"}
{"session_id": "practical-roadmap-becoming-expert-databricks-data-engineer", "title": "A Practical Roadmap to Becoming an Expert Databricks Data Engineer", "track": "DATA ENGINEERING AND STREAMING", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION", "technologies": ["APACHE SPARK", "DELTA LAKE", "DATABRICKS WORKFLOWS"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities. Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights. Key takeaways: /"}
{"session_id": "practitioners-guide-databricks-serverless", "title": "A Practitioner\u2019s Guide to Databricks Serverless", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "Analytics", "Data Engineering"], "speakers": ["Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows. /Product Specialist"}
{"session_id": "prescription-success-leveraging-dabs-faster-deployment-and-better", "title": "A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "AI"], "speakers": ["Principal Data Engineer, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Health Catalyst (HCAT) transformed its CI/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements. HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making. /Sr. Solutions Architect\nDatabricks /Principal Data Engineer"}
{"session_id": "unified-solution-data-management-and-model-training-apache-iceberg-and", "title": "A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, PUBLIC SECTOR", "technologies": ["APACHE ICEBERG", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala", "Machine Learning", "AI", "Streaming"], "speakers": ["machine learning system engineer, ByteDance"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming). Magnus uses Iceberg\u2019s branch/tag to manage massive datasets/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability. When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility. In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training. /Infrastructure Engineer\nByteDance /machine learning system engineer"}
{"session_id": "what-i-wish-i-had-known-my-last-soc-confessions-cybersecurity-executive", "title": "\u201cWhat I wish I had known in my last SOC.\u201d Confessions of a cybersecurity executive.", "track": "DATA AND AI GOVERNANCE", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Janitor, Ziggiz"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Janitor"}
