{"session_id": "advanced-governance-and-auth-databricks-apps", "title": "Advanced Governance and Auth With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore advanced governance and authentication patterns for building secure, enterprise-grade apps with Databricks Apps. Learn how to configure complex permissions and manage access control using Unity Catalog. We\u2019ll dive into \u201con-behalf-of-user\u201d authentication \u2014 allowing agents to enforce user-specific access controls \u2014 and cover API-based authentication, including PATs and OAuth flows for external integrations. We\u2019ll also highlight how Addepar uses these capabilities to securely build and scale applications that handle sensitive financial data. Whether you're building internal tools or customer-facing apps, this session will equip you with the patterns and tools to ensure robust, secure access in your Databricks apps. /Staff Software Engineer"}
{"session_id": "ai-powering-epsilons-identity-strategy-unified-marketing-platform", "title": "AI Powering Epsilon's Identity Strategy: Unified Marketing Platform on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Data Warehouse", "Delta Lake", "ELT"], "speakers": ["Vice President, Decision Sciences, Epsilon Data Management"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to hear about how Epsilon Data Management migrated Epsilon\u2019s unique, AI-powered marketing identity solution from multi-petabyte on-prem Hadoop and data warehouse systems to a unified Databricks Lakehouse platform. This transition enabled Epsilon to further scale its Decision Sciences solution and enable new cloud-based AI research capabilities on time and within budget, without being bottlenecked by the resource constraints of on-prem systems. Learn how Delta Lake, Unity Catalog, MLflow and LLM endpoints powered massive data volume, reduced data duplication, improved lineage visibility, accelerated Data Science and AI, and enabled new data to be immediately available for consumption by the entire Epsilon platform in a privacy-safe way. Using the Databricks platform as the base for AI and Data Science at global internet scale, Epsilon deploys marketing solutions across multiple cloud providers and multiple regions for many customers. /Vice President , Database\nEpsilon Data Management /Vice President, Decision Sciences"}
{"session_id": "apache-iceberg-unity-catalog-hellofresh", "title": "Apache Iceberg with Unity Catalog at HelloFresh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["Senior Staff Data Engineer, HelloFresh"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Table formats like Delta Lake and Iceberg have been game changers for pushing lakehouse architecture into modern Enterprises. The acquisition of Tabular added Iceberg to the Databricks ecosystem, an open format that was already well supported by processing engines across the industry. At HelloFresh we are building a lakehouse architecture that integrates many touchpoints and technologies all across the organization. As such we chose Iceberg as the table format to bridge the gaps in our decentralized managed tech landscape. We are leveraging Unity Catalog as the Iceberg REST catalog of choice for storing metadata and managing tables. In this talk we will outline our architectural setup between Databricks, Spark, Flink and Snowflake and will explain the native Unity Iceberg REST catalog, as well as catalog federation towards connected engines. We will highlight the impact on our business and discuss the advantages and lessons learned from our early adopter experience. /Director of Data Engineering\nHelloFresh /Senior Staff Data Engineer"}
{"session_id": "breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse", "title": "Breaking the Ice(berg): Riskified\u2019s Journey to its Next-Generation Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Warehouse"], "speakers": ["Staff Data Platform Engineer, Riskified"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How many of you manage multiple data stores in your organization, wrestling with different data formats that led to data duplication, infrastructure redundancy, and uncontrolled costs? Imagine reducing these costs by 25% while maintaining your existing SLAs \u2014 this is exactly what we achieved with our next-generation architecture. In this session, we'll show you how we built Riskified's next-generation lakehouse by leveraging Databricks' native Apache Iceberg support and Unity Catalog. We'll share our innovative approach to cross-platform querying without data duplication, and how we transformed our data warehouse into a modern lakehouse architecture. Throughout the session, we'll explore the technical challenges we conquered, from data migration to performance optimization. The result? A simplified world where everything is identical across engines, leaving users with just one choice \u2014 which query engine best suits their use case. /Data Platform Architect\nRiskified /Staff Data Platform Engineer"}
{"session_id": "breaking-silos-enabling-databricks-snowflake-interoperability-iceberg", "title": "Breaking Silos: Enabling Databricks-Snowflake Interoperability With Iceberg and Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT"], "speakers": ["Member of Technical Staff, Sol Arch, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow more complex, organizations often struggle with siloed platforms and fragmented governance. In this session, we\u2019ll explore how our team made Databricks the central hub for cross-platform interoperability, enabling seamless Snowflake integration through Unity Catalog and the Iceberg REST API. We\u2019ll cover: By leveraging Uniform, Delta, and Iceberg, we created a flexible, vendor-agnostic architecture that bridges Databricks and Snowflake without compromising performance or security. /Director of Data Engineering\nT-Mobile /Member of Technical Staff, Sol Arch"}
{"session_id": "build-ai-powered-applications-natively-databricks", "title": "Build AI-Powered Applications Natively on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI", "Analytics", "Data Quality", "Scala"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how to build and deploy AI-powered applications natively on the Databricks Data Intelligence Platform. This session introduces best practices and a standard reference architecture for developing production-ready apps using popular frameworks like Dash, Shiny, Gradio, Streamlit and Flask. Learn how to leverage agents for orchestration and explore primary use cases supported by Databricks Apps, including data visualization, AI applications, self-service analytics and data quality monitoring. With serverless deployment and built-in governance through Unity Catalog, Databricks Apps enables seamless integration with your data and AI models, allowing you to focus on delivering impactful solutions without the complexities of infrastructure management. Whether you're a data engineer or an app developer, this session will equip you with the knowledge to create secure, scalable and efficient applications within a Databricks environment. /Staff Software Engineer"}
{"session_id": "building-seamless-multi-cloud-platform-secure-portable-workloads", "title": "Building a Seamless Multi-Cloud Platform for Secure Portable Workloads", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many challenges to making a data platform actually a platform, something that hides complexity. Data engineers and scientists are looking for a simple and intuitive abstraction to focus on their work, not where it runs to maintain compliance, what credentials it uses to access data or how it generates operational telemetry. At Databricks we\u2019ve developed a data-centric approach to workload development and deployment that enables data workers to stop doing migrations and instead develop with confidence. Attend this session to learn how to run simple, secure and compliant global multi-cloud workloads at scale on Databricks. /Staff Software Engineer"}
{"session_id": "capitalizing-alternatives-data-addepar-platform-private-markets", "title": "Capitalizing Alternatives Data on the Addepar Platform: Private Markets Benchmarking", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Pipeline", "ELT", "Scala"], "speakers": ["Addepar"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Addepar possesses an enormous private investment data set with 40% of the $7T assets on the platform allocated to alternatives. Leveraging the Addepar Data Lakehouse (ADL), built on Databricks, we have built a scalable data pipeline that assesses millions of private fund investment cash flows and translates it to a private fund benchmarks data offering. Investors on the Addepar platform can leverage this data seamlessly integrated against their portfolio investments and obtain actionable investment insights. At a high-level, this data offering consists of an extensive data aggregation, filtering, and construction logic that dynamically updates for clients through the Databricks job workflows. This derived dataset has gone through several iterations with investment strategists and academics that leveraged delta shared tables. Irrespective of the data source, the data pipeline coalesces all relevant cash flow activity against a unique identifier before constructing the benchmarks. /Addepar"}
{"session_id": "cutting-costs-not-performance-optimizing-databricks-scale", "title": "Cutting Costs, Not Performance: Optimizing Databricks at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES, PUBLIC SECTOR, TRAVEL AND HOSPITALITY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Scala"], "speakers": ["Lead Engineer, NTT DATA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As Databricks transforms data processing, analytics and machine learning, managing platform costs has become crucial for organizations aiming to maximize value while staying within budget. While Databricks offers unmatched scalability and performance, inefficient usage can lead to unexpected cost overruns. This presentation will explore common challenges organizations face in controlling Databricks costs and provide actionable best practices for optimizing resource allocation, preventing over-provisioning and eliminating underutilization. Drawing from NTT DATA\u2019s experience, I'll share how we reduced Databricks costs by up to 50% through strategies like choosing the right compute resource, leveraging manage tables and using Unity Catalog features, such as system tables, to monitor consumption. Join this session to gain practical insights and tools that will empower your team to optimize Databricks without overspending. /Project Manager\nNTTDATA /Lead Engineer"}
{"session_id": "daft-and-unity-catalog-multimodalai-native-lakehouse", "title": "Daft and Unity Catalog: A Multimodal/AI-Native Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake"], "speakers": ["Co-Founder, Eventual"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Modern data organizations have moved beyond big data analytics to also incorporate advanced AI/ML data workloads. These workflows often involve multimodal datasets containing documents, images, long-form text, embeddings, URLs and more. Unity Catalog is an ideal solution for organizing and governing this data at scale. When paired with the Daft open source data engine, you can build a truly multimodal, AI-ready data lakehouse. In this session, we\u2019ll explore how Daft integrates with Unity Catalog\u2019s core features (such as volumes and functions) to enable efficient, AI-driven data lakehouses. You will learn how to ingest and process multimodal data (images, text and videos), run AI/ML transformations and feature extractions at scale, and maintain full control and visibility over your data with Unity Catalog\u2019s fine-grained governance. /Co-Founder"}
{"session_id": "data-modeling-101-data-lakehouse-demystified", "title": "Data Modeling 101 for Data Lakehouse Demystified", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Modeling", "Data Warehouse", "Scala"], "speakers": ["Lead Data Engineer, Pythian"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today\u2019s data-driven world, the Data Lakehouse has emerged as a powerful architectural paradigm that unifies the flexibility of data lakes with the reliability and structure of traditional data warehouses. However, organizations must adopt the right data modeling techniques to unlock its full potential to ensure scalability, maintainability and efficiency. This session is designed for beginners looking to demystify the complexities of data modeling for the lakehouse and make informed design decisions. We\u2019ll break down Medallion Architecture, explore key data modeling techniques and walk through the maturity stages of a successful data platform \u2014 transitioning from raw, unstructured data to well-organized, query-efficient models. /Lead Data Engineer"}
{"session_id": "databricks-action-azures-blueprint-secure-and-cost-effective-operations", "title": "Databricks in Action: Azure\u2019s Blueprint for Secure and Cost-Effective Operations", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Real-time", "Scala"], "speakers": ["Information Security Specialist Expert, Erste Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Erste Group's transition to Azure Databricks marked a significant upgrade from a legacy system to a secure, scalable and cost-effective cloud platform. The initial architecture, characterized by a complex hub-spoke design and stringent compliance regulations, was replaced with a more efficient solution. The phased migration addressed high network costs and operational inefficiencies, resulting in a 60% reduction in networking costs and a 30% reduction in compute costs for the central team. This transformation, completed over a year, now supports real-time analytics, advanced machine learning and GenAI while ensuring compliance with European regulations. The new platform features a Unity Catalogue, separate data catalogs and dedicated workspaces, demonstrating a successful shift to a cloud-based machine learning environment with significant improvements in cost, performance and security. /Senior Solution Manager\nErste Group /Information Security Specialist Expert"}
{"session_id": "databricks-good-bad-and-ugly", "title": "Databricks, the Good, the Bad and the Ugly", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks is the bestest platform ever where everything is perfect and nothing else could ever make it any better, right? \u2026right? You and I know, this is not true. Don\u2019t get me wrong, there are features that I absolutely love, but there are also some that require powering through the papercuts. And then there are those that I pretend don\u2019t exist. I\u2019ll be opening up to give my honest take on three of each category, why I do (or don\u2019t) like them, and then telling you which talks to attend to find out more. /Staff Developer Advocate"}
{"session_id": "declarative-pipelines-ask-us-anything", "title": "Declarative Pipelines \u2014 Ask Us Anything", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL", "SQL"], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an insightful Ask Me Anything (AMA) session on Declarative Pipelines \u2014 a powerful approach to simplify and optimize data workflows. Learn how to define data transformations using high-level, SQL-like semantics, reducing boilerplate code while improving performance and maintainability. Whether you're building ETL processes, feature engineering pipelines, or analytical workflows, this session will cover best practices, real-world use cases and how Declarative Pipelines can streamline your data applications. Bring your questions and discover how to make your data processing more intuitive and efficient! /Engineering Director\nDatabricks /PM Director, Developer Relations\nDatabricks /Software Engineer"}
{"session_id": "delta-and-databricks-cost-effective-exabyte-scale-real-time-web", "title": "Delta and Databricks as a Cost-Effective, Exabyte-Scale, Real-Time Web Application Backend", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "ETL", "Real-time", "SQL", "Streaming"], "speakers": ["VP, Distinguished Engineer, Capital One Financial"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Delta Lake architecture promises to provide a single, highly functional, and high-scale copy of data that can be leveraged by a variety of tools to satisfy a broad range of use cases. To date, most use cases have focused on interactive data warehousing, ETL, model training, and streaming. Real-time access is generally delegated to costly and sometimes difficult-to-scale NoSQL, indexed storage, and domain-specific specialty solutions, which provide limited functionality compared to Spark on Delta Lake. In this session, we will explore the Delta data-skipping and optimization model and discuss how Capital One leveraged it along with Databricks photon and Spark Connect to implement a real-time web application backend. We\u2019ll share how we built a highly-functional and performant security information and event management user experience (SIEM UX) that is cost effective. /VP, Distinguished Engineer"}
{"session_id": "delta-kernel-rs-unparalleled-interoperability-across-query-engines", "title": "Delta-Kernel-RS: Unparalleled Interoperability Across Query Engines", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us as we introduce Delta-Kernel-RS, a new Rust implementation of the Delta Lake protocol designed for unparalleled interoperability across query engines. In this session, we will explore how maintaining a native implementation of the Delta specification \u2014 with native C and C++ FFI support \u2014 can deliver consistent benefits across diverse data processing systems, eliminating the need for repetitive, engine-specific reimplementations. We will dive deep into a real-world case study where a query engine harnessed Delta-Kernel-RS to unlock significant data skipping improvements \u2014 enhancements achieved \u201cfor free\u201d by leveraging the kernel. Attendees will gain insights into the architectural decisions, interoperability strategies and the practical impact of this innovation on performance and development efficiency in modern data ecosystems. /software engineer/ski bum\ndatabricks /Staff Developer Advocate"}
{"session_id": "delta-kernel-rust-and-java", "title": "Delta Kernel for Rust and Java", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Code Monkey, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Kernel makes it easy for engines and connectors to read and write Delta tables. It supports many Delta features and robust connectors, including DuckDB, Clickhouse, Spice AI and delta-dotnet. In this session, we'll cover lessons learned about how to build a high-performance library that lets engines integrate the way they want, while not having to worry about the details of the Delta protocol. We'll talk through how we streamlined the API as well as its changes and underlying motivations. We'll discuss some new highlight features like write support, and the ability to do CDF scans. Finally we'll cover the future roadmap for the Kernel project and what you can expect from the project over the coming year. /Code Monkey"}
{"session_id": "delta-lake-data-mesh", "title": "Delta Lake on the Data Mesh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PUBLIC SECTOR", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Mesh", "Delta Lake", "ELT"], "speakers": ["Principal Engineer, Nextdata"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake has proven to be an excellent storage format. Coupled with the Databricks platform, the storage format has shined as a component of a distributed system on the lakehouse. The pairing of Delta and Spark provides an excellent platform, but users often struggle to perform comparable work outside of the Spark ecosystem. Tools such as delta-rs, Polars and DuckDb have brought access to users outside of Spark, but they are only building blocks of a larger system. In this 40-minute talk we will demonstrate how users can use data products on the Nextdata OS data mesh to interact with the Databricks platform to drive Delta Lake workflows. Additionally, we will show how users can build autonomous data products that interact with their Delta tables both inside and outside of the lakehouse platform. Attendees will learn how to integrate the Nextdata OS data mesh with the Databricks platform as both an external and integral component. /Principal Engineer"}
{"session_id": "delta-lake-liquid-clustering-lightning-fast-queries-massive-datasets", "title": "Delta Lake Liquid Clustering: Lightning-Fast Queries on Massive Datasets", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this presentation, we\u2019ll dive into the power of Liquid Clustering\u2014an innovative, out-of-the-box solution that automatically tunes your data layout to scale effortlessly with your datasets. You\u2019ll get a deep look at how Liquid Clustering works, along with real-world examples of customers leveraging it to unlock blazing-fast query performance on petabyte-scale datasets. We\u2019ll also give you an exciting sneak peek into the roadmap ahead, with upcoming features and enhancements to come. /Product Manager\nDatabricks /Software Engineer"}
{"session_id": "disneys-foundational-medallion-journey-next-generation-data", "title": "Disney's Foundational Medallion: A Journey Into Next-Generation Data Architecture", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT", "Scala", "Streaming"], "speakers": ["Director, Data Engineering, Disney"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Step into the world of Disney Streaming as we unveil the creation of our Foundational Medallion, a cornerstone in our architecture that redefines how we manage data at scale. In this session, we'll explore how we tackled the multi-faceted challenges of building a consistent, self-service surrogate key architecture \u2014 a foundational dataset for every ingested stream powering Disney Streaming's data-driven decisions. Learn how we streamlined our architecture and unlocked new efficiencies by leveraging cutting-edge Databricks features such as liquid clustering, Photon with dynamic file pruning, Delta's identity column, Unity Catalog and more \u2014 transforming our implementation into a simpler, more scalable solution. Join us on this thrilling journey as we navigate the twists and turns of designing and implementing a new Medallion at scale \u2014 the very heartbeat of our streaming business! /Director, Data Engineering"}
{"session_id": "doordash-customer-360-data-store-and-its-evolution-become-entity", "title": "Doordash Customer 360 Data Store and its Evolution to Become an Entity Management Framework", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Data Governance", "Delta Lake", "ELT", "Scala"], "speakers": ["Data Engineer, DoorDash"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The \"Doordash Customer 360 Data Store\" represents a foundational step in centralizing and managing customer profile to enable targeting and personalized customer experiences built on Delta Lake. This presentation will explore the initial goals and architecture of the Customer 360 Data Store, its journey to becoming a robust entity management framework, and the challenges and opportunities encountered along the way. We will discuss how the evolution addressed scalability, data governance and integration needs, enabling the system to support dynamic and diverse use cases, including customer lifecycle analytics, marketing campaign targeting using segmentation. Attendees will gain insight into key design principles, technical innovations and strategic decisions that transformed the system into a flexible platform for entity management, positioning it as a critical enabler of data-driven growth at Doordash. /Data Engineering Manager\nDoordash /Data Engineer"}
{"session_id": "embracing-unity-catalog-and-empowering-innovation-genie-room", "title": "Embracing Unity Catalog and Empowering Innovation With Genie Room", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "ETL"], "speakers": ["Data Engineer, Bagelcode"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bagelcode, a leader in the social casino industry, has utilized Databricks since 2018 and manages over 10,000 tables via Hive Metastore. In 2024, we embarked on a transformative journey to resolve inefficiencies and unlock new capabilities. Over five months, we redesigned ETL pipelines with Delta Lake, optimized partitioned table logs and executed a seamless migration with minimal disruption. This effort improved governance, simplified management and unlocked Unity Catalog\u2019s advanced features. Post-migration, we integrated the Genie Room with Slack to enable natural language queries, accelerating decision-making and operational efficiency. Additionally, a lineage-powered internal tool allowed us to quickly identify and resolve issues like backfill needs or data contamination. Unity Catalog has revolutionized our data ecosystem, elevating governance and innovation. Join us to learn how Bagelcode unlocked its data\u2019s full potential and discover strategies for your own transformation. /Data Engineer\nBagelcode /Data Engineer"}
{"session_id": "empowering-progress-building-personalized-training-goal-ecosystem", "title": "Empowering Progress: Building a Personalized Training Goal Ecosystem with Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES", "technologies": ["APACHE SPARK", "DATABRICKS WORKFLOWS", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science"], "speakers": ["Sr Manager, Data Science and AI, Tonal"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Tonal Trainer is the world\u2019s most intelligent home gym, combining cutting-edge hardware and sensors with AI-powered software to deliver personalized fitness experiences. Members share needs with us through interviews and through social media platforms. One item that consistently came up was having difficulty measuring progress on the machine. We created and deployed a robust Training Goal (TG) ecosystem for our users. TG is a four-part solution: Databricks enabled us to deploy each of these components by the feature launch deadline. /Senior Director, Data and AI\nTonal /Sr Manager, Data Science and AI"}
{"session_id": "end-end-interoperable-data-platform-how-bosch-leverages-databricks", "title": "End-to-End Interoperable Data Platform: How Bosch Leverages Databricks Supply Chain Consolidation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING", "technologies": ["DATABRICKS SQL", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake"], "speakers": ["Project Lead Logistics Innovations, Robert Bosch GmbH"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will showcase Bosch\u2019s journey in consolidating supply chain information using the Databricks platform. It will dive into how Databricks not only acts as the central data lakehouse but also integrates seamlessly with transformative components such as dbt and Large Language Models (LLMs). The talk will highlight best practices, architectural considerations, and the value of an interoperable platform in driving actionable insights and operational excellence across complex supply chain processes. Key Topics and Sections /Development Lead\nRobert Bosch GmbH /Project Lead Logistics Innovations"}
{"session_id": "energy-and-utilities-industry-forum", "title": "Energy and Utilities Industry Forum", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["AI/BI", "DELTA SHARING", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Director of Energy and Utilities, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for a compelling forum exploring how energy leaders are harnessing data and AI to build a more sustainable future. As the industry navigates the complex balance between rising global energy demands and ambitious decarbonization goals, innovative companies are discovering that intelligence-driven operations are the key to success. From optimizing renewable energy integration to revolutionizing grid management, learn how energy pioneers are using AI to transform traditional operations while accelerating the path to net zero. This session reveals how Databricks is empowering energy companies to turn their sustainability aspirations into reality, proving that the future of energy is both clean and intelligent. /Director of Energy and Utilities"}
{"session_id": "enterprise-financial-crime-detection-lakehouse-framework-fatf-basel-iii", "title": "Enterprise Financial Crime Detection: A Lakehouse Framework for FATF, Basel III, and BSA Compliance", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Quality"], "speakers": ["Engineering Lead - Data & ML, Barclays"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We will present a framework for FinCrime detection leveraging Databricks lakehouse architecture specifically how institutions can achieve both data flexibility & ACID transaction guarantees essential for FinCrime monitoring. The framework incorporates advanced ML models for anomaly detection, pattern recognition, and predictive analytics, while maintaining clear data lineage & audit trails required by regulatory bodies. We will also discuss some specific improvements in reduction of false positives, improvement in detection speed, and faster regulatory reporting, delve deep into how the architecture addresses specific FATF recommendations, Basel III risk management requirements, and BSA compliance obligations, particularly in transaction monitoring and SAR. The ability to handle structured and unstructured data while maintaining data quality and governance makes it particularly valuable for large financial institutions dealing with complex, multi-jurisdictional compliance requirements. /Field Engineering\nDatabricks /Engineering Lead - Data & ML Engineering"}
{"session_id": "extending-lakehouse-power-interoperable-compute-unity-catalog-open-apis", "title": "Extending the Lakehouse: Power Interoperable Compute With Unity Catalog Open APIs", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The lakehouse is built for storage flexibility, but what about compute? In this session, we\u2019ll explore how Unity Catalog enables you to connect and govern multiple compute engines across your data ecosystem. With open APIs and support for the Iceberg REST Catalog, UC lets you extend access to engines like Trino, DuckDB, and Flink while maintaining centralized security, lineage, and interoperability. Learn how to bring flexibility to your compute layer\u2014without compromising control. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "full-stack-innovation-building-data-and-ai-products-databricks-apps", "title": "The Full Stack of Innovation: Building Data and AI Products With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "DEEP DIVE", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES, FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "MOSAIC AI"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Regional Architect, Africa & Middle East, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this deep-dive technical session, Ivan Trusov (Sr. SSA @ Databricks) and Giran Moodley (SA @ Databricks) \u2014 will explore the full-stack development of Databricks Apps, covering everything from frameworks to deployment. We\u2019ll walk through essential topics, including: Expect a highly practical session with several live demos, showcasing the development loop, testing workflows and CI/CD automation. Whether you\u2019re building internal tools or AI-powered products, this talk will equip you with the knowledge to ship robust, scalable Databricks Apps. /Senior Specialist Solutions Architect\nDatabricks /Regional Architect, Africa & Middle East"}
{"session_id": "future-dsv2-apache-sparktm", "title": "The Future of DSv2 in Apache Spark\u2122", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "DSv2, Spark's next-generation Catalog API, is gaining traction among data source developers. It shifts complexity to Apache Spark\u2122, improves connector reliability and unlocks new functionality such as catalog federation, MERGE operations, storage-partitioned joins, aggregate pushdown, stored procedures and more. This session covers the design of DSv2, current strengths and gaps and its evolving roadmap. It's intended for Spark users and developers working with data sources, whether custom-built or off-the-shelf. /Databricks"}
{"session_id": "future-open-table-formats", "title": "The Future of Open Table Formats", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Member of Technical Staff, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/PM Director, Developer Relations\nDatabricks /Member of Technical Staff"}
{"session_id": "games-industry-forum-games-executive-perspective-impact-data-and-ai", "title": "Games Industry Forum: The Games Executive Perspective on the Impact of Data and AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS SQL", "MOSAIC AI"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Global Games GTM Leader, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Come hear from some of the biggest names in games about how Data and AI is helping them shape their future, build better games and create player-centric experiences. In this session you\u2019ll hear, first, what Databricks is hearing from Games studios globally as their key priorities. We then shift to customers sharing their stories and perspectives. You\u2019ll leave invigorated on the impact Data and AI can have on games, and our global players and have new ideas on ways you can further your impact. /Global Games GTM Leader"}
{"session_id": "games24x7s-revolutionizing-online-skill-gaming-databricks", "title": "Games24x7's Revolutionizing Online Skill Gaming With Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Engineering Manager, Games24x7"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Games24x7 deals with terabytes of player data generated by 5.5B+ games played on RummyCircle and 500M+ teams created on My11Circle every year, to deliver immersive gaming experiences to 120M+ players. In this presentation we will talk about: /Games24x7 /Engineering Manager"}
{"session_id": "how-blue-origin-accelerates-innovation-databricks-and-aws-govcloud", "title": "How Blue Origin Accelerates Innovation With Databricks and AWS GovCloud", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING, PUBLIC SECTOR", "technologies": ["DATABRICKS SQL", "DLT", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Blue Origin is revolutionizing space exploration with a mission-critical data strategy powered by Databricks on AWS GovCloud. Learn how they leverage Databricks to meet ITAR and FedRAMP High compliance, streamline manufacturing and accelerate their vision of a 24/7 factory. Key use cases include predictive maintenance, real-time IoT insights and AI-driven tools that transform CAD designs into factory instructions. Discover how Delta Lake, Structured Streaming and advanced Databricks functionalities like Unity Catalog enable real-time analytics and future-ready infrastructure, helping Blue Origin stay ahead in the race to adopt generative AI and serverless solutions. /Staff Product Manager"}
{"session_id": "how-build-open-lakehouse-best-practices-interoperability", "title": "How to Build an Open Lakehouse: Best Practices for Interoperability", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager"}
{"session_id": "how-serverless-empowered-nationwide-build-cost-efficient-and-world", "title": "How Serverless Empowered Nationwide to Build Cost-Efficient and World Class BI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS APPS", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Scala"], "speakers": ["Lead Data Engineer, Nationwide"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks\u2019 Serverless compute streamlines infrastructure setup and management, delivering unparalleled performance and cost optimization for Data and BI workflows. In this presentation, we will explore how Nationwide is leveraging Databricks\u2019 serverless technology and unified governance through Unity Catalog to build scalable, world-class BI solutions. Key features like AI/BI Dashboards, Genie, Materialized Views, Lakehouse Federation and Lakehouse Apps, all powered by serverless, have empowered business teams to deliver faster, scalable and smarter insights. We will show how Databricks\u2019 serverless technology is enabling Nationwide to unlock new levels of efficiency and business impact, and how other organizations can adopt serverless technology to realize similar benefits. /Lead Data Engineer"}
{"session_id": "how-texas-rangers-use-unified-data-platform-drive-world-class-baseball", "title": "How the Texas Rangers Use a Unified Data Platform to Drive World Class Baseball Analytics", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DATABRICKS WORKFLOWS", "DELTA SHARING", "MLFLOW"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Quality"], "speakers": ["Data Engineer, Texas Rangers"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Don't miss this session where we demonstrate how the Texas Rangers baseball team is staying one step ahead of the competition by going back to the basics. After implementing a modern data strategy with Databricks and winnng the 2023 World Series the rest of the league quickly followed suit. Now more than ever, data and AI are a central pillar of every baseball team's strategy driving profound insights into player performance and game dynamics. With a 'fundamentals win games' back to the basics focus, join us as we explain our commmitment to world-class data quality, engineering, and MLOPS by taking full advantage of the Databricks Data Intelligence Platform. From system tables to federated querying, find out how the Rangers use every tool at their disposal to stay one step ahead in the hyper competitive world of baseball. /Assistant Director, Baseball R&D\nTexas Rangers /Data Engineer"}
{"session_id": "how-we-transformed-two-businesses-databricks-cornerstone", "title": "How We Transformed Two Businesses With Databricks as the Cornerstone", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Index Platform Technology, Nasdaq OMX Group"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this talk, we will discuss the lessons learned and future vision of transforming two business units to a modern financial data platform at Nasdaq. We'll highlight the transition from disjointed systems to a unified platform using Databricks. Our target audience includes financial engineers, data architects and technical leaders. The agenda covers challenges of legacy systems, reasons for choosing Databricks and key architectural decisions. /Vice President Software Engineering\nNasdaq, Inc. /VP, Index Platform Technology"}
{"session_id": "iceberg-geo-type-transforming-geospatial-data-management-scale", "title": "Iceberg Geo Type: Transforming Geospatial Data Management at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, TRAVEL AND HOSPITALITY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "Data Warehouse"], "speakers": ["Co-founder and chief architect, Wherobots Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Apache Iceberg\u2122 community is introducing native geospatial type support, addressing key challenges in managing geospatial data at scale, including fragmented formats and inefficiencies in storing large spatial datasets. This talk will delve into the origins of the Iceberg geo type, its specification design and future goals. We will examine the impact on both the geospatial and Iceberg communities, in introducing a standard data warehouse storage layer to the geospatial community, and enabling optimized geospatial analytics for Iceberg users. We will also present a live demonstration of the Iceberg geo data type with Apache Sedona\u2122 and Apache Spark\u2122, showcasing how it simplifies and accelerates geospatial analytics workflows and queries. Finally, we will also provide an in-depth look at its current capabilities and outline the roadmap for future developments, and offer a perspective on its role in advancing geospatial data management in the industry. /Software Engineer\nDatabricks /Co-founder and chief architect"}
{"session_id": "iceberg-modern-table-standard-apple", "title": "Iceberg as the Modern Table Standard at Apple", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Streaming"], "speakers": ["Software Engineer, Apple"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore Apache Iceberg as a modern table format that has revolutionized data storage and processing. We\u2019ll dive into its core benefits, such as schema evolution, hidden partitioning and time-travel capabilities, and share how Apple leverages these features to optimize internal workflows. The session will highlight Iceberg\u2019s interoperability across compute engines commonly used at Apple: Spark, Trino and Flink, as well as its integration with streaming platforms like Kafka where it supports large scale batch and streaming workloads. Finally, we\u2019ll discuss the emerging support for AWS-managed Iceberg metadata, and how this can greatly improve large-scale data workflows, paving the way for future advancements. /Software Engineer"}
{"session_id": "iceberg-table-format-adoption-and-unified-metadata-catalog", "title": "Iceberg Table Format Adoption and Unified Metadata Catalog Implementation in Lakehouse Platform", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Doordash"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Lead Data Engineer\nDoorDash /Software Engineer"}
{"session_id": "incremental-iceberg-table-replication-scale", "title": "Incremental Iceberg Table Replication at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Apache Spark"], "speakers": ["Software Engineer, Apple"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Apache Iceberg is a popular table format for managing large analytical datasets. But replicating iceberg tables at scale can be a daunting task \u2014 especially when dealing with its hierarchical metadata. In this talk, we present an end-to-end workflow for replicating Apache Iceberg tables, leveraging Apache Spark to ensure that backup tables remain identical to their source counterparts. More excitingly, we have contributed these libraries back to the open-source community. Attendees will gain a comprehensive understanding of how to set up replication workflows for Iceberg tables, as well as practical guidance on how to manage and maintain replicated datasets at scale. This talk is ideal for data engineers, platform architects and practitioners looking to apply replication and disaster recovery for Apache Iceberg in complex data ecosystems. /Software Engineer\nDatabricks /Software Engineer"}
{"session_id": "insights-all-bayer-consumer-healths-journey-self-service-analytics", "title": "Insights for All \u2014 Bayer Consumer Health\u2019s Journey on Self-Service Analytics at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Machine Learning", "Scala"], "speakers": ["Principal Cloud Platform Architect, Bayer AG"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Bayer\u2019s mission, \"health for all, hunger for none,\" focuses on delivering innovative healthcare and agricultural products to address major global challenges in health and nutrition. This presentation will showcase how Bayer\u2019s Consumer Health division, known for products like Aspirin and Bepanthen, utilizes the Databricks Data Intelligence Platform to develop reusable core data assets and scalable data products. This globally distributed platform supports cost-efficient dashboarding, ad hoc analytics, machine learning and AI solutions, empowering thousands of stakeholders worldwide. The management of core data assets, which serve as integrated and reusable data models, will be discussed, highlighting their role in accelerating the creation of targeted data products. Additionally, a self-service analytics strategy will be presented to enhance access to insights and foster a data-driven culture at Bayer, addressing the needs of stakeholders in various markets and global headquarters. /Principal Cloud Platform Architect"}
{"session_id": "intelligent-applications-lakehouse", "title": "Intelligent Applications for the Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Senior Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Product Manager\nDatabricks /Senior Product Manager"}
{"session_id": "introduction-modern-open-table-formats-and-catalogs", "title": "Introduction to Modern Open Table Formats and Catalogs", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "DEEP DIVE", "industry": "ENERGY AND UTILITIES, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "DELTA LAKE"], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this session, learn about why modern open table formats like Delta and Iceberg are a big deal and how they work with catalogs. Learn about what motivated their creation, how they work, what benefits they can bring to your data and AI platform. Hear about how these formats are becoming increasingly interoperable and what our vision is for their future. /Sr. Staff Product Manager\nDatabricks /Principal Software Engineer"}
{"session_id": "iqvias-serverless-journey-enabling-data-and-ai-regulated-world", "title": "IQVIA\u2019s Serverless Journey: Enabling Data and AI in a Regulated World", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES, PUBLIC SECTOR, FINANCIAL SERVICES", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Your data and AI use-cases are multiplying. At the same time, there is increased focus and scrutiny to meet sophisticated security and regulatory requirements. IQVIA utilizes serverless use-cases across data engineering, data analytics, and ML and AI, to empower their customers to make informed decisions, support their R&D processes and improve patient outcomes. By leveraging native controls on the platform, serverless enables them to streamline their use cases while maintaining a strong security posture, top performance and optimized costs. This session will go over IQVIA\u2019s journey to serverless, how they met their security and regulatory requirements, and the latest and upcoming enhancements to the Databricks Platform. /Data Enablement / Architect\nIQVIA /Staff Product Manager"}
{"session_id": "italgas-ai-factory-and-future-gas-distribution", "title": "Italgas\u2019 AI Factory and the Future of Gas Distribution", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["DATABRICKS SQL", "MOSAIC AI", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "SQL"], "speakers": ["Lead Data Architect, Italgas"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Italgas, Europe\u2019s leading gas distributor both by network size and number of customers, we are spearheading digital transformation through a state-of-the-art, fully-fledged Databricks Intelligent platform. We first achieved 50% cost reduction and a 20% performance boost migrating from Azure Synapse to Databricks SQL and ensured that 100% of workloads are governed by Unity Catalog. Now we have 41ML/GenAI models in production: e.g., AI Customer Complaint Resolution. In AI ICT ticket resolution case, Lakeflow connector with ServiceNow allowed to halve the development time (eliminating DataFactory) and an automatic resolution of 40% of cases. Genie Dashboards and self-BI is used by 80% of our employees, while Genie allows the grid control-room operators to analyze network status data in natural language. Finally, our AI faculty will spread and boost AI literacy across the board and empower employees. /Lead\nCluster Reply /Lead Data Architect"}
{"session_id": "kafka-forwarder-simplifying-kafka-consumption-openai", "title": "Kafka Forwarder: Simplifying Kafka Consumption at OpenAI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala", "Streaming"], "speakers": ["Member of Technical Staff, Open AI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At OpenAI, Kafka fuels real-time data streaming at massive scale, but traditional consumers struggle under the burden of partition management, offset tracking, error handling, retries, Dead Letter Queues (DLQ), and dynamic scaling \u2014 all while racing to maintain ultra-high throughput. As deployments scale, complexity multiplies. Enter Kafka Forwarder \u2014 a game-changing Kafka Consumer Proxy that flips the script on traditional Kafka consumption. By offloading client-side complexity and pushing messages to consumers, it ensures at-least-once delivery, automated retries, and seamless DLQ management via Databricks. The result? Scalable, reliable and effortless Kafka consumption that lets teams focus on what truly matters. Curious how OpenAI simplified self-service, high-scale Kafka consumption? Join us as we walk through the motivation, architecture and challenges behind Kafka Forwarder, and share how we structured the pipeline to seamlessly route DLQ data into Databricks for analysis. /Member of Technical Staff"}
{"session_id": "kernel-catalog-action-reimagining-our-delta-spark-connector-dsv2", "title": "Kernel, Catalog, Action! Reimagining our Delta-Spark Connector With DSv2", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Delta Lake", "ELT"], "speakers": ["Sr Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake is redesigning its Spark connector through the combination of three key technologies: First, we're updating our Spark APIs to DSv2 to achieve deeper catalog integration and improved integration with the Spark optimizer. Second, we're fully integrating on top of Delta Kernel to take advantage of its simplified abstraction of Delta protocol complexities, accelerating feature adoption and improving maintainability. Third, we are transforming Delta to become a catalog-aware lakehouse format with Catalog Commits, enabling more efficient metadata management, governance and query performance. Join us to explore how we're advancing Delta Lake's architecture, pushing the boundaries of metadata management and creating a more intelligent, performant data lakehouse platform. /Sr Software Engineer"}
{"session_id": "lessons-learned-building-scalable-game-analytics-platform-netflix", "title": "Lessons Learned: Building a Scalable Game Analytics Platform at Netflix", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Snr. Software Engineer, Netflix"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Over the past three years, Netflix has built a catalog of 100+ mobile and cloud games across TV, mobile and web platforms. With both internal and external studios contributing to this diverse ecosystem, building a robust game analytics platform became crucial for gaining insights into player behavior, optimizing game performance and driving member engagement.In this talk, we\u2019ll share our journey of building Netflix\u2019s Game Analytics platform from the ground up. We\u2019ll highlight key decisions around data strategy, such as whether to develop an in-house solution or adopt an external service. We\u2019ll discuss the challenges of balancing developer autonomy with data integrity and the complexities of managing data contracts for custom game telemetry, with an emphasis on self-service analytics. Attendees will learn how the Games Data team navigated these challenges, the lessons learned and the trade-offs involved in building a multi-tenant data ecosystem that supports diverse stakeholders. /Senior Data Engineer\nNETFLIX INC /Snr. Software Engineer"}
{"session_id": "lets-save-tons-money-cloud-native-data-ingestion", "title": "Let's Save Tons of Money With Cloud-Native Data Ingestion!", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Valued Employee, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Delta Lake is a fantastic technology for quickly querying massive data sets, but first you need those massive data sets! In this session we will dive into the cloud-native architecture Scribd has adopted to ingest data from AWS Aurora, SQS, Kinesis Data Firehose and more. By using off-the-shelf open source tools like kafka-delta-ingest, oxbow and Airbyte, Scribd has redefined its ingestion architecture to be more event-driven, reliable, and most importantly: cheaper. No jobs needed! Attendees will learn how to use third-party tools in concert with a Databricks and Unity Catalog environment to provide a highly efficient and available data platform. This architecture will be presented in the context of AWS but can be adapted for Azure, Google Cloud Platform or even on-premise environments. /Valued Employee"}
{"session_id": "leveling-gaming-analytics-how-supercell-evolved-player-experiences", "title": "Leveling Up Gaming Analytics: How Supercell Evolved Player Experiences With Snowplow and Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "MOSAIC AI"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Real-time", "Scala"], "speakers": ["CEO and Co-Founder, Snowplow"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In the competitive gaming industry, understanding player behavior is key to delivering engaging experiences. Supercell, creators of Clash of Clans and Brawl Stars, faced challenges with fragmented data and limited visibility into user journeys. To address this, they partnered with Snowplow and Databricks to build a scalable, privacy-compliant data platform for real-time insights. By leveraging Snowplow\u2019s behavioral data collection and Databricks\u2019 Lakehouse architecture, Supercell achieved: This session explores Supercell\u2019s data journey and AI-driven player engagement strategies. /CEO and Co-Founder"}
{"session_id": "managed-and-foreign-tables-unity-catalog", "title": "Managed and Foreign Tables in Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog brings open, interoperable table formats to the heart of the Databricks Lakehouse. In this session, we\u2019ll introduce new capabilities that apply fine-grained governance across all data and unify access across catalogs. Learn how Databricks is eliminating data silos, simplifying performance with predictive optimization and advancing an open lakehouse architecture. /Staff Product Manager"}
{"session_id": "manufacturing-and-transportation-industry-forum", "title": "Manufacturing and Transportation Industry Forum", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI", "DELTA SHARING", "UNITY CATALOG"], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["VP Data & AI, Virgin Atlantic Airways"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an inspiring forum showcasing how manufacturers and transportation leaders are turning today's challenges into tomorrow's opportunities. From automotive giants revolutionizing product development with generative AI to logistics providers optimizing routes for both cost and sustainability, discover how industry pioneers are reshaping the future of industrial operations. Highlighting this session is an exciting collaboration between Heathrow Airport and Virgin Atlantic, demonstrating how partnership and innovation are transforming the air travel experience. Learn how these leaders and other companies are using Databricks to tackle their most pressing challenges \u2014 from smart factory transformations to autonomous systems development \u2014 proving that the path to profitability and sustainability runs through intelligent operations. /Global Industry Leader - Manufacturing\nDatabricks /Head of Technology, Cloud and Data\nHeathrow /Manufacturing & Energy Marketing Lead\nDatabricks /VP Data & AI"}
{"session_id": "manufacturing-cleaner-how-data-intelligence-cuts-carbon-not-profits", "title": "Manufacturing Cleaner: How Data Intelligence Cuts Carbon, Not Profits", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Architecture"], "speakers": ["Lead Data & ML Engineer, Dow Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join industry leaders from Dow and Michelin as they reveal how data intelligence is revolutionizing sustainable manufacturing without compromising profitability. Dow demonstrates how their implementation of Databricks' Data Intelligence Platform has transformed their ability to track and reduce carbon footprints while driving operational efficiencies, resulting in significant cost savings through optimized maintenance and reduced downtime. Michelin follows with their ambitious strategy to achieve 3% energy consumption reduction by 2026, leveraging Databricks to turn this environmental challenge into operational excellence. Together, these manufacturing giants showcase how modern data architecture and AI are creating a new paradigm where sustainability and profitability go hand-in-hand. /Senior Solution Manager\nDow Inc. /Product Owner\nCGI /Lead Data & ML Engineer"}
{"session_id": "maximizing-business-value-and-ensuring-data-privacy-databricks", "title": "Maximizing Business Value and Ensuring Data Privacy with Databricks in Connected Vehicles", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Privacy", "Delta Lake", "ELT"], "speakers": ["Manager, Field Engineering, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As global data privacy regulations tighten, balancing user data protection with maximizing its business value is crucial.This presentation explores how integrating Databricks into our connected-vehicle data platform enhances both governance and business outcomes. We\u2019ll highlight a case where migrating from EMR to Databricks improved deletion performance and cut costs by 99% with Delta Lake. This shift not only ensures compliance with data-privacy regulations but also maximizes the potential of connected-vehicle data. We are developing a platform that balances compliance with business value and sets a global standard for data usage, inviting partners to join us in building a secure, efficient mobility ecosystem. /General Manager\nTOYOTA MOTOR CORPORATION /Manager, Field Engineering"}
{"session_id": "metadata-marathon-how-three-storage-projects-are-racing-forward", "title": "The Metadata Marathon: How Three Storage Projects are Racing Forward", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT"], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "With the enormous amount of discussion about open storage formats between nerds and even not-nerds, it can be hard to keep track of who\u2019s doing what and how this actually makes any impact on day to day data projects. I want us to take a closer look at the three big projects in this space; Delta, Hudi and Iceberg. They\u2019re all trying to solve for similar data problems and have tackled the various challenges in different ways. This talk with start with the very basics of how we got here, what the history is before diving deep into the underlying tech, their roadmaps and their impacts on the data landscape as a whole. /Staff Developer Advocate"}
{"session_id": "missing-link-between-lakehouse-and-data-intelligence", "title": "The Missing Link Between the Lakehouse and Data Intelligence", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "What connects your lakehouse to real data intelligence? The answer: the catalog. But not just any catalog. In this session, we break down why Unity Catalog is purpose-built for the lakehouse, and how it goes beyond operational or business catalogs to deliver cross-platform interoperability and a shared understanding of your data. You\u2019ll walk away with a clear view of how the right data foundation unlocks smarter decisions and trusted AI. /Director, Product\nDatabricks /Staff Product Manager"}
{"session_id": "multi-format-multi-table-multi-statement-transactions-unity-catalog", "title": "Multi-Format, Multi-Table, Multi-Statement Transactions on Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Pipeline", "Delta Lake", "ELT"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Get a first look at multi-statement transactions in Databricks. In this session, we will dive into their capabilities, exploring how multi-statement transactions enable atomic updates across multiple tables in your data pipelines, ensuring data consistency and integrity for complex operations. We will also share how we are enabling unified transactions across Delta Lake and Iceberg with Unity Catalog \u2014 powering our vision for an open and interoperable lakehouse. /Staff Software Engineer\nDatabricks /Staff Product Manager"}
{"session_id": "open-source-unity-catalog-getting-started-best-practices-and-governance", "title": "Open Source Unity Catalog: Getting Started, Best Practices and Governance at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Sr Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How to use UC OSS, what features are available, and intro to the ecosystem. We'll dive into the latest release and get hands-on with demos for working with your UC data and AI assets \u2014 including tables, volumes, models and AI functions. /Staff Software Engineer\nDatabricks /Sr Software Engineer"}
{"session_id": "optimizing-analytics-infrastructure-lessons-migrating-snowflake", "title": "Optimizing Analytics Infrastructure: Lessons from Migrating Snowflake to Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DATABRICKS SQL", "DELTA LAKE"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Lake", "Data Quality", "Scala"], "speakers": ["Architect, DeeplearningAPI"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session explores the strategic migration from Snowflake to Databricks, focusing on the journey of transforming a data lake to leverage Databricks\u2019 advanced capabilities. It outlines the assessment of key architectural differences, performance benchmarks, and cost implications driving the decision. Attendees will gain insights into planning and execution, including data ingestion pipelines, schema conversion and metadata migration. Challenges such as maintaining data quality, optimizing compute resources and minimizing downtime are discussed, alongside solutions implemented to ensure a seamless transition. The session highlights the benefits of unified analytics and enhanced scalability achieved through Databricks, delivering actionable takeaways for similar migrations. /Architect"}
{"session_id": "over-architected-live", "title": "Over Architected: LIVE", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "LAKEFLOW", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Developer Advocate, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Staff Developer Advocate\nDatabricks /Staff Developer Advocate"}
{"session_id": "petabyte-scale-chain-insights-real-time-intelligence-next-gen-financial", "title": "Petabyte-Scale On-Chain Insights: Real-Time Intelligence for the Next-Gen Financial Backbone", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Apache Spark", "Data Lake", "Delta Lake", "ELT", "Machine Learning", "Real-time", "SQL"], "speakers": ["Founder, CipherOwl Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "We\u2019ll explore how CipherOwl Inc. constructed a near real-time, multi-chain data lakehouse to power anti-money laundering (AML) monitoring at a petabyte scale. We will walk through the end-to-end architecture, which integrates cutting-edge open-source technologies and AI-driven analytics to handle massive on-chain data volumes seamlessly. Off-chain intelligence complements this to meet rigorous AML requirements. At the core of our solution is ChainStorage, an OSS started by Coinbase that provides robust blockchain data ingestion and block-level serving. We enhanced it with Apache Spark\u2122 and Arrow\u2122, coupled for high-throughput processing and efficient data serialization, backed by Delta Lake and Kafka. For the serving layer, we employ StarRocks to deliver lightning-fast SQL analytics over vast datasets. Finally, our system incorporates machine learning and AI agents for continuous data curation and near real-time insights, which are crucial for tackling on-chain AML challenges. /Founder"}
{"session_id": "practitioners-guide-databricks-serverless", "title": "A Practitioner\u2019s Guide to Databricks Serverless", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Engineering", "Data Pipeline"], "speakers": ["Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows. /Product Specialist"}
{"session_id": "prescription-success-leveraging-dabs-faster-deployment-and-better", "title": "A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Principal Data Engineer, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Health Catalyst (HCAT) transformed its CI/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements. HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making. /Sr. Solutions Architect\nDatabricks /Principal Data Engineer"}
{"session_id": "redesigning-kaizens-cloud-data-lake-future", "title": "Redesigning Kaizen's Cloud Data Lake for the Future", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, PROFESSIONAL SERVICES", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ELT", "Scala"], "speakers": ["DevOps Engineer, Kaizen Gaming"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "At Kaizen Gaming, data drives our decision-making, but rapid growth exposed inefficiencies in our legacy cloud setup \u2014 escalating costs, delayed insights and scalability limits. Operating in 18 countries with 350M daily transactions (1PB+), shared quotas and limited cost transparency hindered efficiency. To address this, we redesigned our cloud architecture with Data Landing Zones, a modular framework that decouples resources, enabling independent scaling and cost accountability. Automation streamlined infrastructure, reduced overhead and enhanced FinOps visibility, while Unity Catalog ensured governance and security. Migration challenges included maintaining stability, managing costs and minimizing latency. A phased approach, Delta Sharing, and DBx Asset Bundles simplified transitions. The result: faster insights, improved cost control and reduced onboarding time, fostering innovation and efficiency. We share our transformation, offering insights for modern cloud optimization. /Data Platform Team Lead\nKaizen Gaming /Senior SRE/DevOps Engineer"}
{"session_id": "reducing-transaction-conflicts-databricks-fundamentals-and-applications", "title": "Reducing Transaction Conflicts in Databricks\u2014Fundamentals and Applications at Asana", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Software Engineer, Asana"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "When using ACID-guaranteed transactions on Databricks concurrently, we can run into transaction conflicts. The first part of this talk discusses the basics of concurrent transaction functionality in Databricks\u2014what happens when various combinations of INSERT, UPDATE and MERGE INTO happen concurrently. We discuss how table isolation level, partitioning and deletion vectors affect this. The second part of this talk focuses on a particular pipeline evolution at Asana to reduce transaction conflicts. As the number of writers to a table grew, we first implemented writer-specific partitioning to reduce transaction conflicts. Later on, we implemented an intermediate blind append stage to be able to avoid transaction conflicts while leveraging liquid clustering rather than partitioning for improved read and write performance. /Software Engineer"}
{"session_id": "rust-and-lakehouse-format-ask-us-anything", "title": "Rust and Lakehouse Format \u2014 Ask Us Anything", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "EDUCATION, PROFESSIONAL SERVICES, PUBLIC SECTOR", "technologies": ["DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT"], "speakers": ["Valued Employee, Scribd"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us for an in-depth Ask Me Anything (AMA) on how Rust is revolutionizing Lakehouse formats like Delta Lake and Apache Iceberg through projects like delta-rs and iceberg-rs! Discover how Rust\u2019s memory safety, zero-cost abstractions and fearless concurrency unlock faster development and higher-performance data operations. Whether you\u2019re a data engineer, Rustacean or Lakehouse enthusiast, bring your questions on how Rust is shaping the future of open table formats! /Staff Developer Advocate\nDatabricks /PM Director, Developer Relations\nDatabricks /Valued Employee"}
{"session_id": "securing-databricks-using-databricks-siem", "title": "Securing Databricks Using Databricks as SIEM", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Sr. Manager, Security Engineering\nDatabricks /Staff Software Engineer"}
{"session_id": "selectively-overwrite-data-delta-lakes-dynamic-insert-overwrite", "title": "Selectively Overwrite Data With Delta Lake\u2019s Dynamic Insert Overwrite", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["APACHE SPARK", "DATABRICKS SQL", "DELTA LAKE"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT"], "speakers": ["Principal Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Dynamic Insert Overwrite is an important Delta Lake feature that allows fine-grained updates by selectively overwriting specific rows, eliminating the need for full-table rewrites. For examples, this capability is essential for: In this lightning talk, we will: /Software Engineer\nDatabricks /Principal Software Engineer"}
{"session_id": "serverless-compute-notebooks-jobs-and-dlt", "title": "Serverless Compute for Notebooks, Jobs and DLT", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how Databricks serverless compute revolutionizes data workflows by eliminating infrastructure management, enabling rapid scaling and optimizing costs for Notebooks, Jobs and DLT. This session will delve into the serverless architecture, highlighting its ability to dynamically allocate resources, reduce idle costs and simplify development cycles. Learn about recent advancements, including cost savings and practical strategies for migration and optimization. Tailored for Data Engineers and Architects, this talk will also explore use cases, features, limitations and future roadmap, empowering you to make informed infrastructure decisions while unlocking the full potential of Databricks\u2019 serverless capabilities. /Product Manager"}
{"session_id": "smart-data-smarter-vehicles-building-foundation-future-transportation", "title": "Smart Data, Smarter Vehicles: Building the Foundation for the Future of Transportation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MANUFACTURING", "technologies": ["AI/BI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Architect, Boeing"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Head of Data Management\nCARIAD SE /Architect"}
{"session_id": "sponsored-onehouse-open-default-fast-design-one-lakehouse-scales-bi-ai", "title": "Sponsored by: Onehouse | Open By Default, Fast By Design: One Lakehouse That Scales From BI to AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["AI/BI", "APACHE ICEBERG", "APACHE SPARK"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "ETL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "You already see the value of the lakehouse. But are you truly maximizing its potential across all workloads, from BI to AI? In this session, Onehouse unveils how our open lakehouse architecture unifies your entire stack, enabling true interoperability across formats, catalogs, and engines. From lightning-fast ingestion at scale to cost-efficient processing and multi-catalog sync, Onehouse helps you go beyond trade-offs. Discover how Apache XTable (Incubating) enables cross-table-format compatibility, how OpenEngines puts your data in front of the best engine for the job, and how OneSync keeps data consistent across Snowflake, Athena, Redshift, BigQuery, and more. Meanwhile, our purpose-built lakehouse runtime slashes ingest and ETL costs. Whether you\u2019re delivering BI, scaling AI, or building the next big thing, you need a lakehouse that\u2019s open and powerful. Onehouse opens everything\u2014so your data can power anything."}
{"session_id": "stop-guessing-spend-where-it-counts-data-driven-decisions-high-impact", "title": "Stop Guessing Spend Where It Counts: Data-Driven Decisions for High-Impact Investments on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, FINANCIAL SERVICES", "technologies": ["AI/BI", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Scala"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Struggling with runaway cloud costs as your organization grows? Join us for an inside look at how Databricks\u2019 own Data Platform team tackled escalating spend in some of the world\u2019s largest workspaces \u2014 saving millions of dollars without sacrificing performance or user experience. We\u2019ll share how we harnessed powerful features like System Tables, Workflows, Unity Catalog, and Photon to monitor and optimize resource usage, all while using data-driven decisions to improve efficiency and ensure we invest in the areas that truly drive business impact. You\u2019ll hear about the real-world challenges we faced balancing governance with velocity and discover the custom tooling and best practices we developed to keep costs in check. By the end of this session, you\u2019ll walk away with a proven roadmap for leveraging Databricks to control cloud spend at scale. /Software Engineer\nDatabricks /Databricks"}
{"session_id": "streamlining-data-platform-architecture-databricks-apps", "title": "Streamlining Data Platform Architecture With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Platform Operations Lead, Takeda"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Apps bring the power of open-source visualization frameworks like Plotly Dash directly into Databricks. Combined with the power of elastic serverless compute, Databricks Apps create a seamless development experience for advanced visualizations with the lowest possible latency to your lakehouse. In this talk, we walk you through a new approach to presenting AI-driven analysis to customers around your organization \u2014 all while eliminating unneeded license costs, halving administrative burden and accelerating delivery time. /Platform Operations Lead"}
{"session_id": "swimming-our-own-lakehouse-how-databricks-uses-databricks", "title": "Swimming at Our Own Lakehouse: How Databricks Uses Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS SQL", "DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Peek behind the curtain to learn how Databricks processes hundreds of petabytes of data across every region and cloud where we operate. Learn how Databricks leverages Data and AI to scale and optimize every aspect of the company. From facilities and legal to sales and marketing and of course product research and development. This session is a high-level tour inside Databricks to see how Data and AI enable us to be a better company. We will go into the architecture of things for how Databricks is used for internal use cases like business analytics and SIEM as well as customer-facing features like system tables and assistant. We will cover how data production of our data flow and how we maintain security and privacy while operating a large multi-cloud, multi-region environment. /Head of Data Platform\nDatabricks /Staff Software Engineer"}
{"session_id": "thredups-journey-databricks-modernizing-our-data-infrastructure", "title": "ThredUp\u2019s Journey with Databricks: Modernizing Our Data Infrastructure", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, MEDIA AND ENTERTAINMENT, RETAIL AND CPG - FOOD", "technologies": ["AI/BI", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Engineering", "Machine Learning", "Real-time"], "speakers": ["VP, Data platform & Enterprise Apps Engg, ThredUp Inc."], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Building an AI-ready data platform requires strong governance, performance optimization, and seamless adoption of new technologies. At ThredUp, our Databricks journey began with a need for better data management and evolved into a full-scale transformation powering analytics, machine learning, and real-time decision-making. In this session, we\u2019ll cover: Whether you\u2019re new to Databricks or scaling an existing platform, you\u2019ll gain practical insights on navigating the transition, avoiding pitfalls, and maximizing AI and data intelligence. /Data Engineering Manager\nThredup /VP, Data platform & Enterprise Apps Engg"}
{"session_id": "tracing-path-row-through-gpu-enabled-query-engine-grace-blackwell", "title": "Tracing the Path of a Row Through a GPU-Enabled Query Engine on the Grace-Blackwell Architecture", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE SPARK"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Apache Spark", "SQL"], "speakers": ["Senior Developer Technology Engineer, NVIDIA"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Grace-Blackwell is NVIDIA\u2019s most recent GPU system architecture. It addresses a key concern of query engines: fast data access. In this session, we will take a close look at how GPUs can accelerate data analytics by tracing how a row flows through a GPU-enabled query engine.Query engines read large data from CPU memory or from disk. On Blackwell GPUs, a query engine can rely on hardware-accelerated decompression of compact formats. The Grace-Blackwell system takes data access performance even further, by reading data at up to 450 GB/s across its CPU to GPU interconnect. We demonstrate full end-to-end SQL query acceleration using GPUs in a prototype query engine using industry standard benchmark queries. We compare the results to existing CPU solutions.Using Apache Spark\u2122 and the RAPIDS Accelerator for Apache Spark, we demonstrate the impact GPU acceleration has on the performance of SQL queries at the 100TB scale using NDS, a suite that simulates real-world business scenarios. /Principal Systems Software Engineer\nNvidia /Senior Developer Technology Engineer"}
{"session_id": "transforming-bio-pharma-manufacturing-eli-lillys-data-driven-journey", "title": "Transforming Bio-Pharma Manufacturing: Eli Lilly's Data-Driven Journey With Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["AI/BI", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Architecture", "Data Fabric", "Real-time", "Streaming"], "speakers": ["Executive Director, Eli Lilly and Company"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Eli Lilly and Company, a leading bio-pharma company, is revolutionizing manufacturing with next-gen fully digital sites. Lilly and Tredence have partnered to establish a Databricks-powered Global Manufacturing Data Fabric (GMDF), laying the groundwork for transformative data products used by various personas at sites and globally. By integrating data from various manufacturing systems into a unified data model, GMDF has delivered actionable insights across several use cases such as batch release by exception, predictive maintenance, anomaly detection, process optimization and more. Our serverless architecture leverages Databricks Auto Loader for real-time data streaming, PySpark for automation and Unity Catalog for governance, ensuring seamless data processing and optimization. This platform is the foundation for data driven processes, self-service analytics, AI and more. This session will provide details on the data architecture and strategy and share a few use cases delivered. /Lead Innovation Architect - Data & AI\nEli Lilly /Head of Delivery - HLS\nTredence /Executive Director"}
{"session_id": "unified-governance-and-enterprise-sharing-data-ai", "title": "Unified Governance and Enterprise Sharing for Data + AI", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "EDUCATION, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["VP, Federal Government, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The Databricks Lakehouse for Public Sector is the only enterprise data platform that allows you to leverage all your data, from any source, on any workload to always offer better citizen services/warfighter support/student success with the best outcomes, at the lowest cost, with the greatest investment protection. /VP and GM, Public Sector\nDatabricks /VP, Federal Government"}
{"session_id": "unified-solution-data-management-and-model-training-apache-iceberg-and", "title": "A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, PUBLIC SECTOR", "technologies": ["APACHE ICEBERG", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala", "Streaming"], "speakers": ["machine learning system engineer, ByteDance"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming). Magnus uses Iceberg\u2019s branch/tag to manage massive datasets/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability. When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility. In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training. /Infrastructure Engineer\nByteDance /machine learning system engineer"}
{"session_id": "unify-your-data-and-governance-lakehouse-federation", "title": "Unify Your Data and Governance With Lakehouse Federation", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Warehouse", "SQL"], "speakers": ["Staff Product Manager, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In today's data landscape, organizations often grapple with fragmented data spread across various databases, data warehouses and catalogs. Lakehouse Federation addresses this challenge by enabling seamless discovery, querying, and governance of distributed data without the need for duplication or migration. This session will explore how Lakehouse Federation integrates external data sources like Hive Metastore, Snowflake, SQL Server and more into a unified interface, providing consistent access controls, lineage tracking and auditing across your entire data estate. Learn how to streamline analytics and AI workloads, enhance compliance and reduce operational complexity by leveraging a single, cohesive platform for all your data needs. /Sr. Staff Product Manager\nDatabricks /Staff Product Manager"}
{"session_id": "unity-catalog-managed-tables-powerful-easy-and-interoperable", "title": "Unity Catalog Managed Tables: Powerful, Easy, and Interoperable", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, TRAVEL AND HOSPITALITY, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["ELT"], "speakers": ["Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Unity Catalog's Managed tables are the best of all worlds. Learn how they harness the Data Intelligence Platform to deliver lightning-fast performance\u2014without requiring a space shuttle cockpit worth of setting. Learn how they're fully interoperable with 3P clients\u2014be they Delta or Iceberg\u2014while still respecting a single source of governance. And learn about our exciting roadmap for how they will get even more powerful in the coming year. /Sr. Staff Product Manager\nDatabricks /Databricks"}
{"session_id": "unleash-your-content-ai-powered-metadata-targeting-personalization-and", "title": "Unleash your Content: AI-Powered Metadata for Targeting, Personalization, and Brand Safety", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "ENTERPRISE TECHNOLOGY, MANUFACTURING, FINANCIAL SERVICES", "technologies": ["DELTA LAKE", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Field CTO & Co-Founder, Coactive Systems Inc"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "/Field CTO & Co-Founder"}
{"session_id": "unlocking-enterprise-potential-key-insights-pgs-deployment-unity", "title": "Unlocking Enterprise Potential: Key Insights from P&G's Deployment of Unity Catalog at Scale", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "RETAIL AND CPG - FOOD", "technologies": ["UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Governance", "Data Lake"], "speakers": ["Engineering Manager, P&G"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session will explore Databricks Unity Catalog (UC) implementation by P&G to enhance data governance, reduce data redundancy and improve the developer experience through the enablement of a Lakehouse architecture. The presentation will cover: The distinction between data treated as a product and standard application data, highlighting how UC's structure maximizes the value of data in P&G's data lake. Real-life examples from two years of using Unity Catalog, demonstrating benefits such as improved governance, reduced waste and enhanced data discovery. Challenges related to disaster recovery and external data access, along with our collaboration with Databricks to address these issues. Sharing our experience can provide valuable insights for organizations planning to adopt Unity Catalog on an enterprise scale. /Engineering Manager"}
{"session_id": "unlocking-power-iceberg-our-journey-unified-lakehouse-databricks", "title": "Unlocking the Power of Iceberg: Our Journey to a Unified Lakehouse on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["APACHE ICEBERG", "UNITY CATALOG"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "Scala"], "speakers": ["Group Manager, LSports"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session showcases our journey of adopting Apache Iceberg\u2122 to build a modern lakehouse architecture and leveraging Databricks advanced Iceberg support to take it to the next level. We\u2019ll dive into the key design principles behind our lakehouse, the operational challenges we tackled and how Databricks enabled us to unlock enhanced performance, scalability and streamlined data workflows. Whether you\u2019re exploring Apache Iceberg\u2122 or building a lakehouse on Databricks, this session offers actionable insights, lessons learned and best practices for modern data engineering. /Group Manager"}
{"session_id": "ursa-augment-your-lakehouse-kafka-compatible-data-streaming", "title": "Ursa: Augment Your Lakehouse With Kafka-Compatible Data Streaming Capabilities", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Batch Processing", "Data Architecture", "Data Lake", "Delta Lake", "ELT", "Real-time", "Streaming"], "speakers": ["Director of Engineering, Automotive Industry"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data architectures evolve to meet the demands of real-time GenAI applications, organizations increasingly need systems that unify streaming and batch processing while maintaining compatibility with existing tools. The Ursa Engine offers a Kafka-API-compatible data streaming engine built on Lakehouse (Iceberg and Delta Lake). Designed to seamlessly integrate with data lakehouse architectures, Ursa extends your lakehouse capabilities by enabling streaming ingestion, transformation and processing \u2014 using a Kafka-compatible interface. In this session, we will explore how Ursa Engine augments your existing lakehouses with Kafka-compatible capabilities. Attendees will gain insights into Ursa Engine architecture and real-world use cases of Ursa Engine. Whether you're modernizing legacy systems or building cutting-edge AI-driven applications, discover how Ursa can help you unlock the full potential of your data. /Founder and CEO\nStreamNative /Director of Engineering"}
