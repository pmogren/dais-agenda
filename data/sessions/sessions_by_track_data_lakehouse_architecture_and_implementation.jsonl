{"session_id": "advanced-governance-and-auth-databricks-apps", "title": "Advanced Governance and Auth With Databricks Apps", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Explore advanced governance and authentication patterns for building secure, enterprise-grade apps with Databricks Apps. Learn how to configure complex permissions and manage access control using Unity Catalog. We\u2019ll dive into \u201con-behalf-of-user\u201d authentication \u2014 allowing agents to enforce user-specific access controls \u2014 and cover API-based authentication, including PATs and OAuth flows for external integrations. We\u2019ll also highlight how Addepar uses these capabilities to securely build and scale applications that handle sensitive financial data. Whether you're building internal tools or customer-facing apps, this session will equip you with the patterns and tools to ensure robust, secure access in your Databricks apps. /Staff Software Engineer"}
{"session_id": "ai-powering-epsilons-identity-strategy-unified-marketing-platform", "title": "AI Powering Epsilon's Identity Strategy: Unified Marketing Platform on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT", "technologies": ["DELTA LAKE", "MLFLOW", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Science", "Data Warehouse", "Delta Lake", "ELT"], "speakers": ["Vice President, Decision Sciences, Epsilon Data Management"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Join us to hear about how Epsilon Data Management migrated Epsilon\u2019s unique, AI-powered marketing identity solution from multi-petabyte on-prem Hadoop and data warehouse systems to a unified Databricks Lakehouse platform. This transition enabled Epsilon to further scale its Decision Sciences solution and enable new cloud-based AI research capabilities on time and within budget, without being bottlenecked by the resource constraints of on-prem systems. Learn how Delta Lake, Unity Catalog, MLflow and LLM endpoints powered massive data volume, reduced data duplication, improved lineage visibility, accelerated Data Science and AI, and enabled new data to be immediately available for consumption by the entire Epsilon platform in a privacy-safe way. Using the Databricks platform as the base for AI and Data Science at global internet scale, Epsilon deploys marketing solutions across multiple cloud providers and multiple regions for many customers. /Vice President , Database\nEpsilon Data Management /Vice President, Decision Sciences"}
{"session_id": "apache-iceberg-unity-catalog-hellofresh", "title": "Apache Iceberg with Unity Catalog at HelloFresh", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "RETAIL AND CPG - FOOD", "technologies": ["APACHE ICEBERG", "APACHE SPARK", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["Senior Staff Data Engineer, HelloFresh"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Table formats like Delta Lake and Iceberg have been game changers for pushing lakehouse architecture into modern Enterprises. The acquisition of Tabular added Iceberg to the Databricks ecosystem, an open format that was already well supported by processing engines across the industry. At HelloFresh we are building a lakehouse architecture that integrates many touchpoints and technologies all across the organization. As such we chose Iceberg as the table format to bridge the gaps in our decentralized managed tech landscape. We are leveraging Unity Catalog as the Iceberg REST catalog of choice for storing metadata and managing tables. In this talk we will outline our architectural setup between Databricks, Spark, Flink and Snowflake and will explain the native Unity Iceberg REST catalog, as well as catalog federation towards connected engines. We will highlight the impact on our business and discuss the advantages and lessons learned from our early adopter experience. /Director of Data Engineering\nHelloFresh /Senior Staff Data Engineer"}
{"session_id": "breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse", "title": "Breaking the Ice(berg): Riskified\u2019s Journey to its Next-Generation Lakehouse", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, RETAIL AND CPG - FOOD, FINANCIAL SERVICES", "technologies": ["APACHE ICEBERG", "DELTA LAKE", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Warehouse"], "speakers": ["Staff Data Platform Engineer, Riskified"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "How many of you manage multiple data stores in your organization, wrestling with different data formats that led to data duplication, infrastructure redundancy, and uncontrolled costs? Imagine reducing these costs by 25% while maintaining your existing SLAs \u2014 this is exactly what we achieved with our next-generation architecture. In this session, we'll show you how we built Riskified's next-generation lakehouse by leveraging Databricks' native Apache Iceberg support and Unity Catalog. We'll share our innovative approach to cross-platform querying without data duplication, and how we transformed our data warehouse into a modern lakehouse architecture. Throughout the session, we'll explore the technical challenges we conquered, from data migration to performance optimization. The result? A simplified world where everything is identical across engines, leaving users with just one choice \u2014 which query engine best suits their use case. /Data Platform Architect\nRiskified /Staff Data Platform Engineer"}
{"session_id": "breaking-silos-enabling-databricks-snowflake-interoperability-iceberg", "title": "Breaking Silos: Enabling Databricks-Snowflake Interoperability With Iceberg and Unity Catalog", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENERGY AND UTILITIES", "technologies": ["UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Engineering", "ELT"], "speakers": ["Member of Technical Staff, Sol Arch, T-Mobile"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "As data ecosystems grow more complex, organizations often struggle with siloed platforms and fragmented governance. In this session, we\u2019ll explore how our team made Databricks the central hub for cross-platform interoperability, enabling seamless Snowflake integration through Unity Catalog and the Iceberg REST API. We\u2019ll cover: By leveraging Uniform, Delta, and Iceberg, we created a flexible, vendor-agnostic architecture that bridges Databricks and Snowflake without compromising performance or security. /Director of Data Engineering\nT-Mobile /Member of Technical Staff, Sol Arch"}
{"session_id": "build-ai-powered-applications-natively-databricks", "title": "Build AI-Powered Applications Natively on Databricks", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["AI/BI", "DATABRICKS APPS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI", "Analytics", "Data Quality", "Scala"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover how to build and deploy AI-powered applications natively on the Databricks Data Intelligence Platform. This session introduces best practices and a standard reference architecture for developing production-ready apps using popular frameworks like Dash, Shiny, Gradio, Streamlit and Flask. Learn how to leverage agents for orchestration and explore primary use cases supported by Databricks Apps, including data visualization, AI applications, self-service analytics and data quality monitoring. With serverless deployment and built-in governance through Unity Catalog, Databricks Apps enables seamless integration with your data and AI models, allowing you to focus on delivering impactful solutions without the complexities of infrastructure management. Whether you're a data engineer or an app developer, this session will equip you with the knowledge to create secure, scalable and efficient applications within a Databricks environment. /Staff Software Engineer"}
{"session_id": "building-seamless-multi-cloud-platform-secure-portable-workloads", "title": "Building a Seamless Multi-Cloud Platform for Secure Portable Workloads", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY, HEALTH AND LIFE SCIENCES, PUBLIC SECTOR", "technologies": ["DELTA LAKE", "DELTA SHARING", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["Staff Software Engineer, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "There are many challenges to making a data platform actually a platform, something that hides complexity. Data engineers and scientists are looking for a simple and intuitive abstraction to focus on their work, not where it runs to maintain compliance, what credentials it uses to access data or how it generates operational telemetry. At Databricks we\u2019ve developed a data-centric approach to workload development and deployment that enables data workers to stop doing migrations and instead develop with confidence. Attend this session to learn how to run simple, secure and compliant global multi-cloud workloads at scale on Databricks. /Staff Software Engineer"}
{"session_id": "capitalizing-alternatives-data-addepar-platform-private-markets", "title": "Capitalizing Alternatives Data on the Addepar Platform: Private Markets Benchmarking", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "LIGHTNING TALK", "industry": "FINANCIAL SERVICES", "technologies": ["DATABRICKS WORKFLOWS", "DELTA LAKE", "DELTA SHARING"], "duration": "20 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Lake", "Data Pipeline", "ELT", "Scala"], "speakers": ["Addepar"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Addepar possesses an enormous private investment data set with 40% of the $7T assets on the platform allocated to alternatives. Leveraging the Addepar Data Lakehouse (ADL), built on Databricks, we have built a scalable data pipeline that assesses millions of private fund investment cash flows and translates it to a private fund benchmarks data offering. Investors on the Addepar platform can leverage this data seamlessly integrated against their portfolio investments and obtain actionable investment insights. At a high-level, this data offering consists of an extensive data aggregation, filtering, and construction logic that dynamically updates for clients through the Databricks job workflows. This derived dataset has gone through several iterations with investment strategists and academics that leveraged delta shared tables. Irrespective of the data source, the data pipeline coalesces all relevant cash flow activity against a unique identifier before constructing the benchmarks. /Addepar"}
{"session_id": "practitioners-guide-databricks-serverless", "title": "A Practitioner\u2019s Guide to Databricks Serverless", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "BEGINNER", "type": "BREAKOUT", "industry": "ENTERPRISE TECHNOLOGY", "technologies": ["DATABRICKS WORKFLOWS", "DLT"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Engineering", "Data Pipeline"], "speakers": ["Product Specialist, Databricks"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows. /Product Specialist"}
{"session_id": "prescription-success-leveraging-dabs-faster-deployment-and-better", "title": "A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "INTERMEDIATE", "type": "BREAKOUT", "industry": "HEALTH AND LIFE SCIENCES", "technologies": ["DATABRICKS WORKFLOWS", "UNITY CATALOG"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics"], "speakers": ["Principal Data Engineer, Health Catalyst"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Health Catalyst (HCAT) transformed its CI/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements. HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making. /Sr. Solutions Architect\nDatabricks /Principal Data Engineer"}
{"session_id": "unified-solution-data-management-and-model-training-apache-iceberg-and", "title": "A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming", "track": "DATA LAKEHOUSE ARCHITECTURE AND IMPLEMENTATION", "level": "ADVANCED", "type": "BREAKOUT", "industry": "MEDIA AND ENTERTAINMENT, PUBLIC SECTOR", "technologies": ["APACHE ICEBERG", "MOSAIC AI"], "duration": "40 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Scala", "Streaming"], "speakers": ["machine learning system engineer, ByteDance"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming). Magnus uses Iceberg\u2019s branch/tag to manage massive datasets/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability. When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility. In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training. /Infrastructure Engineer\nByteDance /machine learning system engineer"}
