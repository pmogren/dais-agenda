{"session_id": "advanced-machine-learning-operations", "title": "Advanced Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Python"], "speakers": ["CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course is designed to cover advanced concepts and workflows in machine learning operations. It starts by introducing participants to continuous integration (CI) and continuous development (CD) workflows within machine learning projects, guiding them through the deployment of a sample CI/CD workflow using Databricks in the first section. Moving on to the second part, participants delve into data and model testing, where they actively create tests and automate CI/CD workflows. Finally, the course concludes with an exploration of model monitoring concepts, demonstrating the use of Lakehouse Monitoring to oversee machine learning models in production settings. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. intermediate-level knowledge of traditional ML concepts, development with CI/CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "ai-agents-hackathon", "title": "AI Agents Hackathon", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "540 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "aibi-data-analysts", "title": "AI/BI for Data Analysts", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence"], "speakers": ["abilities: Labs: Yes"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to use the features Databricks provides for business intelligence needs: AI/BI Dashboards and AI/BI Genie. As a Databricks Data Analyst, you will be tasked with creating AI/BI Dashboards and AI/BI Genie Spaces within the platform, managing the access to these assets by stakeholders and necessary parties, and maintaining these assets as they are edited, refreshed, or decommissioned over the course of their lifespan. This course intends to instruct participants on how to design dashboards for business insights, share those with collaborators and stakeholders, and maintain those assets within the platform. Participants will also learn how to utilize AI/BI Genie Spaces to support self-service analytics through the creation and maintenance of these environments powered by the Databricks Data Intelligence Engine. Pre-requisites: The content was developed for participants with these skills/knowledge/abilities: Labs: Yes"}
{"session_id": "aibi-self-service-analytics", "title": "AI/BI for Self-Service Analytics", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you will learn how to self-serve business insights from your company\u2019s Databricks Data Intelligence Platform using AI/BI. After a tour of the fundamental components of the platform, you\u2019ll learn how to interact with pre-created AI/BI Dashboards to explore your company\u2019s data through existing charts and visualizations. You\u2019ll also learn how to use AI/BI Genie to go beyond dashboards by asking follow-up questions in natural language to self-serve new insights, create visualizations, and share them with your colleagues. Pre-requisites: A working understanding of your organization\u2019s business and key performance indicators. Labs: No Certification Path: N/A"}
{"session_id": "automated-deployment-databricks-asset-bundles", "title": "Automated Deployment with Databricks Asset Bundles", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate project deployments Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge platform, including experience Workspaces, Apache Spark, Delta Lake, Medallion Architecture, Unity Catalog, Live Tables, and Workflows. In particular, leveraging Expectations DLTs. Labs: Yes Certification Path: Certified Data Engineer Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides a comprehensive review of DevOps principles and their application to Databricks projects. It begins with an overview of core DevOps, DataOps, continuous integration (CI), continuous deployment (CD), and testing, and explores how these principles can be applied to data engineering pipelines. The course then focuses on continuous deployment within the CI/CD process, examining tools like the Databricks REST API, SDK, and CLI for project deployment. You will learn about Databricks Asset Bundles (DABs) and how they fit into the CI/CD process. You\u2019ll dive into their key components, folder structure, and how they streamline deployment across various target environments in Databricks. You will also learn how to add variables, modify, validate, deploy, and execute Databricks Asset Bundles for multiple environments with different configurations using the Databricks CLI. Finally, the course introduces Visual Studio Code as an Interactive Development Environment (IDE) for building, testing, and deploying Databricks Asset Bundles locally, optimizing your development process. The course concludes with an introduction to automating deployment pipelines using GitHub Actions to enhance the CI/CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate Databricks project deployments with Databricks Asset Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge of the Databricks platform, including experience with Databricks Workspaces, Apache Spark, Delta Lake, the Medallion Architecture, Unity Catalog, Delta Live Tables, and Workflows. In particular, knowledge of leveraging Expectations with DLTs. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "build-data-pipelines-dlt", "title": "Build Data Pipelines with DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to define and schedule data pipelines that incrementally ingest and process data through multiple tables on the Data Intelligence Platform, using DLT in Spark SQL and Python. We\u2019ll cover topics like how to get started with DLT, how DLT tracks data dependencies in data pipelines, how to configure and run data pipelines using the DLT. UI, how to use Python or Spark SQL to define data pipelines that ingest and process data through multiple tables on the Data Intelligence Platform, using Auto Loader and DLT, how to use APPLY CHANGES INTO syntax to process Change Data Capture feeds, and how to review event logs and data artifacts created by pipelines and troubleshoot syntax. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
