{"session_id": "advanced-machine-learning-operations", "title": "Advanced Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Machine Learning", "Python"], "speakers": ["CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course is designed to cover advanced concepts and workflows in machine learning operations. It starts by introducing participants to continuous integration (CI) and continuous development (CD) workflows within machine learning projects, guiding them through the deployment of a sample CI/CD workflow using Databricks in the first section. Moving on to the second part, participants delve into data and model testing, where they actively create tests and automate CI/CD workflows. Finally, the course concludes with an exploration of model monitoring concepts, demonstrating the use of Lakehouse Monitoring to oversee machine learning models in production settings. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. intermediate-level knowledge of traditional ML concepts, development with CI/CD, the use of Python and Git for ML projects with popular platforms like GitHub) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "ai-agents-hackathon", "title": "AI Agents Hackathon", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "540 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "aibi-data-analysts", "title": "AI/BI for Data Analysts", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Business Intelligence"], "speakers": ["abilities: Labs: Yes"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to use the features Databricks provides for business intelligence needs: AI/BI Dashboards and AI/BI Genie. As a Databricks Data Analyst, you will be tasked with creating AI/BI Dashboards and AI/BI Genie Spaces within the platform, managing the access to these assets by stakeholders and necessary parties, and maintaining these assets as they are edited, refreshed, or decommissioned over the course of their lifespan. This course intends to instruct participants on how to design dashboards for business insights, share those with collaborators and stakeholders, and maintain those assets within the platform. Participants will also learn how to utilize AI/BI Genie Spaces to support self-service analytics through the creation and maintenance of these environments powered by the Databricks Data Intelligence Engine. Pre-requisites: The content was developed for participants with these skills/knowledge/abilities: Labs: Yes"}
{"session_id": "aibi-self-service-analytics", "title": "AI/BI for Self-Service Analytics", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["A"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you will learn how to self-serve business insights from your company\u2019s Databricks Data Intelligence Platform using AI/BI. After a tour of the fundamental components of the platform, you\u2019ll learn how to interact with pre-created AI/BI Dashboards to explore your company\u2019s data through existing charts and visualizations. You\u2019ll also learn how to use AI/BI Genie to go beyond dashboards by asking follow-up questions in natural language to self-serve new insights, create visualizations, and share them with your colleagues. Pre-requisites: A working understanding of your organization\u2019s business and key performance indicators. Labs: No Certification Path: N/A"}
{"session_id": "automated-deployment-databricks-asset-bundles", "title": "Automated Deployment with Databricks Asset Bundles", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Data Engineering", "Delta Lake", "ELT"], "speakers": ["CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate project deployments Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge platform, including experience Workspaces, Apache Spark, Delta Lake, Medallion Architecture, Unity Catalog, Live Tables, and Workflows. In particular, leveraging Expectations DLTs. Labs: Yes Certification Path: Certified Data Engineer Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides a comprehensive review of DevOps principles and their application to Databricks projects. It begins with an overview of core DevOps, DataOps, continuous integration (CI), continuous deployment (CD), and testing, and explores how these principles can be applied to data engineering pipelines. The course then focuses on continuous deployment within the CI/CD process, examining tools like the Databricks REST API, SDK, and CLI for project deployment. You will learn about Databricks Asset Bundles (DABs) and how they fit into the CI/CD process. You\u2019ll dive into their key components, folder structure, and how they streamline deployment across various target environments in Databricks. You will also learn how to add variables, modify, validate, deploy, and execute Databricks Asset Bundles for multiple environments with different configurations using the Databricks CLI. Finally, the course introduces Visual Studio Code as an Interactive Development Environment (IDE) for building, testing, and deploying Databricks Asset Bundles locally, optimizing your development process. The course concludes with an introduction to automating deployment pipelines using GitHub Actions to enhance the CI/CD workflow with Databricks Asset Bundles. By the end of this course, you will be equipped to automate Databricks project deployments with Databricks Asset Bundles, improving efficiency through DevOps practices. Pre-requisites: Strong knowledge of the Databricks platform, including experience with Databricks Workspaces, Apache Spark, Delta Lake, the Medallion Architecture, Unity Catalog, Delta Live Tables, and Workflows. In particular, knowledge of leveraging Expectations with DLTs. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "build-data-pipelines-dlt", "title": "Build Data Pipelines with DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to define and schedule data pipelines that incrementally ingest and process data through multiple tables on the Data Intelligence Platform, using DLT in Spark SQL and Python. We\u2019ll cover topics like how to get started with DLT, how DLT tracks data dependencies in data pipelines, how to configure and run data pipelines using the DLT. UI, how to use Python or Spark SQL to define data pipelines that ingest and process data through multiple tables on the Data Intelligence Platform, using Auto Loader and DLT, how to use APPLY CHANGES INTO syntax to process Change Data Capture feeds, and how to review event logs and data artifacts created by pipelines and troubleshoot syntax. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "data-after-hours", "title": "Data After Hours", "track": "", "level": "", "type": "EVENING EVENT", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "data-ingestion-lakeflow-connect", "title": "Data Ingestion with Lakeflow Connect", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Lake", "Data Warehouse", "Delta Lake", "ELT", "ETL", "Python", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to have efficient data ingestion with Lakeflow Connect and manage that data with Databricks. We\u2019ll cover topics such as ingestion with built-in connectors for popular SaaS applications, databases and file sources, as well as ingestion from cloud object storage, and batch and streaming ingestion. Lakeflow Connect is fully integrated with the Data Intelligence Platform including unified governance, observability, and Delta Lake for the foundation of a data lakehouse architecture. We'll cover the new connector components, setting up the pipeline, validating the source and mapping to the destination for each type of connector. We'll also cover how to ingest data with Batch to Streaming ingestion into Delta tables, using the UI with Auto Loader, automating ETL with DLT or using the API. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc. Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "data-management-and-governance-uc", "title": "Data Management and Governance With UC", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Data Lake", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you'll learn concepts and perform labs that showcase workflows using Unity Catalog - Databricks' unified and open governance solution for data and AI. We'll start off with a brief introduction to Unity Catalog, discuss fundamental data governance concepts, and then dive into a variety of topics including using Unity Catalog for data access control, managing external storage and tables, data segregation, and more. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.). Labs: Yes Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "data-modeling-strategies", "title": "Data Modeling Strategies", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Architecture", "Data Integration", "Delta Lake", "ELT"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course offers a deep dive into designing data models within the Databricks Lakehouse environment, and understanding the data products lifecycle. Participants will learn to align business requirements with data organization and model design leveraging Delta Lake and Unity Catalog for defining data architectures, and techniques for data integration and sharing. Prerequisites: Foundational knowledge equivalent to Databricks Certified Data Engineer Associate and familiarity with many topics covered in Databricks Certified Data Engineer Professional. Experience with: Labs: Yes"}
{"session_id": "data-preparation-machine-learning", "title": "Data Preparation for Machine Learning", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Machine Learning", "Python"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn the fundamentals of preparing data for machine learning using Databricks. We\u2019ll cover topics like exploring, cleaning, and organizing data tailored for traditional machine learning applications. We\u2019ll also cover data visualization, feature engineering, and optimal feature storage strategies. Pre-requisites: Familiarity with Databricks workspace, notebooks, as well as Unity Catalog. An intermediate level knowledge of Python (scikit-learn, Matplotlib), Pandas, and PySpark. As well as with concepts of exploratory data analysis, feature engineering, standardization, and imputation methods). Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "data-warehousing-databricks", "title": "Data Warehousing with Databricks", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Warehouse"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed for data professionals who want to explore the data warehousing capabilities of Databricks. Assuming no prior knowledge of Databricks, it provides an introduction to leveraging Databricks as a modern cloud-based data warehousing solution. Learners will explore how use the Databricks Data Intelligence Platform to ingest, transform, govern, and analyze data efficiently. Learners will also explore Genie, an innovative Databricks feature that simplifies data exploration through natural language queries. By the end of this course, participants will be equipped with the foundational skills to implement and optimize a data warehouse using Databricks. Pre-requisites: Labs: Yes"}
{"session_id": "databricks-data-privacy", "title": "Databricks Data Privacy", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Data Governance", "Delta Lake", "ELT", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to apply patterns to securely store and delete personal information for data governance and compliance on the Data Intelligence Platform. We\u2019ll cover topics like storing sensitive data appropriately to simplify granting access and processing deletes, processing deletes to ensure compliance with the right to be forgotten, performing data masking, and configuring fine-grained access control to configure appropriate privileges to sensitive data. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.). Beginner experience with DLT and streaming workloads. Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "databricks-performance-optimization", "title": "Databricks Performance Optimization", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to optimize workloads and physical layout with Spark and Delta Lake and and analyze the Spark UI to assess performance and debug applications. We\u2019ll cover topics like streaming, liquid clustering, data skipping, caching, photons, and more. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.) Labs: Yes Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "databricks-streaming-and-dlt", "title": "Databricks Streaming and DLT", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Data Quality", "Delta Lake", "ELT", "ETL", "SQL", "Streaming"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to Incrementally process data to power analytic insights with Structured Streaming and Auto Loader, and how to apply design patterns for designing workloads to perform ETL on the Data Intelligence Platform with DLT. First, we\u2019ll cover topics including ingesting raw streaming data, enforcing data quality, implementing CDC, and exploring and tuning state information. Then, we\u2019ll cover options to perform a streaming read on a source, requirements for end-to-end fault tolerance, options to perform a streaming write to a sink, and creating an aggregation and watermark on a streaming dataset. Pre-requisites: Ability to perform basic code development tasks using the Databricks workspace (create clusters, run code in notebooks, use basic notebook operations, import repos from git, etc.), intermediate programming experience with SQL and PySpark (extract data from a variety of file formats and data sources, apply a number of common transformations to clean data, reshape and manipulate complex data using advanced built-in functions), intermediate programming experience with Delta Lake (create tables, perform complete and incremental updates, compact files, restore previous versions etc.). Beginner experience with streaming workloads and familiarity with DLT. Labs: No Certification Path: Databricks Certified Data Engineer Professional"}
{"session_id": "deploy-workloads-lakeflow-jobs-previously-databricks-workflows", "title": "Deploy Workloads with Lakeflow Jobs (previously Databricks Workflows)", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Analytics", "Data Lake", "Data Pipeline", "Data Warehouse", "Python", "SQL"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to orchestrate data pipelines with LakeFlow Jobs (previously Databricks Workflows) and schedule dashboard updates to keep analytics up-to-date. We\u2019ll cover topics like getting started with LakeFlow Jobs, how to use Databricks SQL for on-demand queries, and how to configure and schedule dashboards and alerts to reflect updates to production data pipelines. Pre-requisites: Beginner familiarity with the Databricks Data Intelligence Platform (selecting clusters, navigating the Workspace, executing notebooks), cloud computing concepts (virtual machines, object storage, etc.), production experience working with data warehouses and data lakes, intermediate experience with basic SQL concepts (select, filter, groupby, join, etc), beginner programming experience with Python (syntax, conditions, loops, functions), beginner programming experience with the Spark DataFrame API (Configure DataFrameReader and DataFrameWriter to read and write data, Express query transformations using DataFrame methods and Column expressions, etc.) Labs: No Certification Path: Databricks Certified Data Engineer Associate"}
{"session_id": "devconnect-keynote", "title": "DevConnect Keynote", "track": "", "level": "", "type": "MEETUP", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Analytics", "Data Pipeline"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The DevConnect Meetup is a multi-track event designed to explore the latest news, innovations, and trends in the data, analytics, and AI ecosystem. This technical meetup series focuses on GenAI and agentic systems by connecting researchers, engineers, and industry leaders to share insights, discuss challenges, and showcase cutting-edge developments from data pipelines to AI architectures. Attendees will have the opportunity to hear from researchers and engineers representing academic institutions, startups, and enterprises at the forefront of AI innovation."}
{"session_id": "evaluating-domain-specific-agent-performance-and-metrics", "title": "DISCOVER DATA INTELLIGENCE", "track": "", "level": "", "type": "", "industry": "", "technologies": [], "duration": "", "experience": "", "areas_of_interest": ["AI", "Apache Spark", "Delta Lake", "ELT"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Data + AI Summit speakers include leading experts, researchers and open source contributors \u2014 from Databricks and across the data and AI community Ali\nDimon Chairman and CEO, JPMorgan Chase Kasey\nAmodei Co-founder and CEO, Anthropic Matei\nBrown Product Manager, FedEx Miranda\nXin Co-founder and Chief Architect, Databricks Arvindram\nSutara Field CTO, Databricks 700+ sessions \u2014 from data intelligence to data warehousing, governance to AI. Here are some key highlights. Databricks Walmart Rivian Automotive, LLC Rivian T-Mobile T-Mobile Databricks Mastercard Databricks PepsiCo Join thousands of data leaders, engineers, scientists and architects to explore the convergence of data and AI Explore the latest advances in Delta Lake, Apache Iceberg\u2122, agentic systems, MLflow, Apache Spark\u2122, Unity Catalog, DLT, DSPy, LangChain, PyTorch, dbt, Trino, as well as the newest innovations from our sponsors, partners and the Databricks Data Intelligence Platform. Interact with thousands of your peers from the data and AI community, and grow your professional network through social meetups, the Expo and exclusive event parties. Sharpen your expertise with instructor-led, half-day training. Boost your credentials with our hands-on, lab-based format and get onsite certifications. Reduced prices are available for group purchases and government, military, education and nonprofit attendees GENERAL ADMISSION $1,895 STANDARD PRICE MAY 1 -> JUNE 12 Don\u2019t wait \u2014 secure your spot at Data + AI Summit 2025! Who should attend Data + AI Summit? What is Data + AI Summit? Where in San Francisco will the event take place? Where should I stay for the event? What is included in the full conference pass? Data + AI Summit couldn\u2019t happen without our awesome sponsors. Interested in sponsoring? Reach out to our Sponsorship Management team to learn about available opportunities."}
{"session_id": "gen-ai-application-development", "title": "Gen AI Application Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course provides participants with information and practical experience in building advanced LLM (Large Language Model) applications using multi-stage reasoning LLM chains and agents. In the initial section, participants will learn how to decompose a problem into its components and select the most suitable model for each step to enhance business use cases. Following this, participants will construct a multi-stage reasoning chain utilizing LangChain and HuggingFace transformers. Finally, participants will be introduced to agents and will design an autonomous agent using generative models on Databricks. Pre-requisites: Solid understanding of natural language processing (NLP) concepts, familiarity with prompt engineering and prompt engineering best practices, experience with the Databricks Data Intelligence Platform, experience with retrieval-augmented generation (RAG) techniques including data preparation, building RAG architectures, and concepts like embeddings, vectors, and vector databases Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-deployment-and-monitoring", "title": "Gen AI Deployment and Monitoring", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course introduces learners to deploying, operationalizing, and monitoring generative artificial intelligence (AI) applications. First, learners will develop knowledge and skills in deploying generative AI applications using tools like Model Serving. Next, the course will discuss operationalizing generative AI applications following modern LLMOps best practices and recommended architectures. Finally, learners will be introduced to the idea of monitoring generative AI applications and their components using Lakehouse Monitoring. Pre-requisites: Familiarity with prompt engineering and retrieval-augmented generation (RAG) techniques, including data preparation, embeddings, vectors, and vector databases. A foundational knowledge of Databricks Data Intelligence Platform tools for evaluation and governance (particularly Unity Catalog). Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-evaluation-and-governance", "title": "Gen AI Evaluation and Governance", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": ["security systems. Next, the course will connect evaluation and governance systems to Databricks Data Intelligence Platform. Third, learners be introduced a variety of techniques for specific components types applications. Finally, conclude with an analysis evaluating entire AI respect performance cost. Pre-requisites: Familiarity prompt engineering, experience Additionally, knowledge retrieval-augmented generation (RAG) including data preparation, embeddings, vectors, vector databases Labs: Yes Certification Path: Certified Generative Engineer Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course introduces learners to evaluating and governing GenAI (generative artificial intelligence) systems. First, learners will explore the meaning behind and motivation for building evaluation and governance/security systems. Next, the course will connect evaluation and governance systems to the Databricks Data Intelligence Platform. Third, learners will be introduced to a variety of evaluation techniques for specific components and types of applications. Finally, the course will conclude with an analysis of evaluating entire AI systems with respect to performance and cost. Pre-requisites: Familiarity with prompt engineering, and experience with the Databricks Data Intelligence Platform. Additionally, knowledge of retrieval-augmented generation (RAG) techniques including data preparation, embeddings, vectors, and vector databases Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "gen-ai-solution-development", "title": "Gen AI Solution Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed to introduce participants to contextual GenAI (generative artificial intelligence) solutions using the retrieval-augmented generation (RAG) method. Firstly, participants will be introduced to the RAG architecture and the significance of contextual information using Mosaic AI Playground. Next, the course will demonstrate how to prepare data for GenAI solutions and connect this process with building an RAG architecture. Finally, participants will explore concepts related to context embedding, vectors, vector databases, and the utilization of the Mosaic AI Vector Search product. Pre-requisites: Familiarity with embeddings, prompt engineering best practices, and experience with the Databricks Data Intelligence Platform Labs: Yes Certification Path: Databricks Certified Generative AI Engineer Associate"}
{"session_id": "industry-forum-networking-reception", "title": "Industry Forum Networking Reception", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "60 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "machine-learning-model-deployment", "title": "Machine Learning Model Deployment", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Delta Lake", "ELT", "Machine Learning", "Python", "Real-time"], "speakers": ["ML like Scikit-Learn, awareness of model deployment strategies) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course is designed to introduce three primary machine learning deployment strategies and illustrate the implementation of each strategy on Databricks. Following an exploration of the fundamentals of model deployment, the course delves into batch inference, offering hands-on demonstrations and labs for utilizing a model in batch inference scenarios, along with considerations for performance optimization. The second part of the course comprehensively covers pipeline deployment, while the final segment focuses on real-time deployment. Participants will engage in hands-on demonstrations and labs, deploying models with Model Serving and utilizing the serving endpoint for real-time inference. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. common Python libraries for DS/ML like Scikit-Learn, awareness of model deployment strategies) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-model-development", "title": "Machine Learning Model Development", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Machine Learning", "Python"], "speakers": ["ML like Scikit-Learn, fundamental algorithms regression and classification, model evaluation with common metrics) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "In this course, you\u2019ll learn how to develop traditional machine learning models on Databricks. We\u2019ll cover topics like using popular ML libraries, executing common tasks efficiently with AutoML and MLflow, harnessing Databricks' capabilities to track model training, leveraging feature stores for model development, and implementing hyperparameter tuning. Additionally, the course covers AutoML for rapid and low-code model training, ensuring that participants gain practical, real-world skills for streamlined and effective machine learning model development in the Databricks environment. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. common Python libraries for DS/ML like Scikit-Learn, fundamental ML algorithms like regression and classification, model evaluation with common metrics) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-operations", "title": "Machine Learning Operations", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["AI", "Delta Lake", "ELT", "Machine Learning", "Python"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "This course will guide participants through a comprehensive exploration of machine learning model operations, focusing on MLOps and model lifecycle management. The initial segment covers essential MLOps components and best practices, providing participants with a strong foundation for effectively operationalizing machine learning models. In the latter part of the course, we will delve into the basics of the model lifecycle, demonstrating how to navigate it seamlessly using the Model Registry in conjunction with the Unity Catalog for efficient model management. By the course's conclusion, participants will have gained practical insights and a well-rounded understanding of MLOps principles, equipped with the skills needed to navigate the intricate landscape of machine learning model operations. Pre-requisites: Familiarity with Databricks workspace and notebooks, familiarity with Delta Lake and Lakehouse, intermediate level knowledge of Python (e.g. understanding of basic MLOps concepts and practices as well as infrastructure and importance of monitoring MLOps solutions) Labs: Yes Certification Path: Databricks Certified Machine Learning Associate"}
{"session_id": "machine-learning-scale", "title": "Machine Learning at Scale", "track": "", "level": "", "type": "PAID TRAINING", "industry": "", "technologies": [], "duration": "240 MIN", "experience": "IN PERSON", "areas_of_interest": ["Apache Spark", "Machine Learning", "Python", "Scala"], "speakers": ["ML concepts, common model metrics and python libraries as well a basic understanding of scaling workloads with Spark) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "The course intends to equip professional-level machine learning practitioners with knowledge and hands-on experience in utilizing Apache Spark\u2122 for machine learning purposes, including model fine-tuning. Additionally, the course covers using the Pandas library for scalable machine learning tasks. The initial section of the course focuses on comprehending the fundamentals of Apache Spark\u2122 along with its machine learning capabilities. Subsequently, the second section delves into fine-tuning models using the hyperopt library. The final segment involves learning the implementation of the Pandas API within Apache Spark\u2122, encompassing guidance on Pandas UDFs (User-Defined Functions) and the Functions API for model inference. Pre-requisites: Familiarity with Databricks workspace and notebooks; knowledge of machine learning model development and deployment with MLflow (e.g. basic understanding of DS/ML concepts, common model metrics and python libraries as well as a basic understanding of scaling workloads with Spark) Labs: Yes Certification Path: Databricks Certified Machine Learning Professional"}
{"session_id": "startup-forum", "title": "Startup Forum", "track": "", "level": "BEGINNER", "type": "MEETUP", "industry": "ENTERPRISE TECHNOLOGY", "technologies": [], "duration": "90 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "thursday-keynote", "title": "Thursday Keynote", "track": "", "level": "", "type": "KEYNOTE", "industry": "", "technologies": [], "duration": "180 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Discover the latest advances on the Data Intelligence Platform and hear from the companies who are already enjoying success."}
{"session_id": "wednesday-keynote", "title": "Wednesday Keynote", "track": "", "level": "", "type": "KEYNOTE", "industry": "", "technologies": [], "duration": "180 MIN", "experience": "IN PERSON, VIRTUAL", "areas_of_interest": ["AI"], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": "Be first to witness the latest breakthroughs from Databricks and share the success of innovative data and AI companies."}
{"session_id": "welcome-reception", "title": "Welcome Reception", "track": "", "level": "", "type": "EVENING EVENT", "industry": "", "technologies": [], "duration": "120 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
{"session_id": "women-data-ai", "title": "Women in Data + AI", "track": "", "level": "", "type": "SPECIAL INTEREST", "industry": "", "technologies": [], "duration": "150 MIN", "experience": "IN PERSON", "areas_of_interest": [], "speakers": [], "schedule": {"day": "", "room": "", "start_time": "", "end_time": ""}, "description": ""}
