{"id":"D25L3395","title":"10+ Reasons to Use Databricks\u2019 Delta Live Tables for Your Next Data Processing Project","description":"<p>DLT 's home page says, \u201cIt\u2019s a declarative ETL framework (...) that helps data teams simplify streaming and batch ETL cost-effectively. Simply define the transformations to perform on your data and let DLT pipelines automatically manage task orchestration, cluster management, monitoring, data quality and error handling.<\/p><p>\u00a0<\/p><p>This talk aims to show you how DLT saved me a lot of trouble while on a tight delivery schedule. I\u2019ll show you why the DLT headline is correct. In other words, I hope I will convince you to consider the DLT framework for your next ETL project.<\/p><p>\u00a0<\/p><p>I found over 10 reasons why investing in DLT for your next project is worth your time.<\/p><p>\u00a0<\/p><p>I will discuss the foundational concepts (Spark SQL and Structured Streaming, Delta Lake) and more importantly, how they paved the way for DLT.<\/p><p>\u00a0<\/p><p>The talk is based on my recent experience with two successful projects, which have done very well from their humble beginnings and were so much fun to be part of.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Professional Services"],"category":["Apache Spark, Delta Lake, DLT"],"areas_of_interest":["Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Jacek Laskowski","company":"japila.pl","job_title":"Freelance Data Engineer","bio":"Jacek is a Freelance Data(bricks) Engineer specialising in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Unity Catalog, MLflow, Databricks, and Apache Kafka (incl. Kafka Streams) with brief forays into a wider data engineering space (mostly during Warsaw Data Engineering meetups).<br \/>\n<br \/>\nJacek offers software development and consultancy services, as well as very hands-on, in-depth workshops and mentoring. He is best known for his \"The Internals Of\" online books available free of charge at https:\/\/books.japila.pl\/.<br \/>\n<br \/>\nContact him at jacek@japila.pl to discuss opportunities.<br \/>\n<br \/>\nMotto: \"A day with no git commit is a lost day\" and \"When one teaches, two learn\".","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/jacek_laskowski_20201229_200x200_1746040432653001hNiA.png?h=55541bb6&itok=av6zQjYt","alt":"Jacek Laskowski"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/10-reasons-use-databricks-delta-live-tables-your-next-data-processing","alias":"\/data-ai-summit-2025\/session\/10-reasons-use-databricks-delta-live-tables-your-next-data-processing","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533631+00:00"}
{"id":"D25B3190DBX","title":"A Comprehensive Guide to Streaming on the Data Intelligence Platform","description":"<p>Is stream processing the future? We think so \u2014 and we\u2019re building it with you using the latest capabilities in Apache Spark\u2122 Structured Streaming. If you're a power user, this session is for you: we\u2019ll demo new advanced features, from state transformations to real-time mode. If you prefer simplicity, this session is also for you: we\u2019ll show how DLT simplifies managing streaming pipelines. And if you\u2019re somewhere in between, we\u2019ve got you covered \u2014 we\u2019ll explain when to use your own streaming jobs versus DLT.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities, Manufacturing, Retail and CPG - Food"],"category":["Apache Spark, DLT"],"areas_of_interest":["Databricks Experience (DBX), Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Indrajit Roy","company":"Databricks","job_title":"Director of Engineering","bio":"Indrajit is passionate about making data processing simple and easy to use for everyone. At Databricks, he leads the teams the power Spark Structured Streaming and make materialized views magical. Indrajit used to lead the peta-byte scale and revenue critical Napa datawarehouse at Google. Indrajit started his career as a researcher at HP Labs, sprinkling machine learning over columnar store databases. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ir-headshot_1745213915826001VeWu.jpg?h=6f41954f&itok=BZVh44PX","alt":"Indrajit Roy"}},{"name":"Navneeth Nair","company":"Databricks","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/comprehensive-guide-streaming-data-intelligence-platform","alias":"\/data-ai-summit-2025\/session\/comprehensive-guide-streaming-data-intelligence-platform","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533648+00:00"}
{"id":"D25L1824","title":"A Practical Roadmap to Becoming an Expert Databricks Data Engineer","description":"<p>The demand for skilled Databricks data engineers continues to rise as enterprises accelerate their adoption of the Databricks platform. However, navigating the complex ecosystem of data engineering tools, frameworks and best practices can be overwhelming. This session provides a structured roadmap to becoming an expert Databricks data engineer, offering a clear progression from foundational skills to advanced capabilities.<\/p><p>\u00a0<\/p><p>Acadford, a leading training provider, has successfully trained thousands of data engineers on Databricks, equipping them with the skills needed to excel in their careers and obtain professional certifications. Drawing on this experience, we will guide attendees through the most in-demand skills and knowledge areas through a combination of structured learning and practical insights.<\/p><p>\u00a0<\/p><p>Key takeaways:<\/p><ul>\t<li>Understand the core tech stack in Databricks<\/li>\t<li>Explore real-world code examples and live demonstrations<\/li>\t<li>Receive an actionable learning path with recommended resources<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Education"],"category":["Apache Spark, Delta Lake, Databricks Workflows"],"areas_of_interest":["Developer Experience, Getting started with Databricks, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Derar Alhussein","company":null,"job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/practical-roadmap-becoming-expert-databricks-data-engineer","alias":"\/data-ai-summit-2025\/session\/practical-roadmap-becoming-expert-databricks-data-engineer","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533659+00:00"}
{"id":"D25B1879","title":"Accelerating Data Ingestion with New Innovations in Auto Loader\u2019s Performance and Schema Evolution","description":"<p>Auto Loader is a powerful structured streaming data source connector from Lakeflow Connect, trusted by more than 4,000 Databricks customers to ingest multiple petabytes of file data from cloud storage every day. In this session, we will explore key innovations and enhancements in Auto Loader\u2019s performance and schema evolution capabilities, including:<\/p><ul>\t<li>Accelerated ingestion of millions of files by parallelizing file reading and processing within a single Apache Spark\u2122 task<\/li>\t<li>Faster parallelized processing of large compressed and multi-line files<\/li>\t<li>Improved file discovery and state management for enhanced scalability<\/li>\t<li>Advanced schema evolution with type widening support<\/li>\t<li>Support for the Variant data type, providing greater flexibility in handling evolving schemas<\/li><\/ul><p>You will gain insights into how these enhancements can help overcome data schema challenges while building more performant, scalable, and cost-effective ingestion pipelines with Lakeflow Connect.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Apache Spark, LakeFlow"],"areas_of_interest":["Data Ingestion, ETL"],"delivery":["In Person"],"speakers":[{"name":"Elise Georis","company":"Databricks","job_title":"Staff Product Manager","bio":"Elise Georis is a Staff Product Manager at Databricks, focused on data ingestion. Previously, she worked on AI for ads at LinkedIn. She holds a computer science degree from Princeton University.","image":{}},{"name":"Sandip Agarwala","company":"Databricks","job_title":"Staff Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/accelerating-data-ingestion-new-innovations-auto-loaders-performance","alias":"\/data-ai-summit-2025\/session\/accelerating-data-ingestion-new-innovations-auto-loaders-performance","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533681+00:00"}
{"id":"D25L3370","title":"Apache Spark \u2014 Ask Us Anything","description":"<p>Join us for an interactive Ask Me Anything (AMA) session on the latest advancements in Apache Spark 4, including Spark Connect\u2014the new client-server architecture enabling seamless integration with IDEs, notebooks and custom applications. Learn about performance improvements, enhanced APIs and best practices for leveraging Spark\u2019s next-generation features. Whether you're a data engineer, Spark developer or big data enthusiast, bring your questions on architecture, real-world use cases and how these innovations can optimize your workflows. Don\u2019t miss this chance to dive deep into the future of distributed computing with Spark!<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Lightning Talk","industry":["Professional Services"],"category":["Apache Spark"],"areas_of_interest":["Open Source"],"delivery":["In Person"],"speakers":[{"name":"Allison Wang","company":"Databricks","job_title":"Staff Software Engineer","bio":"Allison is a software engineer at Databricks, working on Spark SQL and PySpark. She holds a Bachelor\u2019s degree in Computer Science from Carnegie Mellon University.","image":{}},{"name":"DB Tsai","company":"Databricks","job_title":"Senior Engineering Manager","bio":"DB Tsai is an engineering leader at the Databricks Spark team. He is an Apache Spark Project Management Committee (PMC) Member and Committer, and he enjoys building teams with great cultures focusing on large scale distributed data infrastructure. Before his transition to a leadership role, he implemented several algorithms including Linear Regression and Binary\/Multinomial Logistic Regression with Elastici-Net (L1\/L2) regularization using LBFGS\/OWL-QN optimizers in Apache Spark project.","image":{}},{"name":"Jules Damji","company":"Databricks","job_title":"Developer Advocate","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/apache-spark-ask-us-anything","alias":"\/data-ai-summit-2025\/session\/apache-spark-ask-us-anything","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533792+00:00"}
{"id":"D25B3261DBX","title":"Authoring Data Pipelines With the New DLT Editor","description":"<p>We\u2019re introducing a new developer experience for DLT designed for data practitioners who prefer a code-first approach and expect robust developer tooling. The new multi-file editor brings an IDE-like environment to declarative pipeline development, making it easy to structure transformation logic, configure pipelines throughout the development lifecycle and iterate efficiently.<\/p><p>\u00a0<\/p><p>Features like contextual data previews and selective table updates enable step-by-step development. UI-driven tools, such as DAG previews and DAG-based actions, enhance productivity for experienced users and provide a bridge for those transitioning to declarative workflows.<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll showcase the new editor in action, highlighting how these enhancements simplify declarative coding and improve development for production-ready data pipelines. Whether you\u2019re an experienced developer or new to declarative data engineering, join us to see how DLT can enhance your data practice.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Adriana Ispas","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":"Adriana Ispas is a Sr. Staff Product Manager at Databricks working on the Databricks Runtime and Databricks SQL. She holds a Ph.D. in Computer Science from ETH Zurich.","image":{}},{"name":"Camiel Steenstra","company":"Databricks","job_title":"Staff Software Engineer","bio":"Camiel Steenstra is a Staff Software Engineer at Databricks. He works in the Lakeflow authoring team and most recently on the Lakeflow DLT Pipelines development experience. Before that he worked on merchant facing tooling at Google Commerce for 7 years.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/profile_picture_1745654784379001Jmvy.JPG?h=c8ce5f30&itok=bOsV5oeL","alt":"Camiel Steenstra"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/authoring-data-pipelines-new-dlt-editor","alias":"\/data-ai-summit-2025\/session\/authoring-data-pipelines-new-dlt-editor","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533800+00:00"}
{"id":"D25L2079","title":"Automating Engineering with AI - LLMs in Metadata Driven Frameworks","description":"<p>The demand for data engineering keeps growing, but data teams are bored by repetitive tasks, stumped by growing complexity and endlessly harassed by an unrelenting need for speed. What if AI could take the heavy lifting off your hands? What if we make the move away from code-generation and into config-generation \u2014 how much more could we achieve?\u00a0<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll explore how AI is revolutionizing data engineering, turning pain points into innovation. Whether you\u2019re grappling with manual schema generation or struggling to ensure data quality, this session offers practical solutions to help you work smarter, not harder.<\/p><p>\u00a0<\/p><p>You\u2019ll walk away with a good idea of where AI is going to disrupt the data engineering workload, some good tips around how to accelerate your own workflows and an impending sense of doom around the future of the industry!<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Apache Spark, Databricks Apps"],"areas_of_interest":["Data Ingestion, Developer Experience, ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Simon Whiteley","company":"Advancing Analytics","job_title":"CTO","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/automating-engineering-ai-llms-metadata-driven-frameworks","alias":"\/data-ai-summit-2025\/session\/automating-engineering-ai-llms-metadata-driven-frameworks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533806+00:00"}
{"id":"D25B1436","title":"Bayada\u2019s Snowflake-to-Databricks Migration: Transforming Data for Speed & Efficiency","description":"<p>Bayada is transforming its data ecosystem by consolidating Matillion+Snowflake and SSIS+SQL Server into a unified Enterprise Data Platform powered by Databricks. Using Databricks' Medallion architecture, this platform enables seamless data integration, advanced analytics and machine learning across critical domains like general ledger, recruitment and activity-based costing.<\/p><p>\u00a0<\/p><p>Databricks was selected for its scalability, real-time analytics and ability to handle both structured and unstructured data, positioning Bayada for future growth. The migration aims to reduce data processing times by 35%, improve reporting accuracy and cut reconciliation efforts by 40%. Operational costs are projected to decrease by 20%, while real-time analytics is expected to boost efficiency by 15%.<\/p><p>\u00a0<\/p><p>Join this session to learn how Bayada is leveraging Databricks to build a high-performance data platform that accelerates insights, drives efficiency and fosters innovation organization-wide.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Health and Life Sciences"],"category":["DLT, Unity Catalog"],"areas_of_interest":["ETL, Migrations, SQL"],"delivery":["In Person"],"speakers":[{"name":"Venkatesh Guruprasad","company":"bayada.com","job_title":"Head of Data Architecture & Governance","bio":null,"image":{}},{"name":"Pradeep Jain","company":"Tredence Inc","job_title":"Sr. Director - HLS","bio":"Pradeep is a Senior Client Partner at Tredence, overseeing the end-to-end P&L for healthcare clients in the U.S. With expertise in enterprise AI, analytics, and cloud infrastructure, Pradeep drives digital transformation, operational excellence, and technological innovation within the healthcare sector. He manages CXO-level relationships, ensuring exceptional service delivery and aligning strategic objectives to create impactful outcomes for clients.<br \/>\n<br \/>\nPradeep has spearheaded initiatives including customized healthcare applications, Gen AI-driven personalized care, and enhanced digital patient experiences. His focus on streamlining workflows, improving patient outcomes, and empowering healthcare organizations positions him as a leader in healthcare innovation and transformation.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Pradeep%2520Jain_1737819048846001SRIi.jpg?h=a7e6d17b&itok=GT69g2_d","alt":"Pradeep Jain"}},{"name":"Elaine O'Neill","company":"BAYADA Home Health Care","job_title":"Chief Data & Analytics Officer","bio":"Elaine O'Neill is the Chief Data and Analytics Officer at BAYADA. Elaine's responsibilities entail strategic oversight and coordination of data-driven initiatives that enhance operational efficiency and patient outcomes contributing to BAYADA's mission of providing high-quality home healthcare services. She plays a pivotal role in integrating data analytics to drive decision-making, improve service delivery, and support BAYADA's support functions.  <br \/>\nHer 16 year growth journey with this mission driven company within departments like: Accounting, Finance and Operations, allow her to operate with an eye toward true value being obtained when solutions are simple, thoughtful, and deeply meaningful to the work BAYADA does day in, day out.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_1288_1745616419394001tZ6M.PNG?h=436b82d4&itok=4u3j1dTD","alt":"Elaine O'Neill"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/bayadas-snowflake-databricks-migration-transforming-data-speed","alias":"\/data-ai-summit-2025\/session\/bayadas-snowflake-databricks-migration-transforming-data-speed","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533816+00:00"}
{"id":"D25B1767","title":"Better Together: Change Data Feed in a Streaming Data Flow","description":"<p>Traditional streaming works great when your data source is append-only, but what if your data source includes updates and deletes? At 84.51 we used DLT and Delta Lake to build a streaming data flow that consumes inserts, updates and deletes while still taking advantage of streaming checkpoints. We combined this flow with a materialized view and Enzyme incremental refresh for a low-code, efficient and robust end-to-end data flow.<\/p><p>\u00a0<\/p><p>We process around 8 million sales transactions each day with 80 million items purchased. This flow not only handles new transactions but also handles updates to previous transactions.\u00a0Join us to learn how 84.51 combined change data feed, data streaming and materialized views to deliver a \u201cbetter together\u201d solution.<\/p><p>\u00a0<\/p><p>84.51 is a retail insights, media & marketing company. We use first-party retail data from 60 million households sourced through a loyalty card program to drive Kroger\u2019s customer-centric journey.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Apache Spark, Delta Lake, DLT"],"areas_of_interest":["Data Ingestion, ETL, Streaming pipelines, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Scott Gordon","company":"84.51\u02da","job_title":"Lead Data Engineer","bio":"Scott is a lead data engineer at 84.51\u02da. He has worked in Data Warehousing and Business Intelligence for several decades. Scott has achieved TDWI's Certified Business Intelligence Professional certification as well as certifications from Databricks, Microsoft and IBM.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Scott_Gordon_8451_1744131356626001Pfdt.png?h=55541bb6&itok=IqoDXKfY","alt":"Scott Gordon"}},{"name":"Mattias Moser","company":"84.51 LLC","job_title":"Data Engineer & Architect","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/better-together-change-data-feed-streaming-data-flow","alias":"\/data-ai-summit-2025\/session\/better-together-change-data-feed-streaming-data-flow","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533823+00:00"}
{"id":"D25B2956","title":"Breaking Barriers: Building Custom Spark 4.0 Data Connectors with Python","description":"<p>Building a custom Spark data source connector once required Java or Scala expertise, making it complex and limiting. This left many proprietary data sources without public SDKs disconnected from Spark. Additionally, data sources with Python SDKs couldn't harness Spark\u2019s distributed power.<\/p><p>\u00a0<\/p><p>Spark 4.0 changes this with a new Python API for data source connectors, allowing developers to build fully functional connectors without Java or Scala. This unlocks new possibilities, from integrating proprietary systems to leveraging untapped data sources. Supporting both batch and streaming, this API makes data ingestion more flexible than ever.<\/p><p>\u00a0<\/p><p>In this talk, we\u2019ll demonstrate how to build a Spark connector for Excel using Python, showcasing schema inference, data reads\/writes and streaming support. Whether you're a data engineer or Spark enthusiast, you\u2019ll gain the knowledge to integrate Spark with any data source \u2014 entirely in Python.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services, Financial Services"],"category":["Apache Spark, Unity Catalog"],"areas_of_interest":["ETL, Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Sourav Gulati","company":"Databricks","job_title":"Senior Resident Solutions Architect","bio":"Sourav is a Senior Resident Solutions Architect at Databricks with over 13 years of experience in data engineering. He has worked extensively with clients across various industries, delivering robust and scalable data solutions. He is passionate about helping organizations unlock the full potential of Databricks by leveraging data effectively to drive smarter decisions and meaningful outcomes.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Sourav_Gulati_1744887714213001raDY.png?h=41d68e6e&itok=m8PXQE3w","alt":"Sourav Gulati"}},{"name":"Ashish Saraswat","company":"Databricks","job_title":"Resident Solutions Architect","bio":"Ashish Saraswat is a Resident Solutions Architect at Databricks with over 11 years of experience in Software and Data Engineering. He is passionate about building scalable, reliable, and secure data architecture & platforms that drive business value and innovation.<br \/>\n<br \/>\nIn his role within Databricks' Professional Services team, he guides strategic customer engagements, helping clients architect, design, and implement robust Data Intelligence Platforms using Databricks.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/me_AshishSaraswat_databricks_1744730438921001qWbQ.jpg?h=3d58393a&itok=5xG2Neyw","alt":"Ashish Saraswat"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/breaking-barriers-building-custom-spark-40-data-connectors-python","alias":"\/data-ai-summit-2025\/session\/breaking-barriers-building-custom-spark-40-data-connectors-python","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533842+00:00"}
{"id":"D25B3254DBX","title":"Breaking With Spark Versions: A Better Way to Manage Workload Compatibility + Dependency Management","description":"<p>This session explains how we\u2019ve made Apache Spark\u2122 versionless for end users by introducing a stable client API, environment versioning and automatic remediation. These capabilities have enabled auto-upgrade of hundreds of millions of workloads with minimal disruption.<\/p><p>\u00a0<\/p><p>We\u2019ll also introduce a new approach to dependency management using environments. Admins will learn how to speed up package installation with Default Base Environments, and users will see how to manage custom environments for their own workloads.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Justin Breese","company":"Databricks","job_title":"Staff Product Manager - serverless jedi","bio":"Justin is a Staff Product Manager at Databricks. He primarily focuses on the runtime; Photon and Spark.<br \/>\n<br \/>\nWhen not thinking about execution engines, you can find him staying busy with soccer, hiking, brewing beer, roasting coffee, and older cars. <br \/>\n<br \/>\nJustin lives in Topanga, CA with his wife and son.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/breaking-spark-versions-better-way-manage-workload-compatibility","alias":"\/data-ai-summit-2025\/session\/breaking-spark-versions-better-way-manage-workload-compatibility","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533859+00:00"}
{"id":"D25L2095","title":"Building a Scalable, Real-Time Concurrency Prediction Service","description":"<p>Dream11's rapid growth has posed critical challenges in scaling infrastructure to handle millions of concurrent users during high-traffic events. Concurrency Prediction Service provides real-time forecasts of peak user activity in 30-minute intervals to optimize resource allocation by the Scaler Service.<\/p><p>\u00a0<\/p><p>This presentation covers the critical aspects of building and optimizing the Concurrency Prediction Service, including:<\/p><ul>\t<li>Real-time data ingestion and processing to handle spiky data patterns and high-variability traffic<\/li>\t<li>Anomaly detection mechanisms to identify and adjust for deviations caused by notifications or external events<\/li>\t<li>Modular and composable architecture for better scalability and maintainability<\/li>\t<li>Incremental processing with Spark Structured Streaming for real-time insights<\/li>\t<li>Granular resource tuning to optimize performance and control costs<\/li>\t<li>Leveraging Databricks for streamlined workflows, enhanced collaboration and efficient pipeline management<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["Apache Spark, Delta Lake, Databricks Workflows"],"areas_of_interest":["Data Applications, Data Science, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Hitesh Kapoor","company":"Dream 11","job_title":"AVP Data Science & Machine Learning","bio":"Hitesh Kapoor is a seasoned Machine Learning and Data Science professional currently serving as the Associate Vice President (AVP) of Machine Learning & Data Science at Dream11, world`s largest fantasy sports platform. With over a decade of experience in building AI systems at scale, Hitesh leads a team of more than 30 data scientists, developing cutting-edge systems that cater to Dream11's massive user base of over 200 million.<br \/>\nAn alumnus of the prestigious Indian Institute of Technology (IIT) Kharagpur, Hitesh has honed his expertise in Artificial Intelligence, Data Science, and Statistics throughout his career. His professional journey includes a notable stint at VMware's Office of the CTO, where he contributed to advanced technological initiatives.<br \/>\nHitesh stands out as a thought leader in the AI and Data Science community, combining academic excellence with practical industry experience to push the boundaries of what's possible in the realm of Machine Learning and AI systems.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/building-scalable-real-time-concurrency-prediction-service","alias":"\/data-ai-summit-2025\/session\/building-scalable-real-time-concurrency-prediction-service","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533887+00:00"}
{"id":"D25B2869","title":"Building a Self-Service Data Platform With a Small Data Team","description":"<p>Discover how Dodo Brands, a global pizza and coffee business with over 1,200 retail locations and 40k employees, revolutionized their analytics infrastructure by creating a self-service data platform. This session explores the approach to empowering analysts, data scientists and ML engineers to independently build analytical pipelines with minimal involvement from data engineers.<\/p><p>\u00a0<\/p><p>By leveraging Databricks as the backbone of their platform, the team developed automated tools like a \"job-generator\" that uses Jinja templates to streamline the creation of data jobs.\u00a0<\/p><p>\u00a0<\/p><p>This approach minimized manual coding and enabled non-data engineers to create over 1,420 data jobs \u2014 90% of which were auto-generated by user configurations.<\/p><p>\u00a0<\/p><p>Supporting thousands of weekly active users via tools like Apache Superset.<\/p><p>\u00a0<\/p><p>This session provides actionable insights for organizations seeking to scale their analytics capabilities efficiently without expanding their data engineering teams.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Delta Lake, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Applications"],"delivery":["In Person"],"speakers":[{"name":"Evgenii Dobrynin","company":"Dodo Brands","job_title":"Senior Data Engineer","bio":"Evgenii Dobrynin is a data engineer with extensive experience working with data across various business domains, including healthcare, social media, and food tech. His professional focus centers on building scalable self-service data platforms. He played a key role in developing an advanced data platform for Dodo Brands\u2014an international chain with over 1,300 Dodo Pizza restaurants and Drinkit coffee shops, operating in 24 countries and employing more than 40,000 people.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Evgenii_Dobrynin_1743965017567001clj4.png?h=3263d642&itok=AxyGpMG-","alt":"Evgenii Dobrynin"}},{"name":"Gleb Lesnikov","company":"Dodo Brands","job_title":"Head of Architecture","bio":"Gleb Lesnikov is Head of Architecture at Dodo Brands, where he leads technology strategy and architectural direction. With over a decade of experience, Gleb has been instrumental in scaling the world\u2019s fastest-growing digital pizza chain, driving initiatives across backend engineering, cloud infrastructure, and analytics.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Gleb_Lesnikov_1744656750241001iLex.png?h=40611f24&itok=4JNvg6vI","alt":"Gleb Lesnikov"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/building-self-service-data-platform-small-data-team","alias":"\/data-ai-summit-2025\/session\/building-self-service-data-platform-small-data-team","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533894+00:00"}
{"id":"D25B1954","title":"Building Real-Time Sport Model Insights with Spark Structured Streaming","description":"<p>In the dynamic world of sports betting, precision and adaptability are key.<\/p><p>\u00a0<\/p><p>Sports traders must navigate risk management, limitations of data feeds, and much more to prevent small model miscalculations from causing significant losses.<\/p><p>\u00a0<\/p><p>To ensure accurate real-time pricing of hundreds of interdependent markets, traders provide key inputs such as player skill-level adjustments, whilst maintaining precise correlations. Black-box models aren\u2019t enough\u2014 constant feedback loops drive informed, accurate decisions.<\/p><p>\u00a0<\/p><p>Join DraftKings as we showcase how we expose real-time metrics from our simulation engine, to empower traders with deeper insights into how their inputs shape the model.<\/p><p>\u00a0<\/p><p>Using Spark Structured Streaming, Kafka, and Databricks dashboards, we transform raw simulation outputs into actionable data. This transparency into our engines enables fine-grained control over pricing\u2015 leading to more accurate odds, a more efficient sportsbook, and an elevated customer experience.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Media and Entertainment"],"category":["Apache Spark, AI\/BI, DLT"],"areas_of_interest":["ETL, Streaming pipelines, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Aaron Hope","company":"Draftkings","job_title":"Lead Machine Learning Engineer","bio":"Aaron Hope is a Lead Machine Learning Engineer at DraftKings, working within the Quant team, focused on enabling sports data scientists to deliver industry-leading real-time pricing models. Playing key roles in migrating to Databricks as an ML platform, developing simulation engine price testing tools, and delivering a market leading Same Game Parlay offering.  Aaron focuses on building scalable, high-availability systems that meet the strict SLAs and accuracy demands of real-time sports betting.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_0291_1745163109539001ipgi.png?h=e0d2f341&itok=4nFibexL","alt":"Aaron Hope"}},{"name":"Ethan Summers","company":"Draftkings","job_title":"Lead Data Science Engineer","bio":"Ethan is a experienced Data Scientist with a demonstrated history of working in the sports analytics industry.<br \/>\nHe looks to solve complex problems through data, analytical reflection and creative solutions. Passionate about using mathematical and analytical skills to help to determine decision making processes in volatile, quick moving industries.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ethan-pp_1738587052245001SssZ.jpg?h=a7ffc51c&itok=uoee9j7W","alt":"Ethan Summers"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/building-real-time-sport-model-insights-spark-structured-streaming","alias":"\/data-ai-summit-2025\/session\/building-real-time-sport-model-insights-spark-structured-streaming","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533916+00:00"}
{"id":"D25B2943","title":"Building Real-Time Trading Dashboards With DLT and Databricks Apps","description":"<p>Barclays Post Trade real-time trade monitoring platform was historically built on a complex set of legacy technologies including Java, Solace, and custom micro-services.This ssession will demonstrate how the power of DLT new real-time mode, in conjunction with the foreach_batch_sink, can enable simple, cost-effective streaming pipelines that can load high volumes of data into our OLTP database with very low latency. Once in our OLTP database, this can be used to update real-time trading dashboards, securely hosted in Databricks Apps, with the latest stock trades - enabling better, more responsive decision-making and alerting.The session will walk-through the architecture, and demonstrate how simple it is to create and manage the pipelines and apps within the Databricks environment.\u00a0<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Apache Spark, DLT, Databricks Apps"],"areas_of_interest":["Data Applications, Data Ingestion, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Matt Slack","company":"Databricks","job_title":"Senior Specialist Solution Architect","bio":"Matt Slack is a Senior Specialist Solutions Architect at Databricks, specialising in helping customers solve their data engineering and geospatial challenges, and building out their modern data platforms. He has a strong background in low latency streaming pipelines at scale, creating innovative solutions with Databricks and other technologies.<br \/>\n<br \/>\nPreviously he has led on the build and architecture of multiple cloud data platforms across a range of industries, including Finance, Transportation and E-commerce. Matt has a Bachelor's in Computer Science from the University of York.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Headshot%2520Shirt%2520Grey_1744709479044001ma5B.jpg?h=b044a8f9&itok=DYK5Y6oq","alt":"Matt Slack"}},{"name":"Matthew Moorcroft","company":"Databricks","job_title":"Specialist Solutions Architect","bio":"Matthew Moorcroft is a Specialist Solutions Architect at Databricks, based in the London office. In his role, Matthew is responsible for architecting customer solutions and accelerating pipeline deployment to production. Matthew holds a Bachelor\u2019s degree in Telematics Engineering, which has provided him with a strong engineering background and an introduction to the world of big data. Matthew is passionate about data processing at scale and helping customers become more data-centric.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Matthew%2520Moorcroft%2520Jan%25202023%2520%25282%2520of%25206%2529_1744278483396001rrWo.jpg?h=814a835d&itok=XY7uRCM7","alt":"Matthew Moorcroft"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/building-real-time-trading-dashboards-dlt-and-databricks-apps","alias":"\/data-ai-summit-2025\/session\/building-real-time-trading-dashboards-dlt-and-databricks-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533919+00:00"}
{"id":"D25B2884","title":"CI\/CD for Databricks: Advanced Asset Bundles and GitHub Actions","description":"<p>Databricks Asset Bundles (DABs) provide a way to use the command line to deploy and run a set of Databricks assets \u2014 like notebooks, Python code, DLT pipelines and workflows. To automate deployments, you create a deployment pipeline that uses the power of DABs along with other validation steps to ensure high quality deployments.<\/p><p>\u00a0<\/p><p>In this session you will learn how to automate CI\/CD processes for Databricks while following best practices to keep deployments easy to scale and maintain. After a brief explanation of why Databricks Asset Bundles are a good option for CI\/CD, we will walk through a working project including advanced variables, target-specific overrides, linting, integration testing and automatic deployment upon code review approval. You will leave the session clear on how to build your first GitHub Action using DABs.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["Apache Spark, Databricks Workflows, DLT"],"areas_of_interest":["Developer Experience"],"delivery":["In Person"],"speakers":[{"name":"Dustin Vannoy","company":"Databricks","job_title":"Sr. Specialist Solutions Architect","bio":"Dustin Vannoy is a data engineer and solutions architect experienced in solving business problems with analytics and big data solutions. He is passionate about all aspects of data engineering, especially building data platforms and streaming data pipelines. He is experienced in using cloud technologies to transition legacy ETL jobs into a modern lakehouse architecture. He currently focuses on guiding others to build successful data platforms and pipelines with Databricks, Apache Spark, Azure, Apache Kafka, Python, and Scala.<br \/>\nHe is co-founder of the Data Engineering San Diego meetup and encourages others to grow their data skills by making mentoring others, speaking at events and creating tutorials at YouTube\/DustinVannoy.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Dustin.Vannoy_1744175146585001Ej7v.PNG?h=cd2a7045&itok=6uGszvDr","alt":"Dustin Vannoy"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/cicd-databricks-advanced-asset-bundles-and-github-actions","alias":"\/data-ai-summit-2025\/session\/cicd-databricks-advanced-asset-bundles-and-github-actions","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533957+00:00"}
{"id":"D25L2481","title":"Creating a Custom PySpark Stream Reader with PySpark 4.0","description":"<p>PySpark supports many data sources out of the box, such as Apache Kafka, JDBC, ODBC, Delta Lake, etc. However, some older systems, such as systems that use JMS protocol, are not supported by default and require considerable extra work for developers to read from them. One such example is ActiveMQ for streaming. Traditionally, users of ActiveMQ have to use a middle-man in order to read the stream with Spark (such as writing to a MySQL DB using Java code and reading that table with Spark JDBC). With PySpark 4.0\u2019s custom data sources (supported in DBR 15.3+) we are able to cut out the middle-man processing using batch or Spark Streaming and consume the queues directly from PySpark, saving developers considerable time and complexity in getting source data into your Delta Lake and governed by Unity Catalog and orchestrated with Databricks Workflows.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Lightning Talk","industry":["Enterprise Technology, Retail and CPG - Food, Travel and Hospitality"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["Data Ingestion, ETL, Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Skyler Myers","company":"Entrada","job_title":"Head of Data Engineering","bio":"I am currently the head of data engineering at Entrada, a boutique consulting firm offering Databricks implementation services. Prior to that, I was a Sr. Solutions Consultant at Databricks for 2 years, helping to create tailored solutions for some of their biggest clients.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/creating-custom-pyspark-stream-reader-pyspark-40","alias":"\/data-ai-summit-2025\/session\/creating-custom-pyspark-stream-reader-pyspark-40","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534000+00:00"}
{"id":"D25B1737","title":"Crypto at Scale: Building a Cost-Efficient, High-Performance Platform for Real-Time Blockchain Data","description":"<p>In today\u2019s fast-evolving crypto landscape, organizations require fast, reliable intelligence to manage risk, investigate financial crime, and stay ahead of evolving threats. In this session, discover how Elliptic built a scalable, high-performance data intelligence platform that delivers real-time, actionable blockchain insights\u2014empowering businesses to future-proof their crypto risk strategies and law enforcement to streamline investigations.<\/p><p>\u00a0<\/p><p>We\u2019ll walk you through how we've built a high-performance data platform, leveraging key components of the Databricks ecosystem such as Structured Streaming and Delta Lake. This transformation has fundamentally changed the way we deliver user-facing analytics\u2014improving not only speed and scalability, but also enabling analytics to directly enhance the accuracy and intelligence of our operational systems. Along the way, we\u2019ll share how we overcame critical challenges such as building efficient incremental pipelines, designing robust partitioning strategies for large-scale crypto datasets, and enabling seamless data sharing with our customers.Whether you\u2019re looking to enhance your streaming capabilities, expand your knowledge of how crypto analytics works or simply discover novel approaches to data processing at scale, this session will provide concrete strategies and valuable lessons learned from the front lines of crypto attribution<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Public Sector, Financial Services"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["ETL, Streaming pipelines, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Matthew Moorcroft","company":"Databricks","job_title":"Specialist Solutions Architect","bio":"Matthew Moorcroft is a Specialist Solutions Architect at Databricks, based in the London office. In his role, Matthew is responsible for architecting customer solutions and accelerating pipeline deployment to production. Matthew holds a Bachelor\u2019s degree in Telematics Engineering, which has provided him with a strong engineering background and an introduction to the world of big data. Matthew is passionate about data processing at scale and helping customers become more data-centric.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Matthew%2520Moorcroft%2520Jan%25202023%2520%25282%2520of%25206%2529_1744278483396001rrWo.jpg?h=814a835d&itok=XY7uRCM7","alt":"Matthew Moorcroft"}},{"name":"Ferran Cabezas Castellvi","company":"Elliptic","job_title":"Lead Data Engineer","bio":"Ferran Cabezas Castellvi is a Lead Data Engineer at Elliptic, based in London. At Elliptic, he is helping to build the world\u2019s most comprehensive blockchain data and intelligence platform\u2014empowering organizations with accurate, actionable insights. Elliptic delivers industry-leading blockchain intelligence that enables businesses and governments to detect and prevent financial crime, fostering trust and transparency in digital finance. He holds a postgraduate degree in Big Data and Data Science from the Universitat Polit\u00e8cnica de Catalunya. Ferran is passionate about large-scale data processing and designing robust, scalable data architectures.<br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Ferran_CabezasCastellvi_1744662506318001hsp5.jpg?h=76a80d6c&itok=WpDM6qnW","alt":"Ferran Cabezas Castellvi"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/crypto-scale-building-cost-efficient-high-performance-platform-real","alias":"\/data-ai-summit-2025\/session\/crypto-scale-building-cost-efficient-high-performance-platform-real","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534007+00:00"}
{"id":"D25B3268DBX","title":"Data Triggers and Advanced Control Flow With Lakeflow Jobs","description":"<p>Lakeflow Jobs is the production-ready fully managed orchestrator for the entire Lakehouse with 99.95% uptime. Join us for a dive into how you can orchestrate your enterprise data operations, from triggering your jobs only when your data is ready to advanced control flow with conditionals, looping and job modularity \u2014 with demos!<\/p><p>\u00a0<\/p><p>Attendees will gain practical insights into optimizing their data operations by orchestrating with Lakeflow Jobs:<\/p><ul>\t<li>New task types: Publish AI\/BI Dashboards, push to Power BI or ingest with Lakeflow Connect<\/li>\t<li>Advanced execution control: Reference SQL Task outputs, run partial DAGs and perform targeted backfills<\/li>\t<li>Repair runs: Re-run failed pipelines with surgical precision using task-level repair<\/li>\t<li>Control flow upgrades: Native for-each loops and conditional logic make DAGs more dynamic + expressive<\/li>\t<li>Smarter triggers: Kick off jobs based on file arrival or Delta table changes, enabling responsive workflows<\/li>\t<li>Code-first approach to pipeline orchestration<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Roland F\u00e4ustlin","company":"Databricks","job_title":"Product Manager","bio":"Roland is a Product Manager at Databricks, focusing on the orchestration of data and AI workloads with Databricks Workflows and fully managed serverless compute since April 2021.<br \/>\n<br \/>\nHe has over 13 years of experience in technology, including roles at Google and as a team leader at McKinsey\u2019s Business Technology Office in Europe and Latin America.<br \/>\n<br \/>\nHe holds a PhD in plasma physics, a master\u2019s degree in laboratory astrophysics from The University of Texas at Austin, and a minor in computer science.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/data-triggers-and-advanced-control-flow-lakeflow-jobs","alias":"\/data-ai-summit-2025\/session\/data-triggers-and-advanced-control-flow-lakeflow-jobs","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534058+00:00"}
{"id":"D25B1638","title":"Databricks as the Backbone of MLOps: From Orchestration to Inference","description":"<p>As machine learning (ML) models scale in complexity and impact, organizations must establish a robust MLOps foundation to ensure seamless model deployment, monitoring and retraining. In this session, we\u2019ll share how we leverage Databricks as the backbone of our MLOps ecosystem \u2014 handling everything from workflow orchestration to large-scale inference.<\/p><p>\u00a0<\/p><p>We\u2019ll walk through our journey of transitioning from fragmented workflows to an integrated, scalable system powered by Databricks Workflows. You\u2019ll learn how we built an automated pipeline that streamlines model development, inference and monitoring while ensuring reliability in production. We\u2019ll also discuss key challenges we faced, lessons learned and best practices for organizations looking to operationalize ML with Databricks.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Iceberg, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Data Science, Machine Learning, ML\/LLMOps, Telecommunications"],"delivery":["In Person"],"speakers":[{"name":"Reinier Veral","company":"Globe Telecoms","job_title":"Asst. Director MLOps","bio":"Reinier Veral is an MLOps Head with over 25 years of experience in software engineering, big data, and machine learning. He currently leads the deployment and monitoring of ML models using Databricks, enabling scalable, production-grade AI solutions. At Nokia, Reinier spearheaded DevOps and IoT innovations, achieving also consecutive victories with Team SymbIoT at AWS Hackdays in 2019 and 2020. He also managed software teams at Quasset. Beginning his career as an Assistant Professor at Map\u00faa University, Reinier taught courses including Software Engineering, Microprocessors, Compiler Theory, Operating Systems and C  and Assembly Language. His technical proficiencies encompass Python, SQL, and ML operationalization in enterprise settings.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/image_2025-04-09_100530516_1744164984565001jEUZ.png?h=15d72140&itok=F_pNY-Qw","alt":"Reinier Veral"}},{"name":"Cyd Kristoff Redelosa","company":"Globe Telecoms","job_title":"Senior Expert - MLOps","bio":"Cyd is an MLOps Team Lead at Globe Telecom, driving the seamless operationalization of ML models at scale. He ensures efficient model deployment, automation, and monitoring, prioritizing Model Developers and Insighters by streamlining workflows with Databricks, Snowflake, and Sagemaker.<br \/>\n<br \/>\nA strong advocate for mentorship and ownership, Cyd fosters a culture of learning and collaboration, guiding his team through Databricks migration, anomaly detection improvements, and model operationalization. He has spearheaded initiatives to optimize MLOps workflows and enhance productivity.<br \/>\n<br \/>\nIn this DBX AI Summit, Cyd will share insights on scaling enterprise MLOps, optimizing model deployment, and ensuring operational excellence in AI-driven workflows.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_1465%2520%25281%2529_1744265382693001OOsR.png?h=18271ffb&itok=AyuZaeLg","alt":"Cyd Kristoff Redelosa"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/databricks-backbone-mlops-orchestration-inference","alias":"\/data-ai-summit-2025\/session\/databricks-backbone-mlops-orchestration-inference","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534074+00:00"}
{"id":"D25B3188DBX","title":"Databricks Lakeflow: the Foundation of Data + AI Innovation for Your Industry","description":"<p>Every analytics, BI and AI project relies on high-quality data. This is why data engineering, the practice of building reliable data pipelines that ingest and transform data, is consequential to the success of these projects.\u00a0\u00a0In this session, we'll show how you can use Lakeflow to accelerate innovation in multiple parts of the organization. We'll review real-world examples of Databricks customers using Lakeflow in different industries such as automotive, healthcare and retail. We'll touch on how the foundational data engineering capabilities Lakeflow provides help power initiatives that improve customer experiences, make real-time decisions and drive business results.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Health and Life Sciences, Manufacturing, Retail and CPG - Food"],"category":["Databricks Workflows, DLT, LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX), ETL"],"delivery":["In Person"],"speakers":[{"name":"Ori Zohar","company":null,"job_title":"Sr. Manager Product Marketing","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/databricks-lakeflow-foundation-data-ai-innovation-your-industry","alias":"\/data-ai-summit-2025\/session\/databricks-lakeflow-foundation-data-ai-innovation-your-industry","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534087+00:00"}
{"id":"D25B3279DBX","title":"Declarative Pipelines: Simplifying Data Engineering Workloads","description":"<p>DLT has made it dramatically easier to build production-grade pipelines, using a declarative framework that abstracts away orchestration and complexity. It\u2019s become a go-to solution for teams who want reliable, maintainable pipelines without reinventing the wheel.<\/p><p>\u00a0<\/p><p>But we\u2019re just getting started. In this session, we\u2019ll take a step back and share how DLT fits into a broader vision for the future of data engineering pipelines \u2014 one that opens the door to a new level of openness, standardization and community momentum.<\/p><p>\u00a0<\/p><p>We\u2019ll cover the core concepts behind declarative pipelines, where the architecture is headed, and what this shift means for data engineers building procedural code. Don\u2019t miss this session \u2014 we\u2019ll be sharing something new that sets the direction for what comes next.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, DLT"],"areas_of_interest":["Databricks Experience (DBX), ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Michael Armbrust","company":"Databricks","job_title":null,"bio":null,"image":{}},{"name":"Sandy Ryza","company":null,"job_title":"Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/declarative-pipelines-simplifying-data-engineering-workloads","alias":"\/data-ai-summit-2025\/session\/declarative-pipelines-simplifying-data-engineering-workloads","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534119+00:00"}
{"id":"D25B3191DBX","title":"Delivering Sub-Second Latency for Operational Workloads on Databricks","description":"<p>As enterprise streaming adoption accelerates, more teams are turning to real-time processing to support operational workloads that require sub-second response times. To address this need, Databricks introduced Project Lightspeed in 2022, which recently delivered Real-Time Mode in Apache Spark\u2122 Structured Streaming.\u00a0This new mode achieves consistent p99 latencies under 300ms for a wide range of stateless and stateful streaming queries. In this session, we\u2019ll define what constitutes an operational use case, outline typical latency requirements and walk through how to meet those SLAs using Real-Time Mode in Structured Streaming.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Media and Entertainment, Financial Services"],"category":["Apache Spark, DLT"],"areas_of_interest":["Databricks Experience (DBX), Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Jerry Peng","company":"Databricks","job_title":"Staff Software Engineer","bio":null,"image":{}},{"name":"Karthikeyan Ramasamy","company":"Databricks","job_title":"Head of Streaming","bio":"Karthik Ramasamy is the Head of Streaming at Databricks. Before Databricks, he was a Senior Director of Engineering, managing the Pulsar team at Splunk. He was the CEO of Streamlio focused on building Apache Pulsar and led the acquisition of Streamlio by Splunk. Before Streamlio, he was the technical lead for real-time infrastructure at Twitter where he created Twitter Heron. He worked with companies such as Greenplum and Juniper in building parallel databases, big data infrastructure and networking. He founded Locomatix that specialized in streaming processing on Hadoop and Cassandra, which was acquired by Twitter. Karthik has a Ph.D. in computer science from the University of Wisconsin, Madison, with a focus on big data and databases. <br \/>\n","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/delivering-sub-second-latency-operational-workloads-databricks","alias":"\/data-ai-summit-2025\/session\/delivering-sub-second-latency-operational-workloads-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534127+00:00"}
{"id":"D25L3377","title":"Delta-rs Turning Five: Growing Pains and Life Lessons","description":"<p>Five years ago, the delta-rs project embarked on a journey to bring Delta Lake's robust capabilities to the Rust & Python ecosystem. In this talk, we'll delve into the triumphs, tribulations and lessons learned along the way. We'll explore how delta-rs has matured alongside the thriving Rust data ecosystem, adapting to its evolving landscape and overcoming the challenges of maintaining a complex data project. Join us as we share insights into the project's evolution, the symbiotic relationship between delta-rs and the Rust community, and the current hurdles and future directions that lie ahead.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology, Professional Services"],"category":["Delta Lake, Apache Iceberg"],"areas_of_interest":["Open Source"],"delivery":["In Person"],"speakers":[{"name":"Robert Pack","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Robert has extensive experience in designing and implementing Data & AI platforms within large multinational organizations. Through this work he has been an avid contributor to the open lakehouse ecosystem - specifically Delta Lake. Now at Databricks, his focus is entirely facilitating and contributing to the open source ecosystem for building lakehouse architectures.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/robert_pack_1736342796943001ZVBj.png?h=7b4ffee1&itok=kMDovM_k","alt":"Robert Pack"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/delta-rs-turning-five-growing-pains-and-life-lessons","alias":"\/data-ai-summit-2025\/session\/delta-rs-turning-five-growing-pains-and-life-lessons","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534154+00:00"}
{"id":"D25B3221DBX","title":"Deploying Databricks Asset Bundles (DABs) at Scale","description":"<p>Managing data and AI workloads in Databricks can be complex. Databricks Asset Bundles (DABs) simplify this by enabling declarative, Git-driven deployment workflows for notebooks, jobs, DLT pipelines, dashboards, ML models and more.<\/p><p>\u00a0<\/p><p>Join the DABs Team for a Deep Dive and learn about:<\/p><ul>\t<li>The Basics: Understanding Databricks asset bundles<\/li>\t<li>Declare, define and deploy assets, follow best practices, use templates and manage dependencies<\/li>\t<li>CI\/CD & Governance: Automate deployments with GitHub Actions\/Azure DevOps, manage Dev vs. Prod differences, and ensure reproducibility<\/li>\t<li>What\u2019s new and what's coming up! AI\/BI Dashboard support, Databricks Apps support, a Pythonic interface and workspace-based deployment<\/li><\/ul><p>\u00a0<\/p><p>If you're a data engineer, ML practitioner or platform architect, this talk will provide practical insights to improve reliability, efficiency and compliance in your Databricks workflows.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), Developer Experience"],"delivery":["In Person"],"speakers":[{"name":"Pieter Noordhuis","company":"Databricks","job_title":"Sr. Staff Software Engineer","bio":"Databricks Engineering","image":{}},{"name":"Saad Ansari","company":"Databricks","job_title":"Product Management","bio":"Saad is a product manager at Databricks working on Data Orchestration (Workflows) and Developer tooling teams. Prior to this he was CTO and co-founder at Connecterra using Artificial Intelligence to improve sustainability, productivity and animal welfare in the Dairy Industry (think Fitbit for cows!). Saad has also worked as a PM and Developer at Microsoft. He holds a masters in computer science from Stanford.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/deploying-databricks-asset-bundles-dabs-scale","alias":"\/data-ai-summit-2025\/session\/deploying-databricks-asset-bundles-dabs-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534170+00:00"}
{"id":"D25B3264DBX","title":"DLT Integrations and Interoperability: Get Data From \u2014 and to \u2014 Anywhere","description":"<p>In this session, you will learn how to integrate DLT with external systems in order to ingest and send data virtually anywhere. DLT is most often used in ingestion and ETL into the Lakehouse. New DLT capabilities like the DLT Sinks API and added support for Python Data Source and ForEachBatch have opened up DLT to support almost any integration. This includes popular Apache Spark\u2122 integrations like JDBC, Kafka, External and managed Delta tables, Azure CosmosDB, MongoDB and more.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities, Enterprise Technology, Manufacturing"],"category":["Apache Spark, DLT"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX), ETL"],"delivery":["In Person"],"speakers":[{"name":"Ryan Nienhuis","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":"Ryan Nienhuis is the PM lead for Databricks streaming engine' and also works on OSS Structured Streaming and Delta Live Tables. Prior to Databricks, Ryan spent ten years at AWS leading the Amazon Managed Service for Apache Flink team and as a product manager for Amazon Kinesis and Amazon Firehose. Ryan has spoken at industry events such as AWS re:Invent and Strata + Hadoop World and has authored several publications, including 'Running Apache Flink on AWS'.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/dlt-integrations-and-interoperability-get-data-and-anywhere","alias":"\/data-ai-summit-2025\/session\/dlt-integrations-and-interoperability-get-data-and-anywhere","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534187+00:00"}
{"id":"D25B2849","title":"Federated Data Pipelines","description":"<p>Are you struggling to keep up with rapid business changes that demand constant updates to your data pipelines? Is your data engineering team growing rapidly just to manage this complexity? Databricks was not immune to this challenge either. Managing our BI with contributions from hundreds of Product Engineering Teams across the company while maintaining central oversight and quality posed significant hurdles. Join us to learn how we developed a config-driven data pipeline framework using Metric Store and UC Metrics that helped us reduce engineering effort \u2014 achieving the work of 100 classical data engineers with just two platform engineers.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Apache Spark, Databricks SQL"],"areas_of_interest":["Collaboration, ETL, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Neo Ni","company":"Databricks","job_title":"Senior Software Engineer","bio":"Neo Ni is a Software Engineer at Databricks, focused on building scalable data infrastructure and developer experience tooling that powers critical internal analytics and business workflows. Neo holds a Master\u2019s degree in Computer Science from Johns Hopkins University, and is passionate about making data engineering more efficient and developer-friendly across the enterprise.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/35a87fef7cdf1f493949136f98bd27f1_1745598319821001mRLX.jpg?h=e3c77aae&itok=KE8r3lRj","alt":"Neo Ni"}},{"name":"Rohit Mathews","company":"Databricks","job_title":"Senior Software Engineer","bio":"Rohit Mathews is a Senior Software Engineer at Databricks with over a decade of deep expertise in the Big Data space and more than 16 years of experience in the software industry. At Databricks, Rohit architects and builds scalable data platforms that power data-driven decision-making across enterprise environments. Rohit has also held key engineering roles at Yieldmo, VMware, and McAfee, contributing across domains including ad tech, security, and analytics.<br \/>\n<br \/>\nHe has led the re-architecture and migration of large-scale data platforms\u2014dramatically improving performance, and cost efficiency for batch\/streaming systems with a strong focus on enabling self-service analytics and is passionate about mentorship and knowledge sharing along the way","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Rohit_Mathews_1745609291335001dUqb.jpg?h=a7e6d17b&itok=Ic4ERO8d","alt":"Rohit Mathews"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/federated-data-pipelines","alias":"\/data-ai-summit-2025\/session\/federated-data-pipelines","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534289+00:00"}
{"id":"D25B1331","title":"From 10 Hours to 10 Minutes: Unleashing the Power of DLT","description":"<p>How do you transform a data pipeline from sluggish 10-hour batch processing into a real-time powerhouse that delivers insights in just 10 minutes? This was the challenge we tackled at one of France's largest manufacturing companies, where data integration and analytics were mission-critical for supply chain optimization.<\/p><p>\u00a0<\/p><p>Power BI dashboards needed to refresh every 15 minutes. Our team struggled with legacy Azure Data Factory batch pipelines. These outdated processes couldn\u2019t keep up, delaying insights and generating up to three daily incident tickets.<\/p><p>\u00a0<\/p><p>We identified DLTs and Databricks SQL as the game-changing solution to modernize our workflow, implement quality checks, and reduce processing times.In this session, we\u2019ll dive into the key factors behind our success:<\/p><ul>\t<li>Pipeline modernization with DLTs: improving scalability<\/li>\t<li>Data quality enforcement: clean, reliable datasets<\/li>\t<li>Seamless BI integration: Using Databricks SQL to power fast, efficient queries in Power BI<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Manufacturing"],"category":["Delta Lake, Databricks SQL, DLT"],"areas_of_interest":["ETL, Orchestration, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Yash Joshi","company":"Accenture","job_title":"Senior Data Engineer","bio":null,"image":{}},{"name":"Fatima CHIKH","company":"Accenture","job_title":"Data Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/10-hours-10-minutes-unleashing-power-dlt","alias":"\/data-ai-summit-2025\/session\/10-hours-10-minutes-unleashing-power-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534306+00:00"}
{"id":"D25B3270DBX","title":"From Apache Airflow to Lakeflow Jobs: A Guide for Workflow Modernization","description":"<p>This is an overview of migrating from Apache Airflow to Lakeflow Jobs for modern data orchestration. It covers key differences, best practices and practical examples of transitioning from traditional Airflow DAGs orchestrating legacy systems to declarative, incremental ETL pipelines with Lakeflow. Attendees will gain actionable tips on how to improve efficiency, scalability and maintainability in their workflows.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Tahir Fayyaz","company":"Databricks","job_title":"Product Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/apache-airflow-lakeflow-jobs-guide-workflow-modernization","alias":"\/data-ai-summit-2025\/session\/apache-airflow-lakeflow-jobs-guide-workflow-modernization","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534310+00:00"}
{"id":"D25L1557","title":"From Code to Insights: Leveraging Advanced Infrastructure and AI Capabilities.","description":"<p>In this talk, we will explore how AI and advanced infrastructure are transforming Insulet's development and operations.<\/p><p>\u00a0<\/p><p>We'll highlight how our innovations have reduced scrap part costs through manufacturing analytics, showcasing efficiency and cost savings.<\/p><p>\u00a0<\/p><p>On leveraging Databricks AI solutions and productivity, it not only identifies errors but also fixes code and assists in writing complex queries. This goes beyond suggestions, providing actual solutions.<\/p><p>\u00a0<\/p><p>On the infrastructure side, integrating Spark with Databricks simplifies setup and reduces costs. Additionally Databricks Lakeflow Connect enables real-time updates and simplification without much coding as we integrate with Salesforce.\u00a0<\/p><p>\u00a0<\/p><p>We'll also discuss real-time processing of patient data, demonstrating how Databricks drives efficiency and productivity.<\/p><p>\u00a0<\/p><p>Join us to learn how these innovations enhance efficiency, cost savings and performance.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Health and Life Sciences"],"category":["Delta Lake, LakeFlow, Unity Catalog"],"areas_of_interest":["Data Applications, ETL, SQL"],"delivery":["In Person"],"speakers":[{"name":"Shweta Shetty","company":"Insulet","job_title":"Director","bio":"Shweta Shetty is a seasoned Cloud and Data engineering leader with over two decades of experience in data analytics platforms, cloud architecture, API development, and mobile applications. Throughout her career, she has worn many hats, showcasing her versatility and expertise. As the Director of Data Engineering at Insulet Corporation, she drives innovative healthcare technology solutions. Previously, as a Technical Architect at Teradata, Shweta played a pivotal role in architecting next-generation developer and data analytics platforms, championing cloud-first initiatives and API security models. A passionate advocate for simplicity, security, and scalability, she has shared her expertise at DataConLA and Women of Silicon Valley.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202023-11-07%2520at%252010.01.55%2520PM_1744925728359001w2Vb.png?h=8bf96674&itok=VxzVKvpz","alt":"Shweta Shetty"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/code-insights-leveraging-advanced-infrastructure-and-ai-capabilities","alias":"\/data-ai-summit-2025\/session\/code-insights-leveraging-advanced-infrastructure-and-ai-capabilities","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534316+00:00"}
{"id":"D25B2780","title":"From Days to Seconds \u2014 Reducing Query Times on Large Geospatial Datasets by 99%","description":"<p>The Global Water Security Center translates environmental science into actionable insights for the U.S. Department of Defense. Prior to incorporating Databricks, responding to these requests required querying approximately five hundred thousand raster files representing over five hundred billion points. By leveraging lakehouse architecture, Databricks Auto Loader, Spark Streaming, Databricks Spatial SQL, H3 geospatial indexing and Databricks Liquid Clustering, we were able to drastically reduce our \u201ctime to analysis\u201d from multiple business days to a matter of seconds. Now, our data scientists execute queries on pre-computed tables in Databricks, resulting in a \u201ctime to analysis\u201d that is 99% faster, giving our teams more time for deeper analysis of the data. Additionally, we\u2019ve incorporated Databricks Workflows, Databricks Asset Bundles, Git and Git Actions to support CI\/CD across workspaces. We completed this work in close partnership with Databricks.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities, Public Sector, Financial Services"],"category":["Apache Spark, Delta Lake, Databricks Workflows"],"areas_of_interest":["Data Ingestion, Orchestration, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Chris Crawford","company":"Databricks","job_title":"Sr. Solutions Archtect","bio":"<br \/>\nChris Crawford is a Senior Solutions Architect at Databricks. Chris has worked in the Big Data space for nearly a decade where he designed on-prem, Hadoop-based architectures architectures and now cloud-based solutions. He has consulted on projects in multiple verticals to include Accenture, Hewlett Packard Enterprise, Wells Fargo, Hartsfield-Jackson Airport in Atlanta, and US Intelligence Agencies. Mr. Crawford is a graduate of Auburn University where he holds an MBA and Bachelor\u2019s in Molecular Biology. Mr. Crawford holds multiple certifications from AWS, The Cloud Native Computing Foundation (CNCF), and Databricks. He also was awarded a patent in 2021 for his design of a SIEM on Kubernetes.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_usethisone_1744220392352001qhFk.png?h=e9b722f3&itok=V7unM5ow","alt":"Chris Crawford"}},{"name":"Hobson Bryan","company":"Global Water Security Center","job_title":"Associate Director of Technology","bio":"Hobson is a senior technology leader with over fifteen years of experience in the agile design, development, and delivery of tech-enabled products and platforms. His experience includes advancing through roles of increasingly higher levels of responsibility focused on statistical analysis, economic\/financial modeling, product management, and systems engineering. Hobson helps organizations explore and unlock the value of their data to provide enhanced engagement, personalization, and analytical insights.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Headshot%25202%2520PNG_1746021854297001Nu8M.png?h=89dec4d3&itok=7ZPsM3AV","alt":"Hobson Bryan"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/days-seconds-reducing-query-times-large-geospatial-datasets-99","alias":"\/data-ai-summit-2025\/session\/days-seconds-reducing-query-times-large-geospatial-datasets-99","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534320+00:00"}
{"id":"D25B2182","title":"From Imperative to Declarative Paradigm: Rebuilding a CI\/CD Infrastructure Using Hatch and DABs","description":"<p>Building and deploying Pyspark pipelines to Databricks should be effortless.<\/p><p>\u00a0<\/p><p>However, our team at FreeWheel has, for the longest time, struggled with a convoluted and hard-to-maintain CI\/CD infrastructure. It followed an imperative paradigm, demanding that every project implement custom scripts to build artifacts and deploy resources, and resulting in redundant boilerplate code and awkward interactions with the Databricks REST API.<\/p><p>\u00a0<\/p><p>We set our mind on rebuilding it from scratch, following a declarative paradigm instead. We will share how we were able to eliminate thousands of lines of code from our repository, create a fully configuration-driven infrastructure where projects can be easily onboarded, and improve the quality of our codebase using Hatch and Databricks Asset Bundles as our tools of choice. In particular, DAB has made deploying across our 3 environments a breeze, and has allowed us to quickly adopt new features as soon as they are released by Databricks.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment"],"category":["Apache Spark, Databricks Workflows"],"areas_of_interest":["Developer Experience, ETL, Orchestration, Telecommunications"],"delivery":["In Person"],"speakers":[{"name":"Luigi Di Tacchio","company":"FreeWheel, a Comcast Company","job_title":"Sr. Software Engineer","bio":"I am a Sr. Software Engineer at FreeWheel, a Comcast Company, where I build scalable data pipelines and products using Spark and Databricks to leverage Comcast's viewership data.<br \/>\nI began my career as a researcher at the University at Buffalo (SUNY) before moving into industry. For over five years at FreeWheel, I have focused on data infrastructure within Comcast advertising.<br \/>\nAt the Data + AI Summit, I will share how my team rebuilt our CI\/CD infrastructure from an imperative to a declarative paradigm using Hatch and Databricks Asset Bundles.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Professional%2520picture_1745950218065001PisZ.jpg?h=08f66884&itok=PxG2mWqc","alt":"Luigi Di Tacchio"}},{"name":"Saswati Bhoi","company":"FreeWheel, a Comcast Company","job_title":"Sr. Site Reliability Engineer","bio":"Saswati is a Senior Site Reliability Engineer with over a decade of experience in designing, securing, and optimizing complex infrastructures. At FreeWheel, her work focuses on streamlining deployments, strengthening security, and optimizing infrastructure costs to drive efficiency and scalability.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/summit_pic_1745964864706001rrJb.jpg?h=abb8db73&itok=lrGEyWHe","alt":"Saswati Bhoi"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/imperative-declarative-paradigm-rebuilding-cicd-infrastructure-using","alias":"\/data-ai-summit-2025\/session\/imperative-declarative-paradigm-rebuilding-cicd-infrastructure-using","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534323+00:00"}
{"id":"D25L1546","title":"From Spaghetti Bowl Pipeline to DLT Efficiency","description":"<p>In today's data-driven world, the ability to efficiently manage and transform data is crucial for any organization. This presentation will explore the process of converting a complex and messy workflow into a clean and simple DLT pipeline at a large integrated health system, Intermountain Health.<\/p><p>\u00a0<\/p><p>Alteryx is a powerful tool for data preparation and blending, but as workflows grow in complexity, they can become difficult to manage and maintain. DLT, on the other hand, offers a more democratized, streamlined and scalable approach to data engineering, leveraging the power of Apache Spark and Delta Lake.<\/p><p>\u00a0<\/p><p>We will begin by examining a typical legacy workflow, identifying common pain points such as tangled logic, performance bottlenecks and maintenance challenges. Next, we will demonstrate how to translate this workflow into a DLT pipeline, highlighting key steps such as data transformation, validation and delivery.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Health and Life Sciences"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["ETL, Orchestration, SQL"],"delivery":["In Person"],"speakers":[{"name":"Peter Jones","company":"Intermountain Healthcare","job_title":"Analytics Engineer","bio":"Peter works as an Analytics Engineer at Intermountain Health and serves as the Databricks account administrator.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-04-14%2520at%252016.23.05_1744669416994001fNvc.png?h=241de609&itok=7txdAbDn","alt":"Peter Jones"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/spaghetti-bowl-pipeline-dlt-efficiency","alias":"\/data-ai-summit-2025\/session\/spaghetti-bowl-pipeline-dlt-efficiency","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534344+00:00"}
{"id":"D25B2007","title":"GenAI Observability in Customer Care","description":"<p>Customer support is going through the GenAI revolution, but how can we use AI to foster deeper empathy with our end users?<\/p><p>\u00a0<\/p><p>To enable this, Earnin has built its GenAI observability platform on Databricks, leveraging DLTs, Kafka and Databricks AI\/BI.<\/p><p>\u00a0<\/p><p>This session covers how we use DLT to monitor our customer care chatbot in near real-time and how we leverage Databricks to better anticipate our customers' needs.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["AI\/BI, Databricks SQL, DLT"],"areas_of_interest":["AI Agents, Generative AI (LLMs), Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Willem Dhaeseleer","company":"Earnin","job_title":"Senior Staff Software Engineer","bio":"Willem has over a decade of experience building products with a DevOps-first mentality.<br \/>\n<br \/>\nHis track record spans developing web UIs, developer tools, and profiling production systems to drive business value.<br \/>\n<br \/>\nAt EarnIn, he focuses on building data products that accelerate GenAI innovation and platformization.  <br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/profile_1744938420898001M4GI.jpg?h=a7e6d17b&itok=qlpTkxsi","alt":"Willem Dhaeseleer"}},{"name":"Matteo Ciccozzi","company":"EarnIn","job_title":"Senior Machine Learning Engineer","bio":"Matteo has experience building platforms to automate data and machine-learning workflows as well as agentic workflows and applications that leverage LLMs. He has experience working with data processing and streaming technologies like Apache Spark, Apache Kafka, and Debezium; generative AI application development frameworks like LangGraph; and machine learning frameworks such as scikit-learn, PyTorch, and MLFlow.<br \/>\nHis interests include learning about distributed systems, software architecture, and computer science theory. Outside of the technical domain, Matteo is an avid surfer.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/dbk_headshot_1745540375418001KFA0.jpg?h=1461d265&itok=HhFFx0Lk","alt":"Matteo Ciccozzi"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/genai-observability-customer-care","alias":"\/data-ai-summit-2025\/session\/genai-observability-customer-care","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534377+00:00"}
{"id":"D25B2619","title":"Genie for Engineering: Optimizing HVAC Design and Operational Insights With Data and AI","description":"<p>In this session, we will explore how Genie, an AI-driven platform transformed HVAC operational insights by leveraging Databricks offerings like Apache Spark, Delta Lake and the Databricks Data Intelligence Platform.\u00a0<\/p><p>\u00a0<\/p><p>Key contributions:<\/p><ul>\t<li>Real-time data processing: DLT and Apache Spark\u2122 for efficient data ingestion and real-time analysis.<\/li>\t<li>Workflow orchestration: Databricks Data Intelligence Platform to orchestrate complex workflows and integrate various data sources and analytical tools.<\/li>\t<li>Field Data Integration: Incorporating real-time field data into design and algorithm development, enabling engineers to make informed adjustments and optimize performance.<\/li><\/ul><p>\u00a0<\/p><p>By analyzing real-time data from HVAC installations, Genie identified discrepancies between design specs and field performance, allowing engineers to optimize algorithms, reduce inefficiencies and improve customer satisfaction. Discover how Genie revolutionized HVAC management and apply to your projects.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Manufacturing"],"category":["Apache Spark, Delta Lake, DLT"],"areas_of_interest":["AI Agents, Data Applications, Data Intelligence"],"delivery":["In Person"],"speakers":[{"name":"Mohamed Hanif Ansari","company":"Lennox","job_title":"Manager, Data Science & AI","bio":null,"image":{}},{"name":"Sridhar Venkatesh","company":"Lennox International","job_title":"Principal AI Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/genie-engineering-optimizing-hvac-design-and-operational-insights-data","alias":"\/data-ai-summit-2025\/session\/genie-engineering-optimizing-hvac-design-and-operational-insights-data","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534395+00:00"}
{"id":"D25B3255DBX","title":"Getting Started With Lakeflow Connect","description":"<p>Hundreds of customers are already ingesting data with Lakeflow Connect from SQL Server, Salesforce, ServiceNow, Google Analytics, SharePoint, PostgreSQL and more to unlock the full power of their data. Lakeflow Connect introduces built-in, no-code ingestion connectors from SaaS applications, databases and file sources to help unlock data intelligence.<\/p><p>\u00a0<\/p><p>In this demo-packed session, you\u2019ll learn how to ingest ready-to-use data for analytics and AI with a few clicks in the UI or a few lines of code. We\u2019ll also demonstrate how Lakeflow Connect is fully integrated with the Databricks Data Intelligence Platform for built-in governance, observability, CI\/CD, automated pipeline maintenance and more. Finally, we\u2019ll explain how to use Lakeflow Connect in combination with downstream analytics and AI tools to tackle common business challenges and drive business impact.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Giselle Goicochea","company":"Databricks","job_title":"Sr. Product Marketing Manager","bio":"Giselle leads product marketing for Lakeflow Connect.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Giselle_Goicochea-600x600_1745450476923001CPmX.png?h=04d92ac6&itok=H1LTGpY5","alt":"Giselle Goicochea"}},{"name":"Peter Pogorski","company":"Databricks","job_title":"Senior Product Manager","bio":"Peter is a Senior Product Manager at Databricks, focused on data ingestion. Previously, he worked on serverless computing at Microsoft. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/getting-started-lakeflow-connect","alias":"\/data-ai-summit-2025\/session\/getting-started-lakeflow-connect","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534409+00:00"}
{"id":"D25D3260DBX","title":"Getting the Most Out of DLT: A Deep Dive on What\u2019s New and Best Practices","description":"<p>This deep dive covers advanced usage patterns, tips and best practices for maximizing the potential of DLT. Attendees will explore new features, enhanced workflows and cost-optimization strategies through a demo-heavy presentation. The session will also address complex use cases, showcasing how DLT simplifies the management of robust data pipelines while maintaining scalability and efficiency across diverse data engineering challenges.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Deep Dive","industry":["Enterprise Technology"],"category":["DLT"],"areas_of_interest":["Databricks Experience (DBX), ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Michael Armbrust","company":"Databricks","job_title":null,"bio":null,"image":{}}],"day":"Wednesday","room":"South, Level 3, Room 305","starts":"2025-06-11T18:30:00","ends":"2025-06-11T20:00:00","starts_pst":"2025-06-11T11:30:00","ends_pst":"2025-06-11T13:00:00","start_time":"6:30 pm","end_time":"8:00 pm","pst_start_time":"11:30 am","pst_end_time":"1:00 pm","duration":"90","path":"\/session\/getting-most-out-dlt-deep-dive-whats-new-and-best-practices","alias":"\/data-ai-summit-2025\/session\/getting-most-out-dlt-deep-dive-whats-new-and-best-practices","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534412+00:00"}
{"id":"D25B1818","title":"GPU Accelerated Spark Connect","description":"<p>Spark Connect, first included for SQL\/DataFrame API in Apache Spark 3.4 and recently extended to MLlib in 4.0, introduced a new way to run Spark applications over a gRPC protocol. This has many benefits, including easier adoption for non-JVM clients, version independence from applications and increased stability and security of the associated Spark clusters.<\/p><p>\u00a0<\/p><p>The recent Spark Connect extension for ML also included a plugin interface to configure enhanced server-side implementations of the MLlib algorithms when launching the server.\u00a0<\/p><p>\u00a0<\/p><p>In this talk, we shall demonstrate how this new interface, together with Spark SQL\u2019s existing plugin interface, can be used with NVIDIA GPU-accelerated plugins for ML and SQL to enable no-code change, end-to-end GPU acceleration of Spark ETL and ML applications over Spark Connect, with optimal performance up to 9x at 80% cost reduction compared to CPU baselines.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Apache Spark"],"areas_of_interest":["ETL, Machine Learning, Open Source, SQL"],"delivery":["In Person"],"speakers":[{"name":"Gera Shegalov","company":"NVIDIA","job_title":"Principal Distributed Systems Engineer","bio":"German \"Gera\" Shegalov is a Principal Systems Engineer on the RAPIDS Acc Team. He received a master's and PhD in Computer Science from Saarland University. Prior to NVIDIA, Gera\u2019s career includes scaling Hadoop clusters to 100s of petabytes and supporting 1000s of A\/B tests daily at Twitter as well as architecting ML apps on Einstein Platform at Salesforce. He has contributed to multiple OSS projects such as Hadoop, Scalding, Spark, and TransmogrifAI.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/gera-portrait_1743734340949001tOW7.jpg?h=cf70f274&itok=7lLBo1G5","alt":"Gera Shegalov"}},{"name":"Erik Ordentlich","company":"NVIDIA","job_title":"Sr. Manager","bio":"Erik Ordentlich is a senior manager at NVIDIA working on GPU accelerated Spark.   He has a PhD in Electrical Engineering from Stanford University and has previously worked at Yahoo and HP Laboratories.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/head-shot-2_1745952000129001mnlU.jpg?h=c5ae2658&itok=S8rF0YyT","alt":"Erik Ordentlich"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/gpu-accelerated-spark-connect","alias":"\/data-ai-summit-2025\/session\/gpu-accelerated-spark-connect","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534420+00:00"}
{"id":"D25B1550","title":"Harnessing Databricks Asset Bundles: Transforming Pipeline Management at Scale at Stack Overflow","description":"<p>Discover how Stack Overflow optimized its data engineering workflows using Databricks Asset Bundles (DABs) for scalable and efficient pipeline deployments. This session explores the structured pipeline architecture, emphasizing code reusability, modular design and bundle variables to ensure clarity and data isolation across projects. Learn how the data team leverages enterprise infrastructure to streamline deployment across multiple environments. Key topics include DRY-principled modular design, essential DAB features for automation and data security strategies using Unity Catalog. Designed for data engineers and teams managing multi-project workflows, this talk offers actionable insights on optimizing pipelines with Databricks evolving toolset.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Education, Enterprise Technology"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Ingestion, Getting started with Databricks"],"delivery":["In Person"],"speakers":[{"name":"Chelsea Zhang","company":"Stack Overflow","job_title":"Staff Data Engineer","bio":"Chelsea Zhang, Staff Data Engineer at Stack Overflow, has 10 years of experience in data engineering, focusing on data platform architecture, pipeline development, and cloud solutions with Databricks and Azure. She built a scalable data pipeline platform, speeding up deployment and meeting business needs. As an early adopter of Databricks Asset Bundles, her input shaped the product pre-release, and she led Delta Live Tables adoption for efficient event processing. Passionate about mentoring, she promotes innovation in data infrastructure. In her free time, she tends to 300 orchid species, enjoys Lego, gardening, and cares for her huskies (ISO, SQL, HASH) and turtle (Shell).","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/1693425840723_1745242211494001e9Le.jpg?h=fbf7a813&itok=x0SqlEKc","alt":"Chelsea Zhang"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/harnessing-databricks-asset-bundles-transforming-pipeline-management","alias":"\/data-ai-summit-2025\/session\/harnessing-databricks-asset-bundles-transforming-pipeline-management","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534426+00:00"}
{"id":"D25B1596","title":"Harnessing Real-Time Data and AI for Retail Innovation","description":"<p>This talk explores using advanced data processing and generative AI techniques to revolutionize the retail industry. Using Databricks, we will discuss how cutting-edge technologies enable real-time data analysis and machine learning applications, creating a powerful ecosystem for large-scale, data-driven retail solutions.<\/p><p>\u00a0<\/p><p>Attendees will gain insights into architecting scalable data pipelines for retail operations and implementing advanced analytics on streaming customer data. Discover how these integrated technologies drive innovation in retail, enhancing customer experiences, streamlining operations and enabling data-driven decision-making.<\/p><p>\u00a0<\/p><p>Learn how retailers can leverage these tools to gain a competitive edge in the rapidly evolving digital marketplace, ultimately driving growth and adaptability in the face of changing consumer behaviors and market dynamics.\u00a0<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Delta Lake, Databricks Workflows, Mosaic AI"],"areas_of_interest":["Data Applications, Generative AI (LLMs), Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Tristen Wentling","company":"Databricks","job_title":"Lead Solutions Architect","bio":"Tristen Wentling is a Lead Solutions Architect at Databricks, where he works with retail and consumer packaged goods companies. Formerly a data scientist and a co-author on Delta Lake: The Definitive Guide, he has also authored several blog posts covering topics like best practices for production stream applications and building generative AI applications for e-commerce and marketing analytics. Tristen holds an M.S. in Mathematics and B.S. in Applied Mathematics. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Tristen_Wentling_1744054635951001e7wz.png?h=07be7ff7&itok=6ynvXNb1","alt":"Tristen Wentling"}},{"name":"Lorenz Verzosa","company":"Databricks","job_title":"Senior Specialist Solutions Architect","bio":"Lorenz Verzosa is a Specialist Solutions Architect at Databricks. He specializes in Data Warehousing and Data Engineering, but supports customers in all topic areas including MLOps and GenAI. He has been an Apache Spark and Databricks user since 2015.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Lorenz_Verzosa_1744047969065001pFJD.JPG?h=93fa4828&itok=7v-wF_4T","alt":"Lorenz Verzosa"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/harnessing-real-time-data-and-ai-retail-innovation","alias":"\/data-ai-summit-2025\/session\/harnessing-real-time-data-and-ai-retail-innovation","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534430+00:00"}
{"id":"D25B1600","title":"Health Data, Delivered: How DLT Powers the HealthVerity Marketplace","description":"<p>Building scalable, reliable ETL pipelines is a challenge for organizations managing large, diverse data sources. Theseus, our custom ETL framework, streamlines data ingestion and transformation by fully leveraging Databricks-native capabilities, including DLT, auto loader and event-driven orchestration. By decoupling supplier logic and implementing structured bronze, silver, and gold layers, Theseus ensures high-performance, fault-tolerant data processing with minimal operational overhead. The result? Faster time-to-value, simplified governance and improved data quality \u2014 all within a declarative framework that reduces engineering effort. In this session, we\u2019ll explore how Theseus automates complex data workflows, optimizes cost efficiency and enhances scalability, showcasing how Databricks-native tools drive real business outcomes.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences"],"category":["DLT, Unity Catalog"],"areas_of_interest":["ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Ron DeFreitas","company":"HealthVerity","job_title":"Principal Data Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/health-data-delivered-how-dlt-powers-healthverity-marketplace","alias":"\/data-ai-summit-2025\/session\/health-data-delivered-how-dlt-powers-healthverity-marketplace","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534433+00:00"}
{"id":"D25B2800","title":"Healthcare Interoperability: End-to-End Streaming FHIR Pipelines With Databricks & Redox","description":"<p>Redox & Databricks direct integration can streamline your interoperability workflows from responding in record time to preauthorization requests to letting attending physicians know about a change in risk for sepsis and readmission in near real time from ADTs. Data engineers will learn how to create fully-streaming ETL pipelines for ingesting, parsing and acting on insights from Redox FHIR bundles delivered directly to Unity Catalog volumes. Once available in the Lakehouse, AI\/BI Dashboards and Agentic Frameworks help write FHIR messages back to Redox for direct push down to EMR systems. Parsing FHIR bundle resources has never been easier with SQL combined with the new VARIANT data type in Delta and streaming table creation against Serverless DBSQL Warehouses. We'll also use Databricks accelerators dbignite and redoxwrite for writing and posting FHIR bundles back to Redox integrated EMRs and we'll extend AI\/BI with Unity Catalog SQL UDFs and the Redox API for use in Genie.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences"],"category":["AI\/BI, Databricks Workflows, DLT"],"areas_of_interest":["Customer Data Platform, Data Intelligence, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Tim Kessler","company":"Redox, Inc.","job_title":"Field CTO","bio":"Tim Kessler is the Field CTO at Redox where he oversees strategy, industry evangelism, enterprise technical sales, Partnerships at Redox. Over the past five years Tim built Redox\u2019s technical partnerships with Google, Microsoft, Amazon, Databricks, and Snowflake.  In almost 9 years at Redox, Tim has held roles in Sales, Implementation, Product, Partnerships, and Field Engineering. Prior to Redox he spent time at Epic helping large health system customers implement Epic\u2019s Access and Revenue Cycle applications.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Tim%2520Kessler%2520Headshot%2520%2528Redox%2529%2520%25281%2529_1744049867245001uD6E.png?h=62938371&itok=XOqBSFXM","alt":"Tim Kessler"}},{"name":"Matthew Giglia","company":"Databricks","job_title":"Solutions Architect","bio":"Matthew Giglia has over 16 years of experience in Analytics & Data Healthcare use cases including nearly 15 years with a BlueCross BlueShield Health Plan where he was Principal Data Scientist and Principal Analytics & Data Architect.  At Databricks, Matt helps our customers design and implement data and AI solutions for complex healthcare problems and serves as an SME on FHIR and Healthcare Interoperability.   Matt lives in Rochester, NY with his brilliant beautiful wife and three small children.  ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/matt-beach-2023%2520head%2520shot_1744735423495001XiAf.png?h=c928a9b7&itok=Hzpqtuq9","alt":"Matthew Giglia"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/healthcare-interoperability-end-end-streaming-fhir-pipelines-databricks","alias":"\/data-ai-summit-2025\/session\/healthcare-interoperability-end-end-streaming-fhir-pipelines-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534437+00:00"}
{"id":"D25B2012","title":"Highways and Hexagons: Processing Large Geospatial Datasets With H3","description":"<p>The problem of matching GPS locations to roads and local government areas (LGAs) involves handling large datasets and a number of geospatial operations. In this deep dive, we will outline the challenges of developing scalable solutions for these tasks.<\/p><p>\u00a0<\/p><p>We will discuss our multi-step approach, first focusing on the use of H3 indexing to isolate matches with single candidates, then explaining use of different geospatial computational techniques to accurately match points with multiple candidates.<\/p><p>\u00a0<\/p><p>From technical perspective, the talk will showcase the use of broadcasting and partitioning techniques, their effect on autoscaling, memory usage and effective data parallelization.<\/p><p>\u00a0<\/p><p>This session is for anyone interested in geospatial data, spark performance optimization and the real-world challenges of large-scale data engineering.<\/p><p>\u00a0<\/p><p>This session will be co-presented by Prad Dias (Austroads Senior Implementation Manager) and Petr Andreev (Mantel Group Senior Data Engineer)<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Public Sector"],"category":["Apache Spark, Databricks SQL, Databricks Workflows"],"areas_of_interest":["Data Applications, Data Science, SQL"],"delivery":["In Person"],"speakers":[{"name":"Petr Andreev","company":"Mantel Group","job_title":"Senior Data Engineer","bio":"Petr is a data engineer specializing in Databricks Azure platform design and development. With a strong focus on optimizing distributed systems, Petr brings extensive experience in creating efficient, scalable solutions that drive performance and innovation in data engineering.\"","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Petr_Andreev_1744331706451001Mli9.png?h=c5ae2658&itok=axkli1oK","alt":"Petr Andreev"}},{"name":"Ahangama (Prad) Dias","company":"Austroads Ltd","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/highways-and-hexagons-processing-large-geospatial-datasets-h3","alias":"\/data-ai-summit-2025\/session\/highways-and-hexagons-processing-large-geospatial-datasets-h3","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534443+00:00"}
{"id":"D25B2520","title":"How Databricks Powers Real-Time Threat Detection at Barracuda XDR","description":"<p>As cybersecurity threats grow in volume and complexity, organizations must efficiently process security telemetry for best-in-class detection and mitigation. Barracuda\u2019s XDR platform is redefining security operations by layering advanced detection methodologies over a broad range of supported technologies. Our vision is to deliver unparalleled protection through automation, machine learning and scalable detection frameworks, ensuring threats are identified and mitigated quickly.<\/p><p>\u00a0<\/p><p>To achieve this, we have adopted Databricks as the foundation of our security analytics platform, providing greater control and flexibility while decoupling from traditional SIEM tools. By leveraging DLTs, Spark Structured Streaming and detection-as-code CI\/CD pipelines, we have built a real-time detection engine that enhances scalability, accuracy and cost efficiency.<\/p><p>\u00a0<\/p><p>This session explores how Databricks is shaping the future of XDR through real-time analytics and cloud-native security.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["MLFlow, Databricks SQL, DLT"],"areas_of_interest":["Cybersecurity, Machine Learning, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Merium Khalid","company":"Barracuda Networks","job_title":"Director, SOC Offensive Security","bio":null,"image":{}},{"name":"Alex Dangel","company":"Barracuda Networks","job_title":"Manager of Detection Engineering","bio":"Based in Atlanta, Georgia, Alex Dangel is the Manager of Cybersecurity Engineering at Barracuda Networks, where he leads the XDR Threat Detection and Platform Engineering team. With a decade of experience in IT security, he has developed expertise across disciplines including data engineering, threat detection, and incident response. Alex is currently pursuing his master\u2019s degree at Georgia Tech and holds technical certifications from ISC(2) and CompTIA, in addition to a bachelor\u2019s in Computer Information Systems and Applied Mathematics. Outside of work, he enjoys playing pickleball, gardening, and strength training.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot24_1744399778791001YOKa.png?h=d1e7c0a3&itok=Oz6pvryO","alt":"Alex Dangel"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/how-databricks-powers-real-time-threat-detection-barracuda-xdr","alias":"\/data-ai-summit-2025\/session\/how-databricks-powers-real-time-threat-detection-barracuda-xdr","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534470+00:00"}
{"id":"D25B2257","title":"Innovating Retail Data: Unilever\u2019s Transformation with Databricks DLT","description":"<p>Retail data is expanding at an unprecedented rate, demanding a scalable, cost-efficient, and near real-time architecture. At Unilever, we transformed our data management approach by leveraging Databricks DLT, achieving approximately $500K in cost savings while accelerating computation speeds by 200\u2013500%.<\/p><p>\u00a0<\/p><p>By adopting a streaming-driven architecture, we built a system where data flows continuously across processing layers, enabling real-time updates with minimal latency.<\/p><p>\u00a0<\/p><p>DLT\u2019s serverless simplicity replaced complex-dependency management, reducing maintenance overhead, and improving pipeline reliability. DLT Direct Publishing further enhanced data segmentation, concurrency, and governance, ensuring efficient and scalable data operations while simplifying workflows.<\/p><p>\u00a0<\/p><p>This transformation empowers Unilever to manage data with greater efficiency, scalability, and reduced costs, creating a future-ready infrastructure that evolves with the needs of our retail partners and customers.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["Data Ingestion, ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Evan Cherney","company":"Unilever","job_title":"Senior Data Science Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/innovating-retail-data-unilevers-transformation-databricks-dlt","alias":"\/data-ai-summit-2025\/session\/innovating-retail-data-unilevers-transformation-databricks-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534572+00:00"}
{"id":"D25B3273DBX","title":"Inside Spark 4.1: A Preview of the Latest Innovations","description":"<p>Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance.<\/p><p>\u00a0<\/p><p>In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility \u2014 including the simplified Python Data Source API \u2014 and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Databricks Experience (DBX), ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Xiao Li","company":"Databricks","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/inside-spark-41-preview-latest-innovations","alias":"\/data-ai-summit-2025\/session\/inside-spark-41-preview-latest-innovations","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534576+00:00"}
{"id":"D25B3277DBX","title":"Introducing Lakeflow: The Future of Data Engineering on Databricks","description":"<p>Join us to explore Lakeflow, Databricks' end-to-end solution for simplifying and unifying the most complex data engineering workflows. This session builds on keynote announcements, offering an accessible introduction for newcomers while emphasizing the transformative value Lakeflow delivers.<\/p><p>\u00a0<\/p><p>We\u2019ll cover:<\/p><ul>\t<li>What is Lakeflow?: A cohesive overview of its components: Lakeflow Connect, Lakeflow DLT and Lakeflow Jobs<\/li>\t<li>Core capabilities in action: Live demos showcasing no-code data ingestion, code-optional declarative pipelines and unified, end-to-end orchestration<\/li>\t<li>Vision for the future: Unveil the roadmap, including details of two major keynote announcements<\/li><\/ul><p>\u00a0<\/p><p>Discover how Lakeflow equips data teams with a seamless experience for ingestion, transformation and orchestration, reducing complexity and driving productivity. By unifying these capabilities, Lakeflow lays the groundwork for scalable, reliable and efficient data pipelines in a governed and high-performing environment.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, DLT, LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX), ETL"],"delivery":["In Person"],"speakers":[{"name":"Michael Armbrust","company":"Databricks","job_title":null,"bio":null,"image":{}},{"name":"Bilal Aslam","company":"Databricks","job_title":null,"bio":"Bilal Aslam is a Senior Director of Product Management at Databricks, focused on data engineering. Prior to joining Databricks, Bilal founded a startup that was acquired by DocuSign. He was a product manager on Microsoft Azure before that. Bilal Aslam lives in Amsterdam with his family and a Siberian cat","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/introducing-lakeflow-future-data-engineering-databricks","alias":"\/data-ai-summit-2025\/session\/introducing-lakeflow-future-data-engineering-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534596+00:00"}
{"id":"D25L2161","title":"Introducing Simplified State Tracking in Apache Spark\u2122 Structured Streaming","description":"<p>This presentation will review the new change feed and snapshot capabilities in Apache Spark\u2122 Structured Streaming\u2019s State Reader API. The State Reader API enables users to access and analyze Structured Streaming's internal state data. Readers will learn how to leverage the new features to debug, troubleshoot and analyze state changes efficiently, making streaming workloads easier to manage at scale.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Lightning Talk","industry":["Manufacturing, Media and Entertainment, Financial Services"],"category":["Apache Spark"],"areas_of_interest":["Developer Experience, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Craig Lukasik","company":"Databricks","job_title":"Sr. SSA","bio":"Craig is a Data Engineering specialist in the Communications, Media, and Entertainment vertical. He has over 25 years of experience building and helping builders.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_1745345894045001YZmw.jpg?h=71ccc96d&itok=x7DUm1KN","alt":"Craig Lukasik"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/introducing-simplified-state-tracking-apache-sparktm-structured","alias":"\/data-ai-summit-2025\/session\/introducing-simplified-state-tracking-apache-sparktm-structured","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534600+00:00"}
{"id":"D25B3257DBX","title":"Lakeflow Connect: Easy, Efficient Ingestion From Databases","description":"<p>Lakeflow Connect streamlines the ingestion of incremental data from popular databases like SQL Server and PostgreSQL.<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll review best practices for networking, security, minimizing database load, monitoring and more \u2014 tailored to common industry scenarios.<\/p><p>\u00a0<\/p><p>Join us to gain practical insights into Lakeflow Connect's functionality so that you\u2019re ready to build your own pipelines. Whether you're looking to optimize data ingestion or enhance your database integrations, this session will provide you with a deep understanding of how Lakeflow Connect works with databases.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Sushil Dhaundiyal","company":"Databricks","job_title":"Software Engineer","bio":null,"image":{}},{"name":"Peter Pogorski","company":"Databricks","job_title":"Senior Product Manager","bio":"Peter is a Senior Product Manager at Databricks, focused on data ingestion. Previously, he worked on serverless computing at Microsoft. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-connect-easy-efficient-ingestion-databases","alias":"\/data-ai-summit-2025\/session\/lakeflow-connect-easy-efficient-ingestion-databases","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534638+00:00"}
{"id":"D25B3256DBX","title":"Lakeflow Connect: Seamless Data Ingestion From Enterprise Apps","description":"<p>Lakeflow Connect enables you to easily and efficiently ingest data from enterprise applications like Salesforce, ServiceNow, Google Analytics, SharePoint, NetSuite, Dynamics 365 and more.<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll dive deep on using connectors for the most popular SaaS applications, reviewing common use cases such as analyzing consumer behavior, predicting churn and centralizing HR analytics. You'll also hear from an early customer about how Lakeflow Connect helped unify their customer data to drive an improved automotive experience. We\u2019ll wrap up with a Q&A so you have the opportunity to learn from our experts.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Manufacturing, Travel and Hospitality"],"category":["LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Manish Dalwadi","company":"Databricks","job_title":"Director, Product Management","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-connect-seamless-data-ingestion-enterprise-apps","alias":"\/data-ai-summit-2025\/session\/lakeflow-connect-seamless-data-ingestion-enterprise-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534643+00:00"}
{"id":"D25B3278DBX","title":"Lakeflow Connect: Simplify Ingestion for IoT, Clickstreams and Telemetry","description":"<p>In this session, we\u2019ll explore new patterns that simplify real-time data ingestion into your lakehouse \u2014 especially for sources like IoT, clickstreams, telemetry and more.<\/p><p>\u00a0<\/p><p>We\u2019ll begin with a quick look at the evolution of the ingestion landscape and the growing need to embed data flows closer to the source. Then, we\u2019ll introduce a modern approach that helps you \u201cshift left\u201d on ingestion \u2014 making it easier to integrate analytics and AI directly into operational systems, rather than treating them as afterthoughts.<\/p><p>\u00a0<\/p><p>This shift can lead to simpler architectures that reduce unnecessary hops and scale with your operations.<\/p><p>\u00a0<\/p><p>To close, we\u2019ll share a practical decision framework to help you determine which ingestion options best fit your use case, followed by a live Q&A to help you get started quickly.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Victoria Bukta","company":"Databricks","job_title":"Member of Technical Staff","bio":"Victoria is a product manager at Databricks on the LakeFlow team. She previously was a PM at Tabular, and an engineering manager at Shopify where she led teams working on large-scale, low-latency ingestion and pioneered lakehouse adoption. She is based out of Toronto and sails in her spare time.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG-20191228-WA0000_1745242341538001urve.jpg?h=d70dd8e3&itok=Ura_z8IB","alt":"Victoria Bukta"}},{"name":"Nikola Obradovic","company":"Databricks","job_title":"Sr. Staff Software Engineer","bio":"Software engineer with extensive experience working on Microsoft SQL products. I was the owner of a major SQL component\u2014SQL Frontend (Programmability)\u2014and contributed to many features shipped in SQL Server 2016\u20132022, including cloud offerings such as SQL Managed Instance and Azure SQL Database. I have also contributed to other SQL components, including High Availability and Query Store, and was part of the teams that delivered the initial Managed Instance offering, Managed Instance Link, and Automatic Query Plan Regression Correction. After Microsoft, I worked at Tenstorrent on an ML compiler for AI-oriented hardware. I am currently member of the Databricks data ingestion team.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Nikola_Obradovic_1746009887410001dMeh.png?h=7ab77a67&itok=Atvzs60C","alt":"Nikola Obradovic"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-connect-simplify-ingestion-iot-clickstreams-and-telemetry","alias":"\/data-ai-summit-2025\/session\/lakeflow-connect-simplify-ingestion-iot-clickstreams-and-telemetry","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534646+00:00"}
{"id":"D25D3258DBX","title":"Lakeflow Connect: Smarter, Simpler File Ingestion With the Next Generation of Auto Loader","description":"<p>Auto Loader is the definitive tool for ingesting data from cloud storage into your lakehouse.<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll unveil new features and best practices that simplify every aspect of cloud storage ingestion. We\u2019ll demo out-of-the-box observability for pipeline health and data quality, walk through improvements for schema management, introduce a series of new data formats and unveil recent strides in Auto Loader performance. Along the way, we\u2019ll provide examples and best practices for optimizing cost and performance.<\/p><p>\u00a0<\/p><p>Finally, we\u2019ll introduce a preview of what\u2019s coming next \u2014 including a REST API for pushing files directly to Delta, a UI for creating cloud storage pipelines and more.<\/p><p>\u00a0<\/p><p>Join us to help shape the future of file ingestion on Databricks.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Deep Dive","industry":["Enterprise Technology"],"category":["DLT, LakeFlow"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Sandip Agarwala","company":"Databricks","job_title":"Staff Software Engineer","bio":null,"image":{}},{"name":"Chavdar Botev","company":"Databricks","job_title":"Sr Staff Software Engineer","bio":null,"image":{}}],"day":"Tuesday","room":"South, Level 3, Room 303","starts":"2025-06-10T23:10:00","ends":"2025-06-11T00:40:00","starts_pst":"2025-06-10T16:10:00","ends_pst":"2025-06-10T17:40:00","start_time":"11:10 pm","end_time":"12:40 am","pst_start_time":"4:10 pm","pst_end_time":"5:40 pm","duration":"90","path":"\/session\/lakeflow-connect-smarter-simpler-file-ingestion-next-generation-auto","alias":"\/data-ai-summit-2025\/session\/lakeflow-connect-smarter-simpler-file-ingestion-next-generation-auto","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534650+00:00"}
{"id":"D25B1944","title":"Lakeflow Connect: The Game-Changer for Complex Event-Driven Architectures","description":"<p>In 2020, Delaware implemented a state-of-the-art, event-driven architecture for EFSA, enabling a highly decoupled system landscape, presented at the Data&AI Summit 2021. By centrally brokering events in near real-time, consumer applications react instantly to events from producer applications as they occur. Event producers are decoupled from consumers via a publisher\/subscriber mechanism. Over the past years, we noticed some drawbacks. The processing of these custom events, primarily aimed for process integration weren\u2019t covering all edge cases, the data quality was not always optimal due to missing events and we needed to create a complex logic for SCD2 tables. Lakeflow Connect allows us to extract the data directly from the source without the complex architecture in between, avoiding data loss and thus, data quality issues, and with some simple adjustments, an SCD2 table is created automatically. Lakeflow Connect allows us to create more efficient and intelligent data provisioning.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Professional Services, Public Sector"],"category":["DLT, LakeFlow, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Ingestion, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Tim Bal","company":"delaware","job_title":"Senior Consultant","bio":"Tim has been a Senior Consultant in delaware's MS Data & AI Team since February 2020. He specializes in Data Engineering, Solutions Architecture, and Data Science, with a focus on building and optimizing efficient data solutions using Databricks. His work ranges from developing greenfield architectures to modernizing legacy systems for enhanced performance and scalability.<br \/>\nTim holds a master\u2019s degree in Theoretical and Experimental Psychology, as well as a second master\u2019s degree in Statistical Data Analysis, and is affiliated with the Royal Belgian Statistical Society. In recognition of his expertise, Tim was awarded Databricks Solutions Architect Champion in April 2023\u2014the highest level of certification in the Databricks Partner Program.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Tim_Bal_1745423483890001JP3w.png?h=7d66a0c3&itok=pY_zKfmf","alt":"Tim Bal"}},{"name":"Jeroen De Clercq","company":"delaware","job_title":"Business Analyst","bio":"Since September 2024, Jeroen has been a Business Analyst in the MS Data & AI team at delaware, with a strong focus on data engineering and data science. He holds a bachelor's degree in Accounting & Tax, a master's degree in Business Administration (IT Management), and a postgraduate degree in Applied Artificial Intelligence. At Ghent University, he graduated summa cum laude and was recognized as the top master's student in IT Management for his year.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Jeroen_DeClercq_1745829090213001qlnh.png?h=fbf7a813&itok=ztBT-91o","alt":"Jeroen De Clercq"}},{"name":"Giancarlo Costa","company":"European Food Safety Authority","job_title":"Cloud Solutions Architect","bio":"Giancarlo joined EFSA in 2006 where he is part of the Architecture & Solutions team. As Cloud Solutions Architect, he oversees the architectural evolution across multiple key projects. He recently had a leading role in the democratization of data and analytics at EFSA, contributing to the development of the EFSA Data Mesh Architecture and Decentralized Cloud Governance. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Giancarlo_Costa_1745514186442001HW48.PNG?h=09d346ff&itok=nky0ckkn","alt":"Giancarlo Costa"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-connect-game-changer-complex-event-driven-architectures","alias":"\/data-ai-summit-2025\/session\/lakeflow-connect-game-changer-complex-event-driven-architectures","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534654+00:00"}
{"id":"D25B3189DBX","title":"Lakeflow in Production: CI\/CD, Testing and Monitoring at Scale","description":"<p>Building robust, production-grade data pipelines goes beyond writing transformation logic \u2014 it requires rigorous testing, version control, automated CI\/CD workflows and a clear separation between development and production. In this talk, we\u2019ll demonstrate how Lakeflow, paired with Databricks Asset Bundles (DABs), enables Git-based workflows, automated deployments and comprehensive testing for data engineering projects. We\u2019ll share best practices for unit testing, CI\/CD automation, data quality monitoring and environment-specific configurations. Additionally, we\u2019ll explore observability techniques and performance tuning to ensure your pipelines are scalable, maintainable and production-ready.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), Developer Experience, ETL"],"delivery":["In Person"],"speakers":[{"name":"Adriana Ispas","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":"Adriana Ispas is a Sr. Staff Product Manager at Databricks working on the Databricks Runtime and Databricks SQL. She holds a Ph.D. in Computer Science from ETH Zurich.","image":{}},{"name":"Lennart Kats","company":"Databricks","job_title":"Sr. Staff Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-production-cicd-testing-and-monitoring-scale","alias":"\/data-ai-summit-2025\/session\/lakeflow-production-cicd-testing-and-monitoring-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534657+00:00"}
{"id":"D25B3269DBX","title":"Lakeflow Observability: From UI Monitoring to Deep Analytics","description":"<p>Monitoring data pipelines is key to reliability at scale. In this session, we\u2019ll dive into the observability experience in Lakeflow, Databricks\u2019 unified DE solution \u2014 from intuitive UI monitoring to advanced event analysis, cost observability and custom dashboards.<\/p><p>\u00a0<\/p><p>We\u2019ll walk through the revamped UX for Lakeflow observability, showing how to:<\/p><ul>\t<li>Monitor runs and task states, dependencies and retry behavior in the UI<\/li>\t<li>Set up alerts for job and pipeline outcomes + failures<\/li>\t<li>Use pipeline and job system tables for historical insights<\/li>\t<li>Explore run events and event logs for root cause analysis<\/li>\t<li>Analyze metadata to understand and optimize pipeline spend<\/li>\t<li>How to build custom dashboards using system tables to track performance data quality, freshness, SLAs and failure trends, and drive automated alerting based on real-time signals<\/li><\/ul><p>\u00a0<\/p><p>This session will help you unlock full visibility into your data workflows.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Saad Ansari","company":"Databricks","job_title":"Product Management","bio":"Saad is a product manager at Databricks working on Data Orchestration (Workflows) and Developer tooling teams. Prior to this he was CTO and co-founder at Connecterra using Artificial Intelligence to improve sustainability, productivity and animal welfare in the Dairy Industry (think Fitbit for cows!). Saad has also worked as a PM and Developer at Microsoft. He holds a masters in computer science from Stanford.","image":{}},{"name":"Theresa Hammer","company":"Databricks","job_title":"Product Manager","bio":"Theresa is a product manager working on the Developer Ecosystem. Previously, she was a team lead and enterprise account manager at Oracle, focusing on the core tech business for the German market. Theresa has also worked in analytics and sales for electronic trading solutions at Bloomberg. She holds a master\u2019s degree in the field of computer science from the University of Pennsylvania. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-observability-ui-monitoring-deep-analytics","alias":"\/data-ai-summit-2025\/session\/lakeflow-observability-ui-monitoring-deep-analytics","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534661+00:00"}
{"id":"D25L1363","title":"Leveraging GenAI for Synthetic Data Generation to Improve Spark Testing and Performance in Big Data","description":"<p>Testing Spark jobs in local environments is often difficult due to the lack of suitable datasets, especially under tight timelines. This creates challenges when jobs work in development clusters but fail in production, or when they run locally but encounter issues in staging clusters due to inadequate documentation or checks. In this session, we\u2019ll discuss how these challenges can be overcome by leveraging Generative AI to create custom synthetic datasets for local testing. By incorporating variations and sampling, a testing framework can be introduced to solve some of these challenges, allowing for the generation of realistic data to aid in performance and load testing. We\u2019ll show how this approach helps identify performance bottlenecks early, optimize job performance and recognize scalability issues while keeping costs low. This methodology fosters better deployment practices and enhances the reliability of Spark jobs across environments.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology, Retail and CPG - Food"],"category":["Apache Spark, Llama, Mosaic AI"],"areas_of_interest":["Data Applications, Developer Experience, Generative AI (LLMs), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Satej Kumar Sahu","company":"Independent Community","job_title":"Principal Data Engineer","bio":"Satej works as Principal Data Engineer with over 14 years of experience in the industry. He has worked with renowned organizations such as Boeing, Adidas, Honeywell specializing in architecture, big data and machine learning use cases. With a strong track record of architecting scalable and efficient systems, Satej has successfully delivered data-driven and ML applied solutions. He's also an author of two programming books named \"Building Secure PHP Applications\" and \"PHP 8 Basics: For Programming and Web Development\" with Apress \/ Springer publications with another one in pipeline on Generative AI Applications in Java.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%2520from%25202025-04-04%252017-23-35_1743767827753001CSRL.png?h=527ad32d&itok=Rf4yqzJL","alt":"Satej Kumar Sahu"}}],"day":"Tuesday","room":"Theater 5","starts":"2025-06-10T20:10:00","ends":"2025-06-10T20:30:00","starts_pst":"2025-06-10T13:10:00","ends_pst":"2025-06-10T13:30:00","start_time":"8:10 pm","end_time":"8:30 pm","pst_start_time":"1:10 pm","pst_end_time":"1:30 pm","duration":"20","path":"\/session\/leveraging-genai-synthetic-data-generation-improve-spark-testing-and","alias":"\/data-ai-summit-2025\/session\/leveraging-genai-synthetic-data-generation-improve-spark-testing-and","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534706+00:00"}
{"id":"D25B3262DBX","title":"Mastering Change Data Capture With DLT","description":"<p>Transactional systems are a common source of data for analytics, and Change Data Capture (CDC) offers an efficient way to extract only what\u2019s changed. However, ingesting CDC data into an analytics system comes with challenges, such as handling out-of-order events or maintaining global order across multiple streams. These issues often require complex, stateful stream processing logic.<\/p><p>\u00a0<\/p><p>This session will explore how DLT simplifies CDC ingestion using the Apply Changes function. With Apply Changes, global ordering across multiple change feeds is handled automatically \u2014 there is no need to manually manage state or understand advanced streaming concepts like watermarks. It supports both snapshot-based inputs from cloud storage and continuous change feeds from systems like message buses, reducing complexity for common streaming use cases.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Ray Zhu","company":"Databricks","job_title":"Product Management","bio":"Ray Zhu is a Sr. Staff Product Manager on the Databricks streaming team. His passion is to help customers across different industries to use the power of streaming to revolutionize their analytical and operational data applications on the Databricks Lakehouse Platform. ","image":{}},{"name":"Jacob Gollub","company":"Square","job_title":"Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/mastering-change-data-capture-dlt","alias":"\/data-ai-summit-2025\/session\/mastering-change-data-capture-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534762+00:00"}
{"id":"D25B1728","title":"Metadata-Driven Streaming Ingestion Using DLT, Azure Event Hubs and a Schema Registry","description":"<p>At Plexure, we ingest hundreds of millions of customer activities and transactions into our data platform every day, fuelling our personalisation engine and providing insights into the effectiveness of marketing campaigns.<\/p><p>\u00a0<\/p><p>We're on a journey to transition from infrequent batch ingestion to near real-time streaming using Azure Event Hubs and DLT. This transformation will allow us to react to customer behaviour as it happens, rather than hours or even days later.<\/p><p>\u00a0<\/p><p>It also enables us to move faster in other ways. By leveraging a Schema Registry, we've created a metadata-driven framework that allows data producers to:<\/p><ul>\t<li>Evolve schemas with confidence, ensuring downstream processes continue running smoothly.<\/li>\t<li>Seamlessly publish new datasets into the data platform without requiring Data Engineering assistance.\t\u00a0<\/li><\/ul><p>Join us to learn more about our journey and see how we're implementing this with DLT meta-programming - including a live demo of the end-to-end process!\u00a0<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food"],"category":["Apache Spark, DLT, Unity Catalog"],"areas_of_interest":["Data Ingestion, Streaming pipelines, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Vicky Avison","company":"Plexure","job_title":"Principal Data Engineer","bio":"Vicky is the Principal Data Engineer at Plexure, where she drives the technical vision and engineering standards for the data platform. She stumbled into the world of Big Data over 10 years ago and has been building and optimising data platforms and pipelines ever since.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Vicky_Avison_1743751887452001oYZ1.png?h=daa376fd&itok=gJ1PvMGX","alt":"Vicky Avison"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/metadata-driven-streaming-ingestion-using-dlt-azure-event-hubs-and","alias":"\/data-ai-summit-2025\/session\/metadata-driven-streaming-ingestion-using-dlt-azure-event-hubs-and","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534785+00:00"}
{"id":"D25B2978","title":"Next-Level PySpark UDF Debugging","description":"<p>Debugging PySpark User Defined Functions (UDFs) has long been challenging due to the distributed execution model and limited runtime visibility. Traditional methods often require manually searching through scattered logs, making debugging slow and inefficient.<\/p><p>\u00a0<\/p><p>In this talk, we introduce a set of powerful UDF debugging improvements, including a new logging framework that provides structured, queryable insights into UDF execution. We also cover timeouts to stop long-running tasks, better error messages for easier debugging, and best practices for common UDF issues.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services, Financial Services"],"category":["Apache Spark"],"areas_of_interest":["Developer Experience, ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Allison Wang","company":"Databricks","job_title":"Staff Software Engineer","bio":"Allison is a software engineer at Databricks, working on Spark SQL and PySpark. She holds a Bachelor\u2019s degree in Computer Science from Carnegie Mellon University.","image":{}},{"name":"Takuya Ueshin","company":"Databricks","job_title":"Sr. Software Engineer","bio":"Takuya is a software engineer at Databricks and an Apache Spark committer and PMC member. His primary focus is on PySpark, including the Pandas API on Spark and the Spark Connect Python client. Additionally, he has experience working on other Spark components, such as Spark Core and Spark SQL.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ueshinphoto_1744162201370001lDcq.jpg?h=a5da4c10&itok=xcF1_vIq","alt":"Takuya Ueshin"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/next-level-pyspark-udf-debugging","alias":"\/data-ai-summit-2025\/session\/next-level-pyspark-udf-debugging","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534821+00:00"}
{"id":"D25L1224","title":"No-Code Change in Your Python UDF for Arrow Optimization","description":"<p>Apache Spark\u2122 has introduced Arrow-optimized APIs such as Pandas UDFs and the Pandas Functions API, providing high performance for Python workloads. Yet, many users continue to rely on regular Python UDFs due to their simple interface, especially when advanced Python expertise is not readily available.<\/p><p>\u00a0<\/p><p>This talk introduces a powerful new feature in Apache Spark that brings Arrow optimization to regular Python UDFs. With this enhancement, users can leverage performance gains without modifying their existing UDFs \u2014 simply by enabling a configuration setting or toggling a UDF-level parameter.<\/p><p>\u00a0<\/p><p>Additionally, we will dive into practical tips and features for using Arrow-optimized Python UDFs effectively, exploring their strengths and limitations. Whether you\u2019re a Spark beginner or an experienced user, this session will allow you to achieve the best of both simplicity and performance in your workflows with regular Python UDFs.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Data Applications, Data Ingestion, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Hyukjin Kwon","company":"Databricks","job_title":"Staff Software Engineer","bio":"Hyukjin is a Databricks software engineer as the tech-lead in OSS PySpark team, ASF member, Apache Spark PMC member and committer, working on many different areas in Apache Spark such as PySpark, Spark SQL, SparkR, infrastructure, etc. He is the top contributor in Apache Spark, and leads efforts such as Project Zen, Pandas API on Spark, and Python Spark Connect.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/SDEGBS_4wYmY0F_1744676161446001dhq3.jpg?h=a7ffc51c&itok=z-N0B8FK","alt":"Hyukjin Kwon"}}],"day":"Thursday","room":"DevRel Theater","starts":"2025-06-12T18:30:00","ends":"2025-06-12T18:50:00","starts_pst":"2025-06-12T11:30:00","ends_pst":"2025-06-12T11:50:00","start_time":"6:30 pm","end_time":"6:50 pm","pst_start_time":"11:30 am","pst_end_time":"11:50 am","duration":"20","path":"\/session\/no-code-change-your-python-udf-arrow-optimization","alias":"\/data-ai-summit-2025\/session\/no-code-change-your-python-udf-arrow-optimization","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534828+00:00"}
{"id":"D25B3267DBX","title":"Orchestration With Lakeflow Jobs","description":"<p>Curious about orchestrating data pipelines on Databricks? Join us for an introduction to Lakeflow Jobs (formerly Databricks Workflows) \u2014 an easy-to-use orchestration service built into the Databricks Data Intelligence Platform. Lakeflow Jobs simplifies automating your data and AI workflows, from ETL pipelines to machine learning model training.<\/p><p>\u00a0<\/p><p>In this beginner-friendly session, you'll learn how to:<\/p><ul>\t<li>Build and manage pipelines using a visual approach<\/li>\t<li>Monitor workflows and rerun failures with repair runs<\/li>\t<li>Automate tasks like publishing dashboards or ingesting data using Lakeflow Connect<\/li>\t<li>Add smart triggers that respond to new files or table updates<\/li>\t<li>Use built-in loops and conditions to reduce manual work and make workflows more dynamic<\/li><\/ul><p>\u00a0<\/p><p>We\u2019ll walk through common use cases, share demos and offer tips to help you get started quickly. If you're new to orchestration or just getting started with Databricks, this session is for you.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Saad Ansari","company":"Databricks","job_title":"Product Management","bio":"Saad is a product manager at Databricks working on Data Orchestration (Workflows) and Developer tooling teams. Prior to this he was CTO and co-founder at Connecterra using Artificial Intelligence to improve sustainability, productivity and animal welfare in the Dairy Industry (think Fitbit for cows!). Saad has also worked as a PM and Developer at Microsoft. He holds a masters in computer science from Stanford.","image":{}},{"name":"Anthony Podgorsak","company":"Databricks","job_title":"Product Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/orchestration-lakeflow-jobs","alias":"\/data-ai-summit-2025\/session\/orchestration-lakeflow-jobs","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534855+00:00"}
{"id":"D25B2858","title":"PDF Document Ingestion Accelerator for GenAI Applications","description":"<p>Databricks Financial Service customers in the GenAI space have a common use case of ingestion and processing of unstructured documents \u2014 PDF\/images \u2014 then performing downstream GenAI tasks such as entity extraction and RAG based knowledge Q&A.<\/p><p>\u00a0<\/p><p>The pain points for the customers for these types of use cases are:<\/p><ul>\t<li>The quality of the PDF\/image documents varies since many older physical documents were scanned into electronic form<\/li>\t<li>The complexity of the PDF\/image documents varies and many contain tables \u2014 images with embedding information \u2014 which require slower Tesseract OCR<\/li>\t<li>They would like to streamline postprocess for downstream workloads<\/li><\/ul><p>In this talk we will present an optimized structured streaming workflow for complex PDF ingestion. The key techniques include Apache Spark\u2122 optimization, multi-threading, PDF object extraction, skew handling and auto retry logics<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Apache Spark, Databricks Workflows"],"areas_of_interest":["Data Ingestion, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Jas Bali","company":"Databricks","job_title":"Lead SSA","bio":null,"image":{}},{"name":"Qian Yu","company":"Databricks","job_title":"Specialist Solution Architect","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/pdf-document-ingestion-accelerator-genai-applications","alias":"\/data-ai-summit-2025\/session\/pdf-document-ingestion-accelerator-genai-applications","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534872+00:00"}
{"id":"D25B2712","title":"Pella Next Generation: Implement Optimization Techniques and Automated Deployments for Cost Savings","description":"<p>In this session, we will explore the \u201cPella Next Generation\u201d project, towards performance optimization and cost savings using various tools within Databricks data analytics in an Azure cloud. Liquid clustering, managed schema\/table within Unity Catalog, BI analytical dashboard to monitor job performance and cost \u2014 compute and storage \u2014 are used towards performance optimization. Auto Loader and DLT with data quality checks\/constraints and a common framework to support data ingestion into the medallion layers of the data lake. Databricks Asset Bundles for workflows\/jobs deployment resulted in significant cost savings of $30K\/year \u2014 conservative number \u2014 with a potential savings of up to $40K\/year for the organization. This presentation will provide valuable takeaways for professionals looking to implement similar solutions in their own organizations.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Manufacturing, Professional Services"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["ETL, Orchestration, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Ragavan Ramasamy","company":"Pella Corporation","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/pella-next-generation-implement-optimization-techniques-and-automated","alias":"\/data-ai-summit-2025\/session\/pella-next-generation-implement-optimization-techniques-and-automated","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534876+00:00"}
{"id":"D25L2772","title":"Race to Real-Time: Low-Latency Streaming ETL With Next-Gen OLTP-DB","description":"<p>In today\u2019s digital economy, real-time insights and rapid responsiveness are paramount to delivering exceptional user experiences and lowering TCO. In this session, discover a pioneering approach that leverages a low-latency streaming ETL pipeline built with Apache Spark\u2122 Structured Streaming and a next-gen OLTP-DB solution. Validated in a live customer scenario, this architecture achieves sub-2 second end-to-end latency by seamlessly ingesting streaming data from Kinesis and merging it into OLTP-DB. This breakthrough not only enhances performance and scalability but also provides a replicable blueprint for transforming data pipelines across various verticals. Join us as we delve into the advanced optimization techniques and best practices that underpin this innovation, demonstrating how Databricks next-generation solutions can revolutionize real-time data processing and unlock a myriad of new use cases in data landscape.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["Apache Spark"],"areas_of_interest":["Data Applications, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Irfan Elahi","company":"Databricks","job_title":"Specialist Solutions Architect","bio":"Irfan Elahi is a Specialist Solutions Architect at Databricks, specializing in data engineering and performance optimization of scalable big data solutions on hyper-scalers. Uniquely positioned by his experience on both consulting and client sides, Irfan brings a holistic perspective and proven track record of delivering high-impact analytics outcomes across telecom, retail, energy, and healthcare sectors. Leveraging Apache Spark, Databricks, Delta Lake, and cloud-native technologies, he has driven numerous data transformation projects successfully. A published author and Udemy instructor, Irfan actively shares insights through courses, blogs, and technical talks to empower data professionals globally.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/irfan_elahi_profile_pic_1744076305959001P8b8.png?h=6f192e6a&itok=j0qe-cO2","alt":"Irfan Elahi"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/race-real-time-low-latency-streaming-etl-next-gen-oltp-db","alias":"\/data-ai-summit-2025\/session\/race-real-time-low-latency-streaming-etl-next-gen-oltp-db","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534918+00:00"}
{"id":"D25B1866","title":"Real-Time Analytics Pipeline for IoT Device Monitoring and Reporting","description":"<p>This session will show how we implemented a solution to support high-frequency data ingestion from smart meters. We implemented a robust API endpoint that interfaces directly with IoT devices. This API processes messages in real time from millions of distributed IoT devices and meters across the network.The architecture leverages cloud storage as a landing zone for the raw data, followed by a streaming pipeline built on DLT. This pipeline implements a multi-layer medallion architecture to progressively clean, transform and enrich the data.<\/p><p>\u00a0<\/p><p>The pipeline operates continuously to maintain near real-time data freshness in our gold layer tables. These datasets connect directly to Databricks Dashboards, providing stakeholders with immediate insights into their operational metrics.<\/p><p>\u00a0<\/p><p>This solution demonstrates how modern data architecture can handle high-volume IoT data streams while maintaining data quality and providing accessible real-time analytics for business users.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities"],"category":["AI\/BI, DLT, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Ingestion, Data Intelligence"],"delivery":["In Person"],"speakers":[{"name":"Padraic Kirrane","company":"CK Delta","job_title":"Data Scientist","bio":null,"image":{}},{"name":"Nayan Sharma","company":"CKDelta","job_title":"Lead Data Engineer","bio":"Nayan Sharma brings over 14 years of experience in Big Data and ETL pipeline development, with expertise in gathering business requirements, designing workflows in Agile environments, and building efficient ETL processes. Proficient in Databricks, AWS (EMR, EC2, S3), Python, Talend, Informatica, SQL, Spark, and R, he has delivered scalable solutions across industries such as healthcare, finance, consulting, and R&D. Holding a Master\u2019s in Data Analytics, Nayan has played a key role in optimizing cloud-based data workflows and enabling actionable insights. His background includes working with global organizations, aligning technical solutions with complex business needs.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Nayan_Profile_pic_1738343600855001Milr.jpg?h=77a3f137&itok=5ca08Y3A","alt":"Nayan Sharma"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/real-time-analytics-pipeline-iot-device-monitoring-and-reporting","alias":"\/data-ai-summit-2025\/session\/real-time-analytics-pipeline-iot-device-monitoring-and-reporting","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534927+00:00"}
{"id":"D25B2830","title":"Real-Time Mode Technical Deep Dive: How We Built Sub-300 Millisecond Streaming Into Apache Spark\u2122","description":"<p>Real-time mode is a new low-latency execution mode for Apache Spark\u2122 Structured Streaming. It can consistently provide p99 latencies less than 300 milliseconds for a broad set of stateless and stateful streaming queries. Our talk focuses on the technical aspects of making this possible in Spark.<\/p><p>\u00a0<\/p><p>We\u2019ll dive into the core architecture that enables these dramatic latency improvements, including a concurrent stage scheduler and a non-blocking shuffle. We\u2019ll explore how we maintained Spark\u2019s fault-tolerance guarantees, and we\u2019ll also share specific optimizations we made to our streaming SQL operators.<\/p><p>\u00a0<\/p><p>These architectural improvements have already enabled Databricks customers to build workloads with latencies up to 10x lower than before. Early adopters in our Private Preview have successfully implemented real-time enrichment pipelines and feature engineering for machine learning \u2014 use cases that were previously impossible at these latencies.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Energy and Utilities, Media and Entertainment, Financial Services"],"category":["Apache Spark, DLT, LakeFlow"],"areas_of_interest":["ETL, SQL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Jerry Peng","company":"Databricks","job_title":"Staff Software Engineer","bio":null,"image":{}},{"name":"Neil Ramaswamy","company":"Databricks","job_title":"Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/real-time-mode-technical-deep-dive-how-we-built-sub-300-millisecond","alias":"\/data-ai-summit-2025\/session\/real-time-mode-technical-deep-dive-how-we-built-sub-300-millisecond","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534939+00:00"}
{"id":"D25B2848","title":"SaaS Data Ingestion With Lakeflow Connect","description":"<p>Are you looking to make informed business decisions by analyzing data from multiple SaaS platforms, but struggling with custom APIs, governance and compliance? Join us as we showcase how Databricks built our ingestion platform using our own Lakeflow Connect product that seamlessly ingests data from all our SaaS systems while addressing challenges like fragmentation, data quality, discoverability, observability and governance. Our streamlined approach empowers your teams to focus on strategic insights instead of the technical burdens of data management, ensuring robust compliance and a faster path to actionable intelligence.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Apache Spark, LakeFlow, Unity Catalog"],"areas_of_interest":["Data Ingestion, ETL, Getting started with Databricks"],"delivery":["In Person"],"speakers":[{"name":"Prashant Gupta","company":"Databricks","job_title":"Software Engineer","bio":"Prashant Gupta is a seasoned engineer with a track record of building high-scale systems across leading tech companies. He currently serves as a Senior Software Engineer at Databricks, where he leads third-party data ingestion within the Data Platform, focusing on scalable no-code SaaS integration with a strong emphasis on security and compliance. Previously at Google, Prashant led data platform in Google Cloud aimed at unifying data sources and enabling petabyte-scale, AI-driven insights. Prashant is passionate about creating resilient systems, streamlining developer experiences, and driving product innovation through data and infrastructure.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Prashant_Gupta_1745465274891001dBh0.png?h=b64bd31a&itok=_5hBwfTp","alt":"Prashant Gupta"}},{"name":"Krishna Bhupatiraju","company":"Databricks","job_title":"Sr. Engineering Manager","bio":"Krishna Bhupatiraju is an engineering leader with nearly two decades of experience building scalable data platforms at top tech companies. He currently leads Data Platform Engineering at Databricks, where he oversees multiple global teams focused on data frameworks, ingestion, governance, and developer experience. Previously, Krishna drove experimentation infrastructure at Airbnb and led data engineering teams at Amazon. His work has empowered data-driven decision-making at scale, and he\u2019s passionate about developing high-impact platforms and growing strong engineering cultures.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/profile_pic_1745370319623001oT9p.png?h=04d92ac6&itok=z1-lLRNO","alt":"Krishna Bhupatiraju"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/saas-data-ingestion-lakeflow-connect","alias":"\/data-ai-summit-2025\/session\/saas-data-ingestion-lakeflow-connect","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535031+00:00"}
{"id":"D25B1938","title":"Saving Millions From Millions: Navigating Towards Cost-Efficiency in Pinterest's Spark Jobs","description":"<p>While Spark offers powerful processing capabilities for massive data volumes, cost-efficiency challenges are always bothering users operating at large scales. At Pinterest, where we run millions of Spark jobs monthly, maintaining infra cost efficiency is crucial to support our rapid business growth.<\/p><p>\u00a0<\/p><p>To tackle this challenge, we have developed several strategies that have saved us tens of millions of dollars across numerous job instances. We will share our analytical methodology for identifying performance bottlenecks, and the technical solutions to overcome various challenges. Our approach includes extracting insights from billions of collected metrics, leveraging remote shuffle services to address shuffle slowness and improve memory utilization and reduce costs while hosting hundreds of millions of pods.<\/p><p>\u00a0<\/p><p>The presentation aims to trigger more discussions about cost efficiency topics of Apache Spark in the community and help the community to tackle the common challenge.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services, Retail and CPG - Food"],"category":["Apache Spark"],"areas_of_interest":["Data Applications, Developer Experience, Machine Learning, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Nan Zhu","company":"Pinterest","job_title":"Staff Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/saving-millions-millions-navigating-towards-cost-efficiency-pinterests","alias":"\/data-ai-summit-2025\/session\/saving-millions-millions-navigating-towards-cost-efficiency-pinterests","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535035+00:00"}
{"id":"D25B2326","title":"Scaling Data Engineering Pipelines: Preparing\u00a0Credit Card Transactions Data for Machine Learning","description":"<p>We discuss two real-world use cases in big data engineering, focusing on constructing stable pipelines and managing storage at a petabyte scale. The first use case highlights the implementation of Delta Lake to optimize data pipelines, resulting in an 80% reduction in query time and a 70% reduction in storage space. The second use case demonstrates the effectiveness of the Workflows \u2018ForEach\u2019 operator in executing compute-intensive pipelines across multiple clusters, significantly reducing processing time from months to days. This approach involves a reusable design pattern that isolates notebooks into units of work, enabling data scientists to independently test and develop.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Financial Services"],"category":["Apache Spark, Delta Lake, Databricks Workflows"],"areas_of_interest":["Data Science, Orchestration, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Brandon DeShon","company":"Mastercard","job_title":"Director, Data Scientist","bio":"Brandon DeShon is a Director of Data Science, where he leads the fraud data engineering team. With four years of experience as a data scientist in the healthcare sector, Brandon has built robust data engineering pipelines to support machine learning models. He later transitioned to Mastercard, focusing on building data engineering pipelines. His diverse background in manufacturing and engineering enables him to continuously improve processes and bring unique perspectives to pipeline development. Outside of his professional life, Brandon is an avid practitioner of Brazilian Jiujitsu","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Picture1_1744032047225001fppD.png?h=28b88a01&itok=3qfkSTsK","alt":"Brandon DeShon"}},{"name":"Luke Garzia","company":"Mastercard","job_title":"Lead Data Engineer","bio":"Luke Garzia: He's a lead data engineering with Mastercard supporting the fraud modeling team. He has extensive history working across the financial sector building cloud centric data pipelines and applications. He is a continuous learner with 3 GCP certifications and earned the nickname, 'Mr ForEach' after extensive usage of databricks Workflow capabilities.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-02-03%2520at%252014.05.50.jpeg_1738613209669001aceS.png?h=cfcfd963&itok=uW4uEi7F","alt":"Luke Garzia"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/scaling-data-engineering-pipelines-preparing-credit-card-transactions","alias":"\/data-ai-summit-2025\/session\/scaling-data-engineering-pipelines-preparing-credit-card-transactions","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535052+00:00"}
{"id":"D25L2719","title":"Scaling Data Quality at Zillow: Migrating and Enhancing Data Quality Systems on Databricks","description":"<p>Zillow has well-established, comprehensive systems for defining and enforcing data quality contracts and detecting anomalies.<\/p><p>\u00a0<\/p><p>In this session, we will share how we evaluated Databricks\u2019 native data quality features and why we chose DLT expectations for DLT pipelines, along with a combination of enforced constraints and self-defined queries for other job types. Our evaluation considered factors such as performance overhead, cost and scalability. We\u2019ll highlight key improvements over our previous system and demonstrate how these choices have enabled Zillow to enforce scalable, production-grade data quality.<\/p><p>\u00a0<\/p><p>Additionally, we are actively testing Databricks\u2019 latest data quality innovations, including enhancements to lakehouse monitoring and the newly released DQX project from Databricks Labs.<\/p><p>\u00a0<\/p><p>In summary, we will cover Zillow\u2019s approach to data quality in the lakehouse, key lessons from our migration and actionable takeaways.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["ETL"],"delivery":["In Person"],"speakers":[{"name":"Laura Zhou","company":"Zillow","job_title":"Software Dev Engineer, Big Data","bio":"Laura is a Big Data Engineer at Zillow, where she builds scalable data pipelines and transforms raw data into insights that drive impact and feed directly into Zillow\u2019s products. With 7 years of experience across diverse data domains\u2014ranging from geo data and sports analytics to user behavior\u2014she enjoys tackling complex data challenges and designing systems that make data processing more efficient and reliable. When not immersed in data, she likes exercising, hiking, and spending time with her family.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Laura_Zhou_1745365416681001ww8d.png?h=596fd5f5&itok=2CJk2biO","alt":"Laura Zhou"}},{"name":"Firas Farah","company":"Databricks","job_title":"Sr. Solutions Architect","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/scaling-data-quality-zillow-migrating-and-enhancing-data-quality","alias":"\/data-ai-summit-2025\/session\/scaling-data-quality-zillow-migrating-and-enhancing-data-quality","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535065+00:00"}
{"id":"D25B2679","title":"Scaling Identity Graph Ingestion to 1M Events\/Sec with Spark Streaming & Delta Lake","description":"<p>Adobe\u2019s Real-Time Customer Data Platform relies on the identity graph to connect over 70 billion identities and deliver personalized experiences. This session will showcase how the platform leverages Databricks, Spark Streaming and Delta Lake, along with 25+ Databricks deployments across multiple regions and clouds \u2014 Azure & AWS \u2014 to process terabytes of data daily and handle over a million records per second.<\/p><p>\u00a0<\/p><p>The talk will highlight the platform\u2019s ability to scale, demonstrating a 10x increase in ingestion pipeline capacity to accommodate peak traffic during events like the Super Bowl. Attendees will learn about the technical strategies employed, including migrating from Flink to Spark Streaming, optimizing data deduplication, and implementing robust monitoring and anomaly detection. Discover how these optimizations enable Adobe to deliver real-time identity resolution at scale while ensuring compliance and privacy.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, Delta Lake"],"areas_of_interest":["Data Ingestion, Generative AI (LLMs), Streaming pipelines, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Akanksha Nagpal","company":"Adobe","job_title":"Sr. Data Engineer","bio":"Akanksha is a Sr. Software Engineer with extensive experience in designing and building large-scale distributed systems within the Adobe Experience Platform (AEP). She has led the development of high-performance data pipelines for efficient big data ingestion into Adobe's Identity Graph, leveraging technologies such as Apache Spark, Spark Streaming, and Flink. Her expertise includes working with Delta Lake for scalable data storage and optimizing real-time and batch data processing workflows. Passionate about innovation and continuous learning, she actively contributes to the tech community, sharing insights on scalable data processing, streaming architectures, and distributed systems.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/SummitBioPic-Ak_1743728835647001cdWU.jpg?h=c5967539&itok=RktwE64v","alt":"Akanksha Nagpal"}},{"name":"Samarth Lad","company":"Adobe","job_title":"Sr Software Engineer","bio":"Samarth joined Adobe in mid-2021 and has since made significant contributions to key projects within Identity Services (UIS). He led the architecture, design, and implementation of streaming ingestion in UIS 2.0, ensuring strict SLTs of 10 minutes for event-based graph updates at production scale. Additionally, Samarth implemented batch ingestion and supported third-party graph ingestion, including Home Depot, at production scale. He also spearheaded the proof of concept for UIS 2.0\u2019s Incremental Graph Export Formats, enabling efficient use of identity graphs in Lakehouse by multiple teams across Adobe.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-02-03%2520at%25204.49.47%25E2%2580%25AFPM_1738630385974001B9yW.png?h=70217200&itok=LmYAkgIw","alt":"Samarth Lad"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/scaling-identity-graph-ingestion-1m-eventssec-spark-streaming-delta","alias":"\/data-ai-summit-2025\/session\/scaling-identity-graph-ingestion-1m-eventssec-spark-streaming-delta","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535084+00:00"}
{"id":"D25B1333","title":"Serverless as the New \"Easy Button\": How HP Inc. Used Serverless to Turbocharge Their Data Pipeline","description":"<p>How do you wrangle over 8TB of granular \u201chit-level\u201d website analytics data with hundreds of columns, all while eliminating the overhead of cluster management, decreasing runtime and saving money? In this session, we\u2019ll dive into how we helped HP Inc. use Databricks serverless compute and DLT to streamline Adobe Analytics data ingestion while making it faster, cheaper and easier to operate. We\u2019ll walk you through our full migration story \u2014 from managing unwieldy custom-defined AWS-based Apache Spark\u2122 clusters to spinning up Databricks serverless pipelines and workflows with on-demand scalability and near-zero overhead. If you want to simplify infrastructure, optimize performance and get more out of your Databricks workloads, this talk is for you.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, DLT, Unity Catalog"],"areas_of_interest":["Data Ingestion, Developer Experience, ETL"],"delivery":["In Person"],"speakers":[{"name":"Matthew Wright","company":"Zahlen Solutions LLC","job_title":"CEO","bio":"Matthew Wright is the founder of Zahlen Solutions LLC, a full-service data science & engineering firm dedicated to engaging with integrity and exceptional execution in the world of digital analytics. With almost two decades of experience collaborating with industry-leading companies\u2014such as Best Buy, Staples, CNN, Walmart, Wells Fargo, and Hewlett-Packard\u2014Matthew has tackled some of the most challenging digital measurement and analytics problems imaginable.  A \"truth-teller\" at heart, Matthew\u2019s approach is built on hard data, iterative testing, and thorough analysis\u2014he\u2019ll never sugar-coat the numbers or rely on gut reactions over facts.<br \/>\n<br \/>\nWhen he's not in front of a computer, he's biking, pickle-balling, or puttering with amateur radio.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Matt_Wright_1744777445192001UJEG.png?h=6dcf9b45&itok=OqdBhi1U","alt":"Matthew Wright"}},{"name":"Jason Hart","company":"Zahlen Solutions","job_title":"Senior Databricks Engineer","bio":"\u200bJason Hart is a seasoned data and analytics professional with over 20 years of experience spanning startups to large enterprises. He specializes in delivering advanced data solutions that transform complex information into actionable strategies. Jason's expertise encompasses AI, data modeling, and analytics engineering, with proficiency in technologies like Databricks, Python, dbt, and Tableau. His deep understanding of marketing technology, business intelligence, and data science has earned him recognition from industry leaders such as Bain & Company, along with certifications from Google and Adobe. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/jason_hart_1743796522368001wNSy.png?h=1e66e246&itok=Z5dFC5AO","alt":"Jason Hart"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/serverless-new-easy-button-how-hp-inc-used-serverless-turbocharge-their","alias":"\/data-ai-summit-2025\/session\/serverless-new-easy-button-how-hp-inc-used-serverless-turbocharge-their","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535170+00:00"}
{"id":"D25B2156","title":"Simplify Data Ingest and Egress with the New Python Data Source API","description":"<p>Data engineering teams are frequently tasked with building bespoke ingest and\/or egress solutions for myriad custom, proprietary, or industry-specific data sources or sinks. Many teams find this work cumbersome and time-consuming. Recognizing these challenges, Databricks interviewed numerous companies across different industries to better understand their diverse data integration needs. This comprehensive feedback led us to develop the Python Data Source API for Apache Spark\u2122.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Energy and Utilities, Manufacturing, Financial Services"],"category":["Apache Spark, Delta Lake, LakeFlow"],"areas_of_interest":["Data Ingestion, ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Craig Lukasik","company":"Databricks","job_title":"Sr. SSA","bio":"Craig is a Data Engineering specialist in the Communications, Media, and Entertainment vertical. He has over 25 years of experience building and helping builders.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_1745345894045001YZmw.jpg?h=71ccc96d&itok=x7DUm1KN","alt":"Craig Lukasik"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/simplify-data-ingest-and-egress-new-python-data-source-api","alias":"\/data-ai-summit-2025\/session\/simplify-data-ingest-and-egress-new-python-data-source-api","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535199+00:00"}
{"id":"D25B3259DBX","title":"Simplifying Data Pipelines With DLT: A Beginner\u2019s Guide","description":"<p>As part of the new Lakeflow data engineering experience, DLT makes it easy to build and manage reliable data pipelines. It unifies batch and streaming, reduces operational complexity and ensures dependable data delivery at scale \u2014 from batch ETL to real-time processing.<\/p><p>\u00a0<\/p><p>DLT excels at declarative change data capture, batch and streaming workloads, and efficient SQL-based pipelines. In this session, you\u2019ll learn how we\u2019ve reimagined data pipelining with DLT, including:<\/p><ul>\t<li>A brand new pipeline editor that simplifies transformations<\/li>\t<li>Serverless compute modes to optimize for performance or cost<\/li>\t<li>Full Unity Catalog integration for governance and lineage<\/li>\t<li>Reading\/writing data with Kafka and custom sources<\/li>\t<li>Monitoring and observability for operational excellence<\/li>\t<li>\u201cReal-time Mode\u201d for ultra-low-latency streaming<\/li><\/ul><p>\u00a0<\/p><p>Join us to see how DLT powers better analytics and AI with reliable, unified pipelines.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Matt Jones","company":"Databricks","job_title":"Senior Product Marketing Manager","bio":"Matt is a Senior Product Marketing Manager at Databricks, where he leads go-to-market efforts for data streaming and DLT. He lives at the intersection of data, storytelling, and serial commas. Matt holds a BBA in Marketing from the University of Texas at Austin and an MBA from UCLA Anderson, and currently lives in his hometown of Austin, Texas.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/MattJones_1746128938385001mPQ7.jpg?h=a7ffc51c&itok=WwM4DREZ","alt":"Matt Jones"}},{"name":"Brad Turnbaugh","company":"84.51","job_title":"Data Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/simplifying-data-pipelines-dlt-beginners-guide","alias":"\/data-ai-summit-2025\/session\/simplifying-data-pipelines-dlt-beginners-guide","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535203+00:00"}
{"id":"D25L2143","title":"Somebody Set Up Us the Bomb: Identifying List Bombing of End Users in an Email Anti-Spam Context","description":"<p>Traditionally, spam emails are messages a user does not want, containing some kind of threat like phishing. Because of this, detection systems can focus on malicious content or sender behavior.\u00a0<\/p><p>\u00a0<\/p><p>List bombing upends this paradigm. By abusing public forms such as marketing signups, attackers can fill a user's inbox with high volumes of legitimate mail. These emails don't contain threats, and each sender is following best practices to confirm the recipient wants to be subscribed, but the net effect for an end user is their inbox being flooded with dozens of emails per minute.<\/p><p>\u00a0<\/p><p>This talk covers the the exploration and implementation for identifying this attack in our company's anti-spam telemetry: from reading and writing to Kafka, Delta table streaming for ETL workflows, multi-table liquid clustering design for efficient table joins, curating gold tables to speed up critical queries and using Delta tables as an auditable integration point for interacting with external services.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Delta Lake, Databricks SQL, Databricks Workflows"],"areas_of_interest":["Cybersecurity, Data Intelligence, SQL, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Doug Sibley","company":"Cisco Talos","job_title":"Security Research Technical Leader","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/somebody-set-us-bomb-identifying-list-bombing-end-users-email-anti-spam","alias":"\/data-ai-summit-2025\/session\/somebody-set-us-bomb-identifying-list-bombing-end-users-email-anti-spam","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535232+00:00"}
{"id":"D25B2193","title":"Spark 4.0 and Delta 4.0 For Streaming Data","description":"<p>Real-time data is one of the most important datasets for any Data and AI Platform across any industry.<\/p><p>\u00a0<\/p><p>Spark 4.0 and Delta 4.0 include new features that make ingestion and querying of real-time data better than ever before.<\/p><p>\u00a0<\/p><p>Features such as:\u00a0<\/p><ul>\t<li>Python custom data sources for simple ingestion of streaming and batch time series data sources using Spark\u00a0<\/li>\t<li>Variant types for managing variable data types and json payloads that are common in the real time domain<\/li>\t<li>Delta liquid clustering for simple data clustering without the overhead or complexity of partitioning<\/li><\/ul><p>\u00a0In this presentation you will learn how data teams can leverage these latest features to build industry-leading, real-time data products using Spark and Delta and includes real world examples and metrics of the improvements they make in performance and processing of data in the real time space.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities, Manufacturing"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["Data Applications, Data Ingestion, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Bryce Bartmann","company":"Shell","job_title":"Chief Digital Technology Advisor","bio":"Bryce Bartmann is the Chief Digital Technology Advisor, focussing on how emerging digital technologies, data and open source contribute to Shell\u2019s Digital strategy and provide solutions for the energy transition. Bryce represents Shell on the LF Energy Governing Body and Technical Advisory Committee and is an active contributor of the LF Energy Real Time Data Ingestion Platform project.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/spark-40-and-delta-40-streaming-data","alias":"\/data-ai-summit-2025\/session\/spark-40-and-delta-40-streaming-data","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535236+00:00"}
{"id":"D25B3274DBX","title":"Spark Connect: Flexible, Local Access to Apache Spark at Scale","description":"<p>What if you could run Spark jobs without worrying about clusters, versions and upgrades? Did you know Spark has this functionality built-in today? Join us to take a look at this functionality \u2014 Spark Connect.<\/p><p>\u00a0<\/p><p>Join us to dig into how Spark Connect works \u2014 abstracting away Spark clusters away in favor of the DataFrame API and unresolved logical plans.<\/p><p>\u00a0<\/p><p>You will learn some of the cool things Spark Connect unlocks, including:<\/p><ul>\t<li>Moving you from thinking about clusters to just thinking about jobs<\/li>\t<li>Making Spark code more portable and platform agnostic<\/li>\t<li>Enabling support for languages such as Go<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Databricks Experience (DBX), Open Source, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"James Malone","company":null,"job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/spark-connect-flexible-local-access-apache-spark-scale","alias":"\/data-ai-summit-2025\/session\/spark-connect-flexible-local-access-apache-spark-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535240+00:00"}
{"id":"D25B3271DBX","title":"Spark on Databricks: Tips and Tricks","description":"<p>This session explores a collection of advanced and lesser-known use cases in Apache Spark\u2122, drawn from real-world scenarios and internal experimentation.<\/p><p>\u00a0<\/p><p>Topics include:<\/p><ul>\t<li>Restarting individual streams without restarting the entire cluster<\/li>\t<li>Priming schemas to handle schema evolution more effectively<\/li>\t<li>Demultiplexing events for cleaner, more scalable stream processing<\/li>\t<li>Using the Delta Kernel directly from Scala and Jupyter notebooks<\/li>\t<li>Key considerations and pitfalls when benchmarking Spark workloads<\/li><\/ul><p>\u00a0<\/p><p>We'll also cover additional patterns and tooling tips that can help solve operational challenges and optimize performance in production Spark environments.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Databricks Experience (DBX), ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Dattatraya Walake","company":"Databricks","job_title":"SSA","bio":null,"image":{}},{"name":"Murali Talluri","company":"Databricks","job_title":"Specialist Solutions Architect","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/spark-databricks-tips-and-tricks","alias":"\/data-ai-summit-2025\/session\/spark-databricks-tips-and-tricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535246+00:00"}
{"id":"D25L3365SPO","title":"Sponsored by: Astronomer | Scaling Data Teams for the Future","description":"<p>The role of data teams and data engineers is evolving. No longer just pipeline builders or dashboard creators, today\u2019s data teams must evolve to drive business strategy, enable automation, and scale with growing demands. Best practices seen in the software engineering world (Agile development, CI\/CD, and Infrastructure-as-code) from the DevOps movement are gradually making their way into data engineering. We believe these changes have led to the rise of DataOps and a new wave of best practices that will transform the discipline of data engineering. But how do you transform a reactive team into a proactive force for innovation? We\u2019ll explore the key principles for building a resilient, high-impact data team\u2014from structuring for collaboration, testing, automation, to leveraging modern orchestration tools. Whether you\u2019re leading a team or looking to future-proof your career, you\u2019ll walk away with actionable insights on how to stay ahead in the rapidly changing data landscape.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["AI\/BI, Databricks Workflows"],"areas_of_interest":["Orchestration, Thought Leadership"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/sponsored-astronomer-scaling-data-teams-future","alias":"\/data-ai-summit-2025\/session\/sponsored-astronomer-scaling-data-teams-future","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535255+00:00"}
{"id":"D25B3155SPO","title":"Sponsored by: Astronomer | Unlocking the Future of Data Orchestration: Introducing Apache Airflow\u00ae 3","description":"<p>Airflow 3 is here, bringing a new era of flexibility, scalability, and security to data orchestration. This release makes building, running, and managing data pipelines easier than ever. In this session, we will cover the key benefits of Airflow 3, including: (1) Ease of Use: Airflow 3 rethinks the user experience\u2014from an intuitive, upgraded UI to DAG Versioning and scheduler-integrated backfills that let teams manage pipelines more effectively than ever before (2) Stronger Security: By decoupling task execution from direct database connections, Airflow 3 enforces task isolation and minimal-privilege access. This meets stringent compliance standards while reducing the risk of unauthorized data exposure. (3) Ultimate Flexibility: Run tasks anywhere, anytime with remote execution and event-driven scheduling. Airflow 3 is designed for global, heterogeneous modern data environments with an architecture that facilitates edge and hybrid-cloud to GPU-based deployments.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Travel and Hospitality, Financial Services"],"category":["AI\/BI"],"areas_of_interest":["ML\/LLMOps, Open Source, Orchestration"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/sponsored-astronomer-unlocking-future-data-orchestration-introducing","alias":"\/data-ai-summit-2025\/session\/sponsored-astronomer-unlocking-future-data-orchestration-introducing","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535260+00:00"}
{"id":"D25L3105SPO","title":"Sponsored by: Coalesce | Bringing Order to Chaos: How to Succeed in a Data & Analytics World","description":"<p>Priorities shift, requirements change, resources fluctuate, and the demands on data teams are only continuing to grow. Join this session, led by Coalesce Sales Engineering Director, Michael Tantrum, to hear about the most efficient way to deliver high quality data to your organization at the speed they need to consume it. Learn how to sidestep the common pitfalls of data development for maximum data team productivity.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["AI\/BI, Databricks SQL, DLT"],"areas_of_interest":["AI Agents, Catalogs, SQL"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/sponsored-coalesce-bringing-order-chaos-how-succeed-data-analytics","alias":"\/data-ai-summit-2025\/session\/sponsored-coalesce-bringing-order-chaos-how-succeed-data-analytics","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535275+00:00"}
{"id":"D25B3106SPO","title":"Sponsored by: Coalesce | From Raw Data to Real-Time Retention: Powering Customer Health Scores on Databricks","description":"<p>Understanding customer engagement and retention isn\u2019t optional\u2014it\u2019s mission-critical. Join us for a live demo to see how you can build a scalable, governed customer health scoring model by transforming raw signals into actionable insights. Discover how Coalesce\u2019s low-code development platform works seamlessly with Databricks\u2019 lakehouse architecture to unify and operationalize customer data at scale. With built-in governance, automation, and metadata intelligence, you\u2019ll deliver trusted scores that support proactive decision-making across the business. Why Attend?<\/p><ul>\t<li>Accelerate time-to-insight with automated, low-code transformations<\/li>\t<li>Build repeatable, enterprise-grade scoring models with full data lineage<\/li>\t<li>Ensure governance, transparency, and compliance at every step<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Delta Lake, Databricks SQL, DLT"],"areas_of_interest":["AI Agents, Catalogs, SQL"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/sponsored-coalesce-raw-data-real-time-retention-powering-customer","alias":"\/data-ai-summit-2025\/session\/sponsored-coalesce-raw-data-real-time-retention-powering-customer","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535279+00:00"}
{"id":"D25B3102SPO","title":"Sponsored by: dbt Labs | Cooking Up Data Success: How Chick-fil-A Optimizes Trust, Efficiency and Speed with dbt & Databricks","description":"<p>Accelerating development, gaining better control over data models, and increasing confidence in data accuracy benefit both internal stakeholders and customers. Join this session to discover how Chick-fil-A\u2014a leading fast-food chain renowned for its efficiency, customer service, and iconic chicken sandwiches\u2014is transforming its data operations with dbt and Databricks. Learn how their \u201cdata recipe,\u201d has enhanced trust, velocity, efficiency, and governance. Through streamlined development, improved collaboration, and higher data quality, Chick-fil-A is achieving faster time-to-market and reducing errors, driving a significant impact across its organization.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Manufacturing, Retail and CPG - Food"],"category":["DBT, Databricks Workflows"],"areas_of_interest":["Collaboration, Data Intelligence"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/sponsored-dbt-labs-cooking-data-success-how-chick-fil-optimizes-trust","alias":"\/data-ai-summit-2025\/session\/sponsored-dbt-labs-cooking-data-success-how-chick-fil-optimizes-trust","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535283+00:00"}
{"id":"D25L3357SPO","title":"Sponsored by: dbt Labs | Leveling Up Data Engineering at Riot: How We Rolled Out dbt and Transformed the Developer Experience","description":"<p>Riot Games reduced its Databricks compute spend and accelerated development cycles by transforming its data engineering workflows\u2014migrating from bespoke Databricks notebooks and Spark pipelines to a scalable, testable, and developer-friendly dbt-based architecture. In this talk, members of the Developer Experience & Automation (DEA) team will walk through how they designed and operationalized dbt to support Riot\u2019s evolving data needs.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["DBT, Databricks Workflows"],"areas_of_interest":["Collaboration, Developer Experience, Gaming"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/sponsored-dbt-labs-leveling-data-engineering-riot-how-we-rolled-out-dbt","alias":"\/data-ai-summit-2025\/session\/sponsored-dbt-labs-leveling-data-engineering-riot-how-we-rolled-out-dbt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535286+00:00"}
{"id":"D25L3343SPO","title":"Sponsored by: Firebolt | The Power of Low-latency Data for AI Apps","description":"<p>Retrieval-augmented generation (RAG) has transformed AI applications by grounding responses with external data. It can be better. By pairing RAG with low latency SQL analytics, you can enrich responses with instant insights, leading to a more interactive and insightful user experience with fresh, data-driven intelligence. In this talk, we\u2019ll demo how low latency SQL combined with an AI application can deliver speed, accuracy, and trust.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["AI\/BI"],"areas_of_interest":["Data Applications, Data Intelligence, SQL"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/sponsored-firebolt-power-low-latency-data-ai-apps","alias":"\/data-ai-summit-2025\/session\/sponsored-firebolt-power-low-latency-data-ai-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535305+00:00"}
{"id":"D25B3367SPO","title":"Sponsored by: Fivetran | Scalable Data Ingestion: Building custom pipelines with the Fivetran Connector SDK and Databricks","description":"<p>Organizations have hundreds of data sources, some of which are very niche or difficult to access. Incorporating this data into your lakehouse requires significant time and resources, hindering your ability to work on more value-add projects. Enter the Fivetran Connector SDK- a powerful new tool that enables your team to create custom pipelines for niche systems, custom APIs, and sources with specific data filtering requirements, seamlessly integrating with Databricks. During this session, Fivetran will demonstrate how to (1) Leverage the Connector SDK to build scalable connectors, enabling the ingestion of diverse data into Databricks (2) Gain flexibility and control over historical and incremental syncs, delete capture, state management, multithreading data extraction, and custom schemas (3) Utilize practical examples, code snippets, and architectural considerations to overcome data integration challenges and unlock the full potential of your Databricks environment.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Databricks Workflows"],"areas_of_interest":["Data Ingestion, ETL"],"delivery":["In Person"],"speakers":[],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/sponsored-fivetran-scalable-data-ingestion-building-custom-pipelines","alias":"\/data-ai-summit-2025\/session\/sponsored-fivetran-scalable-data-ingestion-building-custom-pipelines","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535308+00:00"}
{"id":"D25B3263DBX","title":"SQL-First ETL: Building Easy, Efficient Data Pipelines With DLT","description":"<p>This session explores how SQL-based ETL can accelerate development, simplify maintenance and make data transformation more accessible to both engineers and analysts. We'll walk through how Databricks DLT and Databricks SQL warehouse support building production-grade pipelines using familiar SQL constructs.<\/p><p>\u00a0<\/p><p>Topics include:<\/p><ul>\t<li>Using streaming tables for real-time ingestion and processing<\/li>\t<li>Leveraging materialized views to deliver fast, pre-computed datasets<\/li>\t<li>Integrating with tools like dbt to manage batch and streaming workflows at scale<\/li><\/ul><p>\u00a0<\/p><p>By the end of the session, you\u2019ll understand how SQL-first approaches can streamline ETL development and support both operational and analytical use cases.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks SQL, DLT, LakeFlow"],"areas_of_interest":["Databricks Experience (DBX), ETL, SQL"],"delivery":["In Person"],"speakers":[{"name":"Paul Lappas","company":"Databricks","job_title":"Sr Staff Product Manager","bio":"Paul Lappas manages the Delta Live Tables ETL product at Databricks. Previously, Paul was a principal product manager for Amazon Redshift at AWS.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/sql-first-etl-building-easy-efficient-data-pipelines-dlt","alias":"\/data-ai-summit-2025\/session\/sql-first-etl-building-easy-efficient-data-pipelines-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535332+00:00"}
{"id":"D25B3276DBX","title":"Streaming Meets Governance: Building AI-Ready Tables With Confluent Tableflow and Unity Catalog","description":"<p>Learn how Databricks and Confluent are simplifying the path from real-time data to governed, analytics- and AI-ready tables. This session will cover how Confluent Tableflow automatically materializes Kafka topics into Delta tables and registers them with Unity Catalog \u2014 eliminating the need for custom streaming pipelines. We\u2019ll walk through how this integration helps data engineers reduce ingestion complexity, enforce data governance and make real-time data immediately usable for analytics and AI.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Victoria Bukta","company":"Databricks","job_title":"Member of Technical Staff","bio":"Victoria is a product manager at Databricks on the LakeFlow team. She previously was a PM at Tabular, and an engineering manager at Shopify where she led teams working on large-scale, low-latency ingestion and pioneered lakehouse adoption. She is based out of Toronto and sails in her spare time.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG-20191228-WA0000_1745242341538001urve.jpg?h=d70dd8e3&itok=Ura_z8IB","alt":"Victoria Bukta"}},{"name":"Kasun Indrasiri","company":"Confluent","job_title":"Senior Product Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/streaming-meets-governance-building-ai-ready-tables-confluent-tableflow","alias":"\/data-ai-summit-2025\/session\/streaming-meets-governance-building-ai-ready-tables-confluent-tableflow","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535354+00:00"}
{"id":"D25B2270","title":"Supercharging Sales Intelligence: Processing Billions of Events via Structured Streaming","description":"<p>DigiCert is a digital security company that provides digital certificates, encryption and authentication services and serves 88% of the Fortune 500, securing over 28 billion web connections daily. Our project aggregates and analyzes certificate transparency logs via public APIs to provide comprehensive market and competitive intelligence. Instead of relying on third-party providers with limited data, our project gives full control, deeper insights and automation. Databricks has helped us reliably poll public APIs in a scalable manner that fetches millions of events daily, deduplicate and store them in our Delta tables. We specifically use Spark for parallel processing, structured streaming for real-time ingestion and deduplication, Delta tables for data reliability, pools and jobs to ensure our costs are optimized. These technologies help us keep our data fresh, accurate and cost effective. This data has helped our sales team with real-time intelligence, ensuring DigiCert's success.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, Delta Lake, Databricks Workflows"],"areas_of_interest":["Customer Data Platform, Data Intelligence, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Nikita Raje","company":"DigiCert","job_title":"Director-Data Engineering","bio":"Nikita Raje is the Director of Data Engineering at DigiCert, where she leads the design and implementation of large data infrastructure powering critical business intelligence, compliance, and AI initiatives. With a strong focus on automation, real-time analytics, and cost optimization, she has built a best-in-class data ecosystem from the ground up\u2014enabling everything from structured streaming at scale to AI-powered insights across billions of events.<br \/>\nOutside of work, Nikita brings the same drive and creativity to her life as a mom of two, animal rescue advocate, and backyard homesteader. Whether leading cutting-edge AI initiatives or chasing chickens with her kids, she\u2019s all about building resilient systems\u2014both human and technical.<br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/038-FTbIEnYuTkY_1744816626833001AbTD.jpg?h=436b82d4&itok=ecSCNw-h","alt":"Nikita Raje"}},{"name":"Anurag Bharati","company":"DigiCert","job_title":"Senior Data Engineer","bio":"Anurag works as a Senior Data Engineer at DigiCert and has built data pipelines that have scaled to ingest data of varying volumes. He has built data pipelines that have leveraged technologies of Apache Spark, Structured Streaming and Delta lake tables. Apart from Data Engineering pursuits, he has also built a internal customer facing UI that uses Databricks' Genie API to let DigiCert's internal stakeholders query business questions using natural language.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ab_1744053250811001Y7VW.JPG?h=060fccfc&itok=GlGfainE","alt":"Anurag Bharati"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/supercharging-sales-intelligence-processing-billions-events-structured","alias":"\/data-ai-summit-2025\/session\/supercharging-sales-intelligence-processing-billions-events-structured","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535372+00:00"}
{"id":"D25B3071","title":"The Hitchhiker's Guide to Delta Lake Streaming in an Agentic Universe","description":"<p>As data engineering continues to evolve the shift from batch-oriented to streaming-first has become standard across the enterprise. The reality is these changes have been taking shape for the past decade \u2014 we just now also happen to be standing on the precipice of true disruption through automation, the likes of which we could only dream about before.<\/p><p>\u00a0<\/p><p>Yes, AI Agents and LLMs are already a large part of our daily lives, but we (as data engineers) are ultimately on the frontlines ensuring that the future of AI is powered by consistent, just-in-time data \u2014 and Delta Lake is critical to help us get there.<\/p><p>\u00a0<\/p><p>This session will provide you with best practices learned the hard way by one of the authors of The Delta Lake Definitive Guide including:<\/p><ul>\t<li>Guide to writing generic applications as components<\/li>\t<li>Workflow automation tips and tricks<\/li>\t<li>Tips and tricks for Delta clustering (liquid, z-order, and classic)<\/li>\t<li>Future facing: Leveraging metadata for agentic pipelines and workflow automation<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Delta Lake"],"areas_of_interest":["Data Intelligence, Developer Experience, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Scott Haines","company":"Nike","job_title":"Distinguished Software Engineer","bio":"Scott Haines is a seasoned software engineer with over 20 years of experience. He has worn many hats during his career, across the entire software stack, from front to back. He has worked for a wide variety of companies, from startups to global corporations, and across various industries, from video and telecommunications, to news, sports, and gaming, as well as data, insights, and analytics. He has held positions at notable companies, including Hitachi Data Systems, Convo Communications, Yahoo!, Twilio, and joined Nike in early 2022.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/scott_headshot_1745602314568001XONF.png?h=7bf4c272&itok=-41T3C96","alt":"Scott Haines"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/hitchhikers-guide-delta-lake-streaming-agentic-universe","alias":"\/data-ai-summit-2025\/session\/hitchhikers-guide-delta-lake-streaming-agentic-universe","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535437+00:00"}
{"id":"D25B1287DBX","title":"The LakeFlow Effect","description":"<p>Lakeflow brings much excitement, simplicity and unification to Databricks\u2019 engineering experience. Databricks\u2019 Bilal Aslam (Sr. Director of Product Management) and Josue A. Bogran (Databricks MVP & content creator) provide an overview of the history of Lakeflow, current value to your organization and the direction its capabilities are going toward.<\/p><p>\u00a0<\/p><p>The session covers:<\/p><ul>\t<li>What is Lakeflow?<\/li>\t<li>Differences and similarities between DLTs<\/li>\t<li>Overview of current Lakeflow Connect, Pipelines and Jobs capabilities<\/li>\t<li>How to get started<\/li>\t<li>What's Next?<\/li><\/ul><p>\u00a0<\/p><p>The session will also provide you with an opportunity to ask questions to the team behind Lakeflow.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["Data Ingestion, Databricks Experience (DBX), Open Source, Orchestration, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Bilal Aslam","company":"Databricks","job_title":"Sr. Director of Product Management","bio":"Bilal Aslam is a Senior Director of Product Management at Databricks, focused on data engineering. Prior to joining Databricks, Bilal founded a startup that was acquired by DocuSign. He was a product manager on Microsoft Azure before that. Bilal Aslam lives in Amsterdam with his family and a Siberian cat.","image":{}},{"name":"Josue Bogran","company":"JosueBogran.com & zeb.co","job_title":"VP of Data + AI Architecture","bio":"Tech is worthless without positive, impactful business outcomes.<br \/>\n<br \/>\nI\u2019ve worked in finance, supply chain, and healthcare. In every scenario, technology only mattered when it delivered real business outcomes.<br \/>\n<br \/>\nToday, I reverse-engineer business goals into the right technology to reach them, even if that means using Excel. I do this in a few ways:<br \/>\n1. Publishing no-fluff content on data platforms, architecture, and occasionally, satire.<br \/>\n2. Leading Data + AI Architecture as VP at zeb.co, an outcome-driven consulting firm working across Databricks, AWS, and beyond.<br \/>\n3. Advising Sigma and Lumel on building products that drive real business value.<br \/>\n<br \/>\nLike my approach to business & tech? Follow along:<br \/>\n1) in\/JosueBogran<br \/>\n1) YouTube.com\/@JosueBogranChannel","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Josue%2520PNG_1746118826759001Eqcs.png?h=fbf7a813&itok=WJ5hvVV7","alt":"Josue Bogran"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lakeflow-effect","alias":"\/data-ai-summit-2025\/session\/lakeflow-effect","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535444+00:00"}
{"id":"D25B3023","title":"The Perks of Using Unity Catalog Managed Tables","description":"<p>This session provides actionable insights into how organizations can transition to Unity Catalog managed tables to unlock the full potential of predictive optimization and future-proof their data architecture. Whether you\u2019re managing thousands of tables or looking to streamline operations, this talk will equip you with the tools and strategies to succeed in the era of intelligent data management.<\/p><p>\u00a0<\/p><p>Key highlights include:<\/p><ul>\t<li>Managed tables vs. external tables: Why managed tables are the default choice for performance and ease of use, and how they optimize data automatically<\/li>\t<li>The importance of predictive optimization paired with liquid clustering and automatic statistics collection<\/li>\t<li>Feature enablement: Automatic upgrades to the latest Delta features<\/li><\/ul>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Energy and Utilities, Health and Life Sciences, Manufacturing"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Catalogs, Data Applications, Data Ingestion"],"delivery":["In Person"],"speakers":[{"name":"Youssef Mrini","company":"Databricks","job_title":"Solutions Architect","bio":"Youssef Mrini is part of the pre-sales team in the EMEA Region at Databricks and helps customers to adopt the databricks technologies. Youssef is an ex-data engineer. He holds a degree in big Data and have been working on various areas for the last couple of years","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/1718289391991_1743752881415001nBgn.png?h=fbf7a813&itok=M-CaXQ5a","alt":"Youssef Mrini"}},{"name":"Cindy Jiang","company":"Databricks","job_title":"Product Manager","bio":"Cindy is an Associate Product Manager at Databricks, working on making data management simple and out-of-the-box. Before joining Databricks, Cindy graduated from the M&T Program at the University of Pennsylvania with an MSE and BAS in Computer Science and a BS from Wharton. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/perks-using-unity-catalog-managed-tables","alias":"\/data-ai-summit-2025\/session\/perks-using-unity-catalog-managed-tables","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535462+00:00"}
{"id":"D25B2936DBX","title":"The Upcoming Apache Spark 4.1: The Next Chapter in Unified Analytics","description":"<p>Apache Spark has long been recognized as the leading open-source unified analytics engine, combining a simple yet powerful API with a rich ecosystem and top-notch performance. In the upcoming Spark 4.1 release, the community reimagines Spark to excel at both massive cluster deployments and local laptop development. We\u2019ll start with new single-node optimizations that make PySpark even more efficient for smaller datasets. Next, we\u2019ll delve into a major \u201cPythonizing\u201d overhaul \u2014 simpler installation, clearer error messages and Pythonic APIs. On the ETL side, we\u2019ll explore greater data source flexibility (including the simplified Python Data Source API) and a thriving UDF ecosystem. We\u2019ll also highlight enhanced support for real-time use cases, built-in data quality checks and the expanding Spark Connect ecosystem \u2014 bridging local workflows with fully distributed execution. Don\u2019t miss this chance to see Spark\u2019s next chapter!<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Apache Spark, Delta Lake, Apache Iceberg"],"areas_of_interest":["Databricks Experience (DBX), ETL, Open Source, SQL"],"delivery":["In Person"],"speakers":[{"name":"Xiao Li","company":"Databricks","job_title":"Engineering Director","bio":"Xiao Li is an Engineering Director at Databricks, an Apache Spark Committer, and a PMC member. He has a deep interest in Spark and database engines. Previously, he was an IBM Master Inventor and an expert in asynchronous database replication and consistency verification. Xiao earned his Ph.D. from the University of Florida in 2011.","image":{}},{"name":"DB Tsai","company":"Databricks","job_title":"Senior Engineering Manager","bio":"DB Tsai is an engineering leader at the Databricks Spark team. He is an Apache Spark Project Management Committee (PMC) Member and Committer, and he enjoys building teams with great cultures focusing on large scale distributed data infrastructure. Before his transition to a leadership role, he implemented several algorithms including Linear Regression and Binary\/Multinomial Logistic Regression with Elastici-Net (L1\/L2) regularization using LBFGS\/OWL-QN optimizers in Apache Spark project.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/upcoming-apache-spark-41-next-chapter-unified-analytics","alias":"\/data-ai-summit-2025\/session\/upcoming-apache-spark-41-next-chapter-unified-analytics","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535466+00:00"}
{"id":"D25B3231DBX","title":"Top Performance and Cost Optimizations for DLT","description":"<p>DLT simplifies pipeline development and management \u2014 but how do you optimize for performance and cost? In this session, we\u2019ll explore practical strategies for tuning DLT pipelines, including when and how to use autoscaling, Photon and different node types. We'll also cover how to monitor resource usage and decide when serverless is the right choice. You'll learn best practices drawn from real-world customer implementations, along with an overview of the latest performance enhancements available in serverless DLT.<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["DLT"],"areas_of_interest":["Databricks Experience (DBX), ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Steven Yu","company":"Databricks","job_title":"Principal Solutions Architect","bio":"Steven is a Principal Solutions Architect at Databricks with a background in Data Warehousing, Data Engineering, Streaming, and Performance Tuning with over 20+ years of industry experience.  ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Image%2520from%2520iOS%2520%25281%2529_1744940658412001Bfso.jpg?h=40551f42&itok=f4r0otq3","alt":"Steven Yu"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/top-performance-and-cost-optimizations-dlt","alias":"\/data-ai-summit-2025\/session\/top-performance-and-cost-optimizations-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535479+00:00"}
{"id":"D25B1323","title":"Transforming Customer Processes and Gaining Productivity With DLT","description":"<p>Bradesco Bank is one of the largest private banks in Latin America, with over 75 million customers and over 80 years of presence in FSI. In the digital business, velocity to react to customer interactions is crucial to succeed. In the legacy landscape, acquiring data points on interactions over digital and marketing channels was complex, costly and lacking integrity due to typical fragmentation of tools. With the new in-house Customer Data Platform powered by Databricks Intelligent Platform, it was possible to completely transform the data strategy around customer data. Using some key components such Uniform and DLT, it was possible to increase data integrity, reduce latency and processing time and, most importantly, boost personal productivity and business agility. Months of reprocessing, weeks of human labor and cumbersome and complex data integrations were dramatically simplified achieving significant operational efficiency.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Databricks Workflows, DLT"],"areas_of_interest":["Data Applications, Data Ingestion, Streaming pipelines, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Marcos Abrantes Gomes","company":"Bradesco Bank","job_title":"senior manager","bio":"Bachelor's degree in Statistics with 17 years of experience in the financial market, specializing in CRM and customer data intelligence. Passionate about leveraging data to drive result, build meaningful customer relationships, and generate business insights. Enthusiastic about sports and human connections.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Bradesco_imagem_1744374670891001t2TT.jpg?h=43c03560&itok=M0_Sm03O","alt":"Marcos Abrantes Gomes"}},{"name":"Ademir Francisquini Junior","company":"Banco Bradesco S.A.","job_title":"Sr Manager","bio":"I\u2019m Ademir Francisquini Jr, a professional with 22+ years of experience in the financial market, including 15 years in data analytics. Currently, I work as Senior Manager at Banco Bradesco in the CRM department, one of the largest banks in Brazil. I lead teams focused on customer insights, marketing communications performance, and business intelligence, helping the digital transformation of the areas with focus in a data-driven culture to support the business in identifying opportunities and decision-making. I degree in IT and MBA with focus on management. My life is data, technology, and my family, I very like sports such as boxing, running, and soccer.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/fotoAdemir05_1745961385849001mW5H.jpg?h=e54d7d27&itok=DzgN-otp","alt":"Ademir Francisquini Junior"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/transforming-customer-processes-and-gaining-productivity-dlt","alias":"\/data-ai-summit-2025\/session\/transforming-customer-processes-and-gaining-productivity-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535503+00:00"}
{"id":"D25B2734","title":"Transforming Data Pipeline Management With a Targeted Proof of Concept","description":"<p>At Capital One, data-driven decision making is paramount to our success. This session explores how a focused proof of concept (POC) accelerated a shift in our data pipeline management strategy, resulting in operational improvements and expanded analytical capabilities. We'll cover the business challenges that motivated POC initiation, including data latency, cost savings and scalability limitations, and real-world results. We'll also dive into an examination of the before-and-after architecture with highlights for key technological levers. This session offers insights for data engineering and machine learning practitioners seeking to optimize their data pipelines for improved performance, scalability and business value.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Apache Spark, Delta Lake, AI\/BI"],"areas_of_interest":["Data Ingestion, ETL"],"delivery":["In Person"],"speakers":[{"name":"Kevin O'Dell","company":"Capital One Financial","job_title":"VP Data Engineering CARD","bio":null,"image":{}},{"name":"Raghu Valluri","company":"Capital One Financial","job_title":"VP","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/transforming-data-pipeline-management-targeted-proof-concept","alias":"\/data-ai-summit-2025\/session\/transforming-data-pipeline-management-targeted-proof-concept","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535514+00:00"}
{"id":"D25L2132","title":"Unifying Customer Data to Drive a New Automotive Experience With Lakeflow Connect","description":"<p>The Databricks Data Intelligence Platform and Lakeflow Connect have transformed how Porsche manages and uses its customer data. By opting to use Lakeflow Connect instead of building a custom solution, the company has reaped the benefits of both operational efficiency and cost management.<\/p><p>\u00a0<\/p><p>Internally, teams at Porsche now spend less time managing data integration processes.\u00a0<\/p><p>\u00a0<\/p><p>\u201cLakeflow Connect has enabled our dedicated CRM and Data Science teams to be more productive as they can now focus on their core work to help innovate, instead of spending valuable time on the data ingestion integration with Salesforce,\u201d says Gruber.<\/p><p>\u00a0<\/p><p>This shift in focus is aligned with broader industry trends, where automotive companies are redirecting significant portions of their IT budgets toward customer experience innovations and digital transformation initiatives.<\/p><p>\u00a0<\/p><p>This story was also shared as part of a Databricks Success Story \u2014 Elise Georis, Giselle Goicochea.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Manufacturing, Retail and CPG - Food"],"category":["Delta Lake, LakeFlow, Unity Catalog"],"areas_of_interest":["Data Ingestion, Data Science, Developer Experience"],"delivery":["In Person"],"speakers":[{"name":"Andreas Maier","company":"Porsche Informatik GmbH","job_title":"Team Lead Data-Driven Business Solutions","bio":"- Various Data Science internships within finance, consulting, stardup<br \/>\n- Masters of Industrial Engineering and Management at University of Karlsruhe (KIT), Germany<br \/>\n- Principal Data Scientist at Porsche Informatik<br \/>\n- Team Lead Data-Driven Business Solutions at Porsche Informatik","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/AndreasMaier_1745580428300001yQ7b.png?h=172202e9&itok=VW_yP5SR","alt":"Andreas Maier"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/unifying-customer-data-drive-new-automotive-experience-lakeflow-connect","alias":"\/data-ai-summit-2025\/session\/unifying-customer-data-drive-new-automotive-experience-lakeflow-connect","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535653+00:00"}
{"id":"D25B2563","title":"Unifying Human-Curated Data Ingestion and Real-Time Updates with Databricks DLT, Protobuf and BSR","description":"<p>Red Stapler is a streaming-native system on Databricks that merges file-based ingestion and real-time user edits into one DLT pipeline for near real-time feedback. Protobuf definitions, managed in the Buf Schema Registry (BSR), govern schema and data-quality rules, ensuring backward compatibility. All records \u2014 valid or not \u2014 are stored in an SCD Type 2 table, capturing every version for full history and immediate quarantine views of invalid data. This unified approach boosts data governance, simplifies auditing and streamlines error fixes.<\/p><p>\u00a0<\/p><p>Running on DLT Serverless and the Kafka-compatible Bufstream keeps costs low by scaling down to zero when idle. Red Stapler\u2019s configuration-driven Protobuf logic adapts easily to evolving survey definitions without risking production. The result is consistent validation, quick updates and a complete audit trail \u2014 all critical for trustworthy, flexible data pipelines.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences"],"category":["Databricks Workflows, DLT, Unity Catalog"],"areas_of_interest":["Data Ingestion, Developer Experience, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Dwight Whitlock","company":"Clinician Nexus","job_title":"Data Platform Architect","bio":"Dwight Whitlock is a data and infrastructure engineer leading the Data Platform team at Clinician Nexus. He architects scalable solutions across Python, Spark, Terraform, and Kubernetes, and is driving the firm-wide rollout of a data mesh strategy. His work empowers over 300 analytic users by streamlining data ingestion, schema validation, quality enforcement, and BI acceleration in Databricks. Dwight is passionate about metadata governance, discoverability, and building tools that allow business teams to manage their own data lifecycles.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_1746109061495001EcDH.png?h=fbf7a813&itok=Kih_mgzY","alt":"Dwight Whitlock"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/unifying-human-curated-data-ingestion-and-real-time-updates-databricks","alias":"\/data-ai-summit-2025\/session\/unifying-human-curated-data-ingestion-and-real-time-updates-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535665+00:00"}
{"id":"D25B1237","title":"Unlock Your Use Cases: A Deep Dive on Structured Streaming\u2019s New TransformWithState API","description":"<p>Don\u2019t you just hate telling your customers \u201cNo\u201d? \u201cNo, I can\u2019t get you the data that quickly\u201d, or \u201cNo that logic isn\u2019t possible to implement\u201d really aren\u2019t fun to say. But what if you had a tool that would allow you to implement those use cases? What if it was in a technology you were already familiar with \u2014 say, Spark Structured Streaming? There is a brand new arbitrary stateful operations API called TransformWithState, and after attending this deep dive you won\u2019t have to say \u201cNo\u201d anymore. During this presentation we\u2019ll go through some real-world use cases and build them step-by-step. Everything from state variables, process vs. event time, watermarks, timers, state TTL, and even how you can initialize state with the checkpoint of another stream. Unlock your use cases with the power of Structured Streaming\u2019s TransformWithState!<\/p>","track":"Data Engineering and Streaming","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Angela Chu","company":"Databricks","job_title":"Lead Solutions Architect","bio":"Angela Chu has turned data into information for more than 25 years.  The last three have been spent as a Solutions Architect and Streaming Subject Matter Expert at Databricks.  She enjoys learning about different technologies and solving complex problems with it, and sharing her learnings with her customers and peers.  When not having fun with technology, she is spending time with her family and traveling to different countries so her kids can experience amazing cultures from around the world.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/SelfPicDataAISummit2025_1744047183190001gMwR.jpg?h=f066f77f&itok=iEwn9e6_","alt":"Angela Chu"}},{"name":"Anish Shrigondekar","company":"Databricks","job_title":"Software Engineer","bio":null,"image":{}}],"day":"Tuesday","room":"South, Level 2, Room  211","starts":"2025-06-10T23:10:00","ends":"2025-06-10T23:50:00","starts_pst":"2025-06-10T16:10:00","ends_pst":"2025-06-10T16:50:00","start_time":"11:10 pm","end_time":"11:50 pm","pst_start_time":"4:10 pm","pst_end_time":"4:50 pm","duration":"40","path":"\/session\/unlock-your-use-cases-deep-dive-structured-streamings-new","alias":"\/data-ai-summit-2025\/session\/unlock-your-use-cases-deep-dive-structured-streamings-new","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535695+00:00"}
{"id":"D25B1239","title":"Unlocking Streaming Power: How SEGA Wins With DLT","description":"<p>Streaming data is hard and costly \u2014 that's the default opinion, but it doesn\u2019t have to be.<\/p><p>\u00a0<\/p><p>In this session, discover how SEGA simplified complex streaming pipelines and turned them into a competitive edge. SEGA sees over 40,000 events per second. That's no easy task, but enabling personalised gaming experiences for over 50 million gamers drives a huge competitive advantage. If you\u2019re wrestling with streaming challenges, this talk is your next checkpoint.\u00a0<\/p><p>\u00a0<\/p><p>We\u2019ll unpack how DLT helped SEGA, from automated schema evolution and simple data quality management to seamless streaming reliability. Learn how DLT drives value by transforming chaos emeralds into clarity, delivering results for a global gaming powerhouse. We'll step through the architecture, approach and challenges we overcame.<\/p><p>\u00a0<\/p><p>Join Craig Porteous, Microsoft MVP from Advancing Analytics, and Felix Baker, Head of Data Services at SEGA Europe, for a fast-paced, hands-on journey into DLT\u2019s unique powers.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment"],"category":["DLT"],"areas_of_interest":["Data Ingestion, Migrations, Streaming pipelines, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Craig Porteous","company":"Advancing Analytics","job_title":"Associate Head of Data Engineering","bio":"I'm a lifelong learner with a passion for creative problem-solving. I share my expertise and insights with the data community through talks, video content, and bringing people together with the DATA:Scotland event.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/LUV_9643_cropped_1743755294402001XWIS.jpg?h=03b94cdf&itok=8koe4L0u","alt":"Craig Porteous"}},{"name":"Felix Baker","company":"SEGA Europe Limited","job_title":"Head of Data Services","bio":"Felix Baker manages a team of passionate data scientists and engineers who develop and maintain several cutting-edge data technology projects across SEGA. Databricks forms the backbone of SEGA\u2019s data platform, Felix\u2019s team utilises Databricks to underpin a variety of streaming, machine learning and LLM use cases.<br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-04-22%2520at%25202.15.02%25E2%2580%25AFPM_1745356512467001K5Y8.png?h=a146e9dd&itok=vUtcuigj","alt":"Felix Baker"}}],"day":"Wednesday","room":"South, Level 2, Room 206","starts":"2025-06-11T18:30:00","ends":"2025-06-11T19:10:00","starts_pst":"2025-06-11T11:30:00","ends_pst":"2025-06-11T12:10:00","start_time":"6:30 pm","end_time":"7:10 pm","pst_start_time":"11:30 am","pst_end_time":"12:10 pm","duration":"40","path":"\/session\/unlocking-streaming-power-how-sega-wins-dlt","alias":"\/data-ai-summit-2025\/session\/unlocking-streaming-power-how-sega-wins-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535719+00:00"}
{"id":"D25B1368","title":"Using Delta-rs and Delta-Kernel-rs to Serve CDC Feeds","description":"<p>Change data feeds are a common tool for synchronizing changes between tables and performing data processing in a scalable fashion. Serverless architectures offer a compelling solution for organizations looking to avoid the complexity of managing infrastructure. But how can you bring CDFs into a serverless environment? In this session, we'll explore how to integrate Change Data Feeds into serverless architectures using Delta-rs and Delta-kernel-rs\u2014open-source projects that allow you to read Delta tables and their change data feeds in Rust or Python. We\u2019ll demonstrate how to use these tools with Lakestore\u2019s serverless platform to easily stream and process changes.<\/p><p>\u00a0<\/p><p>You\u2019ll learn how to:<\/p><ol>\t<li>Leverage Delta tables and CDFs in serverless environments<\/li>\t<li>Utilize Databricks and Unity Catalog without needing Apache Spark<\/li><\/ol>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Delta Lake, LakeFlow, Unity Catalog"],"areas_of_interest":["Catalogs, Data Ingestion, ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Stephen Carman","company":"Databricks","job_title":"Senior Resident Solutions Architect","bio":"Steve Carman is currently a Resident Solutions Architect at Databricks.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Carman_Stephen_Default_Badges_Headshot_1744055303089001bytc.jpg?h=c1282a6a&itok=Xkg7rDhS","alt":"Stephen Carman"}},{"name":"Oussama Saoudi","company":"Databricks","job_title":"Software Engineer","bio":"Oussama works for Databricks on delta-kernel-rs, a native Delta implementation that integrates with any query engine.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/E02727P8HV4-U07KAG9GKTQ-c26d76195e6f-512_1737741483402001Pg81.png?h=a7ffc51c&itok=fiCsnsbS","alt":"Oussama Saoudi"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/using-delta-rs-and-delta-kernel-rs-serve-cdc-feeds","alias":"\/data-ai-summit-2025\/session\/using-delta-rs-and-delta-kernel-rs-serve-cdc-feeds","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535755+00:00"}
{"id":"D25B3272DBX","title":"What\u2019s New in Apache Spark\u2122 4.0?","description":"<p>Join this session for a concise tour of Apache Spark\u2122 4.0\u2019s most notable enhancements:<\/p><ul>\t<li>SQL features: ANSI by default, scripting, SQL pipe syntax, SQL UDF, session variable, view schema evolution, etc.<\/li>\t<li>Data type: VARIANT type, string collation<\/li>\t<li>Python features: Python data source, plotting API, etc.<\/li>\t<li>Streaming improvements: State store data source, state store checkpoint v2, arbitrary state v2, etc.<\/li>\t<li>Spark Connect improvements: More API coverage, thin client, unified Scala interface, etc.<\/li>\t<li>Infrastructure: Better error message, structured logging, new Java\/Scala version support, etc.<\/li><\/ul><p>\u00a0<\/p><p>Whether you\u2019re a seasoned Spark user or new to the ecosystem, this talk will prepare you to leverage Spark 4.0\u2019s latest innovations for modern data and AI pipelines.<\/p>","track":"Data Engineering and Streaming","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Databricks Experience (DBX), ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Wenchen Fan","company":"Databricks","job_title":"Senior Staff Software Engineer","bio":"Wenchen Fan is a software engineer at Databricks, working on Spark Core and Spark SQL. He mainly focuses on the Apache Spark open-source community, leading the discussion and reviews of many features\/fixes in Spark. He is a Spark committer and a Spark PMC member.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/whats-new-apache-sparktm-40","alias":"\/data-ai-summit-2025\/session\/whats-new-apache-sparktm-40","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535782+00:00"}
{"id":"D25L3015","title":"Why You Should Move to DLT Serverless","description":"<p>DLT Serverless offers a range of benefits that make it an attractive option for organizations looking to optimize their ETL (Extract, Transform, Load) processes.Key benefits of DLT Serverless:<\/p><ol>\t<li>Automatic infrastructure management<\/li>\t<li>Unified batch and streaming<\/li>\t<li>Cost and performance optimization<\/li>\t<li>Simplified configuration<\/li>\t<li>Granular observability<\/li><\/ol><p>\u00a0<\/p><p>By moving to DLT Serverless, organizations can achieve faster, more reliable, and cost-effective data pipeline management, ultimately driving better business insights and outcomes.<\/p>","track":"Data Engineering and Streaming","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["DLT"],"areas_of_interest":["Data Intelligence, Developer Experience, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Nandini N","company":"Databricks","job_title":"Sr Backline Technical Solutions Engineer","bio":"Nandini is currently working as Backline Technical Solutions Engineer at Databricks where in she helps solve complex customer issues spanning across different Databricks components. Her areas of expertise are DBSQL and Spark SQL. Nandini has 10+ years of experience in Big Data Industry. Previously, she has worked with Cloudera, Walmart and Informatica in different roles. In her previous role, she was a Backline Engineer for Kafka in Cloudera, in Walmart she was part of the Data Lake team and in Informatica she worked on creating a streaming ingestion software. Apart from work, she likes to paint and go for nature walks with her dog. She is passionate about social causes. Previously, she also took a sabbatical to work with a NGO for a year. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_4067_1744872396321001Y0n5.jpg?h=0cc1ff48&itok=9a_QKi3m","alt":"Nandini N"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/why-you-should-move-dlt-serverless","alias":"\/data-ai-summit-2025\/session\/why-you-should-move-dlt-serverless","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535812+00:00"}
