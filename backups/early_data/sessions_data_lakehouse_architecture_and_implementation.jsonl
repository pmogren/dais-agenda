{"id":"D25B3167DBX","title":"A Practitioner\u2019s Guide to Databricks Serverless","description":"<p>Databricks Serverless revolutionizes data engineering and analytics by eliminating the complexities of infrastructure management. This talk will provide an overview of this powerful serverless compute option, highlighting how it enables practitioners to focus solely on building robust data pipelines. We'll explore the core benefits, including automatic scaling, cost optimization and seamless integration with the Databricks ecosystem. Learn how serverless workflows simplify the orchestration of various data tasks, from ingestion to dashboards, ultimately accelerating time-to-insight and boosting productivity. This session is ideal for data engineers, data scientists and analysts looking to leverage the agility and efficiency of serverless computing in their data workflows.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, DLT"],"areas_of_interest":["Databricks Experience (DBX), Getting started with Databricks"],"delivery":["In Person"],"speakers":[{"name":"Prashanth Babu Velanati Venkata","company":"Databricks","job_title":"Product Specialist","bio":"Prashanth is a Lead Product Specialist Solutions Architect at Databricks. He has more than a decade experience working on Big Data. Prior to that for a decade, he was a Java architect for a product. He has been using Spark since 2014 and joined Databricks in Sep 2018. He focuses on all things Data Engineering working closely with both the Product Management and the (EMEA) Field Engineering teams. Also leads EMEA Delta and Performance SME at Databricks and has been working with many enterprises advising them on Databricks Lakehouse best practices and guiding them expedite build, productionize and deploy their pipelines at scale.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/practitioners-guide-databricks-serverless","alias":"\/data-ai-summit-2025\/session\/practitioners-guide-databricks-serverless","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533663+00:00"}
{"id":"D25B1829","title":"A Prescription for Success: Leveraging DABs for Faster Deployment and Better Patient Outcomes","description":"<p>Health Catalyst (HCAT) transformed its CI\/CD strategy by replacing a rigid, internal deployment tool with Databricks Asset Bundles (DABs), unlocking greater agility and efficiency. This shift streamlined deployments across both customer workspaces and HCAT's core platform, accelerating time to insights and driving continuous innovation. By adopting DABs, HCAT ensures feature parity, standardizes metric stores across clients, and rapidly delivers tailored analytics solutions. Attendees will gain practical insights into modernizing CI\/CD pipelines for healthcare analytics, leveraging Databricks to scale data-driven improvements.\u00a0<\/p><p>\u00a0<\/p><p>HCAT's next-generation platform, Health Catalyst Ignite\u2122, integrates healthcare-specific data models, self-service analytics, and domain expertise\u2014powering faster, smarter decision-making.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences"],"category":["Databricks Workflows, Unity Catalog"],"areas_of_interest":["Collaboration, Developer Experience, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Alex Owen","company":"Databricks","job_title":"Sr. Solutions Architect","bio":"Alex is a Senior Solution Architect at Databricks, bringing more than a decade of experience in data engineering use cases. Prior to his role at Databricks, he specialized in utilizing IoT data for safety and compliance purposes within the oil and gas sector. Alex is passionate about all things data and enjoys seeing Databricks in action making an impact across multiple industries.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/alex3_1745543199457001bKG9.jpg?h=76cab243&itok=iAN_ITjP","alt":"Alex Owen"}},{"name":"Brendon Allen","company":"Health Catalyst","job_title":"Principal Data Engineer","bio":"Brendon Allen has spent his career building data and analytics systems that drive real, measurable impact in healthcare. He\u2019s passionate about learning, teaching, and making data more useful for everyone. Grateful for the teams, mentors, and communities who\u2019ve supported him, he\u2019s always looking to pay it forward. He\u2019s especially thankful to the those who made this opportunity possible. In winter, you\u2019ll find him skiing any chance he can get; this year, he\u2019s trying mountain biking to fill the long warm void that is summer. Brendon\u2019s excited to connect and contribute. Continuing technical interests include blending the spectrum of data and software engineering, ML\/AI, & all things data governance.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Brendon_Allen_1746080134313001QcLR.PNG?h=04af0592&itok=CBkLOUs5","alt":"Brendon Allen"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/prescription-success-leveraging-dabs-faster-deployment-and-better","alias":"\/data-ai-summit-2025\/session\/prescription-success-leveraging-dabs-faster-deployment-and-better","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533667+00:00"}
{"id":"D25B1453","title":"A Unified Solution for Data Management and Model Training With Apache Iceberg and Mosaic Streaming","description":"<p>This session introduces ByteDance\u2019s challenges in data management and model training, and addresses them by Magnus (enhanced Apache Iceberg) and Byted Streaming (customized Mosaic Streaming).<\/p><p>\u00a0<\/p><p>Magnus uses Iceberg\u2019s branch\/tag to manage massive datasets\/checkpoints efficiently. With enhanced metadata and a custom C++ data reader, Magnus achieves optimal sharding, shuffling and data loading. Flexible table migration, detailed metrics and built-in full-text indexes on Iceberg tables further ensure training reliability.<\/p><p>\u00a0<\/p><p>When training with ultra-large datasets, ByteDance faced scalability and performance issues. Given Streaming's scalability in distributed training and good code structure, the team chose and customized it to resolve challenges like slow startup, high resource consumption, and limited data source compatibility.<\/p><p>\u00a0<\/p><p>In this session, we will explore Magnus and Byted Streaming, discuss their enhancements and demonstrate how they enable efficient and robust distributed training.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Media and Entertainment, Public Sector"],"category":["Apache Iceberg, Mosaic AI"],"areas_of_interest":["Data Intelligence, Machine Learning, Open Source, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Zilong Zhou","company":"ByteDance","job_title":"Infrastructure Engineer","bio":null,"image":{}},{"name":"jia wei","company":"ByteDance","job_title":"machine learning system engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/unified-solution-data-management-and-model-training-apache-iceberg-and","alias":"\/data-ai-summit-2025\/session\/unified-solution-data-management-and-model-training-apache-iceberg-and","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533670+00:00"}
{"id":"D25B3170DBX","title":"Advanced Governance and Auth With Databricks Apps","description":"<p>Explore advanced governance and authentication patterns for building secure, enterprise-grade apps with Databricks Apps. Learn how to configure complex permissions and manage access control using Unity Catalog. We\u2019ll dive into \u201con-behalf-of-user\u201d authentication \u2014 allowing agents to enforce user-specific access controls \u2014 and cover API-based authentication, including PATs and OAuth flows for external integrations. We\u2019ll also highlight how Addepar uses these capabilities to securely build and scale applications that handle sensitive financial data. Whether you're building internal tools or customer-facing apps, this session will equip you with the patterns and tools to ensure robust, secure access in your Databricks apps.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Unity Catalog, Databricks Apps"],"areas_of_interest":["Databricks Experience (DBX), Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Andre Furlan Bueno","company":"Databricks","job_title":"Staff Software Engineer","bio":"Andre is a Staff Software Engineer at Databricks, where he serves as the technical lead for Databricks Apps.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/mug_shot_1744843413616001K3Md.png?h=fa5d59fe&itok=tc_h6gaL","alt":"Andre Furlan Bueno"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/advanced-governance-and-auth-databricks-apps","alias":"\/data-ai-summit-2025\/session\/advanced-governance-and-auth-databricks-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533706+00:00"}
{"id":"D25B3066","title":"AI Powering Epsilon's Identity Strategy: Unified Marketing Platform on Databricks","description":"<p>Join us to hear about how Epsilon Data Management migrated Epsilon\u2019s unique, AI-powered marketing identity solution from multi-petabyte on-prem Hadoop and data warehouse systems to a unified Databricks Lakehouse platform. This transition enabled Epsilon to further scale its Decision Sciences solution and enable new cloud-based AI research capabilities on time and within budget, without being bottlenecked by the resource constraints of on-prem systems.<\/p><p>\u00a0<\/p><p>Learn how Delta Lake, Unity Catalog, MLflow and LLM endpoints powered massive data volume, reduced data duplication, improved lineage visibility, accelerated Data Science and AI, and enabled new data to be immediately available for consumption by the entire Epsilon platform in a privacy-safe way. Using the Databricks platform as the base for AI and Data Science at global internet scale, Epsilon deploys marketing solutions across multiple cloud providers and multiple regions for many customers.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment"],"category":["Delta Lake, MLFlow, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Science, Generative AI (LLMs), Marketing"],"delivery":["In Person"],"speakers":[{"name":"Gairik Chakraborty","company":"Epsilon Data Management","job_title":"Vice President , Database","bio":"Gairik Chakraborty is a seasoned technology leader with over two decades of experience in designing and deploying large-scale enterprise solutions. As the Vice President of Database at Epsilon, he spearheads the development of scalable database systems, Big Data platforms, and AI-driven solutions in multi-cloud environments, powering cutting-edge AdTech and MarTech applications. He serves on various advisory boards for technology organizations like Oracle , AWS , Databricks, Elastic and speaks at global conferences like AWS Re:Invent , Oracle OpenWorld etc.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Gairik_1745817342249001Y51J.jpg?h=70217200&itok=1KSydcPM","alt":"Gairik Chakraborty"}},{"name":"Boaz Super","company":"Epsilon Data Management","job_title":"Vice President, Decision Sciences","bio":"Leader of technology teams and driver of strategic initiatives, delivering business impact from internet-scale data science, AI, and analytics. Delivered zero-to-one initiatives and continuous-impact streams.<br \/>\n<br \/>\nStrong autodidact, highly portable experience, delivered technology and business innovation in six verticals: retail, manufacturing, adtech\/martech, consumer electronics, TV, public safety.<br \/>\n<br \/>\n30 peer-reviewed publications and 19 patents.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/ai-powering-epsilons-identity-strategy-unified-marketing-platform","alias":"\/data-ai-summit-2025\/session\/ai-powering-epsilons-identity-strategy-unified-marketing-platform","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533737+00:00"}
{"id":"D25B1682","title":"Apache Iceberg with Unity Catalog at HelloFresh","description":"<p>Table formats like Delta Lake and Iceberg have been game changers for pushing lakehouse architecture into modern Enterprises. The acquisition of Tabular added Iceberg to the Databricks ecosystem, an open format that was already well supported by processing engines across the industry. At HelloFresh we are building a lakehouse architecture that integrates many touchpoints and technologies all across the organization. As such we chose Iceberg as the table format to bridge the gaps in our decentralized managed tech landscape. We are leveraging Unity Catalog as the Iceberg REST catalog of choice for storing metadata and managing tables. In this talk we will outline our architectural setup between Databricks, Spark, Flink and Snowflake and will explain the native Unity Iceberg REST catalog, as well as catalog federation towards connected engines. We will highlight the impact on our business and discuss the advantages and lessons learned from our early adopter experience.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Retail and CPG - Food"],"category":["Apache Spark, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Data Ingestion, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Max Schultze","company":"HelloFresh","job_title":"Director of Data Engineering","bio":"Max Schultze is the director of data engineering at the data platform of HelloFresh, the world's leading meal kit company and global integrated food solutions group. His focus lies on offering company wide platform solutions around data infrastructure, usability, and governance, to enable even less technical people to get value out of the organization's data. Previously he was working as engineering manager at Zalando were he was building data pipelines at petabytes scale, productionizing distributed processing engines, and providing services and tooling for data management.<br \/>\nMax originally graduated from Humboldt University of Berlin, actively taking part in the university\u2019s initial development of Apache Flink.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/me_2_1745397800041001QeKI.jpg?h=a7ffc51c&itok=18eleSEy","alt":"Max Schultze"}},{"name":"Adam Komisarek","company":"HelloFresh","job_title":"Senior Staff Data Engineer","bio":"Adam studied computer science in Poland\/Poznan, and started working there in small startup building computer assisted translation tools. <br \/>\n<br \/>\nOver the years, Adam experienced all different assets of software delivery cycle - starting with frontend, and quickly jumped to the backend, ended up after years in Data. Now he works at HelloFresh, changing how people eat forever, and building great Data Platform for the internal users. <br \/>\n<br \/>\nHis current interests include data processing tools such as Spark and Flink, ensuring users can access data easily and securely from various sources. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/shot_1746031589098001R4CZ.jpg?h=4115f0c9&itok=zjUjrlq9","alt":"Adam Komisarek"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/apache-iceberg-unity-catalog-hellofresh","alias":"\/data-ai-summit-2025\/session\/apache-iceberg-unity-catalog-hellofresh","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533789+00:00"}
{"id":"D25B2525","title":"Breaking Silos: Enabling Databricks-Snowflake Interoperability With Iceberg and Unity Catalog","description":"<p>As data ecosystems grow more complex, organizations often struggle with siloed platforms and fragmented governance. In this session, we\u2019ll explore how our team made Databricks the central hub for cross-platform interoperability, enabling seamless Snowflake integration through Unity Catalog and the Iceberg REST API.\u00a0<\/p><p>\u00a0<\/p><p>We\u2019ll cover:\u00a0<\/p><ul>\t<li>Why interoperability matters and the business drivers behind our approach\u00a0<\/li>\t<li>How Unity Catalog and Uniform simplify interoperability, allowing Databricks to expose an Iceberg REST API for external consumption\u00a0<\/li>\t<li>Technical deep dive into data sharing, query performance, and access control across Databricks and Snowflake\u00a0<\/li>\t<li>Lessons learned and best practices for building a multi-engine architecture while maintaining governance and efficiency\u00a0<\/li><\/ul><p>\u00a0<\/p><p>By leveraging Uniform, Delta, and Iceberg, we created a flexible, vendor-agnostic architecture that bridges Databricks and Snowflake without compromising performance or security.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Energy and Utilities"],"category":["Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Applications"],"delivery":["In Person"],"speakers":[{"name":"Geoffrey Freeman","company":"T-Mobile","job_title":"Director of Data Engineering","bio":"Geoffrey Freeman has spent most of his career working in massive scale online data delivery services. He is currently the Director of Data Engineering for the T-Mobile Finance division.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Freeman_Geoffrey_1744656642384001SMhb.jpg?h=b044a8f9&itok=ED9clEfe","alt":"Geoffrey Freeman"}},{"name":"Mohit Kumar","company":"T-Mobile","job_title":"Member of Technical Staff, Sol Arch","bio":"Mohit has 12 years of experience in delivering analytical solutions and advanced BI & Data Science platforms. For the last 7 years he\u2019s been at T-Mobile, leading architecture and implementation of hybrid data and AI platforms across multiple lines of business. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_8508_1744657977904001fnUJ.jpg?h=ef93b635&itok=rfvM01-E","alt":"Mohit Kumar"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/breaking-silos-enabling-databricks-snowflake-interoperability-iceberg","alias":"\/data-ai-summit-2025\/session\/breaking-silos-enabling-databricks-snowflake-interoperability-iceberg","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533849+00:00"}
{"id":"D25B2318","title":"Breaking the Ice(berg): Riskified\u2019s Journey to its Next-Generation Lakehouse","description":"<p>How many of you manage multiple data stores in your organization, wrestling with different data formats that led to data duplication, infrastructure redundancy, and uncontrolled costs? Imagine reducing these costs by 25% while maintaining your existing SLAs \u2014 this is exactly what we achieved with our next-generation architecture.<\/p><p>\u00a0<\/p><p>In this session, we'll show you how we built Riskified's next-generation lakehouse by leveraging Databricks' native Apache Iceberg support and Unity Catalog. We'll share our innovative approach to cross-platform querying without data duplication, and how we transformed our data warehouse into a modern lakehouse architecture. Throughout the session, we'll explore the technical challenges we conquered, from data migration to performance optimization. The result? A simplified world where everything is identical across engines, leaving users with just one choice \u2014 which query engine best suits their use case.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Customer Data Platform, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Hen Ben Hemo","company":"Riskified","job_title":"Data Platform Architect","bio":"Hen is a Data Platform Architect and Spark Advocate at Riskified, where he is leading the next-generation data platform architecture. With a career rich in experience tackling scale and data challenges over the last decade, Hen has skillfully embedded data technologies into the fabric of Riskified\u2019s innovative data solutions. His passion for leveraging cutting-edge technologies is matched by his commitment to the community, where he is dedicated to sharing knowledge and fostering creative problem-solving. Outside of his professional endeavors, Hen is a flight simulator amateur and a food enthusiast, reveling in the joy of traveling with his family.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Chen8184Options_1744103603736001OC4u.jpg?h=6509df0c&itok=Xe-6O9IV","alt":"Hen Ben Hemo"}},{"name":"Yoni Eilon","company":"Riskified","job_title":"Staff Data Platform Engineer","bio":"Yoni is an accomplished DBA and Data Engineer with a wealth of experience across various database systems and platforms. His expertise spans from relational databases to NoSQL and cloud-based solutions, as well as DWH systems and Spark.<br \/>\nWhen he's not helping Riskified manage its Data Platform, Yoni enjoys spending time with his wife and 3 kids, playing basketball and cooking for friends and family.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Yoni%2520Eilon2%25202022_1738611974641001GzYq.jpg?h=c2e81e0a&itok=zq9Pps-q","alt":"Yoni Eilon"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse","alias":"\/data-ai-summit-2025\/session\/breaking-iceberg-riskifieds-journey-its-next-generation-lakehouse","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533856+00:00"}
{"id":"D25B3169DBX","title":"Build AI-Powered Applications Natively on Databricks","description":"<p>Discover how to build and deploy AI-powered applications natively on the Databricks Data Intelligence Platform. This session introduces best practices and a standard reference architecture for developing production-ready apps using popular frameworks like Dash, Shiny, Gradio, Streamlit and Flask. Learn how to leverage agents for orchestration and explore primary use cases supported by Databricks Apps, including data visualization, AI applications, self-service analytics and data quality monitoring. With serverless deployment and built-in governance through Unity Catalog, Databricks Apps enables seamless integration with your data and AI models, allowing you to focus on delivering impactful solutions without the complexities of infrastructure management. Whether you're a data engineer or an app developer, this session will equip you with the knowledge to create secure, scalable and efficient applications within a Databricks environment.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["AI\/BI, Unity Catalog, Databricks Apps"],"areas_of_interest":["Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Andre Furlan Bueno","company":"Databricks","job_title":"Staff Software Engineer","bio":"Andre is a Staff Software Engineer at Databricks, where he serves as the technical lead for Databricks Apps.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/mug_shot_1744843413616001K3Md.png?h=fa5d59fe&itok=tc_h6gaL","alt":"Andre Furlan Bueno"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/build-ai-powered-applications-natively-databricks","alias":"\/data-ai-summit-2025\/session\/build-ai-powered-applications-natively-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533873+00:00"}
{"id":"D25B2847","title":"Building a Seamless Multi-Cloud Platform for Secure Portable Workloads","description":"<p>There are many challenges to making a data platform actually a platform, something that hides complexity. Data engineers and scientists are looking for a simple and intuitive abstraction to focus on their work, not where it runs to maintain compliance, what credentials it uses to access data or how it generates operational telemetry. At Databricks we\u2019ve developed a data-centric approach to workload development and deployment that enables data workers to stop doing migrations and instead develop with confidence. Attend this session to learn how to run simple, secure and compliant global multi-cloud workloads at scale on Databricks.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences, Public Sector"],"category":["Delta Lake, Delta Sharing, Unity Catalog"],"areas_of_interest":["ETL, Thought Leadership, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"James Burns","company":"Databricks","job_title":"Staff Software Engineer","bio":"James has spent more than two decades working at every layer of technology from ASICs to large scale distributed systems. Recently at Twilio, StitchFix, LightStep, and Snowflake, James has worked on secure and scalable observability and data handling. At Databricks, James leads initiatives related to security, privacy, and operational excellence within the Data Platform team handling petabytes of data daily across the globe.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/James_Burns_1745619123313001ikqe.png?h=cdd683f9&itok=gqloMl8n","alt":"James Burns"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/building-seamless-multi-cloud-platform-secure-portable-workloads","alias":"\/data-ai-summit-2025\/session\/building-seamless-multi-cloud-platform-secure-portable-workloads","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533890+00:00"}
{"id":"D25L2395","title":"Capitalizing Alternatives Data on the Addepar Platform: Private Markets Benchmarking","description":"<p>Addepar possesses an enormous private investment data set with 40% of the $7T assets on the platform allocated to alternatives. Leveraging the Addepar Data Lakehouse (ADL), built on Databricks, we have built a scalable data pipeline that assesses millions of private fund investment cash flows and translates it to a private fund benchmarks data offering. Investors on the Addepar platform can leverage this data seamlessly integrated against their portfolio investments and obtain actionable investment insights. At a high-level, this data offering consists of an extensive data aggregation, filtering, and construction logic that dynamically updates for clients through the Databricks job workflows. This derived dataset has gone through several iterations with investment strategists and academics that leveraged delta shared tables. Irrespective of the data source, the data pipeline coalesces all relevant cash flow activity against a unique identifier before constructing the benchmarks.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Financial Services"],"category":["Delta Lake, Databricks Workflows, Delta Sharing"],"areas_of_interest":["Data Intelligence, Data Science"],"delivery":["In Person"],"speakers":[{"name":"Ricky D'Sa","company":"Addepar","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/capitalizing-alternatives-data-addepar-platform-private-markets","alias":"\/data-ai-summit-2025\/session\/capitalizing-alternatives-data-addepar-platform-private-markets","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.533947+00:00"}
{"id":"D25B2160","title":"Cutting Costs, Not Performance: Optimizing Databricks at Scale","description":"<p>As Databricks transforms data processing, analytics and machine learning, managing platform costs has become crucial for organizations aiming to maximize value while staying within budget. While Databricks offers unmatched scalability and performance, inefficient usage can lead to unexpected cost overruns.<\/p><p>\u00a0<\/p><p>This presentation will explore common challenges organizations face in controlling Databricks costs and provide actionable best practices for optimizing resource allocation, preventing over-provisioning and eliminating underutilization.<\/p><p>\u00a0<\/p><p>Drawing from NTT DATA\u2019s experience, I'll share how we reduced Databricks costs by up to 50% through strategies like choosing the right compute resource, leveraging manage tables and using Unity Catalog features, such as system tables, to monitor consumption.<\/p><p>\u00a0<\/p><p>Join this session to gain practical insights and tools that will empower your team to optimize Databricks without overspending.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Energy and Utilities, Public Sector, Travel and Hospitality"],"category":["Databricks SQL, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Applications"],"delivery":["In Person"],"speakers":[{"name":"Pedro Ferreira","company":"NTTDATA","job_title":"Project Manager","bio":"Over the past three years, I have played a key role in driving successful Databricks implementations across multiple organizations, delivering impactful outcomes. As the leader of NTT DATA Portugal\u2019s Databricks practice, I consistently advocate for the platform by coordinating learning initiatives, promoting its adoption, and identifying new opportunities to expand its value. Beyond my leadership within the company, I am deeply engaged with the broader Databricks community. I have authored multiple articles on Databricks features and best practices, sharing insights to help others navigate the platform effectively. My contributions and expertise have been recognized by Databricks, earning me the title of Solution Architect Champion.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/foto_formal_3_1745248563665001ng7k.JPG?h=26fb3927&itok=4Rxp7I89","alt":"Pedro Ferreira"}},{"name":"Artur Sim\u00f5es","company":"NTT DATA","job_title":"Lead Engineer","bio":"Artur is a leading expert on Databricks ML Engineering and Mosaic AI, playing a pivotal role in driving innovation and best practices in the cutting edge fields of AI and GenAI. He is currently leading a pioneering project focused on implementing an end-to-end MLOps and LLMOps platform with Databricks, aiming to operationalize AI at scale for a leading public sector organization.<br \/>\nIn addition to his project work, he is deeply involved in learning initiatives, with one of his key missions being to elevate knowledge and expertise within NTT DATA.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/profile_picture_1745514306389001im99.jpg?h=fbf7a813&itok=ew0Vo97Q","alt":"Artur Sim\u00f5es"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/cutting-costs-not-performance-optimizing-databricks-scale","alias":"\/data-ai-summit-2025\/session\/cutting-costs-not-performance-optimizing-databricks-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534010+00:00"}
{"id":"D25B1465","title":"Daft and Unity Catalog: A Multimodal\/AI-Native Lakehouse","description":"<p>Modern data organizations have moved beyond big data analytics to also incorporate advanced AI\/ML data workloads. These workflows often involve multimodal datasets containing documents, images, long-form text, embeddings, URLs and more. Unity Catalog is an ideal solution for organizing and governing this data at scale. When paired with the Daft open source data engine, you can build a truly multimodal, AI-ready data lakehouse. In this session, we\u2019ll explore how Daft integrates with Unity Catalog\u2019s core features (such as volumes and functions) to enable efficient, AI-driven data lakehouses. You will learn how to ingest and process multimodal data (images, text and videos), run AI\/ML transformations and feature extractions at scale, and maintain full control and visibility over your data with Unity Catalog\u2019s fine-grained governance.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["AI\/BI, Unity Catalog"],"areas_of_interest":["Catalogs, Developer Experience, Generative AI (LLMs), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Jay Chia","company":"Eventual","job_title":"Co-Founder","bio":"Jay is a co-founder of Eventual and maintainer of Daft, the distributed Python query engine. Prior to Eventual, Jay worked in biotech and self-driving on large-scale AI data platforms.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/jay_1745948202836001OTCO.png?h=7f412c8f&itok=co5rw8qE","alt":"Jay Chia"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/daft-and-unity-catalog-multimodalai-native-lakehouse","alias":"\/data-ai-summit-2025\/session\/daft-and-unity-catalog-multimodalai-native-lakehouse","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534013+00:00"}
{"id":"D25B1500","title":"Data Modeling 101 for Data Lakehouse Demystified","description":"<p>In today\u2019s data-driven world, the Data Lakehouse has emerged as a powerful architectural paradigm that unifies the flexibility of data lakes with the reliability and structure of traditional data warehouses. However, organizations must adopt the right data modeling techniques to unlock its full potential to ensure scalability, maintainability and efficiency.<\/p><p>\u00a0<\/p><p>This session is designed for beginners looking to demystify the complexities of data modeling for the lakehouse and make informed design decisions. We\u2019ll break down Medallion Architecture, explore key data modeling techniques and walk through the maturity stages of a successful data platform \u2014 transitioning from raw, unstructured data to well-organized, query-efficient models.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Apache Spark, Databricks SQL, DLT"],"areas_of_interest":["ETL, Getting started with Databricks, Open Source, SQL"],"delivery":["In Person"],"speakers":[{"name":"Luan Moreno Medeiros Maciel","company":"Pythian","job_title":"Lead Data Engineer","bio":"I\u2019m proud to be a recognized Authority in Data Engineering, helping professionals and companies unlock the power of data to drive success. Over the past decade, I\u2019ve worked on the global stage, solving complex data challenges, mentoring aspiring engineers, and building communities that thrive on collaboration and innovation.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/fotos_perfil_luan1_1744238396466001hoHd.png?h=57024e64&itok=8aIVGnv1","alt":"Luan Moreno Medeiros Maciel"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/data-modeling-101-data-lakehouse-demystified","alias":"\/data-ai-summit-2025\/session\/data-modeling-101-data-lakehouse-demystified","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534044+00:00"}
{"id":"D25B1493","title":"Databricks in Action: Azure\u2019s Blueprint for Secure and Cost-Effective Operations","description":"<p>Erste Group's transition to Azure Databricks marked a significant upgrade from a legacy system to a secure, scalable and cost-effective cloud platform. The initial architecture, characterized by a complex hub-spoke design and stringent compliance regulations, was replaced with a more efficient solution. The phased migration addressed high network costs and operational inefficiencies, resulting in a 60% reduction in networking costs and a 30% reduction in compute costs for the central team. This transformation, completed over a year, now supports real-time analytics, advanced machine learning and GenAI while ensuring compliance with European regulations. The new platform features a Unity Catalogue, separate data catalogs and dedicated workspaces, demonstrating a successful shift to a cloud-based machine learning environment with significant improvements in cost, performance and security.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Financial Services"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Migrations, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Vukola Milenkovic","company":"Erste Group","job_title":"Senior Solution Manager","bio":"Vukola Milenkovic is Senior Solution Manager at Erste Group, responsible for the Data & Analytics platform built on Azure Databricks. He oversees infrastructure that enables over 600 users\u2014including data scientists and ML engineers\u2014to build scalable data pipelines, machine learning models, and GenAI solutions in a secure, regulated environment. With a strong technical foundation as a former data engineer, DevOps engineer, and software developer, he combines deep hands-on expertise with strategic vision. Vukola holds an MBA in Digital Transformation & Data Science, bringing together business insight and technical leadership to drive innovation in financial services through data and AI.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Vukola_Milenkovic_1744867742800001KBGY.png?h=64e0b01d&itok=irSRyWic","alt":"Vukola Milenkovic"}},{"name":"Oliuver Schluga","company":"Erste Group","job_title":"Information Security Specialist Expert","bio":"Oliver Schluga is an accomplished Information Security Expert at Erste Digital with over 20 years of experience in IT, specializing in cloud security. He enhances Erste Group's digital security, particularly on cloud platforms.<br \/>\nOliver holds a Master's in Cloud Computing Engineering and a Bachelor's in IT Infrastructure Management. His expertise is further solidified by a background in Technical Mathematics, specializing in Cryptography.<br \/>\nCertified by Microsoft and AWS, Oliver excels in Cloud Computing, IT Infrastructure, Communication, and Security. He has presented at international conferences like TechConf Austria, and his research focuses on information security, Industry 4.0, AI, and IoT, with several IEEE publications.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-04-30%2520at%25203.23.23%25E2%2580%25AFPM_1746051841347001uZIe.png?h=5a4e26cc&itok=RKX6bqU4","alt":"Oliuver Schluga"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/databricks-action-azures-blueprint-secure-and-cost-effective-operations","alias":"\/data-ai-summit-2025\/session\/databricks-action-azures-blueprint-secure-and-cost-effective-operations","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534084+00:00"}
{"id":"D25B1116","title":"Databricks, the Good, the Bad and the Ugly","description":"<p>Databricks is the bestest platform ever where everything is perfect and nothing else could ever make it any better, right?<\/p><p>\u00a0<\/p><p>\u2026right?<\/p><p>\u00a0<\/p><p>You and I know, this is not true. Don\u2019t get me wrong, there are features that I absolutely love, but there are also some that require powering through the papercuts. And then there are those that I pretend don\u2019t exist.<\/p><p>\u00a0<\/p><p>I\u2019ll be opening up to give my honest take on three of each category, why I do (or don\u2019t) like them, and then telling you which talks to attend to find out more.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks SQL, Mosaic AI, Unity Catalog"],"areas_of_interest":["Open Source"],"delivery":["In Person"],"speakers":[{"name":"Holly Smith","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Holly Smith is a developer advocate and multi-award-winning Data and AI expert and tech educator with over a decade of experience working at the cutting edge of Data and AI in various capacities. One of the first employees at Databricks, she is a renowned public speaker and teacher. She is well known for her YouTube series on AI + data engineering.<br \/>\n","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/databricks-good-bad-and-ugly","alias":"\/data-ai-summit-2025\/session\/databricks-good-bad-and-ugly","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534108+00:00"}
{"id":"D25L3380","title":"Declarative Pipelines \u2014 Ask Us Anything","description":"<p>Join us for an insightful Ask Me Anything (AMA) session on Declarative Pipelines \u2014 a powerful approach to simplify and optimize data workflows. Learn how to define data transformations using high-level, SQL-like semantics, reducing boilerplate code while improving performance and maintainability.<\/p><p>\u00a0<\/p><p>Whether you're building ETL processes, feature engineering pipelines, or analytical workflows, this session will cover best practices, real-world use cases and how Declarative Pipelines can streamline your data applications. Bring your questions and discover how to make your data processing more intuitive and efficient!<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Lightning Talk","industry":["Education, Professional Services, Public Sector"],"category":["Apache Spark"],"areas_of_interest":["Open Source, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Xiao Li","company":"Databricks","job_title":"Engineering Director","bio":"Xiao Li is an Engineering Director at Databricks, an Apache Spark Committer, and a PMC member. He has a deep interest in Spark and database engines. Previously, he was an IBM Master Inventor and an expert in asynchronous database replication and consistency verification. Xiao earned his Ph.D. from the University of Florida in 2011.","image":{}},{"name":"Denny Lee","company":"Databricks","job_title":"PM Director, Developer Relations","bio":null,"image":{}},{"name":"Sandy Ryza","company":null,"job_title":"Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/declarative-pipelines-ask-us-anything","alias":"\/data-ai-summit-2025\/session\/declarative-pipelines-ask-us-anything","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534115+00:00"}
{"id":"D25L1909","title":"Delta and Databricks as a Cost-Effective, Exabyte-Scale, Real-Time Web Application Backend","description":"<p>The Delta Lake architecture promises to provide a single, highly functional, and high-scale copy of data that can be leveraged by a variety of tools to satisfy a broad range of use cases. To date, most use cases have focused on interactive data warehousing, ETL, model training, and streaming. Real-time access is generally delegated to costly and sometimes difficult-to-scale NoSQL, indexed storage, and domain-specific specialty solutions, which provide limited functionality compared to Spark on Delta Lake.<\/p><p>\u00a0<\/p><p>In this session, we will explore the Delta data-skipping and optimization model and discuss how Capital One leveraged it along with Databricks photon and Spark Connect to implement a real-time web application backend. We\u2019ll share how we built a highly-functional and performant security information and event management user experience (SIEM UX) that is cost effective.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology, Financial Services"],"category":["Apache Spark, Delta Lake"],"areas_of_interest":["Cybersecurity, Data Applications, Data Ingestion"],"delivery":["In Person"],"speakers":[{"name":"Scott Schenkein","company":"Capital One Financial","job_title":"VP, Distinguished Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/delta-and-databricks-cost-effective-exabyte-scale-real-time-web","alias":"\/data-ai-summit-2025\/session\/delta-and-databricks-cost-effective-exabyte-scale-real-time-web","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534130+00:00"}
{"id":"D25B3165DBX","title":"Delta Kernel for Rust and Java","description":"<p>Delta Kernel makes it easy for engines and connectors to read and write Delta tables. It supports many Delta features and robust connectors, including DuckDB, Clickhouse, Spice AI and delta-dotnet.\u00a0In this session, we'll cover lessons learned about how to build a high-performance library that lets engines integrate the way they want, while not having to worry about the details of the Delta protocol. We'll talk through how we streamlined the API as well as its changes and underlying motivations.\u00a0We'll discuss some new highlight features like write support, and the ability to do CDF scans. Finally we'll cover the future roadmap for the Kernel project and what you can expect from the project over the coming year.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake"],"areas_of_interest":["Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Nick Lanham","company":"Databricks","job_title":"Code Monkey","bio":"Nick is a long time Databricks employee who has more recently begun work on building the delta kernel in Rust. He is excited to present the work the community has done, and to get feedback as we continue to grow.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/delta-kernel-rust-and-java","alias":"\/data-ai-summit-2025\/session\/delta-kernel-rust-and-java","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534133+00:00"}
{"id":"D25B2820DBX","title":"Delta Lake Liquid Clustering: Lightning-Fast Queries on Massive Datasets","description":"<p>In this presentation, we\u2019ll dive into the power of Liquid Clustering\u2014an innovative, out-of-the-box solution that automatically tunes your data layout to scale effortlessly with your datasets. You\u2019ll get a deep look at how Liquid Clustering works, along with real-world examples of customers leveraging it to unlock blazing-fast query performance on petabyte-scale datasets. We\u2019ll also give you an exciting sneak peek into the roadmap ahead, with upcoming features and enhancements to come.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Retail and CPG - Food, Financial Services"],"category":["Delta Lake"],"areas_of_interest":["Data Intelligence, Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Cindy Jiang","company":"Databricks","job_title":"Product Manager","bio":"Cindy is an Associate Product Manager at Databricks, working on making data management simple and out-of-the-box. Before joining Databricks, Cindy graduated from the M&T Program at the University of Pennsylvania with an MSE and BAS in Computer Science and a BS from Wharton. ","image":{}},{"name":"Rahul Mahadev","company":"Databricks","job_title":"Software Engineer","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/delta-lake-liquid-clustering-lightning-fast-queries-massive-datasets","alias":"\/data-ai-summit-2025\/session\/delta-lake-liquid-clustering-lightning-fast-queries-massive-datasets","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534137+00:00"}
{"id":"D25B2597","title":"Delta Lake on the Data Mesh","description":"<p>Delta Lake has proven to be an excellent storage format. Coupled with the Databricks platform, the storage format has shined as a component of a distributed system on the lakehouse. The pairing of Delta and Spark provides an excellent platform, but users often struggle to perform comparable work outside of the Spark ecosystem. Tools such as delta-rs, Polars and DuckDb have brought access to users outside of Spark, but they are only building blocks of a larger system.<\/p><p>\u00a0<\/p><p>In this 40-minute talk we will demonstrate how users can use data products on the Nextdata OS data mesh to interact with the Databricks platform to drive Delta Lake workflows. Additionally, we will show how users can build autonomous data products that interact with their Delta tables both inside and outside of the lakehouse platform. Attendees will learn how to integrate the Nextdata OS data mesh with the Databricks platform as both an external and integral component.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Public Sector"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["Data Ingestion, Developer Experience, ML\/LLMOps, Open Source"],"delivery":["In Person"],"speakers":[{"name":"KyJah Keys","company":"Nextdata","job_title":"Principal Engineer","bio":"KyJah Keys is a principal engineer at Nextdata, developing the data mesh platform, and a Databricks MVP. He is a contributor to multiple Delta Lake rust projects, and the creator of the .NET implementation of Delta Lake.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_1738626279943001ETX8.jpg?h=12b4c257&itok=tRGOjbz1","alt":"KyJah Keys"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/delta-lake-data-mesh","alias":"\/data-ai-summit-2025\/session\/delta-lake-data-mesh","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534140+00:00"}
{"id":"D25B2868","title":"Delta-Kernel-RS: Unparalleled Interoperability Across Query Engines","description":"<p>Join us as we introduce Delta-Kernel-RS, a new Rust implementation of the Delta Lake protocol designed for unparalleled interoperability across query engines. In this session, we will explore how maintaining a native implementation of the Delta specification \u2014 with native C and C++ FFI support \u2014 can deliver consistent benefits across diverse data processing systems, eliminating the need for repetitive, engine-specific reimplementations. We will dive deep into a real-world case study where a query engine harnessed Delta-Kernel-RS to unlock significant data skipping improvements \u2014 enhancements achieved \u201cfor free\u201d by leveraging the kernel. Attendees will gain insights into the architectural decisions, interoperability strategies and the practical impact of this innovation on performance and development efficiency in modern data ecosystems.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Data Intelligence, Developer Experience, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Zachary Schuermann","company":"databricks","job_title":"Software Engineer\/Ski bum","bio":null,"image":{}},{"name":"Robert Pack","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Robert has extensive experience in designing and implementing Data & AI platforms within large multinational organizations. Through this work he has been an avid contributor to the open lakehouse ecosystem - specifically Delta Lake. Now at Databricks, his focus is entirely facilitating and contributing to the open source ecosystem for building lakehouse architectures.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/robert_pack_1736342796943001ZVBj.png?h=7b4ffee1&itok=kMDovM_k","alt":"Robert Pack"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/delta-kernel-rs-unparalleled-interoperability-across-query-engines","alias":"\/data-ai-summit-2025\/session\/delta-kernel-rs-unparalleled-interoperability-across-query-engines","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534151+00:00"}
{"id":"D25L2669","title":"Disney's Foundational Medallion: A Journey Into Next-Generation Data Architecture","description":"<p>Step into the world of Disney Streaming as we unveil the creation of our Foundational Medallion, a cornerstone in our architecture that redefines how we manage data at scale. In this session, we'll explore how we tackled the multi-faceted challenges of building a consistent, self-service surrogate key architecture \u2014 a foundational dataset for every ingested stream powering Disney Streaming's data-driven decisions. Learn how we streamlined our architecture and unlocked new efficiencies by leveraging cutting-edge Databricks features such as liquid clustering, Photon with dynamic file pruning, Delta's identity column, Unity Catalog and more \u2014 transforming our implementation into a simpler, more scalable solution. Join us on this thrilling journey as we navigate the twists and turns of designing and implementing a new Medallion at scale \u2014 the very heartbeat of our streaming business!<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["Apache Spark, Delta Lake, Unity Catalog"],"areas_of_interest":["Data Ingestion, ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"mark.senerth mark.senerth","company":"Disney","job_title":"Director, Data Engineering","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/disneys-foundational-medallion-journey-next-generation-data","alias":"\/data-ai-summit-2025\/session\/disneys-foundational-medallion-journey-next-generation-data","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534183+00:00"}
{"id":"D25L1369","title":"Doordash Customer 360 Data Store and its Evolution to Become an Entity Management Framework","description":"<p>The \"Doordash Customer 360 Data Store\" represents a foundational step in centralizing and managing customer profile to enable targeting and personalized customer experiences built on Delta Lake. This presentation will explore the initial goals and architecture of the Customer 360 Data Store, its journey to becoming a robust entity management framework, and the challenges and opportunities encountered along the way. We will discuss how the evolution addressed scalability, data governance and integration needs, enabling the system to support dynamic and diverse use cases, including customer lifecycle analytics, marketing campaign targeting using segmentation. Attendees will gain insight into key design principles, technical innovations and strategic decisions that transformed the system into a flexible platform for entity management, positioning it as a critical enabler of data-driven growth at Doordash.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Retail and CPG - Food"],"category":["Apache Spark, Delta Lake"],"areas_of_interest":["Customer Data Platform, ETL, Open Source, Marketing"],"delivery":["In Person"],"speakers":[{"name":"Gowri Shankar","company":"Doordash","job_title":"Data Engineering Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/doordash-customer-360-data-store-and-its-evolution-become-entity","alias":"\/data-ai-summit-2025\/session\/doordash-customer-360-data-store-and-its-evolution-become-entity","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534190+00:00"}
{"id":"D25B1340","title":"Embracing Unity Catalog and Empowering Innovation With Genie Room","description":"<p>Bagelcode, a leader in the social casino industry, has utilized Databricks since 2018 and manages over 10,000 tables via Hive Metastore. In 2024, we embarked on a transformative journey to resolve inefficiencies and unlock new capabilities. Over five months, we redesigned ETL pipelines with Delta Lake, optimized partitioned table logs and executed a seamless migration with minimal disruption. This effort improved governance, simplified management and unlocked Unity Catalog\u2019s advanced features. Post-migration, we integrated the Genie Room with Slack to enable natural language queries, accelerating decision-making and operational efficiency. Additionally, a lineage-powered internal tool allowed us to quickly identify and resolve issues like backfill needs or data contamination. Unity Catalog has revolutionized our data ecosystem, elevating governance and innovation. Join us to learn how Bagelcode unlocked its data\u2019s full potential and discover strategies for your own transformation.\u00a0<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Data Intelligence, Migrations, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Seokyun Ha","company":"Bagelcode","job_title":"Data Engineer","bio":"Seokyoon Ha is a data engineer and the leader of the Data Platform team at Bagelcode. A passionate advocate of open source, he is actively involved in the Apache Spark and Kubernetes ecosystems. He excels at solving complex engineering challenges and is skilled at integrating diverse tools to craft effective solutions. With a vision to make the world a better place through data, he enjoys collaborating with people around the globe to tackle meaningful problems. In his free time, he enjoys playing jazz music and has a strong interest in Texas Hold\u2019em, where he favors a mix of counting and intuition-driven play.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/seokyun_1744250148885001snVE.png?h=88b0eb3f&itok=iL7WocD_","alt":"Seokyun Ha"}},{"name":"Junghoon Lee","company":"Bagelcode","job_title":"Data Engineer","bio":"Junghoon Lee is a Data Engineer at BagelCode, specializing in Databricks, Kubernetes, AWS, and Airflow. He works on DataOps tasks and has developed a FastAPI-based backend data cache layer with Couchbase to optimize data processing and enhance system performance. Junghoon focuses on building scalable and efficient data workflows that drive innovation, improve operational efficiency, and ensure seamless data operations.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/junghoon.lee_headshot_1737633018148001vzQH.png?h=14b9d41a&itok=lFzE40hJ","alt":"Junghoon Lee"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/embracing-unity-catalog-and-empowering-innovation-genie-room","alias":"\/data-ai-summit-2025\/session\/embracing-unity-catalog-and-empowering-innovation-genie-room","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534218+00:00"}
{"id":"D25B1804","title":"Empowering Progress: Building a Personalized Training Goal Ecosystem with Databricks","description":"<p>Tonal Trainer is the world\u2019s most intelligent home gym, combining cutting-edge hardware and sensors with AI-powered software to deliver personalized fitness experiences. Members share needs with us through interviews and through social media platforms. One item that consistently came up was having difficulty measuring progress on the machine. We created and deployed a robust Training Goal (TG) ecosystem for our users. TG is a four-part solution:<\/p><ol>\t<li>Creating eight new options for TG preferences so that we could better understand what users wanted to accomplish.<\/li>\t<li>Seven new TG metrics that accumulate weekly as users complete workouts.<\/li>\t<li>TG weekly targets, which set metric ranges for users to achieve.<\/li>\t<li>Enhanced work out details, which tell users how much of an effect the workout has toward each goal, better guiding them to workouts that help them achieve their goals.<\/li><\/ol><p>Databricks enabled us to deploy each of these components by the feature launch deadline.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences"],"category":["Apache Spark, MLFlow, Databricks Workflows"],"areas_of_interest":["Data Applications, Data Ingestion, Data Science"],"delivery":["In Person"],"speakers":[{"name":"Giuseppe Barbalinardo","company":"Tonal","job_title":"Senior Director, Data and AI","bio":"Giuseppe Barbalinardo leads the Data Science and Artificial Intelligence team at Tonal, working across machine learning, computer vision, data engineering, and analytics. His team expands Tonal\u2019s intelligence across human movements, training modalities, and performance goals, building models that power the most intelligent strength training system in the world. Leveraging large-scale sensor and interaction data, they advance machine learning and product innovation to drive human strength and long-term health.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/profile-square%2520copy_1745596886756001Vjlu.png?h=302fa427&itok=eMxTRTSu","alt":"Giuseppe Barbalinardo"}},{"name":"Kristi Korsberg","company":"Tonal","job_title":"Sr Manager, Data Science and AI","bio":"Kristi is a data science manager at Tonal with 9 years of experience in data analytics and data science. At Tonal, she deploys algorithms into the software, manages feature reporting, and partners with business teams on business intelligence forecasting. Prior to Tonal, she worked in Aetna and CVS's Analytics and Behavior Change Department, launching data-driven marketing campaigns. She's based in Boston and enjoys being active in her spare time: skiing, walking her dog, and participating in group workout classes. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Kristi_Korsberg_1745962254764001nUsV.png?h=8d9379ed&itok=OjwgEp5h","alt":"Kristi Korsberg"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/empowering-progress-building-personalized-training-goal-ecosystem","alias":"\/data-ai-summit-2025\/session\/empowering-progress-building-personalized-training-goal-ecosystem","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534235+00:00"}
{"id":"D25B2105","title":"End-to-End Interoperable Data Platform: How Bosch Leverages Databricks Supply Chain Consolidation","description":"<p>This session will showcase Bosch\u2019s journey in consolidating supply chain information using the Databricks platform. It will dive into how Databricks not only acts as the central data lakehouse but also integrates seamlessly with transformative components such as dbt and Large Language Models (LLMs). The talk will highlight best practices, architectural considerations, and the value of an interoperable platform in driving actionable insights and operational excellence across complex supply chain processes.<\/p><p>\u00a0<\/p><p>Key Topics and Sections<\/p><ul>\t<li>Introduction & Business Context<\/li>\t<li>Brief Overview of Bosch\u2019s Supply Chain Challenges and the Need for a Consolidated Data Platform.<\/li>\t<li>Strategic Importance of Data-Driven Decision-Making in a Global Supply Chain Environment.<\/li>\t<li>Databricks as the Core Data Platform<\/li>\t<li>Integrating dbt for Transformation<\/li>\t<li>Leveraging LLM Models for Enhanced Insights<\/li><\/ul>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Manufacturing"],"category":["Delta Lake, Databricks SQL, Unity Catalog"],"areas_of_interest":["Data Intelligence, ML\/LLMOps, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Satish Karunakaran","company":"Robert Bosch GmbH","job_title":"Development Lead","bio":"Satish Karunakaran is a passionate data enthusiast with over two decades of experience working on diverse data workloads. He brings extensive expertise across both transactional and analytical data ecosystems.<br \/>\n<br \/>\nHis primary focus has been on building sustainable and scalable data platforms, both on-premises and in the cloud. <br \/>\n<br \/>\nSince 2023, Satish has taken on the responsibility of establishing the Logistics Data Platform at Robert Bosch, leveraging the power of Azure Databricks to drive innovation and efficiency in the logistics data landscape.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Satish_Bild_1746039041962001CGJZ.JPG?h=17626ad4&itok=vHvqNb3p","alt":"Satish Karunakaran"}},{"name":"Marc-Alexander Frey","company":"Robert Bosch GmbH","job_title":"Project Lead Logistics Innovations","bio":"Over the past three years, Marc has enjoyed building a great crew and setting the sails for Bosch's logistics data intelligence platform, navigating the rushing currents and shallows of a global organization. <br \/>\n<br \/>\nIn his spare time, you can find Marc<br \/>\noutdoors<br \/>\npetting dogs<br \/>\nhitting the gym<br \/>\nand sometimes fiercely competing on the squash court  <br \/>\n<br \/>\nFeel free to reach out - he seems to be a nice guy.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Marc-Alexander_Frey_1745567209007001ZDvg.jpg?h=d1753b66&itok=_mTYtNOQ","alt":"Marc-Alexander Frey"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/end-end-interoperable-data-platform-how-bosch-leverages-databricks","alias":"\/data-ai-summit-2025\/session\/end-end-interoperable-data-platform-how-bosch-leverages-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534246+00:00"}
{"id":"D25B3104","title":"Energy and Utilities Industry Forum","description":"<p>Join us for a compelling forum exploring how energy leaders are harnessing data and AI to build a more sustainable future. As the industry navigates the complex balance between rising global energy demands and ambitious decarbonization goals, innovative companies are discovering that intelligence-driven operations are the key to success. From optimizing renewable energy integration to revolutionizing grid management, learn how energy pioneers are using AI to transform traditional operations while accelerating the path to net zero. This session reveals how Databricks is empowering energy companies to turn their sustainability aspirations into reality, proving that the future of energy is both clean and intelligent.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Energy and Utilities"],"category":["AI\/BI, Delta Sharing, Unity Catalog"],"areas_of_interest":["Industry Experience"],"delivery":["In Person"],"speakers":[{"name":"Julien Debard","company":"Databricks","job_title":"Director of Energy and Utilities","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/energy-and-utilities-industry-forum","alias":"\/data-ai-summit-2025\/session\/energy-and-utilities-industry-forum","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534249+00:00"}
{"id":"D25B1882","title":"Enterprise Financial Crime Detection: A Lakehouse Framework for FATF, Basel III, and BSA Compliance","description":"<p>We will present a framework for FinCrime detection leveraging Databricks lakehouse architecture specifically how institutions can achieve both data flexibility & ACID transaction guarantees essential for FinCrime monitoring. The framework incorporates advanced ML models for anomaly detection, pattern recognition, and predictive analytics, while maintaining clear data lineage & audit trails required by regulatory bodies. We will also discuss some specific improvements in reduction of false positives, improvement in detection speed, and faster regulatory reporting, delve deep into how the architecture addresses specific FATF recommendations, Basel III risk management requirements, and BSA compliance obligations, particularly in transaction monitoring and SAR. The ability to handle structured and unstructured data while maintaining data quality and governance makes it particularly valuable for large financial institutions dealing with complex, multi-jurisdictional compliance requirements.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Delta Lake, MLFlow, Unity Catalog"],"areas_of_interest":["Machine Learning, ML\/LLMOps, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Surya Sai Turaga","company":"Databricks","job_title":"Field Engineering","bio":"My name is Surya! In the past I have played the role of a Field Engineering Leader and evangelist in the Advanced Analytics Space, I come with strong conceptual hands-on experience in Agentic AI, Cloud (AWS+Azure+GCP), Big Data and Data Warehousing environments. I worked as a developer and implemented FSLDMs on Teradata. I am passionate about turning ideas as Agentic AI automation systems. I enjoy developing high-performance teams, mentoring data enthusiasts and working in cross-functional environments.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Surya-linkedIn_1744317580130001URW5.jpg?h=fbf7a813&itok=jyfaLdZS","alt":"Surya Sai Turaga"}},{"name":"Sukhrita Sinha","company":"Barclays","job_title":"Principal Architect","bio":"Extensive experience with leading digital transformation and enterprise architecture for top financial companies. Proven track record of delivering large-scale, complex, global systems leveraging emerging technologies across financial domains in areas of Asset Management, Wealth Management, Consumer and Community Banking, and Treasury Services. Ability to define and execute IT Strategies and Enterprise Architecture Blueprints at the application, data, and technology levels. <br \/>\n<br \/>\nExcellent analytical, business, and interpersonal skills. Highly motivated to take on independent responsibility and contribute to being a productive team member.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/enterprise-financial-crime-detection-lakehouse-framework-fatf-basel-iii","alias":"\/data-ai-summit-2025\/session\/enterprise-financial-crime-detection-lakehouse-framework-fatf-basel-iii","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534260+00:00"}
{"id":"D25B3156DBX","title":"Extending the Lakehouse: Power Interoperable Compute With Unity Catalog Open APIs","description":"<p>The lakehouse is built for storage flexibility, but what about compute? In this session, we\u2019ll explore how Unity Catalog enables you to connect and govern multiple compute engines across your data ecosystem. With open APIs and support for the Iceberg REST Catalog, UC lets you extend access to engines like Trino, DuckDB, and Flink while maintaining centralized security, lineage, and interoperability. Learn how to bring flexibility to your compute layer\u2014without compromising control.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Media and Entertainment, Retail and CPG - Food"],"category":["Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Tathagata Das","company":"Databricks","job_title":"Staff Software Engineer","bio":"Tathagata Das is a Staff Software Engineer at Databricks and has been one of the core developers of Apache Spark (especially Structured Streaming) and Delta Lake. He is a member of Apache Spark PMC, and a Delta Lake committer. He is also one of the authors of Learning Spark: Lighting-fast Data Analytics (2nd edition). Previously, he was a grad student in the UC Berkeley at AMPLab where he conducted research about data-center processing frameworks and networks with Scott Shenker and Ion Stoica.<br \/>\n","image":{}},{"name":"Michelle Leon","company":"Databricks","job_title":"Staff Product Manager","bio":"Michelle is a product manager at Databricks, working on Delta Lake. She previously led teams at Webflow and Airbnb, and is based out of San Francisco.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/extending-lakehouse-power-interoperable-compute-unity-catalog-open-apis","alias":"\/data-ai-summit-2025\/session\/extending-lakehouse-power-interoperable-compute-unity-catalog-open-apis","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534282+00:00"}
{"id":"D25B3239","title":"Games Industry Forum: The Games Executive Perspective on the Impact of Data and AI","description":"<p>Come hear from some of the biggest names in games about how Data and AI is helping them shape their future, build better games and create player-centric experiences. In this session you\u2019ll hear, first, what Databricks is hearing from Games studios globally as their key priorities. We then shift to customers sharing their stories and perspectives. You\u2019ll leave invigorated on the impact Data and AI can have on games, and our global players and have new ideas on ways you can further your impact.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Media and Entertainment"],"category":["AI\/BI, Databricks SQL, Mosaic AI"],"areas_of_interest":["Data Intelligence, Industry Experience, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Huntting Buckley","company":"Databricks","job_title":"Global Games GTM Leader","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/games-industry-forum-games-executive-perspective-impact-data-and-ai","alias":"\/data-ai-summit-2025\/session\/games-industry-forum-games-executive-perspective-impact-data-and-ai","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534355+00:00"}
{"id":"D25B2152","title":"Games24x7's Revolutionizing Online Skill Gaming With Databricks","description":"<p>Games24x7 deals with terabytes of player data generated by 5.5B+ games played on RummyCircle and 500M+ teams created on My11Circle every year, to deliver immersive gaming experiences to 120M+ players.<\/p><p>\u00a0<\/p><p>In this presentation we will talk about:<\/p><ul>\t<li>The journey of our transition to Databricks ApacheSpark\u2122, DeltaLake, UC, MLFlow and DBSQL that led to 20% cost savings of the data platform, unlocking business value through data democratization<\/li>\t<li>The inhouse data products we have built leveraging Databricks to drive efficiency:\t<ul>\t\t<li>EventEcho \u2014 The Digital Campaign Optimization Platform has enabled a 5% increase in user acquisition in the core market. Eliminating Complex S2S event integrations, it has reduced TAT time by 99%.<\/li>\t\t<li>BLITZ \u2014 Helping marketing teams create dashboard without friction and save an estimated 15% overall bandwidth<\/li>\t\t<li>Intelligent Cost Optimizer \u2014 Utilizing generative AI to identify resource inefficiencies, helping reduce the manual efforts in cost optimization by 30%<\/li>\t<\/ul>\t<\/li><\/ul>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment"],"category":["Apache Spark, Delta Lake, Unity Catalog"],"areas_of_interest":["Data Applications, Generative AI (LLMs), Migrations, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Devansh Mehta","company":"Games24x7","job_title":null,"bio":null,"image":{}},{"name":"Anup Tiwari","company":"Games24x7","job_title":"Engineering Manager","bio":"Engineering manager with more than 10 years of experience in developing and managing data platforms.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Anup_Tiwari_1738596821330001msSI.jpg?h=f405c703&itok=DG7ELcoQ","alt":"Anup Tiwari"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/games24x7s-revolutionizing-online-skill-gaming-databricks","alias":"\/data-ai-summit-2025\/session\/games24x7s-revolutionizing-online-skill-gaming-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534358+00:00"}
{"id":"D25B2991","title":"How Blue Origin Accelerates Innovation With Databricks and AWS GovCloud","description":"<p>Blue Origin is revolutionizing space exploration with a mission-critical data strategy powered by Databricks on AWS GovCloud. Learn how they leverage Databricks to meet ITAR and FedRAMP High compliance, streamline manufacturing and accelerate their vision of a 24\/7 factory. Key use cases include predictive maintenance, real-time IoT insights and AI-driven tools that transform CAD designs into factory instructions. Discover how Delta Lake, Structured Streaming and advanced Databricks functionalities like Unity Catalog enable real-time analytics and future-ready infrastructure, helping Blue Origin stay ahead in the race to adopt generative AI and serverless solutions.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Manufacturing, Public Sector"],"category":["Databricks SQL, DLT, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Getting started with Databricks, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Filippo Seracini","company":"Databricks","job_title":"Staff Product Manager","bio":"Joined Databricks as Product Manager in 2021, I lead the company-wide program that ensures that our customers can process data regulated by compliance standards like HIPAA, PCI-DSS, FedRAMP etc. During my tenure at Databricks, I launched the Enhanced Security & Compliance Add-on and more recently, the Databricks on AWS GovCloud offering.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/1731541766511_1745605443253001Ik78.png?h=fbf7a813&itok=qwCznwxp","alt":"Filippo Seracini"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/how-blue-origin-accelerates-innovation-databricks-and-aws-govcloud","alias":"\/data-ai-summit-2025\/session\/how-blue-origin-accelerates-innovation-databricks-and-aws-govcloud","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534457+00:00"}
{"id":"D25L1762","title":"How Serverless Empowered Nationwide to Build Cost-Efficient and World Class BI","description":"<p>Databricks\u2019 Serverless compute streamlines infrastructure setup and management, delivering unparalleled performance and cost optimization for Data and BI workflows. In this presentation, we will explore how Nationwide is leveraging Databricks\u2019 serverless technology and unified governance through Unity Catalog to build scalable, world-class BI solutions. Key features like AI\/BI Dashboards, Genie, Materialized Views, Lakehouse Federation and Lakehouse Apps, all powered by serverless, have empowered business teams to deliver faster, scalable and smarter insights. We will show how Databricks\u2019 serverless technology is enabling Nationwide to unlock new levels of efficiency and business impact, and how other organizations can adopt serverless technology to realize similar benefits.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Financial Services"],"category":["Databricks SQL, Unity Catalog, Databricks Apps"],"areas_of_interest":["Data Intelligence, ETL, SQL"],"delivery":["In Person"],"speakers":[{"name":"Ananya Ghosh","company":"Nationwide","job_title":"Lead Data Engineer","bio":"Ananya Ghosh is a Product Owner and Lead Data Engineer of Data Management Products at Nationwide Insurance. She is responsible for Adoption of Enterprise Data Platforms by strategic Business & Technology stakeholders through platform onboarding, education & feature enhancements. Prior to this role, Ananya has several years of experience in architecting, developing, leading Data Modeling, Data Integration, Data Warehousing & Business Intelligence based applications.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/AnanyaProfilePic_1744406148901001t3EJ.jpg?h=7ee69c4b&itok=yLfEs7It","alt":"Ananya Ghosh"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/how-serverless-empowered-nationwide-build-cost-efficient-and-world","alias":"\/data-ai-summit-2025\/session\/how-serverless-empowered-nationwide-build-cost-efficient-and-world","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534506+00:00"}
{"id":"D25B2190","title":"How the Texas Rangers Use a Unified Data Platform to Drive World Class Baseball Analytics","description":"<p>Don't miss this session where we demonstrate how the Texas Rangers baseball team is staying one step ahead of the competition by going back to the basics. After implementing a modern data strategy with Databricks and winnng the 2023 World Series the rest of the league quickly followed suit. Now more than ever, data and AI are a central pillar of every baseball team's strategy driving profound insights into player performance and game dynamics.<\/p><p>\u00a0<\/p><p>With a 'fundamentals win games' back to the basics focus, join us as we explain our commmitment to world-class data quality, engineering, and MLOPS by taking full advantage of the Databricks Data Intelligence Platform. From system tables to federated querying, find out how the Rangers use every tool at their disposal to stay one step ahead in the hyper competitive world of baseball.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Media and Entertainment"],"category":["MLFlow, Databricks Workflows, Delta Sharing"],"areas_of_interest":["ETL, ML\/LLMOps, Orchestration"],"delivery":["In Person"],"speakers":[{"name":"Michael Topol","company":"Texas Rangers","job_title":"Senior Analyst","bio":null,"image":{}},{"name":"Oliver Dykstra","company":"Texas Rangers","job_title":"Data Engineer","bio":"Oliver Dykstra is a World Champion(2023) data engineer with the Texas Rangers. He builds scalable, future-proof data pipelines to support big data baseball<br \/>\nanalytics and machine learning.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Oliver_Dykstra_1745599341891001jT4g.png?h=f6e1e4ea&itok=fVRXYvoE","alt":"Oliver Dykstra"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/how-texas-rangers-use-unified-data-platform-drive-world-class-baseball","alias":"\/data-ai-summit-2025\/session\/how-texas-rangers-use-unified-data-platform-drive-world-class-baseball","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534513+00:00"}
{"id":"D25B3426DBX","title":"How to Build an Open Lakehouse: Best Practices for Interoperability","description":"Building an open data lakehouse? Start with the right blueprint. This session walks through common reference architectures for interoperable lakehouse deployments across AWS, GCP, Azure, and tools like Snowflake, BigQuery, and Microsoft Fabric. Learn how to design for cross-platform data access, unify governance with Unity Catalog, and ensure your stack is future-ready\u2014no matter where your data lives.","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Aniruth Narayanan","company":null,"job_title":"Product Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/how-build-open-lakehouse-best-practices-interoperability","alias":"\/data-ai-summit-2025\/session\/how-build-open-lakehouse-best-practices-interoperability","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534516+00:00"}
{"id":"D25B2313","title":"How We Transformed Two Businesses With Databricks as the Cornerstone","description":"<p>In this talk, we will discuss the lessons learned and future vision of transforming two business units to a modern financial data platform at Nasdaq. We'll highlight the transition from disjointed systems to a unified platform using Databricks. Our target audience includes financial engineers, data architects and technical leaders. The agenda covers challenges of legacy systems, reasons for choosing Databricks and key architectural decisions.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Financial Services"],"category":["Delta Lake, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Developer Experience, Orchestration, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Leonid Rosenfeld","company":"Nasdaq, Inc.","job_title":"Vice President Software Engineering","bio":"Leonid (Lenny) Rosenfeld is the head of engineering for market data, alternative data, and data intelligence platforms at Nasdaq. He boasts 20+ years of turning ideas into reality, leading technology teams through all stages of growth. His experience includes data-intensive computing, AI,  applied analytics, and distributed systems.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/lenny-rosenfeldheadshot_1745864324127001P33c.jpg?h=b044a8f9&itok=2Sa9bnxJ","alt":"Leonid Rosenfeld"}},{"name":"Steve Schiff","company":"Nasdaq OMX Group","job_title":"VP, Index Platform Technology","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/how-we-transformed-two-businesses-databricks-cornerstone","alias":"\/data-ai-summit-2025\/session\/how-we-transformed-two-businesses-databricks-cornerstone","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534533+00:00"}
{"id":"D25L1282","title":"Iceberg as the Modern Table Standard at Apple","description":"<p>This session will explore Apache Iceberg as a modern table format that has revolutionized data storage and processing. We\u2019ll dive into its core benefits, such as schema evolution, hidden partitioning and time-travel capabilities, and share how Apple leverages these features to optimize internal workflows.<\/p><p>\u00a0<\/p><p>The session will highlight Iceberg\u2019s interoperability across compute engines commonly used at Apple: Spark, Trino and Flink, as well as its integration with streaming platforms like Kafka where it supports large scale batch and streaming workloads. Finally, we\u2019ll discuss the emerging support for AWS-managed Iceberg metadata, and how this can greatly improve large-scale data workflows, paving the way for future advancements.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Apache Spark, Apache Iceberg"],"areas_of_interest":["ETL, Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Nicholas Mehr","company":"Apple","job_title":"Software Engineer","bio":"Nick is an experienced software engineer at Apple. His team focuses on improving the asset delivery and ML integration across Apple services, which powers millions of users devices every day.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/EDUA2R4KG-U04KYKZ3JL8-9316b7553de7-512_1744356953460001zG1e.jpg?h=a7ffc51c&itok=aSTnSpUN","alt":"Nicholas Mehr"}}],"day":"Wednesday","room":"Theater 5","starts":"2025-06-11T21:50:00","ends":"2025-06-11T22:10:00","starts_pst":"2025-06-11T14:50:00","ends_pst":"2025-06-11T15:10:00","start_time":"9:50 pm","end_time":"10:10 pm","pst_start_time":"2:50 pm","pst_end_time":"3:10 pm","duration":"20","path":"\/session\/iceberg-modern-table-standard-apple","alias":"\/data-ai-summit-2025\/session\/iceberg-modern-table-standard-apple","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534543+00:00"}
{"id":"D25B2975","title":"Iceberg Geo Type: Transforming Geospatial Data Management at Scale","description":"<p>The Apache Iceberg\u2122 community is introducing native geospatial type support, addressing key challenges in managing geospatial data at scale, including fragmented formats and inefficiencies in storing large spatial datasets. This talk will delve into the origins of the Iceberg geo type, its specification design and future goals. We will examine the impact on both the geospatial and Iceberg communities, in introducing a standard data warehouse storage layer to the geospatial community, and enabling optimized geospatial analytics for Iceberg users. We will also present a live demonstration of the Iceberg geo data type with Apache Sedona\u2122 and Apache Spark\u2122, showcasing how it simplifies and accelerates geospatial analytics workflows and queries. Finally, we will also provide an in-depth look at its current capabilities and outline the roadmap for future developments, and offer a perspective on its role in advancing geospatial data management in the industry.\u00a0<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Travel and Hospitality"],"category":["Apache Spark, Apache Iceberg"],"areas_of_interest":["Open Source"],"delivery":["In Person"],"speakers":[{"name":"Szehon Ho","company":"Databricks","job_title":"Software Engineer","bio":"Szehon Ho is a software engineer at Databricks working on Apache Spark.  Previously, he was working on Apache Iceberg at Apple, and is an Apache Iceberg PMC.  Prior to that, he was working on Apache Hive at Cloudera and Criteo, and is an Apache Hive PMC.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_8099_1745353394114001L8P0.jpg?h=733e2c11&itok=D7eqqwA_","alt":"Szehon Ho"}},{"name":"Jia Yu","company":"Wherobots Inc.","job_title":"Co-founder and chief architect","bio":"Jia Yu is the co-founder of Wherobots, a Spatial Intelligence Cloud platform for spatial data ETL, analytics, and AI. Previously, he was a Tenure-Track Assistant Professor of Computer Science at Washington State University (2020\u20132023) and earned his Ph.D. from Arizona State University. Jia specializes in large-scale database systems and geospatial data management, with a focus on distributed systems, indexing, and visualization. His work has been featured in top conferences and journals such as SIGMOD, VLDB, ICDE, SIGSPATIAL, and VLDB Journal. He is the primary contributor to Apache Sedona, an open-source big spatial data framework with over 2 million monthly downloads and widespread industry adoption.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/WechatIMG1013-low-res_1744227839453001t4ti.jpg?h=468ef699&itok=Rf0xxc-0","alt":"Jia Yu"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/iceberg-geo-type-transforming-geospatial-data-management-scale","alias":"\/data-ai-summit-2025\/session\/iceberg-geo-type-transforming-geospatial-data-management-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534547+00:00"}
{"id":"D25B2102","title":"Iceberg Table Format Adoption and Unified Metadata Catalog Implementation in Lakehouse Platform","description":"DoorDash Data organization actively adopts LakeHouse paradigm. This presentation describes the methodology which allows to migrate the classic Data Warehouse and Data Lake platforms to unified LakeHouse solution.The objective of this effort include Elimination of excessive data movement.Seamless integration and consolidation of the query engine layers, including Snowflake, Databricks, EMR and Trino.Query performance optimization.Abstracting away complexity of underlying storage layers and table formatsStrategic and justified decision on the Unified Metadata catalog used across varios compute platforms ","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Data Ingestion, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Sergey Zavgorodni","company":"DoorDash","job_title":"Lead Data Engineer","bio":"DoorDash, Disney, DollarShave Club, YellowPages, Cerner and many other companies. <br \/>\nData Engineering expertise  in Data warehouse and Lakehouse environments<br \/>\nData Infrastructure, BigData ETL, AI expert  ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/1568324468757_1744234605200001xABu.jpg?h=7d66a0c3&itok=BSZ0Pjsp","alt":"Sergey Zavgorodni"}},{"name":"Ruotian Wang","company":"Doordash","job_title":"Software Engineer","bio":"Software Engineer in Doordash Data Control team","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Ruotian_Wang_1744325295008001zp7d.png?h=2a934ee8&itok=Gsd0gUvL","alt":"Ruotian Wang"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/iceberg-table-format-adoption-and-unified-metadata-catalog","alias":"\/data-ai-summit-2025\/session\/iceberg-table-format-adoption-and-unified-metadata-catalog","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534550+00:00"}
{"id":"D25B2961","title":"Incremental Iceberg Table Replication at Scale","description":"<p>Apache Iceberg is a popular table format for managing large analytical datasets. But replicating iceberg tables at scale can be a daunting task \u2014 especially when dealing with its hierarchical metadata. In this talk, we present an end-to-end workflow for replicating Apache Iceberg tables, leveraging Apache Spark to ensure that backup tables remain identical to their source counterparts. More excitingly, we have contributed these libraries back to the open-source community.<\/p><p>\u00a0<\/p><p>Attendees will gain a comprehensive understanding of how to set up replication workflows for Iceberg tables, as well as practical guidance on how to manage and maintain replicated datasets at scale. This talk is ideal for data engineers, platform architects and practitioners looking to apply replication and disaster recovery for Apache Iceberg in complex data ecosystems.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, Apache Iceberg"],"areas_of_interest":["Open Source"],"delivery":["In Person"],"speakers":[{"name":"Szehon Ho","company":"Databricks","job_title":"Software Engineer","bio":"Szehon Ho is a software engineer at Databricks working on Apache Spark.  Previously, he was working on Apache Iceberg at Apple, and is an Apache Iceberg PMC.  Prior to that, he was working on Apache Hive at Cloudera and Criteo, and is an Apache Hive PMC.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_8099_1745353394114001L8P0.jpg?h=733e2c11&itok=D7eqqwA_","alt":"Szehon Ho"}},{"name":"Hongyue Zhang","company":"Apple","job_title":"Software Engineer","bio":"Hongyue started his career developing micro services in AWS to help schedule the serverless containers at scale. His journey with data started in 2021 with contribution to Apache Iceberg project. At Apple, Hongyue is building tools and systems around Apache Iceberg to help make data-driven decisions.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/hongyue_zhang_badge_1739388998308001EiMs.jpg?h=773c7aea&itok=C1scZjcB","alt":"Hongyue Zhang"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/incremental-iceberg-table-replication-scale","alias":"\/data-ai-summit-2025\/session\/incremental-iceberg-table-replication-scale","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534564+00:00"}
{"id":"D25B1787","title":"Insights for All \u2014 Bayer Consumer Health\u2019s Journey on Self-Service Analytics at Scale","description":"<p>Bayer\u2019s mission, \"health for all, hunger for none,\" focuses on delivering innovative healthcare and agricultural products to address major global challenges in health and nutrition. This presentation will showcase how Bayer\u2019s Consumer Health division, known for products like Aspirin and Bepanthen, utilizes the Databricks Data Intelligence Platform to develop reusable core data assets and scalable data products. This globally distributed platform supports cost-efficient dashboarding, ad hoc analytics, machine learning and AI solutions, empowering thousands of stakeholders worldwide. The management of core data assets, which serve as integrated and reusable data models, will be discussed, highlighting their role in accelerating the creation of targeted data products. Additionally, a self-service analytics strategy will be presented to enhance access to insights and foster a data-driven culture at Bayer, addressing the needs of stakeholders in various markets and global headquarters.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences, Retail and CPG - Food"],"category":["AI\/BI, Databricks SQL, Unity Catalog"],"areas_of_interest":["Catalogs, Data Intelligence, SQL"],"delivery":["In Person"],"speakers":[{"name":"Andr\u00e9 Wuthenow","company":"Bayer AG","job_title":"Principal Cloud Platform Architect","bio":"Andr\u00e9 Wuthenow is a passionate and pragmatic Principal Cloud Platform Architect at Bayer Consumer Health. He joined Bayer in 2005 and has over 15 year of experience on Data & Analytics Solutions built on SAP HANA, SAP BW, Microsoft Data Platform, Snowflake and Databricks, supporting multiple data domains. Since 2020, Andr\u00e9 is responsible to drive the implementation and operation of the global Data & Analytics Platform of Bayer Consumer Health, intended to address the needs of all Data Analytics use cases like Core Data Asset and Data Product Engineering & Provisioning as well as Machine Learning and Artificial Intelligence as well as GenAI on a global, well-integrated and scalable platform. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/230627_DataAI_Photobooth_5974%2520quadratisch_1744614451548001eKzQ.jpg?h=5e31477a&itok=jpeZEeEp","alt":"Andr\u00e9 Wuthenow"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/insights-all-bayer-consumer-healths-journey-self-service-analytics","alias":"\/data-ai-summit-2025\/session\/insights-all-bayer-consumer-healths-journey-self-service-analytics","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534579+00:00"}
{"id":"D25B3168DBX","title":"Intelligent Applications for the Lakehouse","description":"In this session, you\u2019ll learn about how data engineers can build intelligent applications on Databricks, with production-level concurrency and simplified operations. We'll walk through reference architectures and example use cases while unlocking new possibilities for AI and analytics.","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Apps"],"areas_of_interest":["Databricks Experience (DBX), Getting started with Databricks"],"delivery":["In Person"],"speakers":[{"name":"Dave Nettleton","company":"Databricks","job_title":"Product Manager","bio":null,"image":{}},{"name":"Abbey Russell","company":"Databricks","job_title":"Senior Product Manager","bio":"Abbey is a Product Manager at Databricks. She previously worked on database technology at Cockroach Labs and business consulting at McKinsey & Company. She holds a Bachelors and Masters degree from MIT in Computer Science & Electrical Engineering. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Mikes%252060th%2520Birthday%2520Party-61%2520%25281%2529_1745797211035001IcA8.jpg?h=527ad32d&itok=IAW7rFuf","alt":"Abbey Russell"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/intelligent-applications-lakehouse","alias":"\/data-ai-summit-2025\/session\/intelligent-applications-lakehouse","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534586+00:00"}
{"id":"D25D2827DBX","title":"Introduction to Modern Open Table Formats and Catalogs","description":"<p>In this session, learn about why modern open table formats like Delta and Iceberg are a big deal and how they work with catalogs. Learn about what motivated their creation, how they work, what benefits they can bring to your data and AI platform. Hear about how these formats are becoming increasingly interoperable and what our vision is for their future.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Deep Dive","industry":["Energy and Utilities, Manufacturing, Financial Services"],"category":["Apache Spark, Delta Lake, Apache Iceberg"],"areas_of_interest":["Catalogs, Databricks Experience (DBX), ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Sirui Sun","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":"Sirui is a Sr. Staff Product Manager at Databricks, working on making Delta simpler and more performant. Prior to Databricks, he's spent time working on Google Cloud and at Microsoft. At the Data + AI Summit, he'd love to talk to you about your data goals and challenges! ","image":{}},{"name":"Bart Samwel","company":"Databricks","job_title":"Principal Software Engineer","bio":"Bart is a Principal Engineer at Databricks. He works on performance of Delta Lake in the Databricks Photon engine, and in particular on the MERGE, UPDATE and DELETE commands.","image":{}}],"day":"Wednesday","room":"South, Level 3, Room 303","starts":"2025-06-11T20:50:00","ends":"2025-06-11T22:20:00","starts_pst":"2025-06-11T13:50:00","ends_pst":"2025-06-11T15:20:00","start_time":"8:50 pm","end_time":"10:20 pm","pst_start_time":"1:50 pm","pst_end_time":"3:20 pm","duration":"90","path":"\/session\/introduction-modern-open-table-formats-and-catalogs","alias":"\/data-ai-summit-2025\/session\/introduction-modern-open-table-formats-and-catalogs","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534607+00:00"}
{"id":"D25B2963","title":"IQVIA\u2019s Serverless Journey: Enabling Data and AI in a Regulated World","description":"<p>Your data and AI use-cases are multiplying. At the same time, there is increased focus and scrutiny to meet sophisticated security and regulatory requirements. IQVIA utilizes serverless use-cases across data engineering, data analytics, and ML and AI, to empower their customers to make informed decisions, support their R&D processes and improve patient outcomes. By leveraging native controls on the platform, serverless enables them to streamline their use cases while maintaining a strong security posture, top performance and optimized costs. This session will go over IQVIA\u2019s journey to serverless, how they met their security and regulatory requirements, and the latest and upcoming enhancements to the Databricks Platform.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences, Public Sector, Financial Services"],"category":["Databricks SQL, Databricks Workflows, Mosaic AI"],"areas_of_interest":["Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Matthew Schwartz","company":"IQVIA","job_title":"Data Enablement \/ Architect","bio":"Matt Schwartz is a chaos wrangler, who thrives on developing solutions that enable people to become more effective at what they do. Matt is a self-taught software engineer who loves putting data to work accomplishing a task and measuring the consequences of those decisions. His current work involves empowering data engineers and scientists alike to work effectively and efficiently with the data platforms at their disposal - especially Databricks. Apart from work, Matt enjoys spending time with his family, exploring the heavens aided by a telescope, maintaining his freshwater aquarium, and reading on topics of history, economics, philosophy, politics, and computer science.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Matthew_Schwartz.PNG_1746113579783001JI3a.png?h=e0349028&itok=XQxjn-Qs","alt":"Matthew Schwartz"}},{"name":"Alex Esibov","company":"Databricks","job_title":"Staff Product Manager","bio":"I'm a platform product manager, leading networking for Databricks. If a connection goes into or comes out of Databricks, I care about making it secure, performant, and easy to manage. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/iqvias-serverless-journey-enabling-data-and-ai-regulated-world","alias":"\/data-ai-summit-2025\/session\/iqvias-serverless-journey-enabling-data-and-ai-regulated-world","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534621+00:00"}
{"id":"D25B1914","title":"Italgas\u2019 AI Factory and the Future of Gas Distribution","description":"<p>At Italgas, Europe\u2019s leading gas distributor both by network size and number of customers, we are spearheading digital transformation through a state-of-the-art, fully-fledged Databricks Intelligent platform. We first achieved 50% cost reduction and a 20% performance boost migrating from Azure Synapse to Databricks SQL and ensured that 100% of workloads are governed by Unity Catalog. Now we have 41ML\/GenAI models in production: e.g., AI Customer Complaint Resolution. In AI ICT ticket resolution case, Lakeflow connector with ServiceNow allowed to halve the development time (eliminating DataFactory) and an automatic resolution of 40% of cases. Genie Dashboards and self-BI is used by 80% of our employees, while Genie allows the grid control-room operators to analyze network status data in natural language. Finally, our AI faculty will spread and boost AI literacy across the board and empower employees.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Energy and Utilities"],"category":["Databricks SQL, Mosaic AI, Unity Catalog"],"areas_of_interest":["Data Intelligence, Migrations, ML\/LLMOps"],"delivery":["In Person"],"speakers":[{"name":"Nicola Giorcelli","company":"Cluster Reply","job_title":"Lead","bio":"Nicola is a Lead Consultant at Cluster Reply, specialising in data and AI platform architectures. With deep expertise in technologies such as Databricks and Microsoft Azure, he guides customers through their digital transformation journeys, helping them unlock the full potential of their data assets. His work focuses on designing scalable, efficient, and future-proof solutions that leverage advanced features and innovative approaches, seamlessly combining data and AI capabilities to deliver intelligent, high-performing architectures that accelerate real business value.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/foto_ng_1744629095572001WMpv.png?h=f0d95172&itok=mV6k3H8U","alt":"Nicola Giorcelli"}},{"name":"Serena Delli","company":"Italgas","job_title":"Lead Data Architect","bio":"Serena Delli is the Lead Data & AI Architect at Bludigit,  Italgas' tech hub. With a background in logic research, Serena applies analytical thinking to design effective data architectures. At Bludigit, she focuses on building scalable data platforms that integrate with AI systems, implementing governance frameworks, and supporting team autonomy across departments.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/IMG_5223%25202_1745910079166001P8kf.jpg?h=7503bd68&itok=xLgC68w2","alt":"Serena Delli"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/italgas-ai-factory-and-future-gas-distribution","alias":"\/data-ai-summit-2025\/session\/italgas-ai-factory-and-future-gas-distribution","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534625+00:00"}
{"id":"D25B3050","title":"Kafka Forwarder: Simplifying Kafka Consumption at OpenAI","description":"<p>At OpenAI, Kafka fuels real-time data streaming at massive scale, but traditional consumers struggle under the burden of partition management, offset tracking, error handling, retries, Dead Letter Queues (DLQ), and dynamic scaling \u2014 all while racing to maintain ultra-high throughput. As deployments scale, complexity multiplies.<\/p><p>\u00a0<\/p><p>Enter Kafka Forwarder \u2014 a game-changing Kafka Consumer Proxy that flips the script on traditional Kafka consumption. By offloading client-side complexity and pushing messages to consumers, it ensures at-least-once delivery, automated retries, and seamless DLQ management via Databricks. The result? Scalable, reliable and effortless Kafka consumption that lets teams focus on what truly matters.<\/p><p>\u00a0<\/p><p>Curious how OpenAI simplified self-service, high-scale Kafka consumption? Join us as we walk through the motivation, architecture and challenges behind Kafka Forwarder, and share how we structured the pipeline to seamlessly route DLQ data into Databricks for analysis.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["ETL, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Jigar Bhati","company":"Open AI","job_title":"Member of Technical Staff","bio":"Jigar Bhati has 10+ years of professional software development experience at leading technology companies such as OpenAI, Twitter, and Samsung working on large-scale distributed systems.<br \/>\n<br \/>\nAt OpenAI, he drives AI innovation by designing real-time infrastructure and streaming systems that process millions of events per second. His contributions have been instrumental in launching new products, enhancing safety, and deploying advanced AI capabilities.<br \/>\n<br \/>\nPreviously at Twitter, he played a lead role in developing and scaling Manhattan, a distributed NoSQL database, and productionizing CockroachDB (NewSQL). Together, these systems handled millions of user requests per second, ensuring high availability and platform stability.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/jigar_bhati_high_res_1744645264081001PIMj.jpg?h=6660d6c8&itok=sc9hd3X0","alt":"Jigar Bhati"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/kafka-forwarder-simplifying-kafka-consumption-openai","alias":"\/data-ai-summit-2025\/session\/kafka-forwarder-simplifying-kafka-consumption-openai","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534628+00:00"}
{"id":"D25L3164DBX","title":"Kernel, Catalog, Action! Reimagining our Delta-Spark Connector With DSv2","description":"<p>Delta Lake is redesigning its Spark connector through the combination of three key technologies: First, we're updating our Spark APIs to DSv2 to achieve deeper catalog integration and improved integration with the Spark optimizer. Second, we're fully integrating on top of Delta Kernel to take advantage of its simplified abstraction of Delta protocol complexities, accelerating feature adoption and improving maintainability. Third, we are transforming Delta to become a catalog-aware lakehouse format with Catalog Commits, enabling more efficient metadata management, governance and query performance.\u00a0<\/p><p>Join us to explore how we're advancing Delta Lake's architecture, pushing the boundaries of metadata management and creating a more intelligent, performant data lakehouse platform.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Apache Spark, Delta Lake"],"areas_of_interest":["Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Scott Sandre","company":"Databricks","job_title":"Sr Software Engineer","bio":"Scott Sandre is a Senior Software Engineer at Databricks, where he plays a pivotal role in the Delta Ecosystem team. With a Bachelor's degree in Software Engineering from the University of Waterloo, Scott has been instrumental in driving innovation and performance enhancements for Delta Lake. His notable achievements include contributing to the Delta-Flink Source and Sink connectors, Delta Universal Format, Delta Kernel, and Delta Spark. Based in San Francisco, California, Scott is passionate about open-source technologies and building performance-intensive systems.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/kernel-catalog-action-reimagining-our-delta-spark-connector-dsv2","alias":"\/data-ai-summit-2025\/session\/kernel-catalog-action-reimagining-our-delta-spark-connector-dsv2","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534631+00:00"}
{"id":"D25B2533","title":"Lessons Learned: Building a Scalable Game Analytics Platform at Netflix","description":"<p>Over the past three years, Netflix has built a catalog of 100+ mobile and cloud games across TV, mobile and web platforms. With both internal and external studios contributing to this diverse ecosystem, building a robust game analytics platform became crucial for gaining insights into player behavior, optimizing game performance and driving member engagement.In this talk, we\u2019ll share our journey of building Netflix\u2019s Game Analytics platform from the ground up. We\u2019ll highlight key decisions around data strategy, such as whether to develop an in-house solution or adopt an external service. We\u2019ll discuss the challenges of balancing developer autonomy with data integrity and the complexities of managing data contracts for custom game telemetry, with an emphasis on self-service analytics.<\/p><p>\u00a0<\/p><p>Attendees will learn how the Games Data team navigated these challenges, the lessons learned and the trade-offs involved in building a multi-tenant data ecosystem that supports diverse stakeholders.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Media and Entertainment"],"category":["Apache Spark, Apache Iceberg"],"areas_of_interest":["Customer Data Platform, ETL, Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Bhargavi Reddy Dokuru","company":"NETFLIX INC","job_title":"Senior Data Engineer","bio":"Bhargavi is a Senior Data Engineer at Netflix supporting the Games business. With 10+ years of experience in the data field, she specializes in Big Data technologies and Distributed Computing. She also holds a Master\u2019s Degree in Information Systems from Carnegie Mellon University.\u00a0<br \/>\n<br \/>\nAs one of the early data engineers on Netflix Games, Bhargavi helped build core data infrastructure and game telemetry systems, enabling the Games business to drive quality and engagement of Netflix games across Mobile, TV, and Web platforms.\u00a0<br \/>\n<br \/>\nPassionate about mentorship, Bhargavi is committed to helping students pursue a career in data engineering. With over 20k followers on LinkedIn, she regularly writes about data engineering and IC leadership.\u00a0","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Bhargavi_ReddyDokuru_1744818867220001ncij.png?h=ad75d607&itok=pIPdJAD8","alt":"Bhargavi Reddy Dokuru"}},{"name":"Michael Cuthbert","company":"Netflix","job_title":"Snr. Software Engineer","bio":"Michael Cuthbert is a seasoned software engineer with over 25 years of experience in the technology industry. Holding a Bachelor of Science degree in Computer Science from the University of Cape Town, Michael has built a remarkable career marked by innovation and leadership. Michael spent seven transformative years at Apple Maps, where he contributed to the development of cutting-edge mapping technologies. Now, he's part of the team at Netflix Games, working on the Game Analytics Platform to help enhance gaming experiences through data insights. In addition to his professional work, Michael is passionate about open-source projects, contributing to the community when he is able.<br \/>\n<br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Headshot_1744411357981001ECsv.png?h=ba226ffe&itok=aez-H085","alt":"Michael Cuthbert"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/lessons-learned-building-scalable-game-analytics-platform-netflix","alias":"\/data-ai-summit-2025\/session\/lessons-learned-building-scalable-game-analytics-platform-netflix","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534685+00:00"}
{"id":"D25B2617","title":"Let's Save Tons of Money With Cloud-Native Data Ingestion!","description":"<p>Delta Lake is a fantastic technology for quickly querying massive data sets, but first you need those massive data sets! In this session we will dive into the cloud-native architecture Scribd has adopted to ingest data from AWS Aurora, SQS, Kinesis Data Firehose and more. By using off-the-shelf open source tools like kafka-delta-ingest, oxbow and Airbyte, Scribd has redefined its ingestion architecture to be more event-driven, reliable, and most importantly: cheaper. No jobs needed!<\/p><p>\u00a0<\/p><p>Attendees will learn how to use third-party tools in concert with a Databricks and Unity Catalog environment to provide a highly efficient and available data platform. This architecture will be presented in the context of AWS but can be adapted for Azure, Google Cloud Platform or even on-premise environments.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology, Media and Entertainment"],"category":["Delta Lake, Apache Iceberg"],"areas_of_interest":["Customer Data Platform, Data Ingestion, Open Source, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Tyler Croy","company":"Scribd","job_title":"Valued Employee","bio":"At Scribd Tyler created the delta-rs project and has presented a number of times on Scribd's large scale data and ML workloads combining Rust, Delta Lake, and the Databricks platform. He has written extensively on using Delta Lake from Python and Rust, joining the talented authors of the award-winning #1 best selling book \"Delta Lake: The Definitive Guide\", by contributing Chapter 6.  Tyler is also a Databricks MVP and a generally tall guy.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/halfmoonbay-sm_1745875454288001JbLX.png?h=a7ffc51c&itok=HEo7zFTq","alt":"Tyler Croy"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/lets-save-tons-money-cloud-native-data-ingestion","alias":"\/data-ai-summit-2025\/session\/lets-save-tons-money-cloud-native-data-ingestion","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534696+00:00"}
{"id":"D25L2249","title":"Leveling Up Gaming Analytics: How Supercell Evolved Player Experiences With Snowplow and Databricks","description":"<p>In the competitive gaming industry, understanding player behavior is key to delivering engaging experiences. Supercell, creators of Clash of Clans and Brawl Stars, faced challenges with fragmented data and limited visibility into user journeys. To address this, they partnered with Snowplow and Databricks to build a scalable, privacy-compliant data platform for real-time insights.<\/p><p>\u00a0<\/p><p>By leveraging Snowplow\u2019s behavioral data collection and Databricks\u2019 Lakehouse architecture, Supercell achieved:<\/p><ul>\t<li>Cross-platform data unification: A unified view of player actions across web, mobile and in-game<\/li>\t<li>Real-time analytics: Streaming event data into Delta Lake for dynamic game balancing and engagement<\/li>\t<li>Scalable infrastructure: Supporting terabytes of data during launches and live events<\/li>\t<li>AI & ML use cases: Churn prediction and personalized in-game recommendations<\/li><\/ul><p>\u00a0<\/p><p>This session explores Supercell\u2019s data journey and AI-driven player engagement strategies.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["AI\/BI, Databricks Workflows, Mosaic AI"],"areas_of_interest":["Customer Data Platform, Data Ingestion, Gaming"],"delivery":["In Person"],"speakers":[{"name":"Jordan Hansen","company":null,"job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/leveling-gaming-analytics-how-supercell-evolved-player-experiences","alias":"\/data-ai-summit-2025\/session\/leveling-gaming-analytics-how-supercell-evolved-player-experiences","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534699+00:00"}
{"id":"D25B3160DBX","title":"Managed and Foreign Tables in Unity Catalog","description":"<p>Unity Catalog brings open, interoperable table formats to the heart of the Databricks Lakehouse. In this session, we\u2019ll introduce new capabilities that apply fine-grained governance across all data and unify access across catalogs. Learn how Databricks is eliminating data silos, simplifying performance with predictive optimization and advancing an open lakehouse architecture.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Jonathan Brito","company":"Databricks","job_title":"Staff Product Manager","bio":"Jonathan Brito joined Databricks in 2023 and is on the Delta Lake product management team. He is responsible for Universal Format ('UniForm'). Before joining Databricks, he was at Google, where he developed and launched various Cloud products. ","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/managed-and-foreign-tables-unity-catalog","alias":"\/data-ai-summit-2025\/session\/managed-and-foreign-tables-unity-catalog","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534734+00:00"}
{"id":"D25B3103","title":"Manufacturing and Transportation Industry Forum","description":"<p>Join us for an inspiring forum showcasing how manufacturers and transportation leaders are turning today's challenges into tomorrow's opportunities. From automotive giants revolutionizing product development with generative AI to logistics providers optimizing routes for both cost and sustainability, discover how industry pioneers are reshaping the future of industrial operations. Highlighting this session is an exciting collaboration between Heathrow Airport and Virgin Atlantic, demonstrating how partnership and innovation are transforming the air travel experience. Learn how these leaders and other companies are using Databricks to tackle their most pressing challenges \u2014 from smart factory transformations to autonomous systems development \u2014 proving that the path to profitability and sustainability runs through intelligent operations.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Manufacturing"],"category":["AI\/BI, Delta Sharing, Unity Catalog"],"areas_of_interest":["Industry Experience"],"delivery":["In Person"],"speakers":[{"name":"Shiv Trisal","company":"Databricks","job_title":"Global Industry Leader - Manufacturing","bio":"Shiv is responsible for worldwide adoption of Databricks in the manufacturing, transportation & energy sectors through new solutions, partnerships & GTM programs that bring differentiated value to CxO priorities. Shiv has a proven track record of leading diverse teams and delivering game-changing data & AI-led innovation across Manufacturing and Transportation and Logistics industries, with roles at EY, Strategy& and Raytheon. He regularly connects with senior executives to unfold data & AI strategies and unlock competitive advantage in their businesses. Shiv majored in Electronics Engineering from the Thapar Institute of Engineering and Technology in Punjab, India and holds a MBA from the Wharton School at the University of Pennsylvania. ","image":{}},{"name":"Andy Isenman","company":"Heathrow","job_title":"Head of Technology, Cloud and Data","bio":null,"image":{}},{"name":"Caitlin Gordon","company":"Databricks","job_title":"Manufacturing & Energy Marketing Lead","bio":null,"image":{}},{"name":"Richard Masters","company":"Virgin Atlantic Airways","job_title":"VP Data & AI","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/manufacturing-and-transportation-industry-forum","alias":"\/data-ai-summit-2025\/session\/manufacturing-and-transportation-industry-forum","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534751+00:00"}
{"id":"D25B3098","title":"Manufacturing Cleaner: How Data Intelligence Cuts Carbon, Not Profits","description":"<p>Join industry leaders from Dow and Michelin as they reveal how data intelligence is revolutionizing sustainable manufacturing without compromising profitability. Dow demonstrates how their implementation of Databricks' Data Intelligence Platform has transformed their ability to track and reduce carbon footprints while driving operational efficiencies, resulting in significant cost savings through optimized maintenance and reduced downtime. Michelin follows with their ambitious strategy to achieve 3% energy consumption reduction by 2026, leveraging Databricks to turn this environmental challenge into operational excellence. Together, these manufacturing giants showcase how modern data architecture and AI are creating a new paradigm where sustainability and profitability go hand-in-hand.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Manufacturing"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Industry Experience"],"delivery":["In Person"],"speakers":[{"name":"Jesse Grekowicz","company":"Dow Inc.","job_title":"Senior Solution Manager","bio":"Jesse Grekowicz is a Senior Solution Manager in the Enterprise Data & Analytics organization at Dow. Based in Midland, MI, Jesse specializes in Business Intelligence and Enterprise Data Platforms. Jesse is responsible for defining strategy, designing and architecting innovative solutions, collaborating with various business partners, and ensuring the delivery of high-quality and consistent solutions. Jesse is passionate about leveraging new technologies to enhance business processes to ensure the accuracy, reliability, maintainability, and speed-to-value of data solutions. Jesse earned his Bachelor's degree in Computer Science from Saginaw Valley State University in 2012 where he also taught as an adjunct professor for databases.<br \/>\n","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Grekowicz%252C%2520Jesse_1-22-24-18%2520low%2520res_1745606918229001GeS8.jpg?h=04d92ac6&itok=_99Y5TpG","alt":"Jesse Grekowicz"}},{"name":"Baptiste Andrieux","company":"CGI","job_title":"Product Owner","bio":"Working for Industry 4.0 since 5 years, I've spent 4 years working in Michelin's AVEVA PI Central Team as developer. <br \/>\nThen, I've been working for Energy Digital Product for Michelin as Tech Lead and finally as Product Owner since 2025.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Baptiste_Andrieux_1738002102662001fYRR.jpg?h=023d70b5&itok=7vpDkmZx","alt":"Baptiste Andrieux"}},{"name":"Tim Licquia","company":"Dow Inc.","job_title":"Lead Data & ML Engineer","bio":"Tim Licquia is the Lead Data & ML Engineer in the Enterprise Data & Analytics organization at Dow. He defines strategy and helps drive modern development practices for both DataOps and MLOps in Dow's lakehouse platform, focusing on accuracy, reliability, maintainability, and speed-to-value of data solutions. He has worked as a professional data scientist in various functional areas, using AI\/ML and advanced analytics to capture dozens of value opportunities across Dow's business units. He also has led several internal communities and strategic committees focused on data and AI. Tim earned his Ph.D. in Astrophysics from the University of Pittsburgh where he used ML and advanced statistical methods to make new discoveries in galaxy science.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Licquia_Tim_cropped_1745809020904001MvVE.JPG?h=300d517d&itok=LZZ8gure","alt":"Tim Licquia"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/manufacturing-cleaner-how-data-intelligence-cuts-carbon-not-profits","alias":"\/data-ai-summit-2025\/session\/manufacturing-cleaner-how-data-intelligence-cuts-carbon-not-profits","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534755+00:00"}
{"id":"D25B2010","title":"Maximizing Business Value and Ensuring Data Privacy with Databricks in Connected Vehicles","description":"<p>As global data privacy regulations tighten, balancing user data protection with maximizing its business value is crucial.This presentation explores how integrating Databricks into our connected-vehicle data platform enhances both governance and business outcomes.<\/p><p>\u00a0<\/p><p>We\u2019ll highlight a case where migrating from EMR to Databricks improved deletion performance and cut costs by 99% with Delta Lake.\u00a0<\/p><p>\u00a0<\/p><p>This shift not only ensures compliance with data-privacy regulations but also maximizes the potential of connected-vehicle data.\u00a0<\/p><p>\u00a0<\/p><p>We are developing a platform that balances compliance with business value and sets a global standard for data usage, inviting partners to join us in building a secure, efficient mobility ecosystem.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Manufacturing"],"category":["Apache Spark, Delta Lake, Unity Catalog"],"areas_of_interest":["Catalogs, Migrations, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Yoshihiro Oe","company":"TOYOTA MOTOR CORPORATION","job_title":"General Manager","bio":null,"image":{}},{"name":"Satoshi Kuramitsu","company":"Databricks","job_title":"Manager, Field Engineering","bio":"Satoshi is a Solution Architect Manager at Databricks, responsible for the automotive, manufacturing, and telecommunications industries. He has been instrumental in driving cloud adoption among automotive customers, primarily focusing on connected vehicle platforms and autonomous driving software development platforms. Satoshi emphasizes leveraging Databricks technologies to effectively address complex industry challenges.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/portrait-298_%25202_1746109094754001yB8Q.jpg?h=54a4fb9b&itok=ZfAjZVNA","alt":"Satoshi Kuramitsu"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/maximizing-business-value-and-ensuring-data-privacy-databricks","alias":"\/data-ai-summit-2025\/session\/maximizing-business-value-and-ensuring-data-privacy-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534770+00:00"}
{"id":"D25B3171DBX","title":"Multi-Format, Multi-Table, Multi-Statement Transactions on Unity Catalog","description":"<p>Get a first look at multi-statement transactions in Databricks. In this session, we will dive into their capabilities, exploring how multi-statement transactions enable atomic updates across multiple tables in your data pipelines, ensuring data consistency and integrity for complex operations.\u00a0We will also share how we are enabling unified transactions across Delta Lake and Iceberg with Unity Catalog \u2014 powering our vision for an open and interoperable lakehouse.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Prakhar Jain","company":"Databricks","job_title":"Staff Software Engineer","bio":"Prakhar Jain is a Staff Software Engineer at Databricks. He works on performance of Delta Lake in the Databricks Photon engine, and in particular on the Delta Metadata layer a.k.a. Delta log.<br \/>\n","image":{}},{"name":"Michelle Leon","company":"Databricks","job_title":"Staff Product Manager","bio":"Michelle is a product manager at Databricks, working on Delta Lake. She previously led teams at Webflow and Airbnb, and is based out of San Francisco.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/multi-format-multi-table-multi-statement-transactions-unity-catalog","alias":"\/data-ai-summit-2025\/session\/multi-format-multi-table-multi-statement-transactions-unity-catalog","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534814+00:00"}
{"id":"D25B3162DBX","title":"Open Source Unity Catalog: Getting Started, Best Practices and Governance at Scale","description":"<p>How to use UC OSS, what features are available, and intro to the ecosystem. We'll dive into the latest release and get hands-on with demos for working with your UC data and AI assets\u00a0\u2014 including tables, volumes, models and AI functions.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Tathagata Das","company":"Databricks","job_title":"Staff Software Engineer","bio":"Tathagata Das is a Staff Software Engineer at Databricks and has been one of the core developers of Apache Spark (especially Structured Streaming) and Delta Lake. He is a member of Apache Spark PMC, and a Delta Lake committer. He is also one of the authors of Learning Spark: Lighting-fast Data Analytics (2nd edition). Previously, he was a grad student in the UC Berkeley at AMPLab where he conducted research about data-center processing frameworks and networks with Scott Shenker and Ion Stoica.<br \/>\n","image":{}},{"name":"Ben Wilson","company":"Databricks","job_title":"Sr Software Engineer","bio":"Ben works on the team supporting MLflow and other notable Open Source ML products at Databricks.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ben_1745343978521001HWle.png?h=c35ab9d9&itok=h3yUc2q1","alt":"Ben Wilson"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/open-source-unity-catalog-getting-started-best-practices-and-governance","alias":"\/data-ai-summit-2025\/session\/open-source-unity-catalog-getting-started-best-practices-and-governance","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534838+00:00"}
{"id":"D25B1058","title":"Optimizing Analytics Infrastructure: Lessons from Migrating Snowflake to Databricks","description":"<p>This session explores the strategic migration from Snowflake to Databricks, focusing on the journey of transforming a data lake to leverage Databricks\u2019 advanced capabilities. It outlines the assessment of key architectural differences, performance benchmarks, and cost implications driving the decision. Attendees will gain insights into planning and execution, including data ingestion pipelines, schema conversion and metadata migration. Challenges such as maintaining data quality, optimizing compute resources and minimizing downtime are discussed, alongside solutions implemented to ensure a seamless transition. The session highlights the benefits of unified analytics and enhanced scalability achieved through Databricks, delivering actionable takeaways for similar migrations.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg, Databricks SQL"],"areas_of_interest":["Data Applications, Getting started with Databricks, Migrations"],"delivery":["In Person"],"speakers":[{"name":"AMIT RUSTAGI","company":"DeeplearningAPI","job_title":"Architect","bio":"Amit Rustagi is an experienced AI and Big Data technologist with 20+ years of expertise in transforming business needs into technical solutions. He specializes in AI, Generative AI, and analytics, designing architectures and machine learning models to deliver impactful outcomes. A recognized speaker at several industry events in the past, he drives innovation in AI and data science.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Amit_Photo_1744233821924001npB8.jpg?h=0c3ee085&itok=NPE-0e3D","alt":"AMIT RUSTAGI"}}],"day":"Thursday","room":"South, Level 2, Room 209","starts":"2025-06-12T18:30:00","ends":"2025-06-12T19:10:00","starts_pst":"2025-06-12T11:30:00","ends_pst":"2025-06-12T12:10:00","start_time":"6:30 pm","end_time":"7:10 pm","pst_start_time":"11:30 am","pst_end_time":"12:10 pm","duration":"40","path":"\/session\/optimizing-analytics-infrastructure-lessons-migrating-snowflake","alias":"\/data-ai-summit-2025\/session\/optimizing-analytics-infrastructure-lessons-migrating-snowflake","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534845+00:00"}
{"id":"D25L3371","title":"Over Architected: LIVE","description":"Join a live recording of the Over Architected Databricks podcast with Nick and Holly as they take the hottest features for the coming week and try to shoehorn them into one architecture. ","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["AI\/BI, LakeFlow, Unity Catalog"],"areas_of_interest":["Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Holly Smith","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Holly Smith is a developer advocate and multi-award-winning Data and AI expert and tech educator with over a decade of experience working at the cutting edge of Data and AI in various capacities. One of the first employees at Databricks, she is a renowned public speaker and teacher. She is well known for her YouTube series on AI + data engineering.<br \/>\n","image":{}},{"name":"Nick Karpov","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Nick Karpov is a Developer Advocate at Databricks. He is an Apache Spark and Delta Lake contributor. Prior to joining the Delta Lake advocacy group he was a senior field engineer at Databricks: designing, implementing, & maintaining big data systems. Nick has years of experience delivering end-to-end systems from ingestion & transforms, to model training & serving in production environments.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/over-architected-live","alias":"\/data-ai-summit-2025\/session\/over-architected-live","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534863+00:00"}
{"id":"D25L1123","title":"Petabyte-Scale On-Chain Insights: Real-Time Intelligence for the Next-Gen Financial Backbone","description":"<p>We\u2019ll explore how CipherOwl Inc. constructed a near real-time, multi-chain data lakehouse to power anti-money laundering (AML) monitoring at a petabyte scale. We will walk through the end-to-end architecture, which integrates cutting-edge open-source technologies and AI-driven analytics to handle massive on-chain data volumes seamlessly. Off-chain intelligence complements this to meet rigorous AML requirements.<\/p><p>\u00a0<\/p><p>At the core of our solution is ChainStorage, an OSS started by Coinbase that provides robust blockchain data ingestion and block-level serving. We enhanced it with Apache Spark\u2122 and Arrow\u2122, coupled for high-throughput processing and efficient data serialization, backed by Delta Lake and Kafka. For the serving layer, we employ StarRocks to deliver lightning-fast SQL analytics over vast datasets. Finally, our system incorporates machine learning and AI agents for continuous data curation and near real-time insights, which are crucial for tackling on-chain AML challenges.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology, Financial Services"],"category":["Apache Spark, Delta Lake, Unity Catalog"],"areas_of_interest":["AI Agents, Catalogs, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Leo Liang","company":"CipherOwl Inc","job_title":"Founder","bio":"Leo Liang is a seasoned technologist specializing in large-scale data infrastructure, machine learning, and blockchain technologies. As the co-founder and CEO of CipherOwl, he develops innovative AI-powered platforms focused on onchain intelligence, automation, and compliance solutions. At CipherOwl, Leo emphasizes data-driven, reasoning-focused onchain risk modeling and builds highly scalable infrastructure designed for onchain compliance. Previously, Leo led data platforms and on-chain data infrastructure at Coinbase, and he held key engineering leadership positions at Cruise, Twitter, AWS, and Microsoft, where he contributed significantly to advancements in cloud computing and data technologies.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/A1_00514_1745546668827001guYv.jpg?h=6660d6c8&itok=U5eJAHUr","alt":"Leo Liang"}}],"day":"Tuesday","room":"Theater 6","starts":"2025-06-10T23:30:00","ends":"2025-06-10T23:50:00","starts_pst":"2025-06-10T16:30:00","ends_pst":"2025-06-10T16:50:00","start_time":"11:30 pm","end_time":"11:50 pm","pst_start_time":"4:30 pm","pst_end_time":"4:50 pm","duration":"20","path":"\/session\/petabyte-scale-chain-insights-real-time-intelligence-next-gen-financial","alias":"\/data-ai-summit-2025\/session\/petabyte-scale-chain-insights-real-time-intelligence-next-gen-financial","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534884+00:00"}
{"id":"D25B1589","title":"Redesigning Kaizen's Cloud Data Lake for the Future","description":"<p>At Kaizen Gaming, data drives our decision-making, but rapid growth exposed inefficiencies in our legacy cloud setup \u2014 escalating costs, delayed insights and scalability limits. Operating in 18 countries with 350M daily transactions (1PB+), shared quotas and limited cost transparency hindered efficiency.<\/p><p>\u00a0<\/p><p>To address this, we redesigned our cloud architecture with Data Landing Zones, a modular framework that decouples resources, enabling independent scaling and cost accountability. Automation streamlined infrastructure, reduced overhead and enhanced FinOps visibility, while Unity Catalog ensured governance and security.<\/p><p>\u00a0<\/p><p>Migration challenges included maintaining stability, managing costs and minimizing latency. A phased approach, Delta Sharing, and DBx Asset Bundles simplified transitions. The result: faster insights, improved cost control and reduced onboarding time, fostering innovation and efficiency. We share our transformation, offering insights for modern cloud optimization.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Professional Services"],"category":["Delta Lake, Delta Sharing, Unity Catalog"],"areas_of_interest":["Data Applications, Migrations, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Nikolaos Michail","company":"Kaizen Gaming","job_title":"Data Platform Team Lead","bio":"Nikolaos Michail is the Data Platform Lead at Kaizen Gaming, where he\u2019s on a mission to make data scalable, self-serve, and just a little less chaotic. Whether it's wiring up Data Mesh foundations or convincing people that CI\/CD belongs in data too, he\u2019s all about building systems that are fast, reliable, and kind of magical. He\u2019s especially passionate about clean architecture, automation, and enabling teams to ship data products with confidence.<br \/>\n<br \/>\nAt the 2025 Databricks AI Summit, he\u2019ll walk through Kaizen Gaming\u2019s reimagined data platform\u2014 sharing practical insights, hard-earned lessons, and architectural patterns to help you future-proof your cloud data lake for scale, speed, and cost efficiency.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/headshot_1745417063329001Xwzp.jpg?h=2dd7cee9&itok=ZZEMxrT-","alt":"Nikolaos Michail"}},{"name":"Triantafyllos Tsakmakis","company":"Kaizen Gaming","job_title":"Senior SRE\/DevOps Engineer","bio":"Triantafyllos Tsakmakis is an SRE\/DevOps Engineer at Kaizen Gaming, focused on bringing order and efficiency to cloud data platforms. He designs and automates Databricks environments on Azure using Terraform, integrating CI\/CD pipelines to accelerate deployments while ensuring reliability. With a strong belief in automation, streamlined workflows, and empowering teams, he helps deliver scalable data solutions that balance innovation with stability. His approach combines Infrastructure as Code, continuous integration\/delivery, and SRE principles to maintain robust systems.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/me_1746175544866001fcW9.jpg?h=a7ffc51c&itok=o1ZPqBdj","alt":"Triantafyllos Tsakmakis"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/redesigning-kaizens-cloud-data-lake-future","alias":"\/data-ai-summit-2025\/session\/redesigning-kaizens-cloud-data-lake-future","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534948+00:00"}
{"id":"D25L1545","title":"Reducing Transaction Conflicts in Databricks\u2014Fundamentals and Applications at Asana","description":"<p>When using ACID-guaranteed transactions on Databricks concurrently, we can run into transaction conflicts. The first part of this talk discusses the basics of concurrent transaction functionality in Databricks\u2014what happens when various combinations of INSERT, UPDATE and MERGE INTO happen concurrently. We discuss how table isolation level, partitioning and deletion vectors affect this. The second part of this talk focuses on a particular pipeline evolution at Asana to reduce transaction conflicts. As the number of writers to a table grew, we first implemented writer-specific partitioning to reduce transaction conflicts. Later on, we implemented an intermediate blind append stage to be able to avoid transaction conflicts while leveraging liquid clustering rather than partitioning for improved read and write performance.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Enterprise Technology"],"category":["Apache Spark, Delta Lake"],"areas_of_interest":["Data Ingestion, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"dimakamalov dimakamalov","company":"Asana","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/reducing-transaction-conflicts-databricks-fundamentals-and-applications","alias":"\/data-ai-summit-2025\/session\/reducing-transaction-conflicts-databricks-fundamentals-and-applications","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.534957+00:00"}
{"id":"D25L3381","title":"Rust and Lakehouse Format \u2014 Ask Us Anything","description":"<p>Join us for an in-depth Ask Me Anything (AMA) on how Rust is revolutionizing Lakehouse formats like Delta Lake and Apache Iceberg through projects like delta-rs and iceberg-rs! Discover how Rust\u2019s memory safety, zero-cost abstractions and fearless concurrency unlock faster development and higher-performance data operations. Whether you\u2019re a data engineer, Rustacean or Lakehouse enthusiast, bring your questions on how Rust is shaping the future of open table formats!<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Lightning Talk","industry":["Education, Professional Services, Public Sector"],"category":["Delta Lake"],"areas_of_interest":["Developer Experience, Open Source, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Robert Pack","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Robert has extensive experience in designing and implementing Data & AI platforms within large multinational organizations. Through this work he has been an avid contributor to the open lakehouse ecosystem - specifically Delta Lake. Now at Databricks, his focus is entirely facilitating and contributing to the open source ecosystem for building lakehouse architectures.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/robert_pack_1736342796943001ZVBj.png?h=7b4ffee1&itok=kMDovM_k","alt":"Robert Pack"}},{"name":"Denny Lee","company":"Databricks","job_title":"PM Director, Developer Relations","bio":null,"image":{}},{"name":"Tyler Croy","company":"Scribd","job_title":"Valued Employee","bio":"At Scribd Tyler created the delta-rs project and has presented a number of times on Scribd's large scale data and ML workloads combining Rust, Delta Lake, and the Databricks platform. He has written extensively on using Delta Lake from Python and Rust, joining the talented authors of the award-winning #1 best selling book \"Delta Lake: The Definitive Guide\", by contributing Chapter 6.  Tyler is also a Databricks MVP and a generally tall guy.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/halfmoonbay-sm_1745875454288001JbLX.png?h=a7ffc51c&itok=HEo7zFTq","alt":"Tyler Croy"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/rust-and-lakehouse-format-ask-us-anything","alias":"\/data-ai-summit-2025\/session\/rust-and-lakehouse-format-ask-us-anything","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535026+00:00"}
{"id":"D25B2853","title":"Securing Databricks Using Databricks as SIEM","description":"Securing Databricks using Databricks as SIEM showcases our approach on how we leverage Databricks product capabilities to prevent and mitigate security risks for Databricks. It demonstrates how Databricks can serve as a powerful Security Information and Event Management (SIEM) platform, offering advanced capabilities for data collection and threat detection. This session explores data collection from diverse data sources and real-time threat detection.","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark, Databricks SQL, Unity Catalog"],"areas_of_interest":["Cybersecurity, Data Ingestion, Streaming pipelines"],"delivery":["In Person"],"speakers":[{"name":"Kishore Prabakaran Fernando","company":"Databricks","job_title":"Sr. Manager, Security Engineering","bio":"Kishore Fernando leads Security Data Infrastructure at Databricks, focusing on building multi-cloud data lake systems that enhance security visibility and accelerate incident response. With extensive experience across software engineering, data platforms, cybersecurity, and cloud infrastructure, he has led teams at companies like Capital One and Discover. Kishore specializes in scaling event-driven architectures on AWS, Azure, and GCP, and is passionate about creating secure, resilient platforms that drive innovation.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Prabakaran%2520Fernando_Kishore_Default_Workday%2520_%2520HR_Headshot_1745629203136001Ib9R.jpg?h=b044a8f9&itok=ma2mbLsl","alt":"Kishore Prabakaran Fernando"}},{"name":"Yugi Reddy","company":"Databricks","job_title":"Staff Software Engineer","bio":"Yugi Reddy is a Staff Software Engineer at Databricks, where he leads the implementation of the Security Data LakeHouse - a centralized platform designed to identify and respond to security threats by integrating and analyzing data from diverse sources, including cloud environments, internal products, and third-party vendors. His work focuses on building scalable, reliable systems for threat detection, incident response, and compliance, while driving innovation through the adoption of AI and machine learning use cases. Yugi has previously contributed to the security and technology initiatives at major financial institutions such as Capital One, JPMorgan Chase, American Express, and Wells Fargo.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Reddy_Yugi_Default_Workday%2520_%2520HR_Headshot_1745857107353001raNo.png?h=b044a8f9&itok=sFod9uXd","alt":"Yugi Reddy"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/securing-databricks-using-databricks-siem","alias":"\/data-ai-summit-2025\/session\/securing-databricks-using-databricks-siem","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535142+00:00"}
{"id":"D25L2919","title":"Selectively Overwrite Data With Delta Lake\u2019s Dynamic Insert Overwrite","description":"<p>Dynamic Insert Overwrite is an important Delta Lake feature that allows fine-grained updates by selectively overwriting specific rows, eliminating the need for full-table rewrites.<\/p><p>\u00a0<\/p><p>For examples, this capability is essential for:<\/p><ul>\t<li>DBT-Databricks' incremental models\/workloads, enabling efficient data transformations by processing only new or updated records<\/li>\t<li>ETL Slowly Changing Dimension (SCD) Type 2<\/li><\/ul><p>\u00a0<\/p><p>In this lightning talk, we will:<\/p><ul>\t<li>Introduce Dynamic Insert Overwrite: Understand its functionality and how it works<\/li>\t<li>Explore key use cases: Learn how it optimizes performance and reduces costs<\/li>\t<li>Share best practices: Discover practical tips for leveraging this feature on Databricks, including on the cutting-edge Serverless SQL Warehouses<\/li><\/ul>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Lightning Talk","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["Apache Spark, Delta Lake, Databricks SQL"],"areas_of_interest":["Data Ingestion, ETL, SQL"],"delivery":["In Person"],"speakers":[{"name":"Thang Long Vu","company":"Databricks","job_title":"Software Engineer","bio":null,"image":{}},{"name":"Bart Samwel","company":"Databricks","job_title":"Principal Software Engineer","bio":"Bart is a Principal Engineer at Databricks. He works on performance of Delta Lake in the Databricks Photon engine, and in particular on the MERGE, UPDATE and DELETE commands.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/selectively-overwrite-data-delta-lakes-dynamic-insert-overwrite","alias":"\/data-ai-summit-2025\/session\/selectively-overwrite-data-delta-lakes-dynamic-insert-overwrite","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535151+00:00"}
{"id":"D25B3166DBX","title":"Serverless Compute for Notebooks, Jobs and DLT","description":"<p>Discover how Databricks serverless compute revolutionizes data workflows by eliminating infrastructure management, enabling rapid scaling and optimizing costs for Notebooks, Jobs and DLT. This session will delve into the serverless architecture, highlighting its ability to dynamically allocate resources, reduce idle costs and simplify development cycles. Learn about recent advancements, including cost savings and practical strategies for migration and optimization. Tailored for Data Engineers and Architects, this talk will also explore use cases, features, limitations and future roadmap, empowering you to make informed infrastructure decisions while unlocking the full potential of Databricks\u2019 serverless capabilities.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks Workflows, DLT"],"areas_of_interest":["Databricks Experience (DBX), ETL"],"delivery":["In Person"],"speakers":[{"name":"Roland F\u00e4ustlin","company":"Databricks","job_title":"Product Manager","bio":"Roland is a Product Manager at Databricks, focusing on the orchestration of data and AI workloads with Databricks Workflows and fully managed serverless compute since April 2021.<br \/>\n<br \/>\nHe has over 13 years of experience in technology, including roles at Google and as a team leader at McKinsey\u2019s Business Technology Office in Europe and Latin America.<br \/>\n<br \/>\nHe holds a PhD in plasma physics, a master\u2019s degree in laboratory astrophysics from The University of Texas at Austin, and a minor in computer science.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/serverless-compute-notebooks-jobs-and-dlt","alias":"\/data-ai-summit-2025\/session\/serverless-compute-notebooks-jobs-and-dlt","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535174+00:00"}
{"id":"D25B3100","title":"Smart Data, Smarter Vehicles: Building the Foundation for the Future of Transportation","description":"Join industry pioneers Boeing and CARIAD (Volkswagen Group) as they showcase how advanced data platforms are revolutionizing mobility across air and ground transportation. Boeing's Jeppesen Smart NOTAMs system demonstrates the power of compound AI in aviation safety, processing over 4.5M critical flight notices annually and serving 75% of commercial aviation through an innovative combination of MLflow, GenAI, and Delta Sharing technologies. CARIAD follows with insights into their groundbreaking Unified Data Ecosystem (UDE), the singular data platform powering Volkswagen Group's global mobility transformation across all brands and markets. Together, these leaders illustrate how smart data architecture is building the foundation for the future of transportation, from the skies to the streets.","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Manufacturing"],"category":["AI\/BI"],"areas_of_interest":["Industry Experience"],"delivery":["In Person"],"speakers":[{"name":"Sascha Boerger","company":"CARIAD SE","job_title":"Head of Data Management","bio":"Sascha Boerger is a senior leader with startup and corporate group experience and 20+ years at C- and department director level in IT, telecommunications, and online businesses.<br \/>\n<br \/>\nSascha is an innovative technology evangelist and strategic advisor with hands-on mentality, an eye for details and the big picture, with strong focus on customer needs and the user experience.<br \/>\n<br \/>\nEarly 2022 Sascha founded the Unified Data Ecosystem (UDE) at CARIAD - providing an easy-to-use developer interface to all vehicle data of the VW Group and in a cost efficient way by consolidating all existing data silos into a on-stop-shop for customer and test vehicle data.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Profile_Sascha-Boerger_AI-gen_1744395154131001I9DW.jpg?h=436b82d4&itok=uCi1u8Dq","alt":"Sascha Boerger"}},{"name":"Jon Brown","company":"Boeing","job_title":"Architect","bio":"Jon Brown is a subject matter expert within Digital Aviation Solutions (DAS) with expertise in the Flight Planning, NOTAMs, and Charting domains with over 16 years of experience within Boeing. Jon has contributed to the design and support of many systems across Boeing DAS Solutions with over 500 commercial, business, and military customers. His in-depth knowledge of leveraging technology to integrate data and services to build world class Jeppesen products for airline customers has been key to building out world class Digital Aviation Solutions products in the cloud. Jon is a certified dispatcher and has a pending patent related to the aggregation, management, and correlation of aerospace constraints.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/pic1_1745588181358001fG5y.jpg?h=12de4a96&itok=-suR2MZj","alt":"Jon Brown"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/smart-data-smarter-vehicles-building-foundation-future-transportation","alias":"\/data-ai-summit-2025\/session\/smart-data-smarter-vehicles-building-foundation-future-transportation","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535212+00:00"}
{"id":"D25B2878","title":"Stop Guessing Spend Where It Counts: Data-Driven Decisions for High-Impact Investments on Databricks","description":"<p>Struggling with runaway cloud costs as your organization grows? Join us for an inside look at how Databricks\u2019 own Data Platform team tackled escalating spend in some of the world\u2019s largest workspaces \u2014 saving millions of dollars without sacrificing performance or user experience. We\u2019ll share how we harnessed powerful features like System Tables, Workflows, Unity Catalog, and Photon to monitor and optimize resource usage, all while using data-driven decisions to improve efficiency and ensure we invest in the areas that truly drive business impact. You\u2019ll hear about the real-world challenges we faced balancing governance with velocity and discover the custom tooling and best practices we developed to keep costs in check. By the end of this session, you\u2019ll walk away with a proven roadmap for leveraging Databricks to control cloud spend at scale.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Health and Life Sciences, Financial Services"],"category":["AI\/BI, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Customer Data Platform, Data Intelligence, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Clara MacAvoy","company":"Databricks","job_title":"Software Engineer","bio":"Clara is a Software Engineer on Databricks' Data Platform, operating the largest Databricks workspaces in the world to power the company's Data + AI initiatives. She specializes in cost efficiency and data governance, ensuring Databricks can reliably, safely, and efficiently process and store critical business data. Clara plays a pivotal role in building and managing SWAP (Secure Workload and Regulated Data Platform) workspaces to facilitate internal Data + AI consumption. Passionate about user experience, Clara is committed to designing intuitive systems that empower users to make data-driven decisions seamlessly. Clara has a BS\/MS from Stanford, where she won the Terman Engineering Award and was a 2023 Siebel Scholar. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Clara_MacAvoy_1745427107660001N2JK.png?h=f137dbb5&itok=v7-noVS6","alt":"Clara MacAvoy"}},{"name":"Samira Emmerson","company":null,"job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/stop-guessing-spend-where-it-counts-data-driven-decisions-high-impact","alias":"\/data-ai-summit-2025\/session\/stop-guessing-spend-where-it-counts-data-driven-decisions-high-impact","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535347+00:00"}
{"id":"D25L2484","title":"Streamlining Data Platform Architecture With Databricks Apps","description":"<p>Databricks Apps bring the power of open-source visualization frameworks like Plotly Dash directly into Databricks. Combined with the power of elastic serverless compute, Databricks Apps create a seamless development experience for advanced visualizations with the lowest possible latency to your lakehouse. In this talk, we walk you through a new approach to presenting AI-driven analysis to customers around your organization \u2014 all while eliminating unneeded license costs, halving administrative burden and accelerating delivery time.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Health and Life Sciences"],"category":["Unity Catalog, Databricks Apps"],"areas_of_interest":["Data Applications, Developer Experience, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Clare Bornstein","company":"Takeda","job_title":"Platform Operations Lead","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/streamlining-data-platform-architecture-databricks-apps","alias":"\/data-ai-summit-2025\/session\/streamlining-data-platform-architecture-databricks-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535361+00:00"}
{"id":"D25B2860","title":"Swimming at Our Own Lakehouse: How Databricks Uses Databricks","description":"<p>Peek behind the curtain to learn how Databricks processes hundreds of petabytes of data across every region and cloud where we operate. Learn how Databricks leverages Data and AI to scale and optimize every aspect of the company. From facilities and legal to sales and marketing and of course product research and development. This session is a high-level tour inside Databricks to see how Data and AI enable us to be a better company. We will go into the architecture of things for how Databricks is used for internal use cases like business analytics and SIEM as well as customer-facing features like system tables and assistant. We will cover how data production of our data flow and how we maintain security and privacy while operating a large multi-cloud, multi-region environment.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Databricks SQL, Databricks Workflows, Unity Catalog"],"areas_of_interest":["Orchestration, Thought Leadership, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Bruce Wong","company":"Databricks","job_title":"Head of Data Platform","bio":"Bruce leads our internal Data Platform function at Databricks.   We build, own and operate our enterprise lakehouse for Databricks where all employees and all teams inside use Data and AI to solve problems and guide our decisions.  Previously Bruce held technical leadership roles at Netflix, Twilio and StitchFix where he gained deep experience building planet scale systems in a cloud-native environment.  Bruce also formed the Chaos Engineering team and led the initiative at Netflix to achieve ultimate resilience.  Bruce has also shares his experience and insights serving on product advisory boards previously for DataDog, PagerDuty, currently for DataBricks.","image":{}},{"name":"Alan Jackoway","company":"Databricks","job_title":"Staff Software Engineer","bio":"Alan is the technical lead for our internal Data Platform function at Databricks. Alan has been building data platforms at the companies that make data platforms for the last 15 years, first at Cloudera and now at Databricks. At Databricks, our internal Data Platform enables all Databricks employees to use our own product to interact with data on a daily basis. He is passionate about product feedback, and believes that when more people use data, the company can make better decisions","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Jackoway_Alan_380-1368_1745540082721001SmeY.png?h=1e66e246&itok=GPShu0jK","alt":"Alan Jackoway"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/swimming-our-own-lakehouse-how-databricks-uses-databricks","alias":"\/data-ai-summit-2025\/session\/swimming-our-own-lakehouse-how-databricks-uses-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535375+00:00"}
{"id":"D25D2913","title":"The Full Stack of Innovation: Building Data and AI Products With Databricks Apps","description":"<p>In this deep-dive technical session, Ivan Trusov (Sr. SSA @ Databricks) and Giran Moodley (SA @ Databricks) \u2014 will explore the full-stack development of Databricks Apps, covering everything from frameworks to deployment.<\/p><p>\u00a0<\/p><p>We\u2019ll walk through essential topics, including:<\/p><ul>\t<li>Frameworks & tooling \u2014 Pythonic (Dash, Streamlit, Gradio) vs. JS + Python stack<\/li>\t<li>Development lifecycle \u2014 Debugging, issue resolution and best practices<\/li>\t<li>Testing \u2014 Unit, integration and load testing strategies<\/li>\t<li>CI\/CD & deployment \u2014 Automating with Databricks Asset Bundles<\/li>\t<li>Monitoring & observability \u2014 OpenTelemetry, metrics collection and analysis<\/li><\/ul><p>Expect a highly practical session with several live demos, showcasing the development loop, testing workflows and CI\/CD automation. Whether you\u2019re building internal tools or AI-powered products, this talk will equip you with the knowledge to ship robust, scalable Databricks Apps.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Deep Dive","industry":["Enterprise Technology, Professional Services, Financial Services"],"category":["Databricks SQL, Mosaic AI, Databricks Apps"],"areas_of_interest":["Data Applications, Developer Experience, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Ivan Trusov","company":"Databricks","job_title":"Senior Specialist Solutions Architect","bio":"Ivan Trusov is a Senior Specialist Solutions Architect at Databricks. His main focus specialties are Unity Catalog, Platform Automation, Serverless Security and Databricks Apps.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Ivan_Trusov_1744365606113001vmhJ.png?h=3cd06fc8&itok=R9Jy4-o_","alt":"Ivan Trusov"}},{"name":"Giran Moodley","company":"Databricks","job_title":"Regional Architect, Africa & Middle East","bio":"With over a decade of experience in the technology and banking sectors, Giran is a seasoned professional with roles encompassing Software Engineering, Solution Architecture, Product, and Technical Sales.<br \/>\n<br \/>\nAs a forward-thinking leader, Giran has a proven track record of product innovation, driving digital transformation across enterprises, and leading cross-functional teams to unlock customers' key business goals.<br \/>\n<br \/>\nGiran is dedicated to pushing the boundaries of people, processes, and technology, and is always eager to connect with like-minded professionals to explore new opportunities in the era of Data & AI.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/giran_moodley_headshot_cropped_1744357708752001IOYe.png?h=6ba7b2c7&itok=xfzqLp49","alt":"Giran Moodley"}}],"day":"Thursday","room":"South, Level 3, Room 303","starts":"2025-06-12T18:30:00","ends":"2025-06-12T20:00:00","starts_pst":"2025-06-12T11:30:00","ends_pst":"2025-06-12T13:00:00","start_time":"6:30 pm","end_time":"8:00 pm","pst_start_time":"11:30 am","pst_end_time":"1:00 pm","duration":"90","path":"\/session\/full-stack-innovation-building-data-and-ai-products-databricks-apps","alias":"\/data-ai-summit-2025\/session\/full-stack-innovation-building-data-and-ai-products-databricks-apps","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535424+00:00"}
{"id":"D25B3163DBX","title":"The Future of DSv2 in Apache Spark\u2122","description":"<p>DSv2, Spark's next-generation Catalog API, is gaining traction among data source developers. It shifts complexity to Apache Spark\u2122, improves connector reliability and unlocks new functionality such as catalog federation, MERGE operations, storage-partitioned joins, aggregate pushdown, stored procedures and more. This session covers the design of DSv2, current strengths and gaps and its evolving roadmap. It's intended for Spark users and developers working with data sources, whether custom-built or off-the-shelf.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["Catalogs, Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Anton Okolnychyi","company":"Databricks","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/future-dsv2-apache-sparktm","alias":"\/data-ai-summit-2025\/session\/future-dsv2-apache-sparktm","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535430+00:00"}
{"id":"D25B3161DBX","title":"The Future of Open Table Formats","description":"Open table formats are evolving quickly. In this session, we\u2019ll explore the latest features of Delta Lake and Apache Iceberg\u2122. Join us to learn about what\u2019s driving format innovation in open table formats.","track":"Data Lakehouse Architecture and Implementation","level":"Advanced","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg"],"areas_of_interest":["Databricks Experience (DBX), Open Source"],"delivery":["In Person"],"speakers":[{"name":"Denny Lee","company":"Databricks","job_title":"PM Director, Developer Relations","bio":null,"image":{}},{"name":"Ryan Blue","company":"Databricks","job_title":"Member of Technical Staff","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/future-open-table-formats","alias":"\/data-ai-summit-2025\/session\/future-open-table-formats","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535434+00:00"}
{"id":"D25B1118","title":"The Metadata Marathon: How Three Storage Projects are Racing Forward","description":"<p>With the enormous amount of discussion about open storage formats between nerds and even not-nerds, it can be hard to keep track of who\u2019s doing what and how this actually makes any impact on day to day data projects.<\/p><p>\u00a0<\/p><p>I want us to take a closer look at the three big projects in this space; Delta, Hudi and Iceberg. They\u2019re all trying to solve for similar data problems and have tackled the various challenges in different ways. This talk with start with the very basics of how we got here, what the history is before diving deep into the underlying tech, their roadmaps and their impacts on the data landscape as a whole.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake"],"areas_of_interest":["ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Holly Smith","company":"Databricks","job_title":"Staff Developer Advocate","bio":"Holly Smith is a developer advocate and multi-award-winning Data and AI expert and tech educator with over a decade of experience working at the cutting edge of Data and AI in various capacities. One of the first employees at Databricks, she is a renowned public speaker and teacher. She is well known for her YouTube series on AI + data engineering.<br \/>\n","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/metadata-marathon-how-three-storage-projects-are-racing-forward","alias":"\/data-ai-summit-2025\/session\/metadata-marathon-how-three-storage-projects-are-racing-forward","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535447+00:00"}
{"id":"D25B3172DBX","title":"The Missing Link Between the Lakehouse and Data Intelligence","description":"<p>What connects your lakehouse to real data intelligence? The answer: the catalog. But not just any catalog. In this session, we break down why Unity Catalog is purpose-built for the lakehouse, and how it goes beyond operational or business catalogs to deliver cross-platform interoperability and a shared understanding of your data. You\u2019ll walk away with a clear view of how the right data foundation unlocks smarter decisions and trusted AI.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Jason Reid","company":null,"job_title":"Director, Product","bio":null,"image":{}},{"name":"Michelle Leon","company":"Databricks","job_title":"Staff Product Manager","bio":"Michelle is a product manager at Databricks, working on Delta Lake. She previously led teams at Webflow and Airbnb, and is based out of San Francisco.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/missing-link-between-lakehouse-and-data-intelligence","alias":"\/data-ai-summit-2025\/session\/missing-link-between-lakehouse-and-data-intelligence","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535451+00:00"}
{"id":"D25B2447","title":"ThredUp\u2019s Journey with Databricks: Modernizing Our Data Infrastructure","description":"<p>Building an AI-ready data platform requires strong governance, performance optimization, and seamless adoption of new technologies. At ThredUp, our Databricks journey began with a need for better data management and evolved into a full-scale transformation powering analytics, machine learning, and real-time decision-making.<\/p><p>\u00a0<\/p><p>In this session, we\u2019ll cover:<\/p><ul>\t<li>Key inflection points: Moving from legacy systems to a modernized Delta Lake foundation<\/li>\t<li>Unity Catalog\u2019s impact: Improving governance, access control, and data discovery<\/li>\t<li>Best practices for onboarding: Ensuring smooth adoption for engineering and analytics teams<\/li>\t<li>What\u2019s next? Serverless SQL and conversational analytics with Genie<\/li><\/ul><p>\u00a0<\/p><p>Whether you\u2019re new to Databricks or scaling an existing platform, you\u2019ll gain practical insights on navigating the transition, avoiding pitfalls, and maximizing AI and data intelligence.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology, Media and Entertainment, Retail and CPG - Food"],"category":["Delta Lake, AI\/BI, Unity Catalog"],"areas_of_interest":["Catalogs, Getting started with Databricks, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Chintan Patel","company":"Thredup","job_title":"Data Engineering Manager","bio":"Bringing over a decade of experience building and modernizing data infrastructures, I spearheaded ThredUp\u2019s transformation to a unified Databricks Lakehouse with Delta Lake integration. As Data Engineering Manager at ThredUp, I architect robust, scalable, distributed data processing pipelines and governance frameworks, drive AI-driven innovation while ensuring data quality and reliability. At DAIS 2025, I\u2019ll share practical insights on migrating from legacy data stacks to an end-to-end Lakehouse and unlocking value\u2014drawing on lessons learned throughout my tenure at ThredUp.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Headshot_1745603742183001HFVr.JPG?h=a159b535&itok=16SDR7XR","alt":"Chintan Patel"}},{"name":"Aniket Mane","company":"ThredUp Inc.","job_title":"VP Engineering, Operations","bio":"Aniket Mane is Vice President of Data and Fintech Engineering at thredUP, where he leads the evolution of large-scale data platforms, AI\/ML ecosystems, and fintech systems. With over 14 years of experience in data engineering, marketing analytics, cloud infrastructure, and software development, he has built resilient, scalable systems that powered thredUP\u2019s growth from startup to IPO and beyond. Aniket specializes in cloud platforms, Databricks Lakehouse architecture, MLOps, AI-driven insights, and real-time streaming data applications. He is passionate about scaling innovation, enabling data-driven decision-making, empowering future tech leaders, and transforming business strategies through data and AI innovation","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/LinkedIn-Profile_1746035540466001cFoe.jpg?h=a05dec19&itok=Rab0VcPY","alt":"Aniket Mane"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/thredups-journey-databricks-modernizing-our-data-infrastructure","alias":"\/data-ai-summit-2025\/session\/thredups-journey-databricks-modernizing-our-data-infrastructure","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535469+00:00"}
{"id":"D25B1799","title":"Tracing the Path of a Row Through a GPU-Enabled Query Engine on the Grace-Blackwell Architecture","description":"<p>Grace-Blackwell is NVIDIA\u2019s most recent GPU system architecture. It addresses a key concern of query engines: fast data access. In this session, we will take a close look at how GPUs can accelerate data analytics by tracing how a row flows through a GPU-enabled query engine.Query engines read large data from CPU memory or from disk. On Blackwell GPUs, a query engine can rely on hardware-accelerated decompression of compact formats. The Grace-Blackwell system takes data access performance even further, by reading data at up to 450 GB\/s across its CPU to GPU interconnect.<\/p><p>\u00a0<\/p><p>We demonstrate full end-to-end SQL query acceleration using GPUs in a prototype query engine using industry standard benchmark queries.<\/p><p>\u00a0<\/p><p>We compare the results to existing CPU solutions.Using Apache Spark\u2122 and the RAPIDS Accelerator for Apache Spark, we demonstrate the impact GPU acceleration has on the performance of SQL queries at the 100TB scale using NDS, a suite that simulates real-world business scenarios.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Apache Spark"],"areas_of_interest":["ETL, Open Source"],"delivery":["In Person"],"speakers":[{"name":"Thomas Graves","company":"Nvidia","job_title":"Principal Systems Software Engineer","bio":"Thomas Graves is a distributed systems software engineer at NVIDIA, where he concentrates on accelerating Spark. He is a committer and PMC on Apache Spark and Apache Hadoop. Previously worked for Yahoo on the Big Data Platform team working on Apache Spark, Hadoop, YARN, Storm, and Kafka.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Thomas_Graves_1745852066765001r9br.PNG?h=bc5b215e&itok=5N4Pnt1L","alt":"Thomas Graves"}},{"name":"Clemens Lutz","company":"NVIDIA","job_title":"Senior Developer Technology Engineer","bio":"Clemens is investigating large-scale data management on GPU platforms. He specializes on NVLink C2C due to its high potential to speed-up database workloads. His work on this topic has been recognized with research awards at the SIGMOD and BTW conferences. Prior to joining NVIDIA, Clemens earned his PhD at TU Berlin on GPU-enabled data management, for which he received the GI DBIS Dissertation Award 2025.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/2022-02-10_Lutz_Clemens_casual_wide_1746133972799001GOBH.jpg?h=15c190ef&itok=4KctxUZN","alt":"Clemens Lutz"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"60","path":"\/session\/tracing-path-row-through-gpu-enabled-query-engine-grace-blackwell","alias":"\/data-ai-summit-2025\/session\/tracing-path-row-through-gpu-enabled-query-engine-grace-blackwell","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535482+00:00"}
{"id":"D25B2725","title":"Transforming Bio-Pharma Manufacturing: Eli Lilly's Data-Driven Journey With Databricks","description":"<p>Eli Lilly and Company, a leading bio-pharma company, is revolutionizing manufacturing with next-gen fully digital sites. Lilly and Tredence have partnered to establish a Databricks-powered Global Manufacturing Data Fabric (GMDF), laying the groundwork for transformative data products used by various personas at sites and globally. By integrating data from various manufacturing systems into a unified data model, GMDF has delivered actionable insights across several use cases such as batch release by exception, predictive maintenance, anomaly detection, process optimization and more. Our serverless architecture leverages Databricks Auto Loader for real-time data streaming, PySpark for automation and Unity Catalog for governance, ensuring seamless data processing and optimization. This platform is the foundation for data driven processes, self-service analytics, AI and more. This session will provide details on the data architecture and strategy and share a few use cases delivered.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Health and Life Sciences"],"category":["Delta Lake, AI\/BI, Unity Catalog"],"areas_of_interest":["Data Applications, Data Intelligence, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"SAUNAK DEBROY","company":"Eli Lilly","job_title":"Lead Innovation Architect - Data & AI","bio":"Lead Innovation Architect","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-04-11%2520at%252012.09.41%25E2%2580%25AFPM_1744402785098001aB7S.png?h=d56f964d&itok=TjKjI3EU","alt":"SAUNAK DEBROY"}},{"name":"Abhijay Datta","company":"Tredence","job_title":"Head of Delivery - HLS","bio":"Abhijay has spent over 22 years in Healthcare & Life Sciences technology and analytics sector. During his stints with Infosys, Sutherland Healthcare, Emids and now Tredence, he has worked with some of the largest payers such as Aetna, Humana, Elevance, Cigna and big provider systems like Mercy Health, Northwell, LA County and others. He also spent six years at Novartis Pharma leading their in-house Clinical Informatics group, driving advanced analytics in clinical trials, and pharmacovigilance operations. He currently heads the service delivery of Healthcare and Life Sciences vertical in Tredence and based in India.","image":{}},{"name":"Wilfred Mascarenhas","company":"Eli Lilly and Company","job_title":"Executive Director","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/transforming-bio-pharma-manufacturing-eli-lillys-data-driven-journey","alias":"\/data-ai-summit-2025\/session\/transforming-bio-pharma-manufacturing-eli-lillys-data-driven-journey","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535496+00:00"}
{"id":"D25B3404","title":"Unified Governance and Enterprise Sharing for Data + AI","description":"<p>The Databricks Lakehouse for Public Sector is the only enterprise data platform that allows you to leverage all your data, from any source, on any workload to always offer better citizen services\/warfighter support\/student success with the best outcomes, at the lowest cost, with the greatest investment protection.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Education, Public Sector"],"category":["Delta Lake, Delta Sharing"],"areas_of_interest":["Data Intelligence, Industry Experience"],"delivery":["In Person"],"speakers":[{"name":"Mike Daniels","company":"Databricks","job_title":"VP and GM, Public Sector","bio":"Innovative and accomplished technology executive with a proven track record of success in managing, directing and contributing to large and small organizations. My objective is to make a significant impact for my company and my team.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/MikeDaniels-2019-5_1744989397156001oZ6x.jpg?h=468ef699&itok=XorrqGZN","alt":"Mike Daniels"}},{"name":"Todd Schroeder","company":"Databricks","job_title":"VP, Federal Government","bio":"Strategic Vision: Forward-looking strategic mindset with ability to define clear vision and then translate technology strategy into action.<br \/>\n Innovative Leadership: Dynamic, collaborative leader that inspires and energizes technical teams. Adept at infusing new talent and cultivating internal IT teams committed to excellence and integrity. Can foster environments of innovation and ingenuity by blending control with creative freedom.<br \/>\nCommunications: Aptitude for driving stakeholder engagement through active listening, clear communications, and focus on gaining business perspective and probing for better way to deliver solutions.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/ToddSchroeder-3-e_1745004239869001BQmw.jpg?h=4b07008b&itok=tozoxM0C","alt":"Todd Schroeder"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/unified-governance-and-enterprise-sharing-data-ai","alias":"\/data-ai-summit-2025\/session\/unified-governance-and-enterprise-sharing-data-ai","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535643+00:00"}
{"id":"D25B3158DBX","title":"Unify Your Data and Governance With Lakehouse Federation","description":"<p>In today's data landscape, organizations often grapple with fragmented data spread across various databases, data warehouses and catalogs. Lakehouse Federation addresses this challenge by enabling seamless discovery, querying, and governance of distributed data without the need for duplication or migration. This session will explore how Lakehouse Federation integrates external data sources like Hive Metastore, Snowflake, SQL Server and more into a unified interface, providing consistent access controls, lineage tracking and auditing across your entire data estate. Learn how to streamline analytics and AI workloads, enhance compliance and reduce operational complexity by leveraging a single, cohesive platform for all your data needs.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Enterprise Technology"],"category":["Unity Catalog"],"areas_of_interest":["Catalogs, Databricks Experience (DBX)"],"delivery":["In Person"],"speakers":[{"name":"Fuat Can Efeoglu","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":null,"image":{}},{"name":"Zeashan Pappa","company":"Databricks","job_title":"Staff Product Manager","bio":"Zeashan is a technology executive and architect with 20+ years of expertise in enterprise software architecture, engineering, consulting, and project management. <br \/>\n<br \/>\nZeashan is a product manager at Databricks, focusing on catalog, governance, and security products.","image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/unify-your-data-and-governance-lakehouse-federation","alias":"\/data-ai-summit-2025\/session\/unify-your-data-and-governance-lakehouse-federation","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535650+00:00"}
{"id":"D25B2825DBX","title":"Unity Catalog Managed Tables: Powerful, Easy, and Interoperable","description":"<p>Unity Catalog's Managed tables are the best of all worlds. Learn how they harness the Data Intelligence Platform to deliver lightning-fast performance\u2014without requiring a space shuttle cockpit worth of setting. Learn how they're fully interoperable with 3P clients\u2014be they Delta or Iceberg\u2014while still respecting a single source of governance. And learn about our exciting roadmap for how they will get even more powerful in the coming year.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Breakout","industry":["Media and Entertainment, Travel and Hospitality, Financial Services"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Data Intelligence, Databricks Experience (DBX), ETL"],"delivery":["In Person"],"speakers":[{"name":"Sirui Sun","company":"Databricks","job_title":"Sr. Staff Product Manager","bio":"Sirui is a Sr. Staff Product Manager at Databricks, working on making Delta simpler and more performant. Prior to Databricks, he's spent time working on Google Cloud and at Microsoft. At the Data + AI Summit, he'd love to talk to you about your data goals and challenges! ","image":{}},{"name":"Elizabeth Bowman","company":"Databricks","job_title":null,"bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"40","path":"\/session\/unity-catalog-managed-tables-powerful-easy-and-interoperable","alias":"\/data-ai-summit-2025\/session\/unity-catalog-managed-tables-powerful-easy-and-interoperable","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535681+00:00"}
{"id":"D25L2475","title":"Unlocking Enterprise Potential: Key Insights from P&G's Deployment of Unity Catalog at Scale","description":"<p>This session will explore Databricks Unity Catalog (UC) implementation by P&G to enhance data governance, reduce data redundancy and improve the developer experience through the enablement of a Lakehouse architecture. The presentation will cover:<\/p><p>\u00a0<\/p><p>The distinction between data treated as a product and standard application data, highlighting how UC's structure maximizes the value of data in P&G's data lake.<\/p><p>\u00a0<\/p><p>Real-life examples from two years of using Unity Catalog, demonstrating benefits such as improved governance, reduced waste and enhanced data discovery.<\/p><p>\u00a0<\/p><p>Challenges related to disaster recovery and external data access, along with our collaboration with Databricks to address these issues.<\/p><p>\u00a0<\/p><p>Sharing our experience can provide valuable insights for organizations planning to adopt Unity Catalog on an enterprise scale.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Retail and CPG - Food"],"category":["Unity Catalog"],"areas_of_interest":["Catalogs, Migrations, Security & Compliance"],"delivery":["In Person"],"speakers":[{"name":"Kinga Morawska","company":"P&G","job_title":"Engineering Manager","bio":null,"image":{}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"0","path":"\/session\/unlocking-enterprise-potential-key-insights-pgs-deployment-unity","alias":"\/data-ai-summit-2025\/session\/unlocking-enterprise-potential-key-insights-pgs-deployment-unity","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535709+00:00"}
{"id":"D25L1537","title":"Unlocking the Power of Iceberg: Our Journey to a Unified Lakehouse on Databricks","description":"<p>This session showcases our journey of adopting Apache Iceberg\u2122 to build a modern lakehouse architecture and leveraging Databricks advanced Iceberg support to take it to the next level. We\u2019ll dive into the key design principles behind our lakehouse, the operational challenges we tackled and how Databricks enabled us to unlock enhanced performance, scalability and streamlined data workflows. Whether you\u2019re exploring Apache Iceberg\u2122 or building a lakehouse on Databricks, this session offers actionable insights, lessons learned and best practices for modern data engineering.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Intermediate","type":"Lightning Talk","industry":["Media and Entertainment"],"category":["Apache Iceberg, Unity Catalog"],"areas_of_interest":["Catalogs, Migrations"],"delivery":["In Person"],"speakers":[{"name":"Zevik Gerstner","company":"LSports","job_title":"Group Manager","bio":"I\u2019m a Group Manager at LSports, where I lead teams working at the intersection of data, AI, and machine learning. Over the years, I\u2019ve built several lakehouses on AWS and GCP using Databricks\u2014turning raw, chaotic data into streamlined, high-performing systems. Whether it\u2019s real-time sports data, large-scale ML pipelines, or applied AI solutions, I\u2019m passionate about building smart, scalable technologies that make a real impact.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/Screenshot%25202025-04-22%2520at%25203.48.39%25E2%2580%25AFPM_1745362130829001IKXL.png?h=6d469b7b&itok=TQoke4mi","alt":"Zevik Gerstner"}}],"day":"","room":"","starts":null,"ends":null,"starts_pst":"","ends_pst":"","start_time":"","end_time":"","pst_start_time":"","pst_end_time":"","duration":"20","path":"\/session\/unlocking-power-iceberg-our-journey-unified-lakehouse-databricks","alias":"\/data-ai-summit-2025\/session\/unlocking-power-iceberg-our-journey-unified-lakehouse-databricks","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535726+00:00"}
{"id":"D25B1039","title":"Ursa: Augment Your Lakehouse With Kafka-Compatible Data Streaming Capabilities","description":"<p>As data architectures evolve to meet the demands of real-time GenAI applications, organizations increasingly need systems that unify streaming and batch processing while maintaining compatibility with existing tools. The Ursa Engine offers a Kafka-API-compatible data streaming engine built on Lakehouse (Iceberg and Delta Lake). Designed to seamlessly integrate with data lakehouse architectures, Ursa extends your lakehouse capabilities by enabling streaming ingestion, transformation and processing \u2014 using a Kafka-compatible interface.<\/p><p>\u00a0<\/p><p>In this session, we will explore how Ursa Engine augments your existing lakehouses with Kafka-compatible capabilities.<\/p><p>\u00a0<\/p><p>Attendees will gain insights into Ursa Engine architecture and real-world use cases of Ursa Engine. Whether you're modernizing legacy systems or building cutting-edge AI-driven applications, discover how Ursa can help you unlock the full potential of your data.<\/p>","track":"Data Lakehouse Architecture and Implementation","level":"Beginner","type":"Breakout","industry":["Enterprise Technology"],"category":["Delta Lake, Apache Iceberg, Unity Catalog"],"areas_of_interest":["Data Ingestion, Open Source, Streaming pipelines, Thought Leadership"],"delivery":["In Person"],"speakers":[{"name":"Sijie Guo","company":"StreamNative","job_title":"Founder and CEO","bio":"Sijie Guo is the Co-Founder and CEO of StreamNative, a company pioneering the next generation of real-time data streaming infrastructure. Powered by the Ursa engine, StreamNative Cloud helps enterprises reduce total cost of ownership (TCO) by 90%, offering Kafka compatibility, a leaderless architecture, and lakehouse-native storage, making AI-ready data accessible at scale.<br \/>\n <br \/>\nSijie is a long-time open-source contributor and a PMC member of both Apache BookKeeper and Apache Pulsar.","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/sijieg_1744331638058001TewU.png?h=07a49c3e&itok=RKUkSWHg","alt":"Sijie Guo"}},{"name":"Gaurav Saxena","company":"Automotive Industry","job_title":"Director of Engineering","bio":"Gaurav Saxena is an engineering leader in the field of platform and cloud engineering with over 20 years of experience. His technical expertise includes Stream-based architectures, Kubernetes, Service Mesh, and Observability. ","image":{"url":"https:\/\/microsites.databricks.com\/sites\/default\/files\/styles\/headshot\/public\/media\/images\/dataaisummit_speaker\/gaurav-1000x1000_1745366294040001da3R.jpg?h=1e66e246&itok=iUb39tiX","alt":"Gaurav Saxena"}}],"day":"Wednesday","room":"South, Level 2, Room 213","starts":"2025-06-11T19:40:00","ends":"2025-06-11T20:20:00","starts_pst":"2025-06-11T12:40:00","ends_pst":"2025-06-11T13:20:00","start_time":"7:40 pm","end_time":"8:20 pm","pst_start_time":"12:40 pm","pst_end_time":"1:20 pm","duration":"40","path":"\/session\/ursa-augment-your-lakehouse-kafka-compatible-data-streaming","alias":"\/data-ai-summit-2025\/session\/ursa-augment-your-lakehouse-kafka-compatible-data-streaming","slides":null,"video":null,"scraped_at":"2025-05-02T15:15:21.535737+00:00"}
